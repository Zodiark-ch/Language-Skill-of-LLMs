[2024-07-23 21:06:30,495][explain_satisfiability.py][line:85][INFO] ############ CASE TEXT isThe language used by Juan Bautista de Anza is a bit different from the language used by the
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:86][INFO] ############ CASE Prediction is  Spanish
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:87][INFO] ############ Refined Forward Graph
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:88][INFO] ****** Layer 1
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 0
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 1
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 2
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit24']
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 3
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 4
[2024-07-23 21:06:30,495][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 5
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 6
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 7
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 8
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 9
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 10
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit28']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 11
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 12
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,496][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 13
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 14
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 15
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 16
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 17
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 18
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 19
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 20
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit4', 'circuit10', 'circuit14', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 21
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,497][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 22
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 23
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 24
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit12', 'circuit13', 'circuit15', 'circuit16', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 25
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 26
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 27
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 28
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 0
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 1
[2024-07-23 21:06:30,498][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 2
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit21']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 3
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 4
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit12', 'circuit14', 'circuit15']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 5
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit19', 'circuit20', 'circuit26']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 6
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit26']
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 7
[2024-07-23 21:06:30,499][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 8
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 9
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit28']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit13', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 10
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit21', 'circuit23']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 11
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit12', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 12
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit19', 'circuit21']
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 13
[2024-07-23 21:06:30,500][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit3', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit7', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 14
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 15
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit26']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 16
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 17
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 18
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit23']
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,501][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 19
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit19', 'circuit21', 'circuit22']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 20
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 21
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 22
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 23
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 24
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,502][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 25
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 26
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 27
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 28
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 0
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit6', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 1
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,503][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit26']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 2
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit25']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 3
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 4
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 5
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,504][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 6
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit22', 'circuit27']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 7
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 8
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit22', 'circuit23']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 9
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 10
[2024-07-23 21:06:30,505][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit18']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 11
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit25']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 12
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit22']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 13
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 14
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit3', 'circuit6', 'circuit7', 'circuit14', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 15
[2024-07-23 21:06:30,506][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 16
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 17
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 18
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit3', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit7', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 19
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 20
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 21
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 22
[2024-07-23 21:06:30,507][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit23', 'circuit25']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 23
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 24
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit21', 'circuit24']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 25
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 26
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 27
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 28
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,508][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 0
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit5', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit8', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 1
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit22']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit22', 'circuit26']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 2
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit20']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 3
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 4
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit21', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 5
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,509][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 6
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 7
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit5', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 8
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit28']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit10', 'circuit14']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 9
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit13', 'circuit14']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit4', 'circuit12', 'circuit14', 'circuit20', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 10
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit20', 'circuit24']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 11
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,510][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit21']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit25']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 12
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit22', 'circuit23']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit15']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 13
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit28']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 14
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit6', 'circuit12']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 15
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 16
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,511][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 17
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit15']
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 18
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit20']
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 19
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 20
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 21
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 22
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit23']
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,512][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 23
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 24
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 25
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 26
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 27
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 28
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,513][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 0
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit4', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit5', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 1
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 2
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 3
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 4
[2024-07-23 21:06:30,514][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit20', 'circuit25']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 5
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 6
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 7
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 8
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,515][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 9
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit12', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit11', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit8', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 10
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit10', 'circuit13', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit25']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 11
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit25']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit23']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 12
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit6', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 13
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,516][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 14
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit24']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 15
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit8', 'circuit11', 'circuit12', 'circuit14']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 16
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 17
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit18', 'circuit21', 'circuit22', 'circuit24', 'circuit27']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit23', 'circuit25']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 18
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit21']
[2024-07-23 21:06:30,517][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 19
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 20
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 21
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 22
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 23
[2024-07-23 21:06:30,518][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 24
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 25
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 26
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 27
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,519][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 28
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 0
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 1
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit18', 'circuit22', 'circuit23']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 2
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit8', 'circuit14']
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 3
[2024-07-23 21:06:30,520][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit22', 'circuit23']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 4
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit18', 'circuit22']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit18', 'circuit19', 'circuit20']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 5
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit19', 'circuit20', 'circuit23']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit6', 'circuit14', 'circuit25']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit12', 'circuit20']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 6
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit27']
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 7
[2024-07-23 21:06:30,521][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 8
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 9
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 10
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 11
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,522][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit15', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit20']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 12
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit21', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 13
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit16', 'circuit20', 'circuit22', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 14
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 15
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,523][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 16
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 17
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 18
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 19
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,524][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 20
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 21
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 22
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 23
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,525][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 24
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 25
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 26
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 27
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,526][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 28
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 0
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit8', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit4', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 1
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 2
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit25']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,527][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 3
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit22', 'circuit23']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 4
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 5
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 6
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14']
[2024-07-23 21:06:30,528][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit15', 'circuit18', 'circuit19', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit25']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit7', 'circuit25']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 7
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit8', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit10', 'circuit13', 'circuit21']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 8
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit17', 'circuit18', 'circuit24']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit4', 'circuit7']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 9
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit25']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,529][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 10
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit22']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 11
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 12
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 13
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,530][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 14
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 15
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 16
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 17
[2024-07-23 21:06:30,531][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 18
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 19
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 20
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,532][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 21
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 22
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 23
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 24
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,533][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 25
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 26
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 27
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,534][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 28
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 0
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit8', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit4', 'circuit7', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 1
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit19', 'circuit20', 'circuit21', 'circuit27']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit3']
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 2
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,535][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 3
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit27']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit18', 'circuit19', 'circuit21']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit22']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit12', 'circuit15', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit24']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 4
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 5
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24']
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,536][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 6
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 7
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit2', 'circuit12']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 8
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit21', 'circuit27']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit21', 'circuit23']
[2024-07-23 21:06:30,537][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit17']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit27']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 9
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit19']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 10
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit25']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit24']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit17']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 11
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,538][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 12
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 13
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit18', 'circuit19', 'circuit22', 'circuit26']
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 14
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 15
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,539][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 16
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 17
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 18
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,540][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 19
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 20
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 21
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,541][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 22
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 23
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 24
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,542][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 25
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 26
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 27
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 28
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,543][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 0
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit11', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 1
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 2
[2024-07-23 21:06:30,544][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 3
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 4
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit3', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10']
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit13', 'circuit14', 'circuit16']
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 5
[2024-07-23 21:06:30,545][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 6
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 7
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,546][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 8
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit17']
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 9
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit18', 'circuit20', 'circuit21']
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 10
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,547][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 11
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 12
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 13
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18']
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit24', 'circuit26']
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit22', 'circuit25']
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,548][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 14
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 15
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 16
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,549][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 17
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 18
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 19
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,550][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 20
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 21
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 22
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,551][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 23
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 24
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 25
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,552][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 26
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 27
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 28
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,553][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 0
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit10', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit7', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit22', 'circuit24']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit18', 'circuit21', 'circuit22', 'circuit25']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit16', 'circuit18']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit19']
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 1
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 2
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,554][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 3
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 4
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,555][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 5
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 6
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 7
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit22']
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,556][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 8
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 9
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 10
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,557][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 11
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 12
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 13
[2024-07-23 21:06:30,558][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit10', 'circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit13']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 14
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 15
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,559][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 16
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 17
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 18
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,560][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 19
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 20
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 21
[2024-07-23 21:06:30,561][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 22
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 23
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,562][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 24
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 25
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 26
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,563][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 27
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 28
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,564][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 0
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit20']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit15', 'circuit22']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit17', 'circuit21']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit18']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit6', 'circuit25', 'circuit27']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 1
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 2
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,565][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 3
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 4
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,566][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 5
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 6
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 7
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,567][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 8
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 9
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit3', 'circuit13', 'circuit16', 'circuit22', 'circuit23', 'circuit26']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit5', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit15', 'circuit20']
[2024-07-23 21:06:30,568][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit14', 'circuit19']
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 10
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 11
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit20', 'circuit23', 'circuit24']
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit21', 'circuit23']
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 12
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,569][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit4', 'circuit7', 'circuit8', 'circuit9', 'circuit10']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit18']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 13
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit23']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit28']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit21']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit14', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit25']
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 14
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,570][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 15
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 16
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 17
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,571][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 18
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 19
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,572][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 20
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 21
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 22
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,573][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 23
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 24
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,574][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 25
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 26
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 27
[2024-07-23 21:06:30,575][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 28
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:30,576][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:06:32,320][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:32,326][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,331][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,337][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,343][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,345][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,345][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,346][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,347][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,347][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,350][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,353][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,356][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:32,364][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9306, 0.0694], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,371][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,378][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.8773, 0.1227], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,379][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0757, 0.9243], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,380][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.6789, 0.3211], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,380][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0204, 0.9796], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,381][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.6993, 0.3007], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,385][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.9443, 0.0557], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,391][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9429, 0.0571], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,398][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9166, 0.0834], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,406][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.7551, 0.2449], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,412][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8085, 0.1915], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:32,414][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.5457, 0.3538, 0.1006], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,415][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0012, 0.0074, 0.9914], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,416][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.6911, 0.1941, 0.1148], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,417][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0628, 0.0251, 0.9121], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,417][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.3249, 0.2987, 0.3763], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,421][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0235, 0.0014, 0.9752], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,425][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.4580, 0.4021, 0.1399], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,432][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.3207, 0.5917, 0.0876], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,439][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.7713, 0.0667, 0.1620], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,446][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.6441, 0.1996, 0.1563], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,448][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.5955, 0.0953, 0.3092], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,449][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.6863, 0.1224, 0.1913], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:32,449][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.5934, 0.1280, 0.2245, 0.0541], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,450][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ by] are: tensor([4.7219e-04, 3.8726e-04, 3.4301e-04, 9.9880e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,451][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.4875, 0.0915, 0.1991, 0.2219], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,454][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0301, 0.0034, 0.3061, 0.6604], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,459][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.2891, 0.1021, 0.4708, 0.1380], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,465][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.1267, 0.0204, 0.0118, 0.8411], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,473][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.3579, 0.3661, 0.2188, 0.0572], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,480][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.3428, 0.1505, 0.2709, 0.2358], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,482][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.3895, 0.0843, 0.1263, 0.3999], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,482][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.5151, 0.1336, 0.1302, 0.2212], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,483][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.5033, 0.0815, 0.0818, 0.3334], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,484][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.4917, 0.1388, 0.1413, 0.2282], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:32,485][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.5009, 0.0425, 0.0685, 0.2253, 0.1628], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,486][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([9.5033e-05, 1.2695e-04, 3.7105e-05, 1.3030e-04, 9.9961e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,490][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.5533, 0.1307, 0.0391, 0.1668, 0.1101], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,493][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([9.2316e-04, 8.4138e-05, 3.8049e-05, 9.4415e-05, 9.9886e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,498][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([3.4870e-03, 8.0558e-04, 7.0577e-04, 9.3660e-04, 9.9406e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,502][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([7.5471e-04, 1.0226e-05, 2.1083e-06, 5.1514e-07, 9.9923e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,510][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.4801, 0.1675, 0.0595, 0.0589, 0.2340], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,515][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.2672, 0.0645, 0.2452, 0.3208, 0.1024], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,516][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.5126, 0.0828, 0.0663, 0.1578, 0.1804], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,517][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.5722, 0.1277, 0.0848, 0.1871, 0.0281], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,517][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.4628, 0.0610, 0.0480, 0.1076, 0.3206], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,518][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.2551, 0.2488, 0.1479, 0.2410, 0.1073], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:32,521][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.4291, 0.0765, 0.1451, 0.0690, 0.1916, 0.0887], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,523][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ B] are: tensor([3.6950e-04, 1.3760e-03, 6.4896e-04, 1.5038e-03, 1.6876e-04, 9.9593e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,529][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.3957, 0.1671, 0.0740, 0.1456, 0.1028, 0.1149], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,536][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0047, 0.0015, 0.0041, 0.0026, 0.0118, 0.9753], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,543][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.0915, 0.0425, 0.0288, 0.0704, 0.4245, 0.3424], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,547][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ B] are: tensor([1.0107e-02, 4.2144e-04, 4.2218e-04, 7.0486e-05, 3.0698e-05, 9.8895e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,549][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.2345, 0.3078, 0.1008, 0.0213, 0.2795, 0.0561], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,550][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.1691, 0.1005, 0.0815, 0.1865, 0.2511, 0.2113], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,551][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.2655, 0.0361, 0.0691, 0.1790, 0.0695, 0.3809], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,551][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.3631, 0.0884, 0.0944, 0.1666, 0.1723, 0.1153], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,552][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.4149, 0.0705, 0.0605, 0.1481, 0.0415, 0.2646], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,556][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.2896, 0.2585, 0.1432, 0.1604, 0.1018, 0.0466], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:32,560][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.2467, 0.0763, 0.0968, 0.1069, 0.2233, 0.2141, 0.0360],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,563][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [aut] are: tensor([1.1699e-04, 5.1832e-04, 1.0349e-04, 2.9436e-04, 1.7549e-03, 3.4785e-04,
        9.9686e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,568][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.3996, 0.1858, 0.0508, 0.1283, 0.0873, 0.0700, 0.0782],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,571][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [aut] are: tensor([2.1789e-03, 7.5508e-05, 9.7956e-05, 3.0960e-04, 2.0984e-03, 2.1987e-03,
        9.9304e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,577][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.0138, 0.0043, 0.0055, 0.0062, 0.0312, 0.0211, 0.9179],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,580][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [aut] are: tensor([2.3929e-03, 9.2261e-06, 2.9170e-05, 7.0517e-07, 1.0730e-04, 4.5241e-06,
        9.9746e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,583][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.2575, 0.1229, 0.0913, 0.1111, 0.1553, 0.1900, 0.0719],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,584][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.2573, 0.0291, 0.0666, 0.2383, 0.0170, 0.2843, 0.1074],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,584][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.3098, 0.0468, 0.0640, 0.1804, 0.1323, 0.2385, 0.0282],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,585][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.3668, 0.1026, 0.1116, 0.1332, 0.1299, 0.1414, 0.0145],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,586][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.4165, 0.0825, 0.0671, 0.1110, 0.0537, 0.0622, 0.2070],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,590][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.2917, 0.1603, 0.1479, 0.1623, 0.0965, 0.0501, 0.0911],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:32,594][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.3139, 0.0427, 0.0790, 0.0724, 0.1788, 0.0907, 0.1509, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,597][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ista] are: tensor([6.2069e-05, 2.1995e-04, 3.5065e-04, 6.1633e-05, 1.2478e-03, 3.1181e-05,
        1.2711e-03, 9.9676e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,602][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.2663, 0.0637, 0.0738, 0.0803, 0.4108, 0.0516, 0.0196, 0.0339],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,605][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ista] are: tensor([9.1065e-04, 3.3586e-04, 3.9326e-05, 1.1263e-04, 8.0641e-03, 8.4300e-05,
        5.4129e-01, 4.4916e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,611][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0215, 0.0221, 0.0099, 0.0071, 0.4741, 0.0106, 0.1016, 0.3530],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,614][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ista] are: tensor([7.1239e-05, 3.6730e-06, 3.7730e-07, 2.6393e-08, 1.2105e-06, 1.8285e-09,
        3.8612e-06, 9.9992e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,617][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.2400, 0.0553, 0.0300, 0.0151, 0.2678, 0.0750, 0.1886, 0.1281],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,617][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0495, 0.0077, 0.0246, 0.0342, 0.0937, 0.0630, 0.7162, 0.0111],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,618][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.2800, 0.2031, 0.0859, 0.0985, 0.1245, 0.0877, 0.0602, 0.0602],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,619][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.3081, 0.1821, 0.0675, 0.1173, 0.0955, 0.1322, 0.0738, 0.0236],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,621][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.2455, 0.0706, 0.1018, 0.1108, 0.0844, 0.0484, 0.0553, 0.2832],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,625][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.2188, 0.2709, 0.0662, 0.1065, 0.0885, 0.0289, 0.0336, 0.1868],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:32,630][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.2220, 0.1440, 0.0707, 0.0578, 0.0534, 0.1504, 0.0643, 0.1150, 0.1223],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,633][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ de] are: tensor([7.6683e-05, 6.2478e-04, 4.6740e-04, 2.1791e-04, 3.0341e-03, 1.8713e-04,
        2.7628e-03, 1.6840e-04, 9.9246e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,639][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.2146, 0.2183, 0.0505, 0.1258, 0.1619, 0.0892, 0.0274, 0.0802, 0.0321],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,642][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ de] are: tensor([2.0343e-03, 1.2534e-04, 8.8035e-05, 1.1562e-04, 1.0402e-03, 2.7738e-03,
        1.4046e-03, 7.3843e-04, 9.9168e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,647][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0337, 0.0033, 0.0032, 0.0056, 0.3082, 0.0043, 0.0185, 0.0366, 0.5867],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,650][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ de] are: tensor([3.6768e-03, 9.0371e-05, 5.9336e-05, 4.4453e-05, 8.2236e-04, 4.4068e-05,
        1.0768e-05, 2.1805e-05, 9.9523e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,651][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.1346, 0.1629, 0.0320, 0.0166, 0.3109, 0.0331, 0.0685, 0.1697, 0.0716],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,652][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.1538, 0.0325, 0.0396, 0.0837, 0.0863, 0.0788, 0.0588, 0.1574, 0.3090],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,652][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.2402, 0.0395, 0.0664, 0.1469, 0.1487, 0.2290, 0.0233, 0.0428, 0.0632],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,655][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.2781, 0.0947, 0.0707, 0.1293, 0.1164, 0.1267, 0.0623, 0.0839, 0.0379],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,658][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.2424, 0.0698, 0.0700, 0.1318, 0.1089, 0.0757, 0.0497, 0.0316, 0.2200],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,664][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.2544, 0.1286, 0.0795, 0.1339, 0.0586, 0.0268, 0.0726, 0.1932, 0.0524],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:32,669][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.2975, 0.0795, 0.0886, 0.0601, 0.1087, 0.0744, 0.1116, 0.0912, 0.0493,
        0.0391], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,672][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ An] are: tensor([2.5587e-03, 6.9636e-04, 1.7480e-04, 4.7775e-04, 1.6840e-03, 1.1206e-03,
        1.4216e-03, 2.4080e-04, 8.1480e-04, 9.9081e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,678][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.3571, 0.0873, 0.1093, 0.1391, 0.0289, 0.1019, 0.0434, 0.0390, 0.0445,
        0.0496], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,681][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ An] are: tensor([3.0201e-03, 4.3534e-05, 9.2568e-05, 4.5416e-04, 1.1332e-03, 5.1286e-03,
        1.5847e-03, 1.1297e-03, 1.8175e-02, 9.6924e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,684][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.0347, 0.0163, 0.0114, 0.0175, 0.0825, 0.0345, 0.0162, 0.0345, 0.0686,
        0.6838], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,684][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ An] are: tensor([4.8058e-02, 7.9529e-05, 1.5537e-04, 1.5376e-04, 1.2254e-04, 3.6380e-04,
        1.5380e-04, 5.4233e-06, 2.0852e-04, 9.5070e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,685][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.1337, 0.1612, 0.0622, 0.0211, 0.1650, 0.0510, 0.0914, 0.1787, 0.0843,
        0.0514], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,686][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0661, 0.0259, 0.0164, 0.0425, 0.1174, 0.0582, 0.0745, 0.0755, 0.2436,
        0.2798], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,688][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.1400, 0.0607, 0.0515, 0.1415, 0.0661, 0.2536, 0.0401, 0.1159, 0.0531,
        0.0774], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,692][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.2620, 0.0838, 0.0618, 0.1082, 0.1128, 0.0990, 0.0553, 0.0687, 0.0740,
        0.0744], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,697][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.3282, 0.0439, 0.0548, 0.1089, 0.0689, 0.0660, 0.0465, 0.0344, 0.0394,
        0.2088], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,702][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.1558, 0.1233, 0.1259, 0.1006, 0.1021, 0.0284, 0.0534, 0.2083, 0.0682,
        0.0341], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:32,707][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.2120, 0.0761, 0.0494, 0.0589, 0.1003, 0.0725, 0.2393, 0.0341, 0.0534,
        0.0798, 0.0241], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,710][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [za] are: tensor([1.9942e-05, 7.4627e-05, 3.1164e-05, 7.8103e-05, 9.6882e-04, 5.1507e-06,
        1.7239e-06, 2.2012e-04, 5.6499e-06, 1.4429e-05, 9.9858e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,715][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.2564, 0.1452, 0.0482, 0.0788, 0.0818, 0.0948, 0.0561, 0.0540, 0.0240,
        0.1384, 0.0223], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,717][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [za] are: tensor([5.1910e-04, 3.3563e-05, 3.6124e-05, 5.1507e-05, 7.6285e-03, 1.2347e-04,
        3.3180e-03, 3.7940e-03, 3.3602e-03, 1.9348e-02, 9.6179e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,718][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.0098, 0.0009, 0.0008, 0.0043, 0.0555, 0.0066, 0.0276, 0.0305, 0.0557,
        0.0371, 0.7713], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,719][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [za] are: tensor([3.5073e-03, 1.4400e-04, 9.2759e-06, 2.3598e-06, 1.2716e-04, 1.4093e-05,
        1.7607e-06, 6.5603e-05, 2.3284e-05, 5.9007e-07, 9.9610e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,720][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0985, 0.0290, 0.0275, 0.0172, 0.3324, 0.0358, 0.1053, 0.1107, 0.1429,
        0.0485, 0.0520], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,722][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0610, 0.0050, 0.0064, 0.0254, 0.0165, 0.0493, 0.1034, 0.0704, 0.0794,
        0.5625, 0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,726][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.1869, 0.0101, 0.0397, 0.0710, 0.0771, 0.1498, 0.0686, 0.0407, 0.2207,
        0.0955, 0.0399], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,731][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.2653, 0.0951, 0.0418, 0.0990, 0.1254, 0.0962, 0.0399, 0.0641, 0.0499,
        0.1039, 0.0194], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,736][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.2322, 0.0602, 0.0569, 0.0873, 0.1195, 0.0650, 0.0234, 0.0453, 0.0529,
        0.0466, 0.2106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,742][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.1494, 0.1627, 0.0734, 0.0992, 0.1364, 0.0276, 0.0369, 0.1977, 0.0464,
        0.0279, 0.0425], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:32,748][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.2520, 0.0582, 0.0694, 0.0195, 0.0639, 0.0327, 0.0774, 0.0693, 0.0973,
        0.0887, 0.1300, 0.0415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,751][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ is] are: tensor([2.9907e-03, 6.0827e-04, 2.2329e-03, 1.5016e-03, 7.2650e-05, 2.1604e-03,
        7.9749e-05, 6.9920e-04, 3.0122e-04, 7.3662e-04, 1.0263e-04, 9.8851e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,752][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.2333, 0.0690, 0.1032, 0.1532, 0.0279, 0.0508, 0.0452, 0.0314, 0.0592,
        0.0468, 0.0432, 0.1368], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,753][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ is] are: tensor([1.1132e-02, 1.1418e-04, 8.9555e-04, 4.7994e-04, 1.9732e-04, 4.6174e-04,
        1.0270e-03, 2.2055e-03, 4.0465e-03, 3.9535e-02, 7.7415e-03, 9.3216e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,753][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0794, 0.0118, 0.0446, 0.0316, 0.0203, 0.0142, 0.0227, 0.0321, 0.0486,
        0.1289, 0.0662, 0.4996], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,755][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ is] are: tensor([2.7968e-02, 4.3463e-03, 1.4897e-03, 2.6558e-03, 7.6700e-04, 9.3799e-03,
        3.5013e-04, 2.3004e-04, 1.0313e-03, 1.3376e-03, 3.1535e-04, 9.5013e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,759][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0678, 0.1263, 0.0731, 0.0163, 0.1719, 0.0271, 0.1007, 0.1557, 0.0864,
        0.0579, 0.0934, 0.0235], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,764][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0410, 0.0070, 0.0080, 0.0173, 0.0172, 0.0292, 0.0266, 0.0362, 0.0399,
        0.2604, 0.0885, 0.4286], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,769][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1039, 0.0198, 0.0559, 0.0885, 0.0143, 0.0744, 0.0165, 0.0217, 0.0365,
        0.0908, 0.0123, 0.4654], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,774][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.2034, 0.0638, 0.0814, 0.1176, 0.0580, 0.0758, 0.0416, 0.0424, 0.0547,
        0.0950, 0.0528, 0.1135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,779][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1851, 0.0499, 0.0660, 0.0995, 0.0551, 0.0821, 0.0246, 0.0258, 0.0367,
        0.0809, 0.0310, 0.2633], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,784][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2702, 0.0611, 0.0889, 0.1039, 0.0849, 0.0249, 0.0451, 0.1000, 0.0375,
        0.0436, 0.0399, 0.1000], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:32,785][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.2181, 0.0556, 0.0620, 0.0146, 0.0516, 0.0326, 0.0662, 0.0544, 0.2277,
        0.0663, 0.0913, 0.0380, 0.0216], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,786][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ a] are: tensor([6.9095e-03, 2.6966e-04, 7.7173e-04, 2.4710e-03, 5.2150e-04, 3.6506e-03,
        5.3816e-04, 3.6890e-05, 2.0355e-03, 9.6614e-03, 1.2654e-04, 5.3033e-04,
        9.7248e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,786][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1794, 0.0424, 0.0821, 0.1793, 0.0764, 0.0471, 0.0478, 0.0528, 0.0472,
        0.0460, 0.0676, 0.1017, 0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,788][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ a] are: tensor([1.0969e-02, 2.2260e-04, 8.6075e-04, 7.6704e-04, 4.2792e-04, 8.9915e-04,
        1.2663e-03, 2.5655e-03, 8.2093e-03, 5.1647e-02, 6.5661e-03, 2.0379e-01,
        7.1181e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,792][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0404, 0.0121, 0.0386, 0.0129, 0.0130, 0.0082, 0.0224, 0.0191, 0.0498,
        0.1294, 0.0202, 0.3577, 0.2762], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,797][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1162, 0.0175, 0.0994, 0.0827, 0.0026, 0.0764, 0.0043, 0.0025, 0.0228,
        0.0853, 0.0043, 0.0588, 0.4272], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,802][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0713, 0.1427, 0.0780, 0.0122, 0.1725, 0.0235, 0.0992, 0.1545, 0.0946,
        0.0421, 0.0802, 0.0217, 0.0076], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,806][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0313, 0.0048, 0.0058, 0.0096, 0.0097, 0.0238, 0.0185, 0.0260, 0.0580,
        0.1312, 0.0814, 0.2997, 0.3002], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,811][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0431, 0.0097, 0.0236, 0.0446, 0.0053, 0.0401, 0.0060, 0.0082, 0.0259,
        0.0463, 0.0067, 0.2824, 0.4581], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,817][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.2019, 0.0556, 0.0628, 0.1124, 0.0584, 0.0796, 0.0355, 0.0368, 0.0455,
        0.0797, 0.0472, 0.0869, 0.0976], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,818][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.2124, 0.0375, 0.0559, 0.1016, 0.0521, 0.0691, 0.0324, 0.0278, 0.0416,
        0.0933, 0.0283, 0.0708, 0.1772], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,818][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.2203, 0.0781, 0.0777, 0.0957, 0.0716, 0.0217, 0.0486, 0.0972, 0.0382,
        0.0344, 0.0396, 0.0956, 0.0814], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:32,819][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.1886, 0.2005, 0.0874, 0.0294, 0.0380, 0.0416, 0.0098, 0.0232, 0.0393,
        0.0403, 0.0207, 0.0814, 0.0485, 0.1512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,821][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([9.4997e-05, 4.8588e-04, 1.9858e-04, 5.5468e-04, 2.2799e-04, 9.0573e-04,
        8.0708e-05, 1.7438e-05, 1.1307e-05, 3.5352e-06, 1.7892e-04, 3.6725e-05,
        1.1276e-04, 9.9709e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,825][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.1938, 0.0684, 0.0660, 0.1392, 0.0169, 0.0840, 0.0668, 0.0456, 0.0615,
        0.0649, 0.0522, 0.0624, 0.0636, 0.0147], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,828][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([9.9147e-04, 9.9749e-06, 2.1480e-05, 1.8167e-05, 2.6190e-05, 8.4450e-06,
        8.3337e-05, 1.1320e-04, 5.2093e-05, 6.7052e-05, 3.3360e-04, 1.3724e-03,
        5.6368e-03, 9.9127e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,833][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0306, 0.0058, 0.0060, 0.0070, 0.0041, 0.0048, 0.0054, 0.0138, 0.0234,
        0.0263, 0.0535, 0.0350, 0.1360, 0.6483], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,836][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([1.1338e-02, 4.4360e-04, 4.6777e-05, 1.1349e-05, 3.1843e-05, 1.9900e-04,
        3.2157e-06, 4.7092e-06, 1.5143e-05, 1.2916e-06, 6.9788e-05, 1.9700e-06,
        5.7769e-06, 9.8783e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,841][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.1009, 0.0907, 0.0727, 0.0208, 0.1419, 0.0202, 0.0710, 0.1260, 0.0834,
        0.0457, 0.0472, 0.0243, 0.0296, 0.1256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,845][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0345, 0.0032, 0.0070, 0.0102, 0.0188, 0.0298, 0.0063, 0.0094, 0.0102,
        0.1143, 0.0364, 0.1777, 0.3969, 0.1452], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,850][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.1680, 0.0431, 0.0704, 0.0752, 0.0148, 0.0464, 0.0291, 0.0205, 0.0341,
        0.1091, 0.0129, 0.0972, 0.2526, 0.0266], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,851][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.1804, 0.0644, 0.0616, 0.0902, 0.0613, 0.0559, 0.0406, 0.0778, 0.0562,
        0.0666, 0.0530, 0.0796, 0.0831, 0.0293], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,852][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.1714, 0.0622, 0.0530, 0.0817, 0.0452, 0.0564, 0.0197, 0.0204, 0.0245,
        0.0321, 0.0332, 0.0596, 0.0736, 0.2670], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,852][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.2620, 0.0585, 0.0695, 0.0824, 0.0658, 0.0150, 0.0566, 0.1075, 0.0323,
        0.0319, 0.0536, 0.0427, 0.0285, 0.0935], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:32,855][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.2096, 0.0796, 0.0977, 0.0343, 0.0466, 0.0366, 0.0177, 0.0498, 0.0767,
        0.0779, 0.0545, 0.0493, 0.0661, 0.0645, 0.0391], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,857][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ different] are: tensor([5.3584e-04, 1.7942e-03, 2.9389e-03, 1.2851e-04, 2.5656e-04, 5.6764e-04,
        3.9685e-04, 3.5976e-05, 1.8145e-04, 6.6261e-05, 2.1596e-04, 7.2026e-05,
        3.8574e-03, 3.6123e-03, 9.8534e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,862][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.1440, 0.0631, 0.0775, 0.1180, 0.0489, 0.0559, 0.0318, 0.0529, 0.0538,
        0.0488, 0.0382, 0.0941, 0.0426, 0.0824, 0.0479], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,865][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ different] are: tensor([8.4255e-05, 1.2125e-06, 2.1152e-06, 4.1661e-07, 1.5887e-06, 4.3102e-06,
        3.2587e-06, 1.4473e-06, 4.3104e-05, 1.1102e-05, 2.0438e-05, 1.1974e-04,
        2.1781e-04, 9.2585e-03, 9.9023e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,870][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0129, 0.0028, 0.0053, 0.0047, 0.0015, 0.0033, 0.0067, 0.0054, 0.0135,
        0.0071, 0.0323, 0.0229, 0.0615, 0.0469, 0.7731], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,873][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ different] are: tensor([2.3883e-02, 1.4907e-03, 4.5962e-04, 3.3866e-05, 8.1893e-05, 1.2994e-03,
        2.0543e-04, 2.1466e-05, 2.8967e-04, 3.3625e-05, 2.4764e-05, 3.4374e-06,
        1.9304e-05, 6.1802e-04, 9.7154e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,878][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0738, 0.1196, 0.0608, 0.0146, 0.0770, 0.0149, 0.0430, 0.0678, 0.0495,
        0.0222, 0.0546, 0.0173, 0.0119, 0.2564, 0.1165], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,883][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0293, 0.0021, 0.0031, 0.0078, 0.0035, 0.0097, 0.0054, 0.0060, 0.0244,
        0.0449, 0.0245, 0.1594, 0.2341, 0.1941, 0.2519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,884][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.1119, 0.0198, 0.0554, 0.0797, 0.0220, 0.0437, 0.0220, 0.0228, 0.0304,
        0.0816, 0.0205, 0.2053, 0.2153, 0.0443, 0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,885][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.1807, 0.0610, 0.0546, 0.0838, 0.0534, 0.0553, 0.0329, 0.0424, 0.0431,
        0.0700, 0.0370, 0.0754, 0.0821, 0.0661, 0.0622], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,886][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.1533, 0.0506, 0.0564, 0.0576, 0.0307, 0.0427, 0.0208, 0.0183, 0.0263,
        0.0402, 0.0226, 0.0502, 0.0784, 0.0673, 0.2845], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,888][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.2432, 0.0478, 0.0670, 0.0958, 0.0640, 0.0220, 0.0517, 0.0683, 0.0327,
        0.0345, 0.0426, 0.0564, 0.0473, 0.0642, 0.0626], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:32,891][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.1025, 0.0254, 0.0543, 0.0161, 0.0322, 0.0276, 0.0483, 0.0482, 0.2261,
        0.0715, 0.1165, 0.0492, 0.0465, 0.0224, 0.1029, 0.0102],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,894][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ from] are: tensor([1.3679e-03, 8.6266e-05, 3.3197e-04, 4.0101e-02, 2.8865e-04, 2.0162e-04,
        4.0167e-05, 2.4937e-05, 1.4744e-04, 1.6746e-04, 4.5204e-05, 3.0191e-04,
        2.8320e-04, 3.9010e-04, 8.0528e-05, 9.5614e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,899][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.1730, 0.0265, 0.0606, 0.1396, 0.0342, 0.0447, 0.0532, 0.0483, 0.0435,
        0.0514, 0.0214, 0.0586, 0.0425, 0.0673, 0.0392, 0.0960],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,902][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ from] are: tensor([9.8068e-05, 6.7529e-07, 9.1774e-06, 1.3997e-05, 1.6288e-06, 9.3490e-07,
        9.1485e-06, 6.2641e-06, 7.3511e-05, 6.9360e-05, 9.3441e-05, 4.8535e-04,
        1.4866e-03, 2.2839e-03, 8.1883e-01, 1.7653e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,907][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0343, 0.0084, 0.0215, 0.0048, 0.0039, 0.0035, 0.0130, 0.0041, 0.0071,
        0.0217, 0.0065, 0.0768, 0.0394, 0.0952, 0.4502, 0.2095],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,910][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ from] are: tensor([6.3319e-02, 4.7133e-03, 1.5852e-02, 3.3190e-02, 3.7114e-03, 3.5111e-03,
        8.7924e-04, 2.0163e-03, 1.8932e-03, 2.6095e-03, 6.1554e-04, 2.3093e-03,
        9.3073e-03, 5.0822e-04, 2.7698e-03, 8.5280e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,915][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0632, 0.1036, 0.0603, 0.0249, 0.0484, 0.0286, 0.0815, 0.0476, 0.0530,
        0.0356, 0.0453, 0.0255, 0.0133, 0.1467, 0.0894, 0.1331],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,916][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0247, 0.0023, 0.0031, 0.0047, 0.0089, 0.0077, 0.0058, 0.0058, 0.0069,
        0.0332, 0.0219, 0.0787, 0.1292, 0.0737, 0.3198, 0.2736],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,917][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0619, 0.0148, 0.0285, 0.0692, 0.0064, 0.0400, 0.0066, 0.0060, 0.0239,
        0.0346, 0.0073, 0.2077, 0.2784, 0.0467, 0.0301, 0.1380],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,918][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.1584, 0.0523, 0.0544, 0.0879, 0.0402, 0.0539, 0.0346, 0.0304, 0.0380,
        0.0582, 0.0327, 0.0767, 0.0765, 0.0580, 0.0604, 0.0874],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,919][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.1231, 0.0373, 0.0588, 0.1109, 0.0488, 0.0447, 0.0174, 0.0209, 0.0244,
        0.0543, 0.0214, 0.0718, 0.0711, 0.0467, 0.0301, 0.2183],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,922][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.1764, 0.0517, 0.0761, 0.0979, 0.0592, 0.0175, 0.0407, 0.0761, 0.0313,
        0.0255, 0.0342, 0.0587, 0.0387, 0.0657, 0.0592, 0.0911],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:32,925][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1877, 0.0372, 0.0596, 0.0121, 0.0661, 0.0201, 0.0580, 0.0927, 0.1053,
        0.0459, 0.1001, 0.0372, 0.0177, 0.0503, 0.0727, 0.0151, 0.0222],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,925][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([4.9010e-02, 5.7758e-05, 2.3439e-04, 8.2104e-03, 7.4028e-04, 9.1569e-04,
        1.0556e-04, 1.6306e-05, 5.5944e-04, 2.9701e-03, 3.2195e-05, 8.1778e-04,
        2.5425e-02, 8.7431e-05, 6.6272e-04, 2.9712e-02, 8.8044e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,930][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1332, 0.0353, 0.0576, 0.1120, 0.0580, 0.0402, 0.0267, 0.0300, 0.0394,
        0.0365, 0.0588, 0.0926, 0.0250, 0.0400, 0.0340, 0.1542, 0.0266],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,933][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([5.9429e-03, 6.8710e-05, 2.3454e-04, 1.6407e-04, 1.0247e-04, 1.8217e-04,
        2.1823e-04, 5.2871e-04, 2.0531e-03, 3.3407e-03, 1.2906e-03, 1.8084e-02,
        3.7828e-02, 4.2527e-02, 1.0687e-01, 2.1308e-01, 5.6748e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,938][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0503, 0.0074, 0.0169, 0.0094, 0.0108, 0.0045, 0.0114, 0.0118, 0.0121,
        0.0307, 0.0149, 0.0838, 0.0526, 0.1207, 0.1369, 0.2108, 0.2152],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,943][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1672, 0.0232, 0.0485, 0.0885, 0.0071, 0.0380, 0.0058, 0.0074, 0.0168,
        0.0347, 0.0037, 0.0944, 0.1217, 0.0305, 0.0638, 0.0444, 0.2043],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,948][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0433, 0.0845, 0.0376, 0.0066, 0.1349, 0.0153, 0.0624, 0.1462, 0.0465,
        0.0217, 0.0806, 0.0089, 0.0030, 0.2185, 0.0790, 0.0092, 0.0018],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,951][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0191, 0.0015, 0.0027, 0.0036, 0.0027, 0.0042, 0.0034, 0.0082, 0.0082,
        0.0227, 0.0185, 0.0494, 0.0596, 0.0764, 0.1458, 0.2635, 0.3105],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,952][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0202, 0.0040, 0.0093, 0.0248, 0.0041, 0.0251, 0.0038, 0.0048, 0.0119,
        0.0243, 0.0041, 0.2029, 0.2605, 0.0107, 0.0128, 0.0693, 0.3072],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,953][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1514, 0.0396, 0.0449, 0.0759, 0.0500, 0.0517, 0.0275, 0.0293, 0.0324,
        0.0576, 0.0365, 0.0668, 0.0664, 0.0563, 0.0531, 0.0824, 0.0782],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,954][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2018, 0.0315, 0.0375, 0.0653, 0.0452, 0.0528, 0.0252, 0.0190, 0.0276,
        0.0622, 0.0222, 0.0541, 0.0887, 0.0362, 0.0389, 0.0595, 0.1321],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,957][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1612, 0.0494, 0.0533, 0.0632, 0.0580, 0.0186, 0.0361, 0.0814, 0.0266,
        0.0298, 0.0293, 0.0547, 0.0692, 0.0597, 0.0760, 0.0611, 0.0722],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:32,960][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.2031, 0.0293, 0.0388, 0.0307, 0.0383, 0.0244, 0.0599, 0.0394, 0.0682,
        0.0519, 0.1093, 0.0263, 0.0587, 0.0421, 0.0525, 0.0323, 0.0653, 0.0293],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,963][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ language] are: tensor([7.1919e-04, 4.2462e-01, 1.2797e-03, 8.5193e-04, 5.8788e-04, 1.8423e-03,
        1.3395e-03, 6.7585e-05, 1.0124e-03, 3.1828e-04, 2.8010e-05, 2.1992e-04,
        2.8573e-04, 1.4739e-03, 2.9912e-03, 1.7180e-04, 1.4052e-04, 5.6205e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,968][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1947, 0.0640, 0.0371, 0.0670, 0.0723, 0.0410, 0.0111, 0.0469, 0.0339,
        0.0353, 0.0461, 0.0693, 0.0469, 0.0340, 0.0402, 0.0488, 0.0577, 0.0536],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,971][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ language] are: tensor([1.0906e-03, 8.5680e-04, 7.0573e-06, 3.8967e-06, 9.2452e-05, 2.9903e-05,
        3.3829e-05, 1.9767e-05, 3.2466e-04, 1.1989e-04, 1.0111e-04, 2.9892e-04,
        3.3705e-04, 2.9064e-03, 1.9280e-02, 1.3574e-03, 1.8471e-03, 9.7129e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,975][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0576, 0.0360, 0.0077, 0.0048, 0.0250, 0.0048, 0.0107, 0.0048, 0.0126,
        0.0166, 0.0050, 0.0115, 0.0174, 0.0403, 0.0889, 0.0559, 0.0799, 0.5204],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,978][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ language] are: tensor([9.7461e-04, 7.2789e-01, 2.5997e-05, 8.3918e-06, 1.2320e-05, 3.2928e-05,
        1.1956e-06, 1.6396e-05, 1.0818e-05, 5.7585e-07, 4.4978e-05, 6.4784e-07,
        2.6446e-07, 1.3590e-05, 2.5842e-05, 2.7738e-07, 1.5743e-07, 2.7094e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,983][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0914, 0.1238, 0.0399, 0.0177, 0.1309, 0.0226, 0.0440, 0.0772, 0.0644,
        0.0163, 0.0419, 0.0072, 0.0111, 0.1015, 0.0500, 0.0119, 0.0099, 0.1380],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,984][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0301, 0.0019, 0.0044, 0.0082, 0.0163, 0.0068, 0.0043, 0.0060, 0.0330,
        0.0266, 0.0030, 0.0728, 0.0595, 0.0293, 0.0825, 0.1805, 0.3255, 0.1095],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,985][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.1214, 0.0328, 0.0440, 0.0559, 0.0574, 0.0534, 0.0499, 0.0413, 0.0276,
        0.0594, 0.0321, 0.0717, 0.0602, 0.0232, 0.0577, 0.0659, 0.1063, 0.0397],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,986][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.1242, 0.0178, 0.0577, 0.0686, 0.0644, 0.0443, 0.0280, 0.0563, 0.0439,
        0.0575, 0.0507, 0.0700, 0.0601, 0.0466, 0.0531, 0.0732, 0.0681, 0.0154],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,989][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0744, 0.1625, 0.0433, 0.0576, 0.0452, 0.0443, 0.0238, 0.0164, 0.0319,
        0.0309, 0.0210, 0.0443, 0.0339, 0.0394, 0.0357, 0.0403, 0.0424, 0.2123],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,992][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.1649, 0.0491, 0.0624, 0.0808, 0.0476, 0.0180, 0.0513, 0.0545, 0.0261,
        0.0324, 0.0327, 0.0680, 0.0687, 0.0495, 0.0457, 0.0622, 0.0437, 0.0424],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:32,997][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.1371, 0.1344, 0.0398, 0.0133, 0.0286, 0.0091, 0.0109, 0.0389, 0.0289,
        0.0389, 0.0199, 0.0326, 0.0250, 0.0565, 0.0424, 0.0221, 0.0336, 0.2337,
        0.0543], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,000][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ used] are: tensor([2.4647e-04, 6.9571e-04, 3.3377e-01, 1.9394e-04, 6.0545e-05, 3.0947e-04,
        4.5464e-05, 3.0807e-04, 1.6176e-04, 7.7119e-05, 3.7686e-05, 3.6531e-04,
        2.1530e-04, 8.8508e-05, 2.5208e-04, 1.9346e-04, 9.9450e-05, 5.2547e-04,
        6.6235e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,005][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.1114, 0.0486, 0.0370, 0.0940, 0.0411, 0.0379, 0.0245, 0.0166, 0.0290,
        0.0375, 0.0768, 0.0644, 0.0427, 0.0324, 0.0434, 0.1092, 0.0516, 0.0584,
        0.0437], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,007][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ used] are: tensor([2.0244e-04, 1.7321e-06, 4.6932e-05, 8.5876e-07, 5.6303e-07, 1.6743e-06,
        3.8209e-06, 1.7659e-06, 1.6691e-05, 2.0791e-05, 6.5248e-06, 1.8590e-04,
        1.3886e-04, 2.1229e-03, 4.7188e-03, 9.7972e-04, 1.5261e-03, 4.5459e-03,
        9.8548e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,012][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0125, 0.0046, 0.0084, 0.0026, 0.0022, 0.0009, 0.0021, 0.0017, 0.0039,
        0.0039, 0.0026, 0.0157, 0.0203, 0.0362, 0.0615, 0.0532, 0.0682, 0.1277,
        0.5717], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,015][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ used] are: tensor([1.1543e-03, 1.3892e-04, 7.6385e-01, 5.0831e-05, 1.0294e-05, 3.0082e-04,
        2.4375e-05, 1.0568e-05, 7.2196e-05, 6.9220e-06, 3.8416e-06, 4.0824e-06,
        1.0201e-05, 3.8675e-06, 1.3087e-05, 8.8635e-06, 1.7045e-06, 4.9600e-06,
        2.3433e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,016][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0676, 0.0889, 0.0385, 0.0114, 0.0668, 0.0136, 0.0330, 0.0695, 0.0307,
        0.0203, 0.0352, 0.0118, 0.0082, 0.1636, 0.0904, 0.0156, 0.0085, 0.1627,
        0.0636], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,017][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ used] are: tensor([9.0691e-03, 3.7558e-03, 6.0839e-04, 1.8742e-03, 2.5666e-04, 1.0801e-03,
        5.0950e-04, 1.1275e-03, 1.0385e-03, 4.7279e-03, 1.8680e-03, 1.5749e-02,
        2.6926e-02, 2.4409e-02, 5.1263e-02, 8.4124e-02, 1.2668e-01, 5.3284e-01,
        1.1209e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,018][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0851, 0.0143, 0.0300, 0.0674, 0.0109, 0.0310, 0.0283, 0.0095, 0.0213,
        0.0386, 0.0139, 0.1289, 0.1323, 0.0518, 0.0396, 0.0916, 0.1271, 0.0223,
        0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,019][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0991, 0.0522, 0.0332, 0.0582, 0.0390, 0.0408, 0.0343, 0.0273, 0.0370,
        0.0506, 0.0297, 0.0696, 0.0612, 0.0528, 0.0581, 0.0660, 0.0679, 0.0759,
        0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,022][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0695, 0.0341, 0.1415, 0.0433, 0.0249, 0.0305, 0.0201, 0.0267, 0.0292,
        0.0339, 0.0194, 0.0466, 0.0480, 0.0311, 0.0445, 0.0497, 0.0490, 0.0425,
        0.2154], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,026][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.2022, 0.0372, 0.0734, 0.0854, 0.0479, 0.0194, 0.0431, 0.0540, 0.0296,
        0.0334, 0.0320, 0.0423, 0.0281, 0.0507, 0.0406, 0.0761, 0.0222, 0.0268,
        0.0556], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,030][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.1201, 0.0282, 0.0566, 0.0099, 0.0563, 0.0200, 0.0666, 0.0289, 0.0568,
        0.0469, 0.0375, 0.0365, 0.0325, 0.0520, 0.1045, 0.0171, 0.0369, 0.0521,
        0.1219, 0.0185], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,033][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ by] are: tensor([2.1836e-04, 5.5391e-05, 8.7528e-05, 3.9130e-01, 3.3238e-05, 1.0597e-04,
        2.9313e-05, 1.2519e-05, 6.7839e-05, 4.8087e-05, 1.9735e-05, 3.2532e-05,
        6.1825e-05, 6.5534e-05, 5.6675e-06, 1.0731e-02, 7.1616e-05, 1.6761e-05,
        4.0279e-05, 5.9700e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,038][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.1037, 0.0238, 0.0720, 0.0683, 0.0333, 0.0274, 0.0276, 0.0245, 0.0253,
        0.0271, 0.0270, 0.0560, 0.0218, 0.0411, 0.0269, 0.1584, 0.0244, 0.0250,
        0.0953, 0.0910], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,041][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ by] are: tensor([4.1651e-04, 7.4853e-07, 6.0601e-05, 6.6888e-05, 3.0540e-05, 2.2588e-06,
        9.8370e-05, 6.3545e-06, 4.4755e-05, 3.3930e-05, 1.8076e-04, 1.2376e-04,
        6.6982e-04, 1.6928e-03, 6.6586e-04, 1.0238e-02, 5.6471e-03, 2.4176e-04,
        2.7728e-01, 7.0250e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,044][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ by] are: tensor([1.3002e-02, 1.4241e-03, 8.2150e-03, 1.9827e-03, 9.0295e-04, 3.2395e-04,
        1.6464e-03, 9.4614e-04, 1.8208e-03, 2.7023e-03, 3.0269e-03, 7.2747e-03,
        5.1783e-03, 1.5578e-02, 4.5208e-02, 3.1482e-02, 1.7545e-02, 3.7088e-02,
        6.1665e-01, 1.8801e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,047][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ by] are: tensor([1.7326e-02, 5.8039e-03, 8.8489e-03, 6.5894e-01, 1.2839e-03, 3.9386e-03,
        4.2965e-04, 5.8223e-04, 5.0394e-03, 1.0700e-03, 5.0733e-04, 1.7764e-03,
        3.4111e-03, 1.1116e-03, 4.0424e-04, 7.9971e-03, 1.7502e-03, 7.3140e-04,
        1.5598e-03, 2.7748e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,049][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0464, 0.0584, 0.0439, 0.0096, 0.0636, 0.0114, 0.0692, 0.0829, 0.0481,
        0.0301, 0.0386, 0.0171, 0.0087, 0.1266, 0.0861, 0.0170, 0.0075, 0.1220,
        0.0957, 0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,050][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0167, 0.0011, 0.0020, 0.0015, 0.0026, 0.0018, 0.0048, 0.0013, 0.0054,
        0.0071, 0.0092, 0.0156, 0.0194, 0.0240, 0.0734, 0.0598, 0.1209, 0.0917,
        0.2678, 0.2738], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,051][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0284, 0.0108, 0.0138, 0.0373, 0.0047, 0.0237, 0.0046, 0.0042, 0.0140,
        0.0269, 0.0058, 0.1317, 0.1915, 0.0233, 0.0210, 0.0786, 0.2188, 0.0296,
        0.0353, 0.0960], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,052][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.1140, 0.0435, 0.0362, 0.0588, 0.0395, 0.0418, 0.0220, 0.0272, 0.0331,
        0.0425, 0.0284, 0.0549, 0.0560, 0.0425, 0.0478, 0.0685, 0.0672, 0.0507,
        0.0448, 0.0806], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,055][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0871, 0.0298, 0.0320, 0.1464, 0.0234, 0.0449, 0.0169, 0.0133, 0.0287,
        0.0430, 0.0142, 0.0422, 0.0564, 0.0291, 0.0205, 0.0717, 0.0566, 0.0308,
        0.0324, 0.1806], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,059][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.1612, 0.0445, 0.0570, 0.0756, 0.0567, 0.0170, 0.0328, 0.0625, 0.0234,
        0.0229, 0.0263, 0.0659, 0.0342, 0.0466, 0.0385, 0.0682, 0.0257, 0.0352,
        0.0442, 0.0617], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,064][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1612, 0.0298, 0.0489, 0.0098, 0.0539, 0.0160, 0.0474, 0.0765, 0.0869,
        0.0374, 0.0819, 0.0299, 0.0138, 0.0398, 0.0599, 0.0115, 0.0172, 0.0462,
        0.0949, 0.0164, 0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,067][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.3972e-02, 2.2607e-05, 9.0531e-05, 2.9786e-03, 3.1779e-04, 3.5875e-04,
        4.3726e-05, 6.8169e-06, 2.1299e-04, 1.3648e-03, 1.4931e-05, 3.3854e-04,
        1.1295e-02, 3.6637e-05, 3.2227e-04, 1.4624e-02, 4.4078e-01, 7.8729e-06,
        6.0073e-05, 3.3100e-03, 4.9984e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,071][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1001, 0.0247, 0.0439, 0.0830, 0.0468, 0.0297, 0.0202, 0.0237, 0.0306,
        0.0279, 0.0452, 0.0701, 0.0184, 0.0302, 0.0254, 0.1171, 0.0195, 0.0309,
        0.0666, 0.1246, 0.0215], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,074][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([3.7427e-03, 2.9707e-05, 8.8226e-05, 5.1500e-05, 3.0302e-05, 3.5174e-05,
        5.0346e-05, 1.1751e-04, 2.7242e-04, 3.4287e-04, 2.0837e-04, 1.6493e-03,
        3.1988e-03, 4.1740e-03, 9.7471e-03, 1.7357e-02, 4.7455e-02, 9.2345e-03,
        1.3454e-01, 1.6348e-01, 6.0419e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,079][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0293, 0.0038, 0.0087, 0.0045, 0.0042, 0.0015, 0.0043, 0.0042, 0.0032,
        0.0074, 0.0044, 0.0196, 0.0115, 0.0271, 0.0310, 0.0443, 0.0455, 0.0563,
        0.2978, 0.2151, 0.1762], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,082][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1192, 0.0175, 0.0427, 0.0731, 0.0051, 0.0289, 0.0040, 0.0054, 0.0129,
        0.0300, 0.0028, 0.0807, 0.1002, 0.0271, 0.0563, 0.0399, 0.1676, 0.0026,
        0.0098, 0.0246, 0.1498], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,083][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0332, 0.0642, 0.0288, 0.0050, 0.1055, 0.0116, 0.0488, 0.1117, 0.0353,
        0.0165, 0.0599, 0.0065, 0.0021, 0.1733, 0.0609, 0.0070, 0.0012, 0.1506,
        0.0670, 0.0093, 0.0015], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,084][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0138, 0.0009, 0.0016, 0.0018, 0.0012, 0.0016, 0.0013, 0.0030, 0.0022,
        0.0053, 0.0052, 0.0099, 0.0112, 0.0155, 0.0282, 0.0474, 0.0555, 0.0514,
        0.1618, 0.2677, 0.3137], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,085][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0137, 0.0028, 0.0063, 0.0163, 0.0029, 0.0169, 0.0027, 0.0034, 0.0078,
        0.0165, 0.0027, 0.1382, 0.1709, 0.0072, 0.0086, 0.0458, 0.2015, 0.0093,
        0.0198, 0.0494, 0.2573], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,088][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1184, 0.0308, 0.0344, 0.0580, 0.0400, 0.0402, 0.0218, 0.0234, 0.0254,
        0.0449, 0.0293, 0.0522, 0.0504, 0.0437, 0.0409, 0.0628, 0.0594, 0.0343,
        0.0426, 0.0797, 0.0672], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,093][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.1439, 0.0226, 0.0267, 0.0462, 0.0350, 0.0395, 0.0197, 0.0155, 0.0220,
        0.0496, 0.0185, 0.0434, 0.0709, 0.0301, 0.0331, 0.0495, 0.1100, 0.0236,
        0.0296, 0.0554, 0.1153], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,097][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1339, 0.0398, 0.0454, 0.0520, 0.0486, 0.0166, 0.0314, 0.0687, 0.0223,
        0.0247, 0.0258, 0.0430, 0.0553, 0.0473, 0.0585, 0.0460, 0.0565, 0.0407,
        0.0403, 0.0457, 0.0574], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,121][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:33,126][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,130][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,134][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,137][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,141][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,142][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,143][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,146][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,147][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,148][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,149][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,149][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,150][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.9306, 0.0694], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,153][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,156][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.8773, 0.1227], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,160][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0757, 0.9243], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,165][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.6789, 0.3211], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,170][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0204, 0.9796], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,174][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.6993, 0.3007], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,179][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.9443, 0.0557], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,180][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9429, 0.0571], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,181][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.9166, 0.0834], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,181][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.7551, 0.2449], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,182][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.8085, 0.1915], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,184][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.5457, 0.3538, 0.1006], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,187][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0012, 0.0074, 0.9914], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,192][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.6911, 0.1941, 0.1148], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,196][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0628, 0.0251, 0.9121], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,201][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.3249, 0.2987, 0.3763], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,206][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0235, 0.0014, 0.9752], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,210][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.4580, 0.4021, 0.1399], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,212][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.3207, 0.5917, 0.0876], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,213][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.7713, 0.0667, 0.1620], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,213][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.6441, 0.1996, 0.1563], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,214][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.5955, 0.0953, 0.3092], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,215][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.6863, 0.1224, 0.1913], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:33,218][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.5934, 0.1280, 0.2245, 0.0541], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,219][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([4.7219e-04, 3.8726e-04, 3.4301e-04, 9.9880e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,224][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.4875, 0.0915, 0.1991, 0.2219], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,228][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0301, 0.0034, 0.3061, 0.6604], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,233][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.2891, 0.1021, 0.4708, 0.1380], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,238][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.1267, 0.0204, 0.0118, 0.8411], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,242][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.3579, 0.3661, 0.2188, 0.0572], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,244][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.3428, 0.1505, 0.2709, 0.2358], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,245][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.3895, 0.0843, 0.1263, 0.3999], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,246][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.5151, 0.1336, 0.1302, 0.2212], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,246][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.5033, 0.0815, 0.0818, 0.3334], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,248][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.4917, 0.1388, 0.1413, 0.2282], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:33,251][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.5009, 0.0425, 0.0685, 0.2253, 0.1628], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,253][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([9.5033e-05, 1.2695e-04, 3.7105e-05, 1.3030e-04, 9.9961e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,258][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.5533, 0.1307, 0.0391, 0.1668, 0.1101], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,261][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([9.2316e-04, 8.4138e-05, 3.8049e-05, 9.4415e-05, 9.9886e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,263][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([3.4870e-03, 8.0558e-04, 7.0577e-04, 9.3660e-04, 9.9406e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,266][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([7.5471e-04, 1.0226e-05, 2.1083e-06, 5.1514e-07, 9.9923e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,271][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.4801, 0.1675, 0.0595, 0.0589, 0.2340], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,276][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.2672, 0.0645, 0.2452, 0.3208, 0.1024], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,277][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.5126, 0.0828, 0.0663, 0.1578, 0.1804], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,277][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.5722, 0.1277, 0.0848, 0.1871, 0.0281], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,278][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.4628, 0.0610, 0.0480, 0.1076, 0.3206], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,279][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.2551, 0.2488, 0.1479, 0.2410, 0.1073], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:33,281][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.4291, 0.0765, 0.1451, 0.0690, 0.1916, 0.0887], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,283][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([3.6950e-04, 1.3760e-03, 6.4896e-04, 1.5038e-03, 1.6876e-04, 9.9593e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,287][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.3957, 0.1671, 0.0740, 0.1456, 0.1028, 0.1149], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,292][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0047, 0.0015, 0.0041, 0.0026, 0.0118, 0.9753], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,297][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.0915, 0.0425, 0.0288, 0.0704, 0.4245, 0.3424], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,300][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([1.0107e-02, 4.2144e-04, 4.2218e-04, 7.0486e-05, 3.0698e-05, 9.8895e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,304][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.2345, 0.3078, 0.1008, 0.0213, 0.2795, 0.0561], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,309][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.1691, 0.1005, 0.0815, 0.1865, 0.2511, 0.2113], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,309][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.2655, 0.0361, 0.0691, 0.1790, 0.0695, 0.3809], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,310][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.3631, 0.0884, 0.0944, 0.1666, 0.1723, 0.1153], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,311][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.4149, 0.0705, 0.0605, 0.1481, 0.0415, 0.2646], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,312][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.2896, 0.2585, 0.1432, 0.1604, 0.1018, 0.0466], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:33,315][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.2467, 0.0763, 0.0968, 0.1069, 0.2233, 0.2141, 0.0360],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,318][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([1.1699e-04, 5.1832e-04, 1.0349e-04, 2.9436e-04, 1.7549e-03, 3.4785e-04,
        9.9686e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,322][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.3996, 0.1858, 0.0508, 0.1283, 0.0873, 0.0700, 0.0782],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,325][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([2.1789e-03, 7.5508e-05, 9.7956e-05, 3.0960e-04, 2.0984e-03, 2.1987e-03,
        9.9304e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,330][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.0138, 0.0043, 0.0055, 0.0062, 0.0312, 0.0211, 0.9179],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,333][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([2.3929e-03, 9.2261e-06, 2.9170e-05, 7.0517e-07, 1.0730e-04, 4.5241e-06,
        9.9746e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,337][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.2575, 0.1229, 0.0913, 0.1111, 0.1553, 0.1900, 0.0719],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,341][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.2573, 0.0291, 0.0666, 0.2383, 0.0170, 0.2843, 0.1074],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,342][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.3098, 0.0468, 0.0640, 0.1804, 0.1323, 0.2385, 0.0282],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,342][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.3668, 0.1026, 0.1116, 0.1332, 0.1299, 0.1414, 0.0145],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,343][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.4165, 0.0825, 0.0671, 0.1110, 0.0537, 0.0622, 0.2070],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,344][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.2917, 0.1603, 0.1479, 0.1623, 0.0965, 0.0501, 0.0911],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:33,347][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.3139, 0.0427, 0.0790, 0.0724, 0.1788, 0.0907, 0.1509, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,349][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([6.2069e-05, 2.1995e-04, 3.5065e-04, 6.1633e-05, 1.2478e-03, 3.1181e-05,
        1.2711e-03, 9.9676e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,352][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.2663, 0.0637, 0.0738, 0.0803, 0.4108, 0.0516, 0.0196, 0.0339],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,355][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([9.1065e-04, 3.3586e-04, 3.9326e-05, 1.1263e-04, 8.0641e-03, 8.4300e-05,
        5.4129e-01, 4.4916e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,359][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0215, 0.0221, 0.0099, 0.0071, 0.4741, 0.0106, 0.1016, 0.3530],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,362][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([7.1239e-05, 3.6730e-06, 3.7730e-07, 2.6393e-08, 1.2105e-06, 1.8285e-09,
        3.8612e-06, 9.9992e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,367][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.2400, 0.0553, 0.0300, 0.0151, 0.2678, 0.0750, 0.1886, 0.1281],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,371][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0495, 0.0077, 0.0246, 0.0342, 0.0937, 0.0630, 0.7162, 0.0111],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,376][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.2800, 0.2031, 0.0859, 0.0985, 0.1245, 0.0877, 0.0602, 0.0602],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,377][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.3081, 0.1821, 0.0675, 0.1173, 0.0955, 0.1322, 0.0738, 0.0236],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,377][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.2455, 0.0706, 0.1018, 0.1108, 0.0844, 0.0484, 0.0553, 0.2832],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,378][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.2188, 0.2709, 0.0662, 0.1065, 0.0885, 0.0289, 0.0336, 0.1868],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:33,380][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.2220, 0.1440, 0.0707, 0.0578, 0.0534, 0.1504, 0.0643, 0.1150, 0.1223],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,382][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([7.6683e-05, 6.2478e-04, 4.6740e-04, 2.1791e-04, 3.0341e-03, 1.8713e-04,
        2.7628e-03, 1.6840e-04, 9.9246e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,386][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.2146, 0.2183, 0.0505, 0.1258, 0.1619, 0.0892, 0.0274, 0.0802, 0.0321],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,389][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([2.0343e-03, 1.2534e-04, 8.8035e-05, 1.1562e-04, 1.0402e-03, 2.7738e-03,
        1.4046e-03, 7.3843e-04, 9.9168e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,393][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0337, 0.0033, 0.0032, 0.0056, 0.3082, 0.0043, 0.0185, 0.0366, 0.5867],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,396][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([3.6768e-03, 9.0371e-05, 5.9336e-05, 4.4453e-05, 8.2236e-04, 4.4068e-05,
        1.0768e-05, 2.1805e-05, 9.9523e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,400][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.1346, 0.1629, 0.0320, 0.0166, 0.3109, 0.0331, 0.0685, 0.1697, 0.0716],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,405][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.1538, 0.0325, 0.0396, 0.0837, 0.0863, 0.0788, 0.0588, 0.1574, 0.3090],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,408][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.2402, 0.0395, 0.0664, 0.1469, 0.1487, 0.2290, 0.0233, 0.0428, 0.0632],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,409][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.2781, 0.0947, 0.0707, 0.1293, 0.1164, 0.1267, 0.0623, 0.0839, 0.0379],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,409][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.2424, 0.0698, 0.0700, 0.1318, 0.1089, 0.0757, 0.0497, 0.0316, 0.2200],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,410][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.2544, 0.1286, 0.0795, 0.1339, 0.0586, 0.0268, 0.0726, 0.1932, 0.0524],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:33,413][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.2975, 0.0795, 0.0886, 0.0601, 0.1087, 0.0744, 0.1116, 0.0912, 0.0493,
        0.0391], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,414][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([2.5587e-03, 6.9636e-04, 1.7480e-04, 4.7775e-04, 1.6840e-03, 1.1206e-03,
        1.4216e-03, 2.4080e-04, 8.1480e-04, 9.9081e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,418][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.3571, 0.0873, 0.1093, 0.1391, 0.0289, 0.1019, 0.0434, 0.0390, 0.0445,
        0.0496], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,421][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([3.0201e-03, 4.3534e-05, 9.2568e-05, 4.5416e-04, 1.1332e-03, 5.1286e-03,
        1.5847e-03, 1.1297e-03, 1.8175e-02, 9.6924e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,426][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.0347, 0.0163, 0.0114, 0.0175, 0.0825, 0.0345, 0.0162, 0.0345, 0.0686,
        0.6838], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,428][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([4.8058e-02, 7.9529e-05, 1.5537e-04, 1.5376e-04, 1.2254e-04, 3.6380e-04,
        1.5380e-04, 5.4233e-06, 2.0852e-04, 9.5070e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,433][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.1337, 0.1612, 0.0622, 0.0211, 0.1650, 0.0510, 0.0914, 0.1787, 0.0843,
        0.0514], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,437][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0661, 0.0259, 0.0164, 0.0425, 0.1174, 0.0582, 0.0745, 0.0755, 0.2436,
        0.2798], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,440][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.1400, 0.0607, 0.0515, 0.1415, 0.0661, 0.2536, 0.0401, 0.1159, 0.0531,
        0.0774], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,441][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.2620, 0.0838, 0.0618, 0.1082, 0.1128, 0.0990, 0.0553, 0.0687, 0.0740,
        0.0744], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,442][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.3282, 0.0439, 0.0548, 0.1089, 0.0689, 0.0660, 0.0465, 0.0344, 0.0394,
        0.2088], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,442][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.1558, 0.1233, 0.1259, 0.1006, 0.1021, 0.0284, 0.0534, 0.2083, 0.0682,
        0.0341], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:33,445][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.2120, 0.0761, 0.0494, 0.0589, 0.1003, 0.0725, 0.2393, 0.0341, 0.0534,
        0.0798, 0.0241], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,447][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([1.9942e-05, 7.4627e-05, 3.1164e-05, 7.8103e-05, 9.6882e-04, 5.1507e-06,
        1.7239e-06, 2.2012e-04, 5.6499e-06, 1.4429e-05, 9.9858e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,450][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.2564, 0.1452, 0.0482, 0.0788, 0.0818, 0.0948, 0.0561, 0.0540, 0.0240,
        0.1384, 0.0223], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,453][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([5.1910e-04, 3.3563e-05, 3.6124e-05, 5.1507e-05, 7.6285e-03, 1.2347e-04,
        3.3180e-03, 3.7940e-03, 3.3602e-03, 1.9348e-02, 9.6179e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,458][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.0098, 0.0009, 0.0008, 0.0043, 0.0555, 0.0066, 0.0276, 0.0305, 0.0557,
        0.0371, 0.7713], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,460][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([3.5073e-03, 1.4400e-04, 9.2759e-06, 2.3598e-06, 1.2716e-04, 1.4093e-05,
        1.7607e-06, 6.5603e-05, 2.3284e-05, 5.9007e-07, 9.9610e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,465][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0985, 0.0290, 0.0275, 0.0172, 0.3324, 0.0358, 0.1053, 0.1107, 0.1429,
        0.0485, 0.0520], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,469][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0610, 0.0050, 0.0064, 0.0254, 0.0165, 0.0493, 0.1034, 0.0704, 0.0794,
        0.5625, 0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,472][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.1869, 0.0101, 0.0397, 0.0710, 0.0771, 0.1498, 0.0686, 0.0407, 0.2207,
        0.0955, 0.0399], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,473][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.2653, 0.0951, 0.0418, 0.0990, 0.1254, 0.0962, 0.0399, 0.0641, 0.0499,
        0.1039, 0.0194], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,474][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.2322, 0.0602, 0.0569, 0.0873, 0.1195, 0.0650, 0.0234, 0.0453, 0.0529,
        0.0466, 0.2106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,475][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.1494, 0.1627, 0.0734, 0.0992, 0.1364, 0.0276, 0.0369, 0.1977, 0.0464,
        0.0279, 0.0425], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:33,477][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.2520, 0.0582, 0.0694, 0.0195, 0.0639, 0.0327, 0.0774, 0.0693, 0.0973,
        0.0887, 0.1300, 0.0415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,479][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([2.9907e-03, 6.0827e-04, 2.2329e-03, 1.5016e-03, 7.2650e-05, 2.1604e-03,
        7.9749e-05, 6.9920e-04, 3.0122e-04, 7.3662e-04, 1.0263e-04, 9.8851e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,482][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.2333, 0.0690, 0.1032, 0.1532, 0.0279, 0.0508, 0.0452, 0.0314, 0.0592,
        0.0468, 0.0432, 0.1368], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,485][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([1.1132e-02, 1.1418e-04, 8.9555e-04, 4.7994e-04, 1.9732e-04, 4.6174e-04,
        1.0270e-03, 2.2055e-03, 4.0465e-03, 3.9535e-02, 7.7415e-03, 9.3216e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,490][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0794, 0.0118, 0.0446, 0.0316, 0.0203, 0.0142, 0.0227, 0.0321, 0.0486,
        0.1289, 0.0662, 0.4996], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,493][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([2.7968e-02, 4.3463e-03, 1.4897e-03, 2.6558e-03, 7.6700e-04, 9.3799e-03,
        3.5013e-04, 2.3004e-04, 1.0313e-03, 1.3376e-03, 3.1535e-04, 9.5013e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,497][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0678, 0.1263, 0.0731, 0.0163, 0.1719, 0.0271, 0.1007, 0.1557, 0.0864,
        0.0579, 0.0934, 0.0235], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,502][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0410, 0.0070, 0.0080, 0.0173, 0.0172, 0.0292, 0.0266, 0.0362, 0.0399,
        0.2604, 0.0885, 0.4286], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,504][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1039, 0.0198, 0.0559, 0.0885, 0.0143, 0.0744, 0.0165, 0.0217, 0.0365,
        0.0908, 0.0123, 0.4654], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,505][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.2034, 0.0638, 0.0814, 0.1176, 0.0580, 0.0758, 0.0416, 0.0424, 0.0547,
        0.0950, 0.0528, 0.1135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,506][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.1851, 0.0499, 0.0660, 0.0995, 0.0551, 0.0821, 0.0246, 0.0258, 0.0367,
        0.0809, 0.0310, 0.2633], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,507][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.2702, 0.0611, 0.0889, 0.1039, 0.0849, 0.0249, 0.0451, 0.1000, 0.0375,
        0.0436, 0.0399, 0.1000], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:33,509][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2181, 0.0556, 0.0620, 0.0146, 0.0516, 0.0326, 0.0662, 0.0544, 0.2277,
        0.0663, 0.0913, 0.0380, 0.0216], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,511][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([6.9095e-03, 2.6966e-04, 7.7173e-04, 2.4710e-03, 5.2150e-04, 3.6506e-03,
        5.3816e-04, 3.6890e-05, 2.0355e-03, 9.6614e-03, 1.2654e-04, 5.3033e-04,
        9.7248e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,514][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1794, 0.0424, 0.0821, 0.1793, 0.0764, 0.0471, 0.0478, 0.0528, 0.0472,
        0.0460, 0.0676, 0.1017, 0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,517][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([1.0969e-02, 2.2260e-04, 8.6075e-04, 7.6704e-04, 4.2792e-04, 8.9915e-04,
        1.2663e-03, 2.5655e-03, 8.2093e-03, 5.1647e-02, 6.5661e-03, 2.0379e-01,
        7.1181e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,522][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0404, 0.0121, 0.0386, 0.0129, 0.0130, 0.0082, 0.0224, 0.0191, 0.0498,
        0.1294, 0.0202, 0.3577, 0.2762], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,526][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1162, 0.0175, 0.0994, 0.0827, 0.0026, 0.0764, 0.0043, 0.0025, 0.0228,
        0.0853, 0.0043, 0.0588, 0.4272], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,531][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0713, 0.1427, 0.0780, 0.0122, 0.1725, 0.0235, 0.0992, 0.1545, 0.0946,
        0.0421, 0.0802, 0.0217, 0.0076], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,536][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0313, 0.0048, 0.0058, 0.0096, 0.0097, 0.0238, 0.0185, 0.0260, 0.0580,
        0.1312, 0.0814, 0.2997, 0.3002], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,537][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0431, 0.0097, 0.0236, 0.0446, 0.0053, 0.0401, 0.0060, 0.0082, 0.0259,
        0.0463, 0.0067, 0.2824, 0.4581], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,538][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2019, 0.0556, 0.0628, 0.1124, 0.0584, 0.0796, 0.0355, 0.0368, 0.0455,
        0.0797, 0.0472, 0.0869, 0.0976], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,538][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.2124, 0.0375, 0.0559, 0.1016, 0.0521, 0.0691, 0.0324, 0.0278, 0.0416,
        0.0933, 0.0283, 0.0708, 0.1772], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,539][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.2203, 0.0781, 0.0777, 0.0957, 0.0716, 0.0217, 0.0486, 0.0972, 0.0382,
        0.0344, 0.0396, 0.0956, 0.0814], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:33,543][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.1886, 0.2005, 0.0874, 0.0294, 0.0380, 0.0416, 0.0098, 0.0232, 0.0393,
        0.0403, 0.0207, 0.0814, 0.0485, 0.1512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,545][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([9.4997e-05, 4.8588e-04, 1.9858e-04, 5.5468e-04, 2.2799e-04, 9.0573e-04,
        8.0708e-05, 1.7438e-05, 1.1307e-05, 3.5352e-06, 1.7892e-04, 3.6725e-05,
        1.1276e-04, 9.9709e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,550][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.1938, 0.0684, 0.0660, 0.1392, 0.0169, 0.0840, 0.0668, 0.0456, 0.0615,
        0.0649, 0.0522, 0.0624, 0.0636, 0.0147], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,553][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([9.9147e-04, 9.9749e-06, 2.1480e-05, 1.8167e-05, 2.6190e-05, 8.4450e-06,
        8.3337e-05, 1.1320e-04, 5.2093e-05, 6.7052e-05, 3.3360e-04, 1.3724e-03,
        5.6368e-03, 9.9127e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,557][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0306, 0.0058, 0.0060, 0.0070, 0.0041, 0.0048, 0.0054, 0.0138, 0.0234,
        0.0263, 0.0535, 0.0350, 0.1360, 0.6483], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,560][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([1.1338e-02, 4.4360e-04, 4.6777e-05, 1.1349e-05, 3.1843e-05, 1.9900e-04,
        3.2157e-06, 4.7092e-06, 1.5143e-05, 1.2916e-06, 6.9788e-05, 1.9700e-06,
        5.7769e-06, 9.8783e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,565][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.1009, 0.0907, 0.0727, 0.0208, 0.1419, 0.0202, 0.0710, 0.1260, 0.0834,
        0.0457, 0.0472, 0.0243, 0.0296, 0.1256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,568][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0345, 0.0032, 0.0070, 0.0102, 0.0188, 0.0298, 0.0063, 0.0094, 0.0102,
        0.1143, 0.0364, 0.1777, 0.3969, 0.1452], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,569][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.1680, 0.0431, 0.0704, 0.0752, 0.0148, 0.0464, 0.0291, 0.0205, 0.0341,
        0.1091, 0.0129, 0.0972, 0.2526, 0.0266], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,570][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.1804, 0.0644, 0.0616, 0.0902, 0.0613, 0.0559, 0.0406, 0.0778, 0.0562,
        0.0666, 0.0530, 0.0796, 0.0831, 0.0293], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,571][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.1714, 0.0622, 0.0530, 0.0817, 0.0452, 0.0564, 0.0197, 0.0204, 0.0245,
        0.0321, 0.0332, 0.0596, 0.0736, 0.2670], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,573][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.2620, 0.0585, 0.0695, 0.0824, 0.0658, 0.0150, 0.0566, 0.1075, 0.0323,
        0.0319, 0.0536, 0.0427, 0.0285, 0.0935], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:33,576][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.2096, 0.0796, 0.0977, 0.0343, 0.0466, 0.0366, 0.0177, 0.0498, 0.0767,
        0.0779, 0.0545, 0.0493, 0.0661, 0.0645, 0.0391], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,578][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([5.3584e-04, 1.7942e-03, 2.9389e-03, 1.2851e-04, 2.5656e-04, 5.6764e-04,
        3.9685e-04, 3.5976e-05, 1.8145e-04, 6.6261e-05, 2.1596e-04, 7.2026e-05,
        3.8574e-03, 3.6123e-03, 9.8534e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,583][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.1440, 0.0631, 0.0775, 0.1180, 0.0489, 0.0559, 0.0318, 0.0529, 0.0538,
        0.0488, 0.0382, 0.0941, 0.0426, 0.0824, 0.0479], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,586][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([8.4255e-05, 1.2125e-06, 2.1152e-06, 4.1661e-07, 1.5887e-06, 4.3102e-06,
        3.2587e-06, 1.4473e-06, 4.3104e-05, 1.1102e-05, 2.0438e-05, 1.1974e-04,
        2.1781e-04, 9.2585e-03, 9.9023e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,590][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0129, 0.0028, 0.0053, 0.0047, 0.0015, 0.0033, 0.0067, 0.0054, 0.0135,
        0.0071, 0.0323, 0.0229, 0.0615, 0.0469, 0.7731], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,593][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([2.3883e-02, 1.4907e-03, 4.5962e-04, 3.3866e-05, 8.1893e-05, 1.2994e-03,
        2.0543e-04, 2.1466e-05, 2.8967e-04, 3.3625e-05, 2.4764e-05, 3.4374e-06,
        1.9304e-05, 6.1802e-04, 9.7154e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,598][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0738, 0.1196, 0.0608, 0.0146, 0.0770, 0.0149, 0.0430, 0.0678, 0.0495,
        0.0222, 0.0546, 0.0173, 0.0119, 0.2564, 0.1165], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,600][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0293, 0.0021, 0.0031, 0.0078, 0.0035, 0.0097, 0.0054, 0.0060, 0.0244,
        0.0449, 0.0245, 0.1594, 0.2341, 0.1941, 0.2519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,601][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.1119, 0.0198, 0.0554, 0.0797, 0.0220, 0.0437, 0.0220, 0.0228, 0.0304,
        0.0816, 0.0205, 0.2053, 0.2153, 0.0443, 0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,602][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.1807, 0.0610, 0.0546, 0.0838, 0.0534, 0.0553, 0.0329, 0.0424, 0.0431,
        0.0700, 0.0370, 0.0754, 0.0821, 0.0661, 0.0622], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,603][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.1533, 0.0506, 0.0564, 0.0576, 0.0307, 0.0427, 0.0208, 0.0183, 0.0263,
        0.0402, 0.0226, 0.0502, 0.0784, 0.0673, 0.2845], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,606][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.2432, 0.0478, 0.0670, 0.0958, 0.0640, 0.0220, 0.0517, 0.0683, 0.0327,
        0.0345, 0.0426, 0.0564, 0.0473, 0.0642, 0.0626], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:33,610][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.1025, 0.0254, 0.0543, 0.0161, 0.0322, 0.0276, 0.0483, 0.0482, 0.2261,
        0.0715, 0.1165, 0.0492, 0.0465, 0.0224, 0.1029, 0.0102],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,612][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([1.3679e-03, 8.6266e-05, 3.3197e-04, 4.0101e-02, 2.8865e-04, 2.0162e-04,
        4.0167e-05, 2.4937e-05, 1.4744e-04, 1.6746e-04, 4.5204e-05, 3.0191e-04,
        2.8320e-04, 3.9010e-04, 8.0528e-05, 9.5614e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,617][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.1730, 0.0265, 0.0606, 0.1396, 0.0342, 0.0447, 0.0532, 0.0483, 0.0435,
        0.0514, 0.0214, 0.0586, 0.0425, 0.0673, 0.0392, 0.0960],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,619][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([9.8068e-05, 6.7529e-07, 9.1774e-06, 1.3997e-05, 1.6288e-06, 9.3490e-07,
        9.1485e-06, 6.2641e-06, 7.3511e-05, 6.9360e-05, 9.3441e-05, 4.8535e-04,
        1.4866e-03, 2.2839e-03, 8.1883e-01, 1.7653e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,624][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0343, 0.0084, 0.0215, 0.0048, 0.0039, 0.0035, 0.0130, 0.0041, 0.0071,
        0.0217, 0.0065, 0.0768, 0.0394, 0.0952, 0.4502, 0.2095],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,627][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([6.3319e-02, 4.7133e-03, 1.5852e-02, 3.3190e-02, 3.7114e-03, 3.5111e-03,
        8.7924e-04, 2.0163e-03, 1.8932e-03, 2.6095e-03, 6.1554e-04, 2.3093e-03,
        9.3073e-03, 5.0822e-04, 2.7698e-03, 8.5280e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,632][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0632, 0.1036, 0.0603, 0.0249, 0.0484, 0.0286, 0.0815, 0.0476, 0.0530,
        0.0356, 0.0453, 0.0255, 0.0133, 0.1467, 0.0894, 0.1331],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,633][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0247, 0.0023, 0.0031, 0.0047, 0.0089, 0.0077, 0.0058, 0.0058, 0.0069,
        0.0332, 0.0219, 0.0787, 0.1292, 0.0737, 0.3198, 0.2736],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,634][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0619, 0.0148, 0.0285, 0.0692, 0.0064, 0.0400, 0.0066, 0.0060, 0.0239,
        0.0346, 0.0073, 0.2077, 0.2784, 0.0467, 0.0301, 0.1380],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,635][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.1584, 0.0523, 0.0544, 0.0879, 0.0402, 0.0539, 0.0346, 0.0304, 0.0380,
        0.0582, 0.0327, 0.0767, 0.0765, 0.0580, 0.0604, 0.0874],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,637][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.1231, 0.0373, 0.0588, 0.1109, 0.0488, 0.0447, 0.0174, 0.0209, 0.0244,
        0.0543, 0.0214, 0.0718, 0.0711, 0.0467, 0.0301, 0.2183],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,640][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.1764, 0.0517, 0.0761, 0.0979, 0.0592, 0.0175, 0.0407, 0.0761, 0.0313,
        0.0255, 0.0342, 0.0587, 0.0387, 0.0657, 0.0592, 0.0911],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:33,644][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1877, 0.0372, 0.0596, 0.0121, 0.0661, 0.0201, 0.0580, 0.0927, 0.1053,
        0.0459, 0.1001, 0.0372, 0.0177, 0.0503, 0.0727, 0.0151, 0.0222],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,647][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([4.9010e-02, 5.7758e-05, 2.3439e-04, 8.2104e-03, 7.4028e-04, 9.1569e-04,
        1.0556e-04, 1.6306e-05, 5.5944e-04, 2.9701e-03, 3.2195e-05, 8.1778e-04,
        2.5425e-02, 8.7431e-05, 6.6272e-04, 2.9712e-02, 8.8044e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,651][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1332, 0.0353, 0.0576, 0.1120, 0.0580, 0.0402, 0.0267, 0.0300, 0.0394,
        0.0365, 0.0588, 0.0926, 0.0250, 0.0400, 0.0340, 0.1542, 0.0266],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,654][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([5.9429e-03, 6.8710e-05, 2.3454e-04, 1.6407e-04, 1.0247e-04, 1.8217e-04,
        2.1823e-04, 5.2871e-04, 2.0531e-03, 3.3407e-03, 1.2906e-03, 1.8084e-02,
        3.7828e-02, 4.2527e-02, 1.0687e-01, 2.1308e-01, 5.6748e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,659][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0503, 0.0074, 0.0169, 0.0094, 0.0108, 0.0045, 0.0114, 0.0118, 0.0121,
        0.0307, 0.0149, 0.0838, 0.0526, 0.1207, 0.1369, 0.2108, 0.2152],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,664][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1672, 0.0232, 0.0485, 0.0885, 0.0071, 0.0380, 0.0058, 0.0074, 0.0168,
        0.0347, 0.0037, 0.0944, 0.1217, 0.0305, 0.0638, 0.0444, 0.2043],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,665][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0433, 0.0845, 0.0376, 0.0066, 0.1349, 0.0153, 0.0624, 0.1462, 0.0465,
        0.0217, 0.0806, 0.0089, 0.0030, 0.2185, 0.0790, 0.0092, 0.0018],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,666][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0191, 0.0015, 0.0027, 0.0036, 0.0027, 0.0042, 0.0034, 0.0082, 0.0082,
        0.0227, 0.0185, 0.0494, 0.0596, 0.0764, 0.1458, 0.2635, 0.3105],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,667][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0202, 0.0040, 0.0093, 0.0248, 0.0041, 0.0251, 0.0038, 0.0048, 0.0119,
        0.0243, 0.0041, 0.2029, 0.2605, 0.0107, 0.0128, 0.0693, 0.3072],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,669][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1514, 0.0396, 0.0449, 0.0759, 0.0500, 0.0517, 0.0275, 0.0293, 0.0324,
        0.0576, 0.0365, 0.0668, 0.0664, 0.0563, 0.0531, 0.0824, 0.0782],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,672][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2018, 0.0315, 0.0375, 0.0653, 0.0452, 0.0528, 0.0252, 0.0190, 0.0276,
        0.0622, 0.0222, 0.0541, 0.0887, 0.0362, 0.0389, 0.0595, 0.1321],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,675][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1612, 0.0494, 0.0533, 0.0632, 0.0580, 0.0186, 0.0361, 0.0814, 0.0266,
        0.0298, 0.0293, 0.0547, 0.0692, 0.0597, 0.0760, 0.0611, 0.0722],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:33,680][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.2031, 0.0293, 0.0388, 0.0307, 0.0383, 0.0244, 0.0599, 0.0394, 0.0682,
        0.0519, 0.1093, 0.0263, 0.0587, 0.0421, 0.0525, 0.0323, 0.0653, 0.0293],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,683][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([7.1919e-04, 4.2462e-01, 1.2797e-03, 8.5193e-04, 5.8788e-04, 1.8423e-03,
        1.3395e-03, 6.7585e-05, 1.0124e-03, 3.1828e-04, 2.8010e-05, 2.1992e-04,
        2.8573e-04, 1.4739e-03, 2.9912e-03, 1.7180e-04, 1.4052e-04, 5.6205e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,688][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.1947, 0.0640, 0.0371, 0.0670, 0.0723, 0.0410, 0.0111, 0.0469, 0.0339,
        0.0353, 0.0461, 0.0693, 0.0469, 0.0340, 0.0402, 0.0488, 0.0577, 0.0536],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,690][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([1.0906e-03, 8.5680e-04, 7.0573e-06, 3.8967e-06, 9.2452e-05, 2.9903e-05,
        3.3829e-05, 1.9767e-05, 3.2466e-04, 1.1989e-04, 1.0111e-04, 2.9892e-04,
        3.3705e-04, 2.9064e-03, 1.9280e-02, 1.3574e-03, 1.8471e-03, 9.7129e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,695][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0576, 0.0360, 0.0077, 0.0048, 0.0250, 0.0048, 0.0107, 0.0048, 0.0126,
        0.0166, 0.0050, 0.0115, 0.0174, 0.0403, 0.0889, 0.0559, 0.0799, 0.5204],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,696][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([9.7461e-04, 7.2789e-01, 2.5997e-05, 8.3918e-06, 1.2320e-05, 3.2928e-05,
        1.1956e-06, 1.6396e-05, 1.0818e-05, 5.7585e-07, 4.4978e-05, 6.4784e-07,
        2.6446e-07, 1.3590e-05, 2.5842e-05, 2.7738e-07, 1.5743e-07, 2.7094e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,697][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0914, 0.1238, 0.0399, 0.0177, 0.1309, 0.0226, 0.0440, 0.0772, 0.0644,
        0.0163, 0.0419, 0.0072, 0.0111, 0.1015, 0.0500, 0.0119, 0.0099, 0.1380],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,698][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0301, 0.0019, 0.0044, 0.0082, 0.0163, 0.0068, 0.0043, 0.0060, 0.0330,
        0.0266, 0.0030, 0.0728, 0.0595, 0.0293, 0.0825, 0.1805, 0.3255, 0.1095],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,699][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.1214, 0.0328, 0.0440, 0.0559, 0.0574, 0.0534, 0.0499, 0.0413, 0.0276,
        0.0594, 0.0321, 0.0717, 0.0602, 0.0232, 0.0577, 0.0659, 0.1063, 0.0397],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,702][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.1242, 0.0178, 0.0577, 0.0686, 0.0644, 0.0443, 0.0280, 0.0563, 0.0439,
        0.0575, 0.0507, 0.0700, 0.0601, 0.0466, 0.0531, 0.0732, 0.0681, 0.0154],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,706][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0744, 0.1625, 0.0433, 0.0576, 0.0452, 0.0443, 0.0238, 0.0164, 0.0319,
        0.0309, 0.0210, 0.0443, 0.0339, 0.0394, 0.0357, 0.0403, 0.0424, 0.2123],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,710][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.1649, 0.0491, 0.0624, 0.0808, 0.0476, 0.0180, 0.0513, 0.0545, 0.0261,
        0.0324, 0.0327, 0.0680, 0.0687, 0.0495, 0.0457, 0.0622, 0.0437, 0.0424],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:33,715][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.1371, 0.1344, 0.0398, 0.0133, 0.0286, 0.0091, 0.0109, 0.0389, 0.0289,
        0.0389, 0.0199, 0.0326, 0.0250, 0.0565, 0.0424, 0.0221, 0.0336, 0.2337,
        0.0543], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,718][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([2.4647e-04, 6.9571e-04, 3.3377e-01, 1.9394e-04, 6.0545e-05, 3.0947e-04,
        4.5464e-05, 3.0807e-04, 1.6176e-04, 7.7119e-05, 3.7686e-05, 3.6531e-04,
        2.1530e-04, 8.8508e-05, 2.5208e-04, 1.9346e-04, 9.9450e-05, 5.2547e-04,
        6.6235e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,722][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.1114, 0.0486, 0.0370, 0.0940, 0.0411, 0.0379, 0.0245, 0.0166, 0.0290,
        0.0375, 0.0768, 0.0644, 0.0427, 0.0324, 0.0434, 0.1092, 0.0516, 0.0584,
        0.0437], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,725][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([2.0244e-04, 1.7321e-06, 4.6932e-05, 8.5876e-07, 5.6303e-07, 1.6743e-06,
        3.8209e-06, 1.7659e-06, 1.6691e-05, 2.0791e-05, 6.5248e-06, 1.8590e-04,
        1.3886e-04, 2.1229e-03, 4.7188e-03, 9.7972e-04, 1.5261e-03, 4.5459e-03,
        9.8548e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,728][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0125, 0.0046, 0.0084, 0.0026, 0.0022, 0.0009, 0.0021, 0.0017, 0.0039,
        0.0039, 0.0026, 0.0157, 0.0203, 0.0362, 0.0615, 0.0532, 0.0682, 0.1277,
        0.5717], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,729][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([1.1543e-03, 1.3892e-04, 7.6385e-01, 5.0831e-05, 1.0294e-05, 3.0082e-04,
        2.4375e-05, 1.0568e-05, 7.2196e-05, 6.9220e-06, 3.8416e-06, 4.0824e-06,
        1.0201e-05, 3.8675e-06, 1.3087e-05, 8.8635e-06, 1.7045e-06, 4.9600e-06,
        2.3433e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,730][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0676, 0.0889, 0.0385, 0.0114, 0.0668, 0.0136, 0.0330, 0.0695, 0.0307,
        0.0203, 0.0352, 0.0118, 0.0082, 0.1636, 0.0904, 0.0156, 0.0085, 0.1627,
        0.0636], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,731][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([9.0691e-03, 3.7558e-03, 6.0839e-04, 1.8742e-03, 2.5666e-04, 1.0801e-03,
        5.0950e-04, 1.1275e-03, 1.0385e-03, 4.7279e-03, 1.8680e-03, 1.5749e-02,
        2.6926e-02, 2.4409e-02, 5.1263e-02, 8.4124e-02, 1.2668e-01, 5.3284e-01,
        1.1209e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,734][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0851, 0.0143, 0.0300, 0.0674, 0.0109, 0.0310, 0.0283, 0.0095, 0.0213,
        0.0386, 0.0139, 0.1289, 0.1323, 0.0518, 0.0396, 0.0916, 0.1271, 0.0223,
        0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,738][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0991, 0.0522, 0.0332, 0.0582, 0.0390, 0.0408, 0.0343, 0.0273, 0.0370,
        0.0506, 0.0297, 0.0696, 0.0612, 0.0528, 0.0581, 0.0660, 0.0679, 0.0759,
        0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,742][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0695, 0.0341, 0.1415, 0.0433, 0.0249, 0.0305, 0.0201, 0.0267, 0.0292,
        0.0339, 0.0194, 0.0466, 0.0480, 0.0311, 0.0445, 0.0497, 0.0490, 0.0425,
        0.2154], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,747][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.2022, 0.0372, 0.0734, 0.0854, 0.0479, 0.0194, 0.0431, 0.0540, 0.0296,
        0.0334, 0.0320, 0.0423, 0.0281, 0.0507, 0.0406, 0.0761, 0.0222, 0.0268,
        0.0556], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:33,752][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.1201, 0.0282, 0.0566, 0.0099, 0.0563, 0.0200, 0.0666, 0.0289, 0.0568,
        0.0469, 0.0375, 0.0365, 0.0325, 0.0520, 0.1045, 0.0171, 0.0369, 0.0521,
        0.1219, 0.0185], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,754][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([2.1836e-04, 5.5391e-05, 8.7528e-05, 3.9130e-01, 3.3238e-05, 1.0597e-04,
        2.9313e-05, 1.2519e-05, 6.7839e-05, 4.8087e-05, 1.9735e-05, 3.2532e-05,
        6.1825e-05, 6.5534e-05, 5.6675e-06, 1.0731e-02, 7.1616e-05, 1.6761e-05,
        4.0279e-05, 5.9700e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,759][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.1037, 0.0238, 0.0720, 0.0683, 0.0333, 0.0274, 0.0276, 0.0245, 0.0253,
        0.0271, 0.0270, 0.0560, 0.0218, 0.0411, 0.0269, 0.1584, 0.0244, 0.0250,
        0.0953, 0.0910], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,760][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([4.1651e-04, 7.4853e-07, 6.0601e-05, 6.6888e-05, 3.0540e-05, 2.2588e-06,
        9.8370e-05, 6.3545e-06, 4.4755e-05, 3.3930e-05, 1.8076e-04, 1.2376e-04,
        6.6982e-04, 1.6928e-03, 6.6586e-04, 1.0238e-02, 5.6471e-03, 2.4176e-04,
        2.7728e-01, 7.0250e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,761][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([1.3002e-02, 1.4241e-03, 8.2150e-03, 1.9827e-03, 9.0295e-04, 3.2395e-04,
        1.6464e-03, 9.4614e-04, 1.8208e-03, 2.7023e-03, 3.0269e-03, 7.2747e-03,
        5.1783e-03, 1.5578e-02, 4.5208e-02, 3.1482e-02, 1.7545e-02, 3.7088e-02,
        6.1665e-01, 1.8801e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,762][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([1.7326e-02, 5.8039e-03, 8.8489e-03, 6.5894e-01, 1.2839e-03, 3.9386e-03,
        4.2965e-04, 5.8223e-04, 5.0394e-03, 1.0700e-03, 5.0733e-04, 1.7764e-03,
        3.4111e-03, 1.1116e-03, 4.0424e-04, 7.9971e-03, 1.7502e-03, 7.3140e-04,
        1.5598e-03, 2.7748e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,763][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0464, 0.0584, 0.0439, 0.0096, 0.0636, 0.0114, 0.0692, 0.0829, 0.0481,
        0.0301, 0.0386, 0.0171, 0.0087, 0.1266, 0.0861, 0.0170, 0.0075, 0.1220,
        0.0957, 0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,766][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0167, 0.0011, 0.0020, 0.0015, 0.0026, 0.0018, 0.0048, 0.0013, 0.0054,
        0.0071, 0.0092, 0.0156, 0.0194, 0.0240, 0.0734, 0.0598, 0.1209, 0.0917,
        0.2678, 0.2738], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,770][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0284, 0.0108, 0.0138, 0.0373, 0.0047, 0.0237, 0.0046, 0.0042, 0.0140,
        0.0269, 0.0058, 0.1317, 0.1915, 0.0233, 0.0210, 0.0786, 0.2188, 0.0296,
        0.0353, 0.0960], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,774][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.1140, 0.0435, 0.0362, 0.0588, 0.0395, 0.0418, 0.0220, 0.0272, 0.0331,
        0.0425, 0.0284, 0.0549, 0.0560, 0.0425, 0.0478, 0.0685, 0.0672, 0.0507,
        0.0448, 0.0806], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,779][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0871, 0.0298, 0.0320, 0.1464, 0.0234, 0.0449, 0.0169, 0.0133, 0.0287,
        0.0430, 0.0142, 0.0422, 0.0564, 0.0291, 0.0205, 0.0717, 0.0566, 0.0308,
        0.0324, 0.1806], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,784][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.1612, 0.0445, 0.0570, 0.0756, 0.0567, 0.0170, 0.0328, 0.0625, 0.0234,
        0.0229, 0.0263, 0.0659, 0.0342, 0.0466, 0.0385, 0.0682, 0.0257, 0.0352,
        0.0442, 0.0617], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:33,788][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1612, 0.0298, 0.0489, 0.0098, 0.0539, 0.0160, 0.0474, 0.0765, 0.0869,
        0.0374, 0.0819, 0.0299, 0.0138, 0.0398, 0.0599, 0.0115, 0.0172, 0.0462,
        0.0949, 0.0164, 0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,791][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([2.3972e-02, 2.2607e-05, 9.0531e-05, 2.9786e-03, 3.1779e-04, 3.5875e-04,
        4.3726e-05, 6.8169e-06, 2.1299e-04, 1.3648e-03, 1.4931e-05, 3.3854e-04,
        1.1295e-02, 3.6637e-05, 3.2227e-04, 1.4624e-02, 4.4078e-01, 7.8729e-06,
        6.0073e-05, 3.3100e-03, 4.9984e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,792][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1001, 0.0247, 0.0439, 0.0830, 0.0468, 0.0297, 0.0202, 0.0237, 0.0306,
        0.0279, 0.0452, 0.0701, 0.0184, 0.0302, 0.0254, 0.1171, 0.0195, 0.0309,
        0.0666, 0.1246, 0.0215], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,793][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([3.7427e-03, 2.9707e-05, 8.8226e-05, 5.1500e-05, 3.0302e-05, 3.5174e-05,
        5.0346e-05, 1.1751e-04, 2.7242e-04, 3.4287e-04, 2.0837e-04, 1.6493e-03,
        3.1988e-03, 4.1740e-03, 9.7471e-03, 1.7357e-02, 4.7455e-02, 9.2345e-03,
        1.3454e-01, 1.6348e-01, 6.0419e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,794][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0293, 0.0038, 0.0087, 0.0045, 0.0042, 0.0015, 0.0043, 0.0042, 0.0032,
        0.0074, 0.0044, 0.0196, 0.0115, 0.0271, 0.0310, 0.0443, 0.0455, 0.0563,
        0.2978, 0.2151, 0.1762], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,796][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1192, 0.0175, 0.0427, 0.0731, 0.0051, 0.0289, 0.0040, 0.0054, 0.0129,
        0.0300, 0.0028, 0.0807, 0.1002, 0.0271, 0.0563, 0.0399, 0.1676, 0.0026,
        0.0098, 0.0246, 0.1498], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,799][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0332, 0.0642, 0.0288, 0.0050, 0.1055, 0.0116, 0.0488, 0.1117, 0.0353,
        0.0165, 0.0599, 0.0065, 0.0021, 0.1733, 0.0609, 0.0070, 0.0012, 0.1506,
        0.0670, 0.0093, 0.0015], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,802][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0138, 0.0009, 0.0016, 0.0018, 0.0012, 0.0016, 0.0013, 0.0030, 0.0022,
        0.0053, 0.0052, 0.0099, 0.0112, 0.0155, 0.0282, 0.0474, 0.0555, 0.0514,
        0.1618, 0.2677, 0.3137], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,807][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0137, 0.0028, 0.0063, 0.0163, 0.0029, 0.0169, 0.0027, 0.0034, 0.0078,
        0.0165, 0.0027, 0.1382, 0.1709, 0.0072, 0.0086, 0.0458, 0.2015, 0.0093,
        0.0198, 0.0494, 0.2573], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,812][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1184, 0.0308, 0.0344, 0.0580, 0.0400, 0.0402, 0.0218, 0.0234, 0.0254,
        0.0449, 0.0293, 0.0522, 0.0504, 0.0437, 0.0409, 0.0628, 0.0594, 0.0343,
        0.0426, 0.0797, 0.0672], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,816][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1439, 0.0226, 0.0267, 0.0462, 0.0350, 0.0395, 0.0197, 0.0155, 0.0220,
        0.0496, 0.0185, 0.0434, 0.0709, 0.0301, 0.0331, 0.0495, 0.1100, 0.0236,
        0.0296, 0.0554, 0.1153], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,821][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1339, 0.0398, 0.0454, 0.0520, 0.0486, 0.0166, 0.0314, 0.0687, 0.0223,
        0.0247, 0.0258, 0.0430, 0.0553, 0.0473, 0.0585, 0.0460, 0.0565, 0.0407,
        0.0403, 0.0457, 0.0574], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:33,825][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:33,826][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[19124],
        [ 6144],
        [ 5447],
        [ 3534],
        [ 5787],
        [20044],
        [40088],
        [14716],
        [ 8623],
        [ 9260],
        [17691],
        [14634],
        [ 7307],
        [22009],
        [ 6366],
        [ 3074],
        [ 3563],
        [ 2515],
        [ 2498],
        [ 2030],
        [ 3153]], device='cuda:0')
[2024-07-23 21:06:33,828][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[43406],
        [10528],
        [33256],
        [27864],
        [  473],
        [29398],
        [41107],
        [ 7070],
        [14921],
        [35395],
        [27646],
        [40616],
        [28188],
        [40203],
        [22939],
        [31015],
        [28242],
        [ 3412],
        [33339],
        [28521],
        [26744]], device='cuda:0')
[2024-07-23 21:06:33,830][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[  637],
        [  476],
        [  250],
        [ 1793],
        [10275],
        [ 3317],
        [ 6948],
        [ 6733],
        [ 3906],
        [ 4568],
        [ 5635],
        [ 3906],
        [ 3045],
        [ 6434],
        [ 5668],
        [ 6370],
        [ 6798],
        [ 6007],
        [ 3838],
        [ 8459],
        [ 6873]], device='cuda:0')
[2024-07-23 21:06:33,833][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[10079],
        [27589],
        [39936],
        [ 1061],
        [16429],
        [42595],
        [30935],
        [37472],
        [43325],
        [32449],
        [24546],
        [39528],
        [21669],
        [20170],
        [39328],
        [ 1914],
        [16194],
        [21544],
        [39984],
        [ 1003],
        [16340]], device='cuda:0')
[2024-07-23 21:06:33,836][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[32482],
        [32118],
        [32784],
        [32115],
        [30810],
        [29601],
        [29164],
        [28790],
        [25899],
        [29240],
        [26083],
        [30136],
        [28798],
        [27305],
        [27493],
        [26248],
        [26028],
        [25492],
        [24414],
        [25696],
        [25509]], device='cuda:0')
[2024-07-23 21:06:33,839][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[28158],
        [10071],
        [15753],
        [30615],
        [ 3059],
        [29280],
        [31212],
        [ 8515],
        [38526],
        [30077],
        [  474],
        [ 8178],
        [28831],
        [15481],
        [15184],
        [13392],
        [26588],
        [13704],
        [16251],
        [35473],
        [35148]], device='cuda:0')
[2024-07-23 21:06:33,842][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[  327],
        [  703],
        [  311],
        [  243],
        [18566],
        [ 5416],
        [ 6333],
        [10798],
        [27511],
        [ 7631],
        [21956],
        [ 3309],
        [ 3096],
        [ 9811],
        [ 2042],
        [ 2109],
        [ 3038],
        [ 7654],
        [  337],
        [  504],
        [ 1360]], device='cuda:0')
[2024-07-23 21:06:33,846][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[21221],
        [41159],
        [33732],
        [11577],
        [44057],
        [28413],
        [43024],
        [38723],
        [18010],
        [12228],
        [30114],
        [14058],
        [14053],
        [29392],
        [25622],
        [14627],
        [16157],
        [40486],
        [33389],
        [ 9312],
        [14525]], device='cuda:0')
[2024-07-23 21:06:33,849][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[44904],
        [40627],
        [37164],
        [41310],
        [26266],
        [10970],
        [43022],
        [ 8381],
        [ 2055],
        [ 7846],
        [ 5353],
        [ 7828],
        [ 6840],
        [16609],
        [22103],
        [41552],
        [ 6842],
        [ 9656],
        [16740],
        [21757],
        [ 6368]], device='cuda:0')
[2024-07-23 21:06:33,852][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[41908],
        [40126],
        [  948],
        [19400],
        [23604],
        [33452],
        [47736],
        [28936],
        [38779],
        [34149],
        [31026],
        [ 6749],
        [11496],
        [17053],
        [18420],
        [23727],
        [24770],
        [13784],
        [  420],
        [11591],
        [17492]], device='cuda:0')
[2024-07-23 21:06:33,855][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[25426],
        [26296],
        [27997],
        [ 7490],
        [23107],
        [ 8066],
        [13761],
        [29500],
        [21328],
        [19693],
        [31814],
        [ 4130],
        [ 1962],
        [ 5510],
        [ 4556],
        [ 2303],
        [ 1442],
        [ 9686],
        [ 3709],
        [ 1278],
        [ 1201]], device='cuda:0')
[2024-07-23 21:06:33,858][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[23850],
        [21338],
        [22144],
        [31741],
        [28715],
        [21220],
        [20475],
        [13710],
        [15711],
        [13736],
        [11284],
        [22114],
        [19942],
        [17406],
        [19019],
        [23864],
        [21740],
        [20481],
        [18245],
        [23559],
        [23211]], device='cuda:0')
[2024-07-23 21:06:33,861][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[23538],
        [32043],
        [ 5071],
        [ 1663],
        [27201],
        [ 7112],
        [ 7320],
        [17613],
        [10556],
        [13982],
        [25739],
        [ 3644],
        [10306],
        [ 9493],
        [12117],
        [  822],
        [11303],
        [20335],
        [  985],
        [  767],
        [10488]], device='cuda:0')
[2024-07-23 21:06:33,863][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[17508],
        [19567],
        [19867],
        [19597],
        [22356],
        [23546],
        [21691],
        [26788],
        [26515],
        [27698],
        [27779],
        [28244],
        [31822],
        [28223],
        [29150],
        [27876],
        [32527],
        [31124],
        [26331],
        [26348],
        [30228]], device='cuda:0')
[2024-07-23 21:06:33,865][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[18265],
        [ 6513],
        [17917],
        [ 8348],
        [ 9660],
        [44124],
        [36770],
        [35633],
        [30809],
        [28102],
        [39061],
        [44888],
        [18634],
        [28915],
        [ 9328],
        [10360],
        [ 8346],
        [ 5535],
        [17798],
        [ 8168],
        [ 8211]], device='cuda:0')
[2024-07-23 21:06:33,867][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[3032],
        [2687],
        [1757],
        [1499],
        [4060],
        [2590],
        [3347],
        [3807],
        [2730],
        [2388],
        [3275],
        [2447],
        [3295],
        [2530],
        [2268],
        [3778],
        [2931],
        [2744],
        [2512],
        [2446],
        [2342]], device='cuda:0')
[2024-07-23 21:06:33,869][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[16986],
        [15796],
        [ 9625],
        [13953],
        [19631],
        [ 6700],
        [10623],
        [11286],
        [10522],
        [ 8896],
        [14729],
        [ 7295],
        [10241],
        [14399],
        [13496],
        [18646],
        [13393],
        [12881],
        [ 7402],
        [14755],
        [12760]], device='cuda:0')
[2024-07-23 21:06:33,873][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[36871],
        [35640],
        [33207],
        [34743],
        [36723],
        [35295],
        [36127],
        [37189],
        [32071],
        [36091],
        [36161],
        [36731],
        [37424],
        [36579],
        [32832],
        [34955],
        [35594],
        [34572],
        [33478],
        [33888],
        [34984]], device='cuda:0')
[2024-07-23 21:06:33,876][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[17947],
        [16633],
        [19549],
        [22822],
        [ 7568],
        [22411],
        [12154],
        [18951],
        [13882],
        [23423],
        [37920],
        [32894],
        [26950],
        [15248],
        [14012],
        [18287],
        [27705],
        [23708],
        [25849],
        [29704],
        [27257]], device='cuda:0')
[2024-07-23 21:06:33,879][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[30603],
        [27926],
        [28694],
        [28750],
        [32290],
        [22505],
        [35963],
        [39735],
        [28158],
        [26526],
        [46742],
        [21745],
        [25464],
        [20987],
        [30962],
        [29775],
        [26357],
        [22607],
        [24997],
        [24277],
        [25921]], device='cuda:0')
[2024-07-23 21:06:33,882][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[19584],
        [11956],
        [14845],
        [31922],
        [ 9767],
        [19580],
        [30563],
        [18787],
        [26206],
        [23922],
        [14664],
        [22462],
        [21601],
        [18109],
        [15459],
        [30004],
        [20034],
        [11094],
        [14850],
        [32005],
        [21100]], device='cuda:0')
[2024-07-23 21:06:33,885][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[47185],
        [46610],
        [45361],
        [45467],
        [33643],
        [18098],
        [37054],
        [12366],
        [ 4647],
        [11539],
        [ 8124],
        [14743],
        [13391],
        [22483],
        [22484],
        [36745],
        [12302],
        [16078],
        [21339],
        [23263],
        [11639]], device='cuda:0')
[2024-07-23 21:06:33,889][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[48501],
        [48399],
        [11216],
        [45477],
        [46979],
        [46268],
        [46186],
        [27635],
        [36138],
        [32145],
        [31864],
        [42397],
        [40646],
        [38430],
        [39482],
        [42652],
        [45218],
        [44498],
        [ 8995],
        [40387],
        [44275]], device='cuda:0')
[2024-07-23 21:06:33,892][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[25028],
        [25372],
        [28404],
        [25699],
        [26078],
        [28850],
        [27398],
        [27119],
        [27892],
        [24595],
        [27154],
        [16644],
        [23891],
        [20745],
        [19802],
        [20079],
        [22827],
        [19537],
        [19108],
        [20828],
        [23480]], device='cuda:0')
[2024-07-23 21:06:33,895][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[32541],
        [31730],
        [30834],
        [33507],
        [33394],
        [33909],
        [33358],
        [33004],
        [33842],
        [33962],
        [33594],
        [33971],
        [33615],
        [33670],
        [33910],
        [35264],
        [34802],
        [35136],
        [34822],
        [35535],
        [35160]], device='cuda:0')
[2024-07-23 21:06:33,898][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[17415],
        [19721],
        [26668],
        [22971],
        [19829],
        [19968],
        [22628],
        [25385],
        [21481],
        [19862],
        [21235],
        [19635],
        [19210],
        [19448],
        [21518],
        [23914],
        [18618],
        [23591],
        [28004],
        [23058],
        [18542]], device='cuda:0')
[2024-07-23 21:06:33,900][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[ 8990],
        [ 8249],
        [12009],
        [31871],
        [44274],
        [44570],
        [43603],
        [42794],
        [39267],
        [42834],
        [42706],
        [43927],
        [44149],
        [40135],
        [41406],
        [44287],
        [41244],
        [43548],
        [44553],
        [45907],
        [42730]], device='cuda:0')
[2024-07-23 21:06:33,901][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 4887],
        [ 6998],
        [ 9548],
        [ 5679],
        [ 7153],
        [ 8804],
        [ 6885],
        [ 9683],
        [12111],
        [11948],
        [10181],
        [10707],
        [ 9616],
        [11156],
        [10318],
        [ 5647],
        [ 9054],
        [ 8315],
        [10414],
        [ 6582],
        [ 8994]], device='cuda:0')
[2024-07-23 21:06:33,903][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[13282],
        [31816],
        [18242],
        [26831],
        [27090],
        [ 2157],
        [ 6256],
        [ 5839],
        [ 8989],
        [10137],
        [ 4210],
        [ 1393],
        [17096],
        [ 9867],
        [28632],
        [25588],
        [28994],
        [34778],
        [18679],
        [28048],
        [29377]], device='cuda:0')
[2024-07-23 21:06:33,906][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074],
        [27074]], device='cuda:0')
[2024-07-23 21:06:33,924][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:33,929][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,932][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,936][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,940][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,941][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,942][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,942][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,943][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,945][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,947][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,951][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,954][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:33,959][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5013, 0.4987], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,964][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.2373, 0.7627], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,968][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.3881, 0.6119], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,973][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.6042, 0.3958], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,973][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.7852, 0.2148], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,974][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.6647, 0.3353], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,975][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9977, 0.0023], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,976][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.9849, 0.0151], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,978][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9901, 0.0099], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,981][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.9782, 0.0218], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,986][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.3035, 0.6965], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,991][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9480, 0.0520], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:33,995][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.3344, 0.3328, 0.3328], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,000][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0408, 0.3208, 0.6383], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,005][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.1857, 0.4903, 0.3240], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,006][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.5444, 0.2158, 0.2397], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,006][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.7112, 0.1425, 0.1463], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,007][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.6355, 0.2006, 0.1639], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,008][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.9537, 0.0448, 0.0015], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,010][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.7594, 0.1728, 0.0679], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,013][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.9374, 0.0166, 0.0459], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,016][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ used] are: tensor([3.5758e-03, 3.3523e-04, 9.9609e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,020][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.1771, 0.4108, 0.4121], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,025][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.8742, 0.0454, 0.0804], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,030][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.2509, 0.2496, 0.2496, 0.2499], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,034][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0293, 0.1992, 0.4799, 0.2916], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,038][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.1389, 0.3810, 0.2387, 0.2414], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,038][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.4134, 0.1709, 0.1874, 0.2283], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,039][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.5955, 0.1238, 0.1146, 0.1662], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,040][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0782, 0.0666, 0.0159, 0.8393], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,041][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.9500, 0.0424, 0.0064, 0.0012], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,043][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.5103, 0.1564, 0.2177, 0.1156], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,047][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.7864, 0.0216, 0.0541, 0.1380], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,050][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ by] are: tensor([4.4088e-03, 4.9225e-05, 5.8052e-01, 4.1502e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,054][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.1297, 0.3032, 0.3045, 0.2627], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,059][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.7182, 0.0702, 0.1125, 0.0991], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,064][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.2007, 0.1997, 0.1997, 0.2000, 0.1998], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,068][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0163, 0.1174, 0.2988, 0.4090, 0.1585], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,070][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0915, 0.3378, 0.2008, 0.1700, 0.1999], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,071][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.3674, 0.1544, 0.1727, 0.1817, 0.1238], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,072][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.6173, 0.0885, 0.0922, 0.1166, 0.0854], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,073][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.1824, 0.0598, 0.0314, 0.4888, 0.2377], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,073][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.7958, 0.1067, 0.0321, 0.0130, 0.0524], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,077][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.3836, 0.2499, 0.1505, 0.1311, 0.0849], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,081][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.7529, 0.0328, 0.0505, 0.1374, 0.0264], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,084][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([6.1092e-04, 7.0447e-05, 9.4327e-01, 5.1876e-02, 4.1699e-03],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,089][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.1002, 0.2345, 0.2342, 0.2039, 0.2273], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,094][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.8120, 0.0486, 0.0706, 0.0596, 0.0093], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,098][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1673, 0.1664, 0.1665, 0.1666, 0.1665, 0.1667], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,103][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0129, 0.0801, 0.2718, 0.3023, 0.2371, 0.0958], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,103][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0862, 0.2509, 0.1650, 0.1581, 0.1818, 0.1580], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,104][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.3144, 0.1359, 0.1423, 0.1465, 0.1094, 0.1515], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,105][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.4429, 0.0898, 0.0924, 0.1337, 0.1110, 0.1302], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,106][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.0895, 0.0244, 0.0103, 0.8322, 0.0162, 0.0273], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,109][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.7650, 0.0289, 0.0099, 0.0047, 0.1871, 0.0043], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,113][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.3000, 0.1704, 0.2022, 0.1362, 0.1572, 0.0340], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,118][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.7490, 0.0204, 0.0359, 0.0879, 0.0460, 0.0608], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,121][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ B] are: tensor([3.7598e-04, 3.0804e-05, 9.5279e-01, 3.9334e-02, 7.2437e-03, 2.2770e-04],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,126][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0804, 0.1902, 0.1900, 0.1646, 0.1856, 0.1892], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,130][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.7336, 0.0459, 0.0707, 0.0599, 0.0094, 0.0804], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,135][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.1434, 0.1427, 0.1427, 0.1429, 0.1427, 0.1429, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,136][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0085, 0.0638, 0.2380, 0.2418, 0.1939, 0.1627, 0.0914],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,136][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0646, 0.2422, 0.1454, 0.1279, 0.1524, 0.1108, 0.1567],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,137][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.2782, 0.1242, 0.1350, 0.1414, 0.0998, 0.1274, 0.0939],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,138][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.4139, 0.0800, 0.0766, 0.1071, 0.0907, 0.1090, 0.1226],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,141][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.1969, 0.0757, 0.0324, 0.5287, 0.0602, 0.0835, 0.0227],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,146][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.6851, 0.0063, 0.0034, 0.0101, 0.2526, 0.0216, 0.0208],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,151][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.2615, 0.2156, 0.0825, 0.2568, 0.1396, 0.0318, 0.0121],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,155][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.6354, 0.0158, 0.0297, 0.1120, 0.0481, 0.0991, 0.0599],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,158][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [aut] are: tensor([8.2637e-04, 9.3959e-05, 9.3702e-01, 5.1059e-02, 9.2076e-03, 6.8611e-04,
        1.1027e-03], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,162][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0676, 0.1585, 0.1585, 0.1380, 0.1542, 0.1575, 0.1657],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,167][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.7351, 0.0445, 0.0701, 0.0553, 0.0084, 0.0684, 0.0182],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,168][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.1254, 0.1248, 0.1248, 0.1250, 0.1249, 0.1250, 0.1249, 0.1252],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,168][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0050, 0.0653, 0.2268, 0.2193, 0.1744, 0.1342, 0.1175, 0.0574],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,169][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0645, 0.2032, 0.1228, 0.1142, 0.1453, 0.1160, 0.1363, 0.0977],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,171][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.2614, 0.0953, 0.1195, 0.1108, 0.0864, 0.1169, 0.1045, 0.1051],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,174][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.4428, 0.0516, 0.0591, 0.0824, 0.0804, 0.0857, 0.1015, 0.0964],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,179][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.1634, 0.1022, 0.0318, 0.3277, 0.1248, 0.0498, 0.0550, 0.1453],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,183][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.3277, 0.0185, 0.0116, 0.0819, 0.1782, 0.3132, 0.0564, 0.0126],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,188][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.2920, 0.0889, 0.0333, 0.1107, 0.1953, 0.0217, 0.1421, 0.1161],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,193][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.5602, 0.0356, 0.0622, 0.1080, 0.0464, 0.0850, 0.0538, 0.0488],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,196][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ista] are: tensor([7.2834e-03, 4.2775e-05, 5.5722e-01, 4.2444e-01, 7.1859e-03, 1.8925e-03,
        1.9150e-03, 1.5843e-05], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,199][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0593, 0.1328, 0.1324, 0.1145, 0.1298, 0.1320, 0.1391, 0.1601],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,200][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.7400, 0.0424, 0.0628, 0.0554, 0.0083, 0.0633, 0.0168, 0.0110],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,200][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.1115, 0.1109, 0.1110, 0.1111, 0.1110, 0.1111, 0.1110, 0.1113, 0.1111],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,201][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0076, 0.0597, 0.1681, 0.2018, 0.1372, 0.0857, 0.1146, 0.0688, 0.1565],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,203][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0576, 0.1863, 0.1134, 0.1065, 0.1144, 0.1107, 0.1258, 0.0912, 0.0940],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,206][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.2308, 0.1013, 0.1105, 0.1028, 0.0827, 0.1099, 0.0810, 0.0985, 0.0825],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,210][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.3157, 0.0547, 0.0579, 0.0915, 0.0781, 0.0912, 0.1150, 0.0766, 0.1192],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,214][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.1219, 0.0879, 0.0578, 0.0504, 0.0508, 0.0289, 0.0135, 0.1010, 0.4879],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,219][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.1660, 0.0029, 0.0041, 0.0018, 0.3204, 0.0052, 0.0017, 0.4259, 0.0720],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,224][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.1246, 0.0755, 0.0973, 0.0695, 0.2921, 0.0176, 0.0390, 0.1961, 0.0883],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,228][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.5515, 0.0161, 0.0243, 0.1179, 0.0388, 0.0698, 0.0595, 0.0457, 0.0764],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,231][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ de] are: tensor([6.6945e-04, 1.1536e-04, 7.5093e-01, 2.2495e-01, 1.2442e-02, 1.8797e-03,
        1.3584e-03, 8.9126e-05, 7.5605e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,232][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0487, 0.1153, 0.1147, 0.0996, 0.1129, 0.1148, 0.1219, 0.1394, 0.1328],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,232][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.6857, 0.0449, 0.0680, 0.0608, 0.0092, 0.0764, 0.0193, 0.0105, 0.0252],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,233][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.1004, 0.0999, 0.0999, 0.1000, 0.0999, 0.1000, 0.0999, 0.1002, 0.1000,
        0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,235][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0042, 0.0327, 0.0960, 0.1526, 0.1041, 0.0603, 0.0642, 0.0542, 0.2797,
        0.1520], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,238][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0554, 0.1830, 0.1054, 0.0970, 0.1174, 0.0965, 0.1154, 0.0849, 0.0909,
        0.0541], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,243][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.2163, 0.0778, 0.0885, 0.0883, 0.0685, 0.0910, 0.0727, 0.0876, 0.0724,
        0.1370], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,247][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.2448, 0.0471, 0.0501, 0.0719, 0.0662, 0.0769, 0.0865, 0.0732, 0.1276,
        0.1558], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,252][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0806, 0.0939, 0.0266, 0.1261, 0.0081, 0.5323, 0.0239, 0.0225, 0.0246,
        0.0615], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,257][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.2902, 0.0213, 0.0078, 0.0133, 0.0598, 0.0112, 0.0343, 0.4561, 0.0832,
        0.0228], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,261][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.1623, 0.1816, 0.1065, 0.1099, 0.0784, 0.0279, 0.0265, 0.1256, 0.1115,
        0.0698], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,263][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.5248, 0.0332, 0.0327, 0.0797, 0.0322, 0.0526, 0.0526, 0.0420, 0.0769,
        0.0733], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,264][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ An] are: tensor([6.0448e-04, 6.4009e-05, 8.7805e-01, 1.0688e-01, 9.0919e-03, 1.4951e-03,
        4.2386e-04, 2.2901e-05, 2.1602e-03, 1.2086e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,264][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0439, 0.1032, 0.1029, 0.0889, 0.1009, 0.1026, 0.1090, 0.1248, 0.1189,
        0.1049], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,265][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.6387, 0.0434, 0.0652, 0.0589, 0.0094, 0.0713, 0.0190, 0.0118, 0.0244,
        0.0580], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,268][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0912, 0.0908, 0.0908, 0.0909, 0.0908, 0.0909, 0.0909, 0.0911, 0.0909,
        0.0908, 0.0909], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,271][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0028, 0.0380, 0.0962, 0.1277, 0.0951, 0.0639, 0.0521, 0.0347, 0.2204,
        0.2375, 0.0315], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,275][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.0456, 0.1555, 0.0963, 0.0976, 0.1087, 0.0915, 0.1043, 0.0732, 0.0870,
        0.0515, 0.0889], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,279][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.1737, 0.0736, 0.0959, 0.0872, 0.0621, 0.0779, 0.0722, 0.0807, 0.0642,
        0.1428, 0.0698], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,284][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.2891, 0.0306, 0.0357, 0.0525, 0.0448, 0.0561, 0.0749, 0.0572, 0.0924,
        0.1574, 0.1094], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,289][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.0700, 0.0320, 0.0195, 0.2288, 0.1520, 0.0168, 0.0597, 0.1460, 0.1767,
        0.0393, 0.0592], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,293][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.1958, 0.2905, 0.0131, 0.0446, 0.1370, 0.0817, 0.0896, 0.0028, 0.0880,
        0.0428, 0.0142], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,295][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.1739, 0.0638, 0.0142, 0.0548, 0.1642, 0.0282, 0.0860, 0.2487, 0.0687,
        0.0782, 0.0193], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,296][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.4305, 0.0164, 0.0286, 0.0924, 0.0354, 0.0546, 0.0386, 0.0480, 0.0794,
        0.1393, 0.0367], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,297][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [za] are: tensor([6.6511e-04, 5.5892e-05, 8.8144e-01, 1.1169e-01, 2.5984e-03, 1.3897e-04,
        6.3452e-04, 2.4367e-05, 1.1591e-03, 6.7358e-04, 9.2580e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,297][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0406, 0.0924, 0.0919, 0.0795, 0.0905, 0.0918, 0.0970, 0.1115, 0.1062,
        0.0938, 0.1047], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,300][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.6577, 0.0442, 0.0627, 0.0559, 0.0086, 0.0658, 0.0174, 0.0096, 0.0207,
        0.0535, 0.0039], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:34,303][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0836, 0.0832, 0.0832, 0.0833, 0.0832, 0.0833, 0.0833, 0.0835, 0.0833,
        0.0832, 0.0833, 0.0834], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,307][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0093, 0.0359, 0.1056, 0.0966, 0.0815, 0.0460, 0.0697, 0.0517, 0.1915,
        0.1982, 0.0601, 0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,311][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0336, 0.1261, 0.0988, 0.0842, 0.0961, 0.0730, 0.0915, 0.0632, 0.0737,
        0.0460, 0.0783, 0.1355], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,316][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.1504, 0.0556, 0.0895, 0.0898, 0.0613, 0.0761, 0.0646, 0.0806, 0.0628,
        0.1282, 0.0695, 0.0717], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,321][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1995, 0.0393, 0.0473, 0.0554, 0.0510, 0.0530, 0.0581, 0.0460, 0.0872,
        0.1243, 0.0793, 0.1597], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,325][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1421, 0.0630, 0.0584, 0.0870, 0.0251, 0.0219, 0.0202, 0.0759, 0.0635,
        0.0654, 0.0519, 0.3256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,330][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.4716, 0.0069, 0.0028, 0.0016, 0.0448, 0.0057, 0.0036, 0.1708, 0.0332,
        0.0505, 0.2047, 0.0038], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,330][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0540, 0.1094, 0.2202, 0.0332, 0.1020, 0.0147, 0.0141, 0.0647, 0.0743,
        0.0752, 0.0310, 0.2071], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,331][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.4441, 0.0483, 0.0422, 0.0814, 0.0423, 0.0525, 0.0444, 0.0285, 0.0685,
        0.0630, 0.0265, 0.0585], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,332][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ is] are: tensor([6.4060e-04, 3.5791e-06, 9.1302e-01, 8.0673e-02, 1.5283e-03, 4.9010e-05,
        2.0595e-04, 6.6889e-06, 1.2857e-03, 1.0056e-03, 1.5484e-03, 2.9106e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,335][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0367, 0.0854, 0.0855, 0.0737, 0.0840, 0.0851, 0.0903, 0.1044, 0.0990,
        0.0874, 0.0978, 0.0707], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,338][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.5050, 0.0396, 0.0615, 0.0553, 0.0100, 0.0711, 0.0193, 0.0140, 0.0275,
        0.0565, 0.0061, 0.1340], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:34,342][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0772, 0.0768, 0.0768, 0.0769, 0.0768, 0.0769, 0.0769, 0.0771, 0.0769,
        0.0768, 0.0769, 0.0770, 0.0769], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,346][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0057, 0.0217, 0.0859, 0.1102, 0.0815, 0.0379, 0.0615, 0.0413, 0.1925,
        0.1403, 0.0412, 0.0957, 0.0848], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,351][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0367, 0.1428, 0.0779, 0.0730, 0.0924, 0.0650, 0.0907, 0.0672, 0.0656,
        0.0415, 0.0827, 0.0986, 0.0659], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,356][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1595, 0.0570, 0.0710, 0.0743, 0.0539, 0.0724, 0.0579, 0.0728, 0.0579,
        0.1112, 0.0706, 0.0562, 0.0853], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,360][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1637, 0.0349, 0.0336, 0.0488, 0.0425, 0.0496, 0.0518, 0.0414, 0.0830,
        0.1147, 0.0684, 0.1346, 0.1330], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,362][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0755, 0.0684, 0.0360, 0.1005, 0.0150, 0.3336, 0.0213, 0.0251, 0.1247,
        0.0352, 0.0271, 0.0744, 0.0633], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,363][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.2446, 0.0015, 0.0011, 0.0007, 0.0398, 0.0026, 0.0023, 0.3071, 0.0347,
        0.0281, 0.3326, 0.0023, 0.0026], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,363][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0659, 0.0785, 0.1016, 0.0502, 0.1225, 0.0219, 0.0163, 0.0988, 0.0824,
        0.0770, 0.0321, 0.1021, 0.1509], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,364][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.4415, 0.0374, 0.0375, 0.1004, 0.0398, 0.0498, 0.0414, 0.0245, 0.0494,
        0.0598, 0.0184, 0.0438, 0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,366][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ a] are: tensor([1.1116e-03, 2.4921e-05, 8.2192e-01, 1.4132e-01, 4.2852e-03, 2.2093e-04,
        8.0202e-04, 3.4204e-05, 2.3436e-03, 1.8954e-03, 4.7915e-03, 3.7039e-04,
        2.0885e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,369][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0340, 0.0796, 0.0799, 0.0688, 0.0783, 0.0794, 0.0843, 0.0975, 0.0925,
        0.0816, 0.0912, 0.0658, 0.0669], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,370][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.4370, 0.0366, 0.0562, 0.0466, 0.0085, 0.0596, 0.0169, 0.0117, 0.0214,
        0.0474, 0.0048, 0.1089, 0.1443], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:34,373][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0717, 0.0713, 0.0713, 0.0714, 0.0714, 0.0714, 0.0714, 0.0716, 0.0714,
        0.0714, 0.0714, 0.0715, 0.0714, 0.0714], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,378][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0025, 0.0248, 0.0821, 0.0750, 0.0507, 0.0399, 0.0497, 0.0282, 0.1594,
        0.1720, 0.0301, 0.0683, 0.1424, 0.0749], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,382][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0359, 0.1296, 0.0758, 0.0736, 0.0841, 0.0588, 0.0864, 0.0620, 0.0685,
        0.0357, 0.0802, 0.1089, 0.0430, 0.0575], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,387][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.1442, 0.0541, 0.0717, 0.0757, 0.0522, 0.0643, 0.0520, 0.0610, 0.0563,
        0.1012, 0.0726, 0.0586, 0.0628, 0.0733], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,392][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.1447, 0.0319, 0.0323, 0.0425, 0.0353, 0.0441, 0.0497, 0.0370, 0.0752,
        0.0947, 0.0648, 0.1206, 0.1164, 0.1106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,395][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.1329, 0.0522, 0.0209, 0.1110, 0.0122, 0.0113, 0.0214, 0.0293, 0.0216,
        0.0564, 0.0507, 0.1123, 0.2271, 0.1407], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,396][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.1991, 0.0296, 0.0043, 0.0071, 0.1065, 0.0175, 0.0066, 0.2781, 0.0563,
        0.0397, 0.1575, 0.0384, 0.0336, 0.0260], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,397][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.1044, 0.0865, 0.0401, 0.0246, 0.0811, 0.0175, 0.0388, 0.0754, 0.0512,
        0.0489, 0.0315, 0.1533, 0.1864, 0.0604], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,398][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.3923, 0.0354, 0.0362, 0.0866, 0.0246, 0.0521, 0.0400, 0.0214, 0.0625,
        0.0505, 0.0181, 0.0569, 0.0778, 0.0456], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,399][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([4.9559e-03, 8.1651e-05, 5.0385e-01, 3.3444e-01, 1.2537e-02, 7.3900e-04,
        2.4918e-03, 5.0469e-05, 1.1332e-02, 7.8043e-03, 5.8551e-03, 7.1914e-04,
        1.1513e-01, 1.5704e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,402][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0313, 0.0742, 0.0740, 0.0640, 0.0727, 0.0735, 0.0780, 0.0896, 0.0853,
        0.0751, 0.0841, 0.0606, 0.0616, 0.0760], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,406][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.4841, 0.0327, 0.0513, 0.0418, 0.0065, 0.0515, 0.0133, 0.0073, 0.0153,
        0.0361, 0.0028, 0.0982, 0.1236, 0.0355], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:34,411][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0669, 0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666, 0.0668, 0.0667,
        0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,416][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0025, 0.0174, 0.0824, 0.0719, 0.0460, 0.0299, 0.0389, 0.0253, 0.1470,
        0.1377, 0.0334, 0.0766, 0.1178, 0.1414, 0.0317], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,420][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0295, 0.1400, 0.0742, 0.0714, 0.0814, 0.0568, 0.0807, 0.0568, 0.0579,
        0.0298, 0.0658, 0.0891, 0.0403, 0.0637, 0.0627], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,425][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.1421, 0.0493, 0.0622, 0.0666, 0.0463, 0.0575, 0.0512, 0.0636, 0.0481,
        0.0894, 0.0681, 0.0499, 0.0597, 0.0764, 0.0697], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,427][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.1458, 0.0263, 0.0272, 0.0398, 0.0349, 0.0375, 0.0383, 0.0299, 0.0644,
        0.0874, 0.0513, 0.1091, 0.1030, 0.0982, 0.1068], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,428][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.1164, 0.0678, 0.0303, 0.0640, 0.0194, 0.0103, 0.0643, 0.0431, 0.0205,
        0.1014, 0.0222, 0.0639, 0.2038, 0.1462, 0.0263], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,429][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.4808, 0.0095, 0.0018, 0.0016, 0.1003, 0.0043, 0.0005, 0.2507, 0.0334,
        0.0223, 0.0545, 0.0035, 0.0042, 0.0256, 0.0067], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,430][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.1193, 0.0453, 0.0437, 0.0461, 0.1110, 0.0238, 0.0129, 0.1850, 0.0709,
        0.0539, 0.0327, 0.0602, 0.0672, 0.1121, 0.0160], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,432][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.3292, 0.0223, 0.0235, 0.0604, 0.0220, 0.0578, 0.0402, 0.0206, 0.0517,
        0.0731, 0.0165, 0.0684, 0.0805, 0.0459, 0.0877], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,434][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ different] are: tensor([5.1450e-03, 8.0556e-05, 1.7132e-01, 3.9707e-01, 1.7206e-02, 5.6256e-03,
        2.4823e-03, 6.6972e-05, 1.7182e-02, 1.0276e-02, 6.0470e-03, 1.3363e-03,
        3.6245e-01, 7.9067e-05, 3.6313e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,437][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0290, 0.0690, 0.0689, 0.0597, 0.0674, 0.0684, 0.0727, 0.0833, 0.0792,
        0.0701, 0.0782, 0.0566, 0.0576, 0.0712, 0.0688], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,442][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.4993, 0.0262, 0.0430, 0.0358, 0.0052, 0.0440, 0.0106, 0.0055, 0.0118,
        0.0287, 0.0020, 0.0867, 0.1087, 0.0278, 0.0646], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:34,446][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0627, 0.0624, 0.0624, 0.0625, 0.0624, 0.0625, 0.0625, 0.0626, 0.0625,
        0.0624, 0.0625, 0.0626, 0.0625, 0.0624, 0.0625, 0.0625],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,451][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0038, 0.0191, 0.0649, 0.0670, 0.0743, 0.0343, 0.0396, 0.0319, 0.1852,
        0.1228, 0.0261, 0.0544, 0.0985, 0.1023, 0.0366, 0.0393],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,456][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0349, 0.1145, 0.0672, 0.0638, 0.0761, 0.0514, 0.0736, 0.0568, 0.0549,
        0.0326, 0.0695, 0.0801, 0.0475, 0.0603, 0.0713, 0.0453],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,459][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.1317, 0.0445, 0.0579, 0.0723, 0.0450, 0.0557, 0.0417, 0.0571, 0.0449,
        0.0910, 0.0531, 0.0455, 0.0551, 0.0629, 0.0650, 0.0766],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,460][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.1146, 0.0254, 0.0232, 0.0326, 0.0310, 0.0348, 0.0346, 0.0300, 0.0598,
        0.0753, 0.0503, 0.0905, 0.0939, 0.0870, 0.1007, 0.1164],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,461][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0098, 0.0025, 0.0062, 0.0206, 0.0014, 0.0020, 0.0014, 0.0016, 0.0012,
        0.0051, 0.0010, 0.0078, 0.0257, 0.0058, 0.0146, 0.8931],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,462][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ from] are: tensor([1.8730e-01, 7.3069e-03, 6.6432e-04, 4.6998e-04, 2.3112e-02, 1.9669e-03,
        9.9438e-04, 2.9192e-01, 5.8215e-03, 8.6098e-03, 4.4196e-01, 2.5394e-03,
        3.9536e-03, 2.0891e-02, 2.2130e-03, 2.7353e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,464][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0809, 0.0681, 0.0810, 0.0382, 0.0751, 0.0197, 0.0181, 0.0632, 0.0627,
        0.0422, 0.0199, 0.0911, 0.0981, 0.0946, 0.0815, 0.0656],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,467][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.3523, 0.0238, 0.0278, 0.0785, 0.0286, 0.0321, 0.0343, 0.0192, 0.0449,
        0.0433, 0.0168, 0.0350, 0.0382, 0.0429, 0.0945, 0.0877],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,470][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ from] are: tensor([5.7569e-04, 1.1159e-05, 6.8235e-01, 9.4992e-02, 4.0161e-03, 4.2827e-04,
        6.6772e-04, 5.4623e-05, 4.4566e-03, 2.3686e-03, 2.9202e-03, 2.4292e-04,
        4.1585e-02, 2.5808e-05, 2.0676e-03, 1.6324e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,474][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0267, 0.0647, 0.0649, 0.0559, 0.0635, 0.0643, 0.0683, 0.0791, 0.0749,
        0.0662, 0.0740, 0.0535, 0.0546, 0.0673, 0.0653, 0.0568],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,479][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.3300, 0.0281, 0.0461, 0.0425, 0.0074, 0.0493, 0.0145, 0.0099, 0.0194,
        0.0401, 0.0044, 0.0887, 0.1110, 0.0348, 0.0680, 0.1057],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:34,483][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0591, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0589, 0.0588,
        0.0588, 0.0588, 0.0589, 0.0588, 0.0588, 0.0588, 0.0588, 0.0587],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,488][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0030, 0.0183, 0.0605, 0.0724, 0.0495, 0.0325, 0.0410, 0.0253, 0.1243,
        0.1004, 0.0254, 0.0625, 0.0994, 0.0984, 0.0369, 0.0903, 0.0600],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,491][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0259, 0.1151, 0.0621, 0.0525, 0.0740, 0.0489, 0.0714, 0.0566, 0.0520,
        0.0290, 0.0619, 0.0747, 0.0464, 0.0638, 0.0652, 0.0408, 0.0597],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,492][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1172, 0.0413, 0.0533, 0.0513, 0.0422, 0.0522, 0.0449, 0.0554, 0.0425,
        0.0810, 0.0524, 0.0375, 0.0592, 0.0711, 0.0701, 0.0655, 0.0628],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,493][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1077, 0.0225, 0.0197, 0.0291, 0.0267, 0.0297, 0.0309, 0.0254, 0.0524,
        0.0666, 0.0391, 0.0818, 0.0824, 0.0837, 0.0819, 0.1026, 0.1176],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,494][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0584, 0.0211, 0.0149, 0.1061, 0.0214, 0.0425, 0.0308, 0.0322, 0.0209,
        0.0516, 0.0324, 0.0717, 0.0942, 0.1323, 0.0122, 0.1645, 0.0928],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,497][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.3226, 0.0026, 0.0008, 0.0013, 0.0991, 0.0038, 0.0028, 0.3132, 0.0566,
        0.0251, 0.1444, 0.0022, 0.0028, 0.0130, 0.0007, 0.0012, 0.0078],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,501][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0468, 0.0478, 0.0599, 0.0349, 0.0635, 0.0209, 0.0200, 0.1000, 0.0651,
        0.0476, 0.0306, 0.0519, 0.1128, 0.0743, 0.0461, 0.0479, 0.1298],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,505][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3284, 0.0234, 0.0272, 0.0838, 0.0292, 0.0337, 0.0290, 0.0154, 0.0481,
        0.0383, 0.0145, 0.0208, 0.0393, 0.0382, 0.1006, 0.0765, 0.0538],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,508][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([9.4794e-04, 1.8380e-05, 7.3893e-01, 7.1560e-02, 5.2349e-03, 5.0469e-04,
        5.1654e-04, 3.0038e-05, 3.8658e-03, 2.1122e-03, 5.6247e-03, 4.0710e-04,
        3.3643e-02, 4.4486e-05, 3.0498e-03, 1.0295e-01, 3.0562e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,513][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0256, 0.0614, 0.0615, 0.0528, 0.0604, 0.0612, 0.0652, 0.0754, 0.0716,
        0.0630, 0.0706, 0.0505, 0.0514, 0.0640, 0.0620, 0.0536, 0.0498],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,517][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2885, 0.0271, 0.0417, 0.0350, 0.0066, 0.0425, 0.0129, 0.0093, 0.0159,
        0.0344, 0.0038, 0.0752, 0.1006, 0.0306, 0.0585, 0.0841, 0.1333],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:34,522][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0558, 0.0555, 0.0555, 0.0556, 0.0555, 0.0556, 0.0555, 0.0557, 0.0556,
        0.0555, 0.0556, 0.0556, 0.0555, 0.0555, 0.0556, 0.0555, 0.0555, 0.0556],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,524][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0025, 0.0103, 0.0526, 0.0564, 0.0424, 0.0259, 0.0358, 0.0228, 0.0985,
        0.1266, 0.0309, 0.0361, 0.0834, 0.1276, 0.0360, 0.1043, 0.0892, 0.0188],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,524][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0297, 0.0936, 0.0676, 0.0634, 0.0637, 0.0506, 0.0617, 0.0458, 0.0481,
        0.0320, 0.0591, 0.0825, 0.0441, 0.0557, 0.0520, 0.0402, 0.0479, 0.0622],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,525][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.1194, 0.0397, 0.0521, 0.0640, 0.0375, 0.0508, 0.0407, 0.0489, 0.0392,
        0.0860, 0.0447, 0.0459, 0.0530, 0.0572, 0.0558, 0.0680, 0.0458, 0.0514],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,526][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.1231, 0.0267, 0.0246, 0.0287, 0.0249, 0.0294, 0.0295, 0.0223, 0.0420,
        0.0589, 0.0351, 0.0771, 0.0682, 0.0639, 0.0692, 0.0869, 0.0896, 0.0997],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,529][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0119, 0.0202, 0.0206, 0.0467, 0.0006, 0.0135, 0.0306, 0.0054, 0.0114,
        0.0322, 0.0311, 0.0326, 0.1097, 0.0857, 0.0337, 0.1674, 0.1299, 0.2170],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,533][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.2019, 0.0193, 0.0068, 0.0145, 0.4795, 0.0201, 0.0310, 0.0132, 0.0881,
        0.0110, 0.0797, 0.0021, 0.0049, 0.0064, 0.0060, 0.0047, 0.0080, 0.0028],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,537][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0247, 0.0061, 0.0121, 0.0242, 0.0727, 0.0242, 0.0540, 0.0725, 0.0589,
        0.0493, 0.0351, 0.0912, 0.0688, 0.1190, 0.0729, 0.0668, 0.1427, 0.0049],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,542][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.3311, 0.0084, 0.0281, 0.0512, 0.0126, 0.0282, 0.0209, 0.0144, 0.0518,
        0.0428, 0.0118, 0.0767, 0.0430, 0.0293, 0.0647, 0.0778, 0.0890, 0.0183],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,545][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ language] are: tensor([9.3405e-04, 1.5612e-05, 7.0323e-01, 7.3938e-02, 5.7797e-03, 3.7507e-04,
        1.9582e-03, 5.4748e-05, 3.1222e-03, 2.7517e-03, 6.5657e-03, 6.9291e-04,
        3.1062e-02, 3.6468e-05, 2.6525e-03, 1.3377e-01, 3.3054e-02, 7.2647e-06],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,549][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0251, 0.0584, 0.0579, 0.0501, 0.0569, 0.0575, 0.0609, 0.0702, 0.0666,
        0.0589, 0.0659, 0.0475, 0.0484, 0.0597, 0.0577, 0.0505, 0.0469, 0.0608],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,554][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.3521, 0.0257, 0.0356, 0.0316, 0.0052, 0.0375, 0.0099, 0.0055, 0.0112,
        0.0273, 0.0021, 0.0695, 0.0902, 0.0258, 0.0512, 0.0812, 0.1261, 0.0124],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:34,556][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0528, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0527, 0.0526,
        0.0526, 0.0526, 0.0527, 0.0526, 0.0526, 0.0526, 0.0526, 0.0525, 0.0526,
        0.0526], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,556][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0023, 0.0167, 0.0307, 0.0443, 0.0614, 0.0381, 0.0385, 0.0281, 0.1107,
        0.1087, 0.0269, 0.0545, 0.0787, 0.0868, 0.0285, 0.0817, 0.0826, 0.0345,
        0.0464], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,557][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0259, 0.0940, 0.0598, 0.0618, 0.0693, 0.0456, 0.0579, 0.0457, 0.0481,
        0.0261, 0.0518, 0.0695, 0.0355, 0.0498, 0.0497, 0.0402, 0.0425, 0.0618,
        0.0649], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,558][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.1175, 0.0359, 0.0464, 0.0584, 0.0365, 0.0442, 0.0399, 0.0440, 0.0365,
        0.0741, 0.0450, 0.0404, 0.0505, 0.0562, 0.0550, 0.0712, 0.0447, 0.0527,
        0.0509], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,562][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.1097, 0.0195, 0.0191, 0.0309, 0.0252, 0.0282, 0.0263, 0.0196, 0.0417,
        0.0547, 0.0286, 0.0607, 0.0637, 0.0587, 0.0646, 0.0883, 0.0850, 0.0785,
        0.0971], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,565][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0124, 0.0135, 0.0086, 0.0152, 0.0015, 0.0152, 0.0047, 0.0028, 0.0128,
        0.0291, 0.0121, 0.0536, 0.0780, 0.0578, 0.0050, 0.0305, 0.0928, 0.4418,
        0.1125], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,568][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ used] are: tensor([1.8328e-02, 2.0157e-03, 8.5929e-05, 8.6724e-05, 9.1649e-03, 1.2327e-04,
        7.1004e-05, 4.9163e-01, 4.0620e-04, 6.9774e-03, 4.2423e-01, 3.2298e-04,
        5.6603e-04, 1.0237e-03, 5.8036e-05, 9.4113e-05, 8.6144e-04, 4.3805e-02,
        1.4065e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,573][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0449, 0.0682, 0.0147, 0.0221, 0.0234, 0.0145, 0.0106, 0.0131, 0.0361,
        0.0297, 0.0240, 0.0924, 0.1140, 0.1047, 0.1479, 0.0665, 0.1335, 0.0266,
        0.0128], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,577][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.3398, 0.0139, 0.0184, 0.0585, 0.0206, 0.0197, 0.0280, 0.0180, 0.0451,
        0.0337, 0.0145, 0.0394, 0.0340, 0.0337, 0.0708, 0.0884, 0.0572, 0.0288,
        0.0377], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,580][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ used] are: tensor([5.9908e-04, 7.4863e-05, 2.1634e-01, 9.9101e-02, 7.6018e-03, 6.6391e-04,
        8.3160e-04, 2.8964e-05, 1.1101e-02, 2.0919e-03, 3.2823e-03, 4.1963e-04,
        5.2817e-02, 2.3153e-05, 2.7579e-03, 1.4532e-01, 4.6598e-02, 3.2317e-05,
        4.1032e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,585][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0236, 0.0550, 0.0548, 0.0472, 0.0538, 0.0544, 0.0579, 0.0664, 0.0630,
        0.0558, 0.0625, 0.0449, 0.0457, 0.0570, 0.0549, 0.0477, 0.0442, 0.0578,
        0.0534], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,587][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.3251, 0.0208, 0.0367, 0.0321, 0.0049, 0.0373, 0.0099, 0.0059, 0.0125,
        0.0275, 0.0023, 0.0688, 0.0876, 0.0248, 0.0508, 0.0826, 0.1155, 0.0115,
        0.0435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:34,588][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0502, 0.0499, 0.0499, 0.0500, 0.0500, 0.0500, 0.0500, 0.0501, 0.0500,
        0.0500, 0.0500, 0.0501, 0.0500, 0.0500, 0.0500, 0.0500, 0.0499, 0.0500,
        0.0499, 0.0500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,589][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0029, 0.0176, 0.0424, 0.0254, 0.0450, 0.0357, 0.0391, 0.0216, 0.0930,
        0.1257, 0.0226, 0.0476, 0.0950, 0.0781, 0.0339, 0.0671, 0.0765, 0.0363,
        0.0647, 0.0298], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,590][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0246, 0.0861, 0.0526, 0.0511, 0.0615, 0.0419, 0.0595, 0.0465, 0.0450,
        0.0268, 0.0551, 0.0592, 0.0360, 0.0476, 0.0563, 0.0363, 0.0450, 0.0634,
        0.0586, 0.0468], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,593][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.1083, 0.0341, 0.0438, 0.0565, 0.0330, 0.0417, 0.0382, 0.0477, 0.0354,
        0.0681, 0.0414, 0.0343, 0.0439, 0.0539, 0.0522, 0.0644, 0.0435, 0.0465,
        0.0463, 0.0664], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,597][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.1015, 0.0213, 0.0183, 0.0275, 0.0218, 0.0266, 0.0252, 0.0190, 0.0369,
        0.0490, 0.0262, 0.0541, 0.0549, 0.0498, 0.0546, 0.0702, 0.0692, 0.0744,
        0.0810, 0.1185], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,600][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ by] are: tensor([1.4708e-03, 3.1588e-03, 9.6727e-04, 6.6931e-02, 6.0438e-04, 4.8177e-04,
        2.6260e-04, 9.9840e-04, 1.4158e-03, 2.4503e-03, 9.3040e-04, 5.9478e-04,
        7.4864e-03, 1.6306e-03, 2.9001e-04, 1.7752e-02, 6.4340e-03, 3.7927e-02,
        1.0581e-02, 8.3763e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,603][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ by] are: tensor([4.0141e-02, 2.8100e-03, 3.2343e-04, 7.3167e-05, 9.8943e-03, 3.2559e-04,
        1.0961e-04, 1.3334e-01, 9.7229e-04, 4.6733e-03, 7.7723e-01, 6.9356e-04,
        1.1005e-03, 1.7193e-03, 3.1403e-05, 1.0686e-04, 2.9931e-03, 2.3006e-02,
        3.9001e-04, 6.7909e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,608][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0299, 0.0560, 0.0478, 0.0326, 0.0585, 0.0289, 0.0143, 0.0570, 0.0560,
        0.0594, 0.0177, 0.0404, 0.0700, 0.0817, 0.0462, 0.0895, 0.0686, 0.0387,
        0.0502, 0.0566], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,612][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.3061, 0.0179, 0.0221, 0.0714, 0.0199, 0.0215, 0.0252, 0.0149, 0.0436,
        0.0272, 0.0118, 0.0222, 0.0287, 0.0310, 0.0732, 0.0777, 0.0440, 0.0272,
        0.0367, 0.0778], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,615][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ by] are: tensor([7.2598e-04, 1.3128e-05, 1.5599e-01, 1.4885e-01, 5.9508e-03, 6.1919e-04,
        6.7348e-04, 2.0658e-05, 8.9501e-03, 2.8863e-03, 2.3460e-03, 1.4580e-04,
        4.0358e-02, 9.8949e-06, 1.4624e-03, 1.9867e-01, 2.5050e-02, 5.2015e-06,
        2.6369e-01, 1.4358e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,619][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0218, 0.0522, 0.0522, 0.0447, 0.0513, 0.0517, 0.0552, 0.0642, 0.0605,
        0.0534, 0.0600, 0.0429, 0.0438, 0.0544, 0.0529, 0.0455, 0.0424, 0.0555,
        0.0514, 0.0441], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,620][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.2249, 0.0236, 0.0369, 0.0353, 0.0066, 0.0384, 0.0123, 0.0093, 0.0167,
        0.0338, 0.0042, 0.0673, 0.0865, 0.0288, 0.0516, 0.0793, 0.1149, 0.0155,
        0.0447, 0.0694], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:34,621][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0478, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0477, 0.0476,
        0.0476, 0.0476, 0.0477, 0.0476, 0.0476, 0.0476, 0.0476, 0.0475, 0.0476,
        0.0476, 0.0477, 0.0477], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,622][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0026, 0.0145, 0.0470, 0.0561, 0.0396, 0.0256, 0.0327, 0.0203, 0.0944,
        0.0772, 0.0202, 0.0484, 0.0761, 0.0762, 0.0293, 0.0700, 0.0460, 0.0295,
        0.0743, 0.0667, 0.0536], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,625][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0204, 0.0917, 0.0491, 0.0415, 0.0584, 0.0385, 0.0562, 0.0446, 0.0408,
        0.0228, 0.0490, 0.0589, 0.0363, 0.0503, 0.0514, 0.0321, 0.0468, 0.0705,
        0.0552, 0.0377, 0.0477], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,628][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0951, 0.0328, 0.0424, 0.0418, 0.0336, 0.0416, 0.0357, 0.0434, 0.0339,
        0.0651, 0.0409, 0.0292, 0.0467, 0.0571, 0.0562, 0.0530, 0.0492, 0.0513,
        0.0486, 0.0510, 0.0513], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,633][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0806, 0.0166, 0.0138, 0.0202, 0.0179, 0.0193, 0.0200, 0.0157, 0.0331,
        0.0410, 0.0223, 0.0514, 0.0508, 0.0492, 0.0480, 0.0611, 0.0693, 0.0730,
        0.0767, 0.1113, 0.1087], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,638][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0260, 0.0098, 0.0062, 0.0542, 0.0125, 0.0245, 0.0216, 0.0184, 0.0127,
        0.0343, 0.0259, 0.0369, 0.0641, 0.0887, 0.0066, 0.1099, 0.0669, 0.0705,
        0.0255, 0.2018, 0.0830], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,640][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([1.5317e-01, 1.3867e-03, 3.4698e-04, 6.3330e-04, 5.2685e-02, 1.2909e-03,
        1.2563e-03, 4.0881e-01, 1.7988e-02, 1.6962e-02, 3.1625e-01, 1.0967e-03,
        1.7864e-03, 6.3909e-03, 2.3888e-04, 5.4525e-04, 5.8946e-03, 9.9880e-03,
        2.8747e-04, 3.2460e-04, 2.6642e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,645][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0369, 0.0427, 0.0445, 0.0320, 0.0433, 0.0192, 0.0158, 0.0779, 0.0547,
        0.0427, 0.0243, 0.0395, 0.0909, 0.0487, 0.0318, 0.0418, 0.1048, 0.0309,
        0.0441, 0.0438, 0.0895], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,649][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.2874, 0.0194, 0.0241, 0.0674, 0.0225, 0.0260, 0.0223, 0.0111, 0.0372,
        0.0301, 0.0102, 0.0134, 0.0318, 0.0292, 0.0792, 0.0630, 0.0436, 0.0261,
        0.0399, 0.0737, 0.0426], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,651][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([3.6741e-04, 7.1504e-06, 2.7651e-01, 2.7095e-02, 2.1998e-03, 2.2451e-04,
        2.0671e-04, 1.2132e-05, 1.6260e-03, 8.4581e-04, 2.3471e-03, 1.6695e-04,
        1.2922e-02, 2.0430e-05, 1.3694e-03, 4.1003e-02, 1.0910e-02, 2.8921e-06,
        5.8027e-01, 3.1627e-02, 1.0270e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,652][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0209, 0.0502, 0.0502, 0.0429, 0.0494, 0.0498, 0.0533, 0.0617, 0.0584,
        0.0514, 0.0578, 0.0410, 0.0417, 0.0523, 0.0507, 0.0435, 0.0404, 0.0530,
        0.0491, 0.0420, 0.0403], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,653][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2073, 0.0219, 0.0334, 0.0290, 0.0057, 0.0343, 0.0108, 0.0083, 0.0138,
        0.0289, 0.0035, 0.0589, 0.0788, 0.0250, 0.0457, 0.0659, 0.1056, 0.0133,
        0.0371, 0.0566, 0.1161], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:34,683][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:34,684][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,685][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,687][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,689][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,692][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,695][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,699][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,703][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,706][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,710][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,714][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,714][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:34,715][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.6218, 0.3782], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,716][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.3175, 0.6825], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,717][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.6926, 0.3074], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,719][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.5003, 0.4997], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,722][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.6726, 0.3274], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,726][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.8380, 0.1620], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,731][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.9965, 0.0035], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,735][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.7872, 0.2128], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,740][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9398, 0.0602], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,744][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0031, 0.9969], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,746][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0939, 0.9061], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,747][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.5662, 0.4338], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:34,748][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.4970, 0.2817, 0.2213], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,748][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0782, 0.2992, 0.6226], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,749][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.4819, 0.2472, 0.2709], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,752][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.3336, 0.3333, 0.3331], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,756][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.4454, 0.2044, 0.3502], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,760][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.7699, 0.0402, 0.1898], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,765][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.9944, 0.0029, 0.0026], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,769][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.4339, 0.2411, 0.3250], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,774][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.8026, 0.0929, 0.1046], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,776][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([6.8652e-04, 1.8413e-01, 8.1518e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,778][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0679, 0.4964, 0.4358], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,779][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.5997, 0.0556, 0.3447], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:34,780][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.3961, 0.2537, 0.1843, 0.1659], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,780][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0557, 0.1952, 0.4559, 0.2932], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,782][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.3569, 0.1935, 0.2086, 0.2409], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,785][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.2502, 0.2500, 0.2498, 0.2499], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,789][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.3293, 0.1392, 0.2482, 0.2833], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,794][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.7355, 0.0507, 0.1435, 0.0704], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,798][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.9562, 0.0050, 0.0263, 0.0126], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,803][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.1853, 0.2644, 0.3236, 0.2267], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,808][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.6265, 0.1048, 0.1046, 0.1641], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,810][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0008, 0.1826, 0.7516, 0.0651], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,811][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0457, 0.3555, 0.3027, 0.2962], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,811][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.4528, 0.0303, 0.0289, 0.4880], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:34,812][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.3330, 0.1647, 0.1781, 0.1305, 0.1937], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,813][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0336, 0.1311, 0.3151, 0.3596, 0.1606], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,816][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.2946, 0.1704, 0.1765, 0.1978, 0.1608], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,820][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.2002, 0.2000, 0.1999, 0.2000, 0.1999], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,824][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.3358, 0.1205, 0.2275, 0.2411, 0.0750], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,829][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.8645, 0.0311, 0.0731, 0.0223, 0.0090], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,833][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.8797, 0.0011, 0.0083, 0.0107, 0.1001], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,838][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.5485, 0.0956, 0.1364, 0.1520, 0.0675], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,842][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.6644, 0.0944, 0.0871, 0.1105, 0.0436], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,843][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0022, 0.2391, 0.6215, 0.0832, 0.0539], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,844][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.0364, 0.2795, 0.2376, 0.2197, 0.2268], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,844][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.5389, 0.0085, 0.0076, 0.0406, 0.4044], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:34,845][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.2962, 0.1494, 0.1508, 0.1315, 0.1846, 0.0876], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,848][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0277, 0.0931, 0.2864, 0.2941, 0.2062, 0.0925], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,852][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.2451, 0.1314, 0.1431, 0.1636, 0.1342, 0.1827], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,857][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.1668, 0.1667, 0.1666, 0.1666, 0.1666, 0.1667], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,861][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.2932, 0.1058, 0.1921, 0.2127, 0.0663, 0.1299], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,866][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.8146, 0.0298, 0.0959, 0.0320, 0.0068, 0.0209], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,871][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.7654, 0.0033, 0.0124, 0.0114, 0.1070, 0.1005], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,874][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.2222, 0.1702, 0.2194, 0.1744, 0.1029, 0.1109], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,875][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.4191, 0.0898, 0.0610, 0.0708, 0.0833, 0.2760], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,876][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0020, 0.2619, 0.6111, 0.0674, 0.0505, 0.0071], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,876][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.0283, 0.2231, 0.1914, 0.1774, 0.1832, 0.1966], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,878][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.7274, 0.0402, 0.0358, 0.0529, 0.0398, 0.1038], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:34,881][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.1593, 0.0945, 0.1008, 0.0889, 0.1337, 0.0723, 0.3505],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,886][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0200, 0.0786, 0.2634, 0.2403, 0.1729, 0.1329, 0.0919],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,890][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.2123, 0.1201, 0.1247, 0.1399, 0.1165, 0.1519, 0.1346],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,895][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.1430, 0.1429, 0.1428, 0.1428, 0.1428, 0.1429, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,900][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.2567, 0.1002, 0.1814, 0.1978, 0.0635, 0.1236, 0.0767],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,904][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.7820, 0.0428, 0.1085, 0.0274, 0.0080, 0.0228, 0.0085],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,906][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([8.8239e-01, 4.2629e-04, 5.4011e-03, 9.1797e-03, 1.0261e-02, 5.6112e-02,
        3.6230e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,907][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.3636, 0.1086, 0.1553, 0.1504, 0.0663, 0.0873, 0.0685],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,907][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.3264, 0.0466, 0.0365, 0.0783, 0.0733, 0.3332, 0.1057],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,908][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0020, 0.2059, 0.5678, 0.0721, 0.0460, 0.0095, 0.0967],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,910][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.0271, 0.1847, 0.1567, 0.1497, 0.1556, 0.1644, 0.1619],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,913][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.7814, 0.0313, 0.0254, 0.0392, 0.0685, 0.0127, 0.0414],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:34,917][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.1302, 0.0976, 0.1471, 0.1421, 0.1451, 0.0837, 0.1596, 0.0946],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,922][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0138, 0.0734, 0.2370, 0.2226, 0.1572, 0.1207, 0.1047, 0.0706],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,926][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.1872, 0.1044, 0.1065, 0.1217, 0.1038, 0.1396, 0.1171, 0.1198],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,931][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.1251, 0.1250, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1251],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,935][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.2289, 0.0933, 0.1676, 0.1792, 0.0627, 0.1146, 0.0700, 0.0837],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,938][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.8228, 0.0375, 0.0811, 0.0188, 0.0070, 0.0197, 0.0063, 0.0069],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,939][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([8.9412e-01, 9.9869e-05, 2.4473e-03, 4.7654e-03, 1.8377e-02, 7.3528e-02,
        3.6762e-03, 2.9844e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,939][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.5195, 0.0705, 0.1014, 0.1129, 0.0413, 0.0618, 0.0505, 0.0420],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,940][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.3196, 0.1066, 0.1099, 0.0639, 0.0701, 0.2107, 0.0749, 0.0443],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,942][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0018, 0.1918, 0.5280, 0.0619, 0.0430, 0.0071, 0.0929, 0.0734],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,945][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.0246, 0.1481, 0.1294, 0.1197, 0.1290, 0.1360, 0.1344, 0.1788],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,949][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.7877, 0.0385, 0.0231, 0.0428, 0.0518, 0.0111, 0.0079, 0.0371],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:34,954][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.1366, 0.0813, 0.0991, 0.0879, 0.1342, 0.0634, 0.1669, 0.1381, 0.0924],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,958][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0177, 0.0664, 0.1759, 0.1923, 0.1213, 0.0765, 0.0987, 0.0742, 0.1770],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,963][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.1680, 0.0928, 0.0966, 0.1109, 0.0895, 0.1251, 0.1052, 0.1080, 0.1038],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,967][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.1112, 0.1111, 0.1110, 0.1111, 0.1111, 0.1111, 0.1111, 0.1112, 0.1111],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,970][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.2208, 0.0832, 0.1489, 0.1635, 0.0531, 0.1010, 0.0623, 0.0732, 0.0940],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,971][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.6834, 0.0551, 0.1541, 0.0254, 0.0126, 0.0223, 0.0113, 0.0116, 0.0242],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,972][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.2233, 0.0054, 0.0134, 0.0097, 0.2141, 0.0908, 0.2027, 0.1347, 0.1059],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,972][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.3093, 0.0945, 0.1270, 0.1249, 0.0623, 0.0825, 0.0670, 0.0655, 0.0671],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,975][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.3065, 0.0532, 0.0316, 0.0946, 0.0530, 0.2139, 0.1073, 0.0610, 0.0789],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,978][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0013, 0.1731, 0.3816, 0.0510, 0.0385, 0.0055, 0.0980, 0.0629, 0.1882],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,982][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.0192, 0.1310, 0.1151, 0.1057, 0.1104, 0.1225, 0.1187, 0.1636, 0.1138],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,986][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.4923, 0.0189, 0.0167, 0.0762, 0.1161, 0.0086, 0.0057, 0.0054, 0.2602],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:34,991][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0963, 0.0708, 0.0855, 0.0808, 0.1015, 0.0646, 0.1644, 0.1377, 0.1195,
        0.0788], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:34,996][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0099, 0.0394, 0.1108, 0.1487, 0.0926, 0.0530, 0.0604, 0.0599, 0.2588,
        0.1664], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,000][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.1471, 0.0817, 0.0847, 0.0968, 0.0806, 0.1084, 0.0919, 0.0945, 0.0944,
        0.1199], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,002][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.1001, 0.1000, 0.0999, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,
        0.1000], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,003][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.1920, 0.0717, 0.1274, 0.1428, 0.0466, 0.0890, 0.0537, 0.0612, 0.0791,
        0.1363], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,004][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.7342, 0.0413, 0.1148, 0.0276, 0.0069, 0.0257, 0.0098, 0.0077, 0.0073,
        0.0249], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,004][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.4014, 0.0025, 0.0128, 0.0128, 0.0238, 0.0712, 0.1240, 0.0220, 0.0141,
        0.3155], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,007][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.2258, 0.1114, 0.1503, 0.1256, 0.0676, 0.0786, 0.0618, 0.0677, 0.0703,
        0.0408], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,010][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.2583, 0.1229, 0.0422, 0.0454, 0.0415, 0.1254, 0.0798, 0.0494, 0.0698,
        0.1654], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,014][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0010, 0.1620, 0.3463, 0.0490, 0.0352, 0.0039, 0.0873, 0.0472, 0.1772,
        0.0909], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,018][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.0164, 0.1176, 0.1014, 0.0913, 0.0964, 0.1044, 0.1047, 0.1429, 0.1018,
        0.1233], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,023][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.7679, 0.0268, 0.0094, 0.0342, 0.0618, 0.0178, 0.0074, 0.0051, 0.0066,
        0.0628], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,028][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0798, 0.0517, 0.0837, 0.0595, 0.0698, 0.0552, 0.1494, 0.1316, 0.1272,
        0.0882, 0.1039], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,032][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0073, 0.0449, 0.1110, 0.1218, 0.0820, 0.0557, 0.0495, 0.0409, 0.2120,
        0.2254, 0.0494], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,034][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.1340, 0.0762, 0.0777, 0.0900, 0.0742, 0.0999, 0.0844, 0.0858, 0.0863,
        0.1103, 0.0812], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,035][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0910, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,
        0.0909, 0.0909], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,036][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.1837, 0.0641, 0.1161, 0.1293, 0.0441, 0.0841, 0.0508, 0.0600, 0.0757,
        0.1310, 0.0611], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,037][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.8001, 0.0284, 0.0852, 0.0232, 0.0062, 0.0115, 0.0070, 0.0056, 0.0069,
        0.0232, 0.0026], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,039][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.5681, 0.0020, 0.0024, 0.0080, 0.0633, 0.0648, 0.0170, 0.0243, 0.0165,
        0.2293, 0.0041], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,042][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.4585, 0.0596, 0.0847, 0.0963, 0.0401, 0.0574, 0.0474, 0.0430, 0.0484,
        0.0392, 0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,046][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.1763, 0.0594, 0.0381, 0.0370, 0.0395, 0.1190, 0.0437, 0.0490, 0.0475,
        0.3539, 0.0366], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,050][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0011, 0.1345, 0.3116, 0.0424, 0.0308, 0.0043, 0.0678, 0.0555, 0.2049,
        0.1060, 0.0411], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,055][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.0171, 0.1045, 0.0879, 0.0834, 0.0900, 0.0956, 0.0910, 0.1240, 0.0932,
        0.1127, 0.1006], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,060][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.6949, 0.0486, 0.0380, 0.0655, 0.0837, 0.0221, 0.0071, 0.0098, 0.0106,
        0.0109, 0.0089], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,064][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0883, 0.0481, 0.0453, 0.0411, 0.0631, 0.0383, 0.1927, 0.1356, 0.1117,
        0.0680, 0.0965, 0.0713], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,066][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0166, 0.0427, 0.1064, 0.0950, 0.0735, 0.0404, 0.0583, 0.0552, 0.1830,
        0.1781, 0.0716, 0.0792], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,067][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.1149, 0.0676, 0.0765, 0.0843, 0.0701, 0.0915, 0.0789, 0.0800, 0.0820,
        0.1043, 0.0764, 0.0736], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,068][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0834, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0834, 0.0833,
        0.0833, 0.0833, 0.0834], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,069][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.1472, 0.0568, 0.1030, 0.1176, 0.0368, 0.0710, 0.0417, 0.0475, 0.0623,
        0.1113, 0.0495, 0.1553], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,071][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.5720, 0.0333, 0.1085, 0.0209, 0.0067, 0.0116, 0.0098, 0.0084, 0.0127,
        0.0295, 0.0044, 0.1821], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,074][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.4987, 0.0017, 0.0042, 0.0069, 0.0216, 0.0473, 0.1038, 0.0324, 0.0215,
        0.1683, 0.0049, 0.0886], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,078][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0688, 0.1406, 0.1547, 0.1039, 0.0777, 0.0792, 0.0605, 0.0667, 0.0692,
        0.0371, 0.0446, 0.0970], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,083][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.2072, 0.1397, 0.0440, 0.0433, 0.0595, 0.1137, 0.0627, 0.0353, 0.0610,
        0.1189, 0.0332, 0.0816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,087][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0012, 0.1442, 0.2919, 0.0380, 0.0301, 0.0037, 0.0814, 0.0481, 0.1433,
        0.0844, 0.0384, 0.0953], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,092][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0137, 0.0945, 0.0875, 0.0790, 0.0814, 0.0885, 0.0871, 0.1159, 0.0874,
        0.1077, 0.0954, 0.0619], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,096][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.3901, 0.0243, 0.0178, 0.0381, 0.0287, 0.0130, 0.0032, 0.0065, 0.0077,
        0.0083, 0.0011, 0.4612], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,098][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0715, 0.0462, 0.0558, 0.0446, 0.0632, 0.0403, 0.1608, 0.1180, 0.1053,
        0.0696, 0.0898, 0.0740, 0.0608], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,099][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0110, 0.0276, 0.0904, 0.0977, 0.0691, 0.0330, 0.0525, 0.0451, 0.1752,
        0.1280, 0.0553, 0.1118, 0.1033], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,100][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1074, 0.0653, 0.0669, 0.0766, 0.0655, 0.0842, 0.0744, 0.0775, 0.0762,
        0.0968, 0.0733, 0.0658, 0.0700], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,101][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0770, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,
        0.0769, 0.0769, 0.0770, 0.0769], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,104][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.1333, 0.0501, 0.0912, 0.1044, 0.0320, 0.0622, 0.0367, 0.0415, 0.0540,
        0.0973, 0.0432, 0.1359, 0.1180], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,107][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.5040, 0.0191, 0.0591, 0.0180, 0.0051, 0.0250, 0.0060, 0.0060, 0.0094,
        0.0209, 0.0039, 0.0896, 0.2340], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,112][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2410, 0.0008, 0.0070, 0.0035, 0.0150, 0.0339, 0.0326, 0.0175, 0.0112,
        0.1755, 0.0067, 0.1543, 0.3009], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,116][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0860, 0.1201, 0.1477, 0.1008, 0.0675, 0.0700, 0.0547, 0.0591, 0.0603,
        0.0328, 0.0383, 0.0981, 0.0644], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,121][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1590, 0.1104, 0.0340, 0.0529, 0.0491, 0.1068, 0.0539, 0.0243, 0.0360,
        0.1106, 0.0162, 0.0425, 0.2043], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,125][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0010, 0.1280, 0.2632, 0.0367, 0.0266, 0.0033, 0.0668, 0.0382, 0.1299,
        0.0705, 0.0298, 0.0788, 0.1271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,130][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0128, 0.0924, 0.0794, 0.0731, 0.0765, 0.0832, 0.0824, 0.1129, 0.0811,
        0.0986, 0.0906, 0.0557, 0.0613], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,131][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.2364, 0.0088, 0.0068, 0.0138, 0.0148, 0.0146, 0.0017, 0.0021, 0.0032,
        0.0163, 0.0010, 0.0352, 0.6453], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,132][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0637, 0.0393, 0.0520, 0.0461, 0.0487, 0.0325, 0.1235, 0.0813, 0.0774,
        0.0546, 0.0651, 0.0713, 0.0599, 0.1846], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,133][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0058, 0.0280, 0.0836, 0.0691, 0.0452, 0.0338, 0.0408, 0.0302, 0.1435,
        0.1479, 0.0415, 0.0850, 0.1516, 0.0941], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,135][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.1114, 0.0619, 0.0639, 0.0733, 0.0603, 0.0782, 0.0704, 0.0700, 0.0718,
        0.0877, 0.0690, 0.0628, 0.0583, 0.0613], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,138][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0715, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0715, 0.0714,
        0.0714, 0.0714, 0.0715, 0.0714, 0.0714], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,142][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.1204, 0.0476, 0.0873, 0.0963, 0.0301, 0.0575, 0.0342, 0.0400, 0.0509,
        0.0885, 0.0403, 0.1226, 0.1059, 0.0785], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,147][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.4798, 0.0164, 0.0489, 0.0117, 0.0019, 0.0078, 0.0046, 0.0031, 0.0031,
        0.0170, 0.0019, 0.0840, 0.2863, 0.0337], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,149][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([2.0925e-01, 2.1514e-04, 1.5660e-03, 9.9631e-04, 1.7471e-03, 2.1583e-02,
        1.3441e-03, 7.7525e-04, 1.1468e-03, 8.1634e-02, 1.3383e-03, 9.3540e-02,
        5.6924e-01, 1.5627e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,154][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.1627, 0.0907, 0.1169, 0.1012, 0.0503, 0.0614, 0.0482, 0.0466, 0.0523,
        0.0332, 0.0292, 0.0917, 0.0679, 0.0478], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,159][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.1229, 0.1038, 0.0367, 0.0409, 0.0231, 0.1061, 0.0510, 0.0189, 0.0462,
        0.0730, 0.0167, 0.0510, 0.2862, 0.0238], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,162][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0007, 0.0923, 0.2708, 0.0371, 0.0216, 0.0035, 0.0522, 0.0335, 0.1005,
        0.0673, 0.0242, 0.0607, 0.1080, 0.1277], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,163][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0126, 0.0866, 0.0715, 0.0681, 0.0739, 0.0777, 0.0758, 0.1008, 0.0769,
        0.0922, 0.0853, 0.0519, 0.0542, 0.0725], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,164][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.4098, 0.0418, 0.0334, 0.0482, 0.0295, 0.0189, 0.0059, 0.0051, 0.0040,
        0.0076, 0.0009, 0.1908, 0.1376, 0.0665], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,165][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0564, 0.0381, 0.0287, 0.0366, 0.0521, 0.0239, 0.1065, 0.0949, 0.0585,
        0.0407, 0.0509, 0.0511, 0.0502, 0.1461, 0.1650], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,168][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0056, 0.0210, 0.0793, 0.0647, 0.0408, 0.0270, 0.0332, 0.0281, 0.1325,
        0.1222, 0.0430, 0.0872, 0.1301, 0.1409, 0.0445], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,171][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.0991, 0.0602, 0.0613, 0.0714, 0.0579, 0.0747, 0.0668, 0.0670, 0.0662,
        0.0809, 0.0634, 0.0579, 0.0553, 0.0611, 0.0568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,176][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667,
        0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,180][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.1126, 0.0472, 0.0857, 0.0913, 0.0279, 0.0532, 0.0327, 0.0382, 0.0494,
        0.0842, 0.0376, 0.1154, 0.0992, 0.0755, 0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,185][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.5029, 0.0241, 0.0715, 0.0151, 0.0024, 0.0067, 0.0050, 0.0030, 0.0034,
        0.0170, 0.0016, 0.0832, 0.1869, 0.0214, 0.0558], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,188][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([3.6399e-01, 6.9935e-04, 1.4995e-03, 1.9040e-03, 8.3163e-04, 1.1836e-02,
        1.6079e-03, 2.9402e-04, 1.5292e-03, 7.8173e-02, 4.4612e-04, 1.6526e-01,
        3.4804e-01, 1.3848e-02, 1.0048e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,192][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.2209, 0.0711, 0.0954, 0.0969, 0.0447, 0.0541, 0.0462, 0.0425, 0.0468,
        0.0319, 0.0264, 0.0829, 0.0635, 0.0419, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,194][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0918, 0.0668, 0.0191, 0.0239, 0.0244, 0.1130, 0.0517, 0.0206, 0.0364,
        0.1243, 0.0171, 0.0684, 0.2762, 0.0259, 0.0405], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,195][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0004, 0.0523, 0.1605, 0.0311, 0.0173, 0.0033, 0.0359, 0.0254, 0.0833,
        0.0570, 0.0164, 0.0473, 0.0827, 0.0846, 0.3025], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,196][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.0114, 0.0786, 0.0671, 0.0647, 0.0667, 0.0727, 0.0709, 0.0969, 0.0696,
        0.0850, 0.0784, 0.0483, 0.0514, 0.0686, 0.0697], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,197][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.4555, 0.0302, 0.0226, 0.0305, 0.0251, 0.0117, 0.0054, 0.0031, 0.0064,
        0.0055, 0.0010, 0.0785, 0.1018, 0.0049, 0.2176], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:35,199][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.0535, 0.0373, 0.0280, 0.0286, 0.0382, 0.0219, 0.0833, 0.0950, 0.0566,
        0.0421, 0.0553, 0.0470, 0.0445, 0.1235, 0.1543, 0.0910],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,203][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0074, 0.0237, 0.0652, 0.0614, 0.0598, 0.0292, 0.0353, 0.0340, 0.1497,
        0.1045, 0.0348, 0.0664, 0.1054, 0.1115, 0.0495, 0.0622],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,208][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.0920, 0.0538, 0.0565, 0.0634, 0.0541, 0.0683, 0.0621, 0.0639, 0.0618,
        0.0776, 0.0610, 0.0523, 0.0540, 0.0573, 0.0558, 0.0660],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,212][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0626, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,
        0.0625, 0.0625, 0.0626, 0.0625, 0.0625, 0.0625, 0.0624],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,217][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.1078, 0.0422, 0.0769, 0.0824, 0.0248, 0.0478, 0.0290, 0.0341, 0.0436,
        0.0761, 0.0337, 0.1060, 0.0912, 0.0696, 0.0450, 0.0898],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,221][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.4684, 0.0224, 0.0519, 0.0212, 0.0054, 0.0068, 0.0047, 0.0038, 0.0043,
        0.0175, 0.0021, 0.0515, 0.1357, 0.0129, 0.0484, 0.1430],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,226][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.1740, 0.0005, 0.0066, 0.0018, 0.0032, 0.0363, 0.0288, 0.0009, 0.0046,
        0.0865, 0.0017, 0.1330, 0.4707, 0.0196, 0.0264, 0.0053],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,227][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0671, 0.1170, 0.1291, 0.0788, 0.0609, 0.0629, 0.0459, 0.0492, 0.0551,
        0.0284, 0.0343, 0.0859, 0.0535, 0.0503, 0.0473, 0.0344],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,228][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.1219, 0.0938, 0.0249, 0.0408, 0.0461, 0.0785, 0.0552, 0.0280, 0.0354,
        0.0928, 0.0221, 0.0530, 0.1708, 0.0299, 0.0560, 0.0508],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,228][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([2.9201e-04, 4.3633e-02, 1.1391e-01, 1.9847e-02, 1.2617e-02, 1.7729e-03,
        2.9249e-02, 1.8539e-02, 6.7478e-02, 3.7621e-02, 1.3240e-02, 4.3362e-02,
        7.0315e-02, 7.6165e-02, 3.0901e-01, 1.4295e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,231][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.0107, 0.0717, 0.0629, 0.0613, 0.0615, 0.0670, 0.0675, 0.0915, 0.0642,
        0.0805, 0.0729, 0.0443, 0.0478, 0.0650, 0.0673, 0.0640],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,233][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([1.3793e-01, 6.7748e-03, 5.8850e-03, 3.4590e-02, 1.7499e-02, 1.9949e-03,
        1.1182e-03, 1.2738e-03, 2.3241e-03, 3.5218e-03, 4.5981e-04, 1.9049e-02,
        2.1106e-02, 9.8913e-04, 6.4491e-03, 7.3904e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:35,237][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0439, 0.0310, 0.0278, 0.0231, 0.0321, 0.0228, 0.0799, 0.0814, 0.0621,
        0.0489, 0.0599, 0.0484, 0.0426, 0.1119, 0.1507, 0.0736, 0.0599],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,241][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0061, 0.0217, 0.0628, 0.0611, 0.0428, 0.0266, 0.0358, 0.0290, 0.1124,
        0.0846, 0.0341, 0.0732, 0.0964, 0.1017, 0.0478, 0.0980, 0.0657],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,246][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0814, 0.0512, 0.0530, 0.0589, 0.0517, 0.0654, 0.0588, 0.0618, 0.0604,
        0.0736, 0.0570, 0.0510, 0.0531, 0.0560, 0.0524, 0.0628, 0.0516],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,250][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0589, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,
        0.0588, 0.0588, 0.0589, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,255][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0971, 0.0371, 0.0670, 0.0746, 0.0227, 0.0439, 0.0262, 0.0299, 0.0382,
        0.0694, 0.0307, 0.0977, 0.0845, 0.0618, 0.0399, 0.0803, 0.0991],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,260][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.3264, 0.0122, 0.0265, 0.0125, 0.0051, 0.0096, 0.0051, 0.0050, 0.0035,
        0.0163, 0.0025, 0.0475, 0.1302, 0.0152, 0.0255, 0.0347, 0.3223],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,261][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1522, 0.0006, 0.0028, 0.0028, 0.0044, 0.0214, 0.0393, 0.0080, 0.0045,
        0.0855, 0.0025, 0.0814, 0.1695, 0.0179, 0.0072, 0.0063, 0.3937],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,262][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0584, 0.1058, 0.1291, 0.0812, 0.0590, 0.0569, 0.0446, 0.0491, 0.0502,
        0.0255, 0.0327, 0.0844, 0.0500, 0.0493, 0.0466, 0.0353, 0.0420],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,263][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1086, 0.0825, 0.0245, 0.0488, 0.0435, 0.0787, 0.0433, 0.0176, 0.0448,
        0.0768, 0.0166, 0.0168, 0.1757, 0.0225, 0.0644, 0.0445, 0.0905],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,265][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.7387e-04, 3.0415e-02, 8.0865e-02, 1.4521e-02, 8.5697e-03, 1.1701e-03,
        1.8422e-02, 1.2791e-02, 5.3376e-02, 2.6874e-02, 8.9163e-03, 2.8437e-02,
        5.0601e-02, 5.4761e-02, 2.2528e-01, 1.0443e-01, 2.8040e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,268][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0101, 0.0707, 0.0600, 0.0559, 0.0590, 0.0631, 0.0640, 0.0879, 0.0619,
        0.0750, 0.0698, 0.0425, 0.0463, 0.0629, 0.0626, 0.0594, 0.0489],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,272][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.3656, 0.0080, 0.0057, 0.0132, 0.0158, 0.0054, 0.0020, 0.0028, 0.0036,
        0.0066, 0.0010, 0.0312, 0.0720, 0.0019, 0.0114, 0.0818, 0.3718],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:35,277][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0475, 0.0315, 0.0348, 0.0269, 0.0348, 0.0194, 0.0757, 0.0699, 0.0440,
        0.0353, 0.0407, 0.0379, 0.0346, 0.1025, 0.1690, 0.0895, 0.0586, 0.0474],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,281][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0048, 0.0134, 0.0530, 0.0493, 0.0359, 0.0220, 0.0295, 0.0248, 0.0899,
        0.1029, 0.0377, 0.0490, 0.0880, 0.1193, 0.0456, 0.1135, 0.0926, 0.0291],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,286][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0861, 0.0488, 0.0524, 0.0598, 0.0476, 0.0625, 0.0549, 0.0561, 0.0539,
        0.0717, 0.0548, 0.0488, 0.0485, 0.0515, 0.0472, 0.0586, 0.0456, 0.0511],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,291][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0556, 0.0556, 0.0555, 0.0555, 0.0555, 0.0556, 0.0555, 0.0556, 0.0556,
        0.0555, 0.0556, 0.0556, 0.0555, 0.0555, 0.0556, 0.0555, 0.0555, 0.0556],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,292][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0958, 0.0355, 0.0655, 0.0699, 0.0213, 0.0411, 0.0248, 0.0302, 0.0379,
        0.0659, 0.0295, 0.0909, 0.0789, 0.0598, 0.0388, 0.0790, 0.0945, 0.0406],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,293][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.2837, 0.0469, 0.0610, 0.0122, 0.0010, 0.0053, 0.0028, 0.0021, 0.0024,
        0.0104, 0.0011, 0.0420, 0.1154, 0.0125, 0.0261, 0.0265, 0.3354, 0.0132],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,294][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([1.5299e-01, 1.2014e-03, 1.1164e-03, 8.3001e-04, 1.5853e-02, 1.1306e-02,
        2.1928e-03, 1.0164e-03, 1.2995e-02, 3.7818e-02, 4.7155e-04, 4.0283e-02,
        2.0427e-01, 4.1589e-03, 2.5186e-03, 4.6542e-03, 5.0396e-01, 2.3577e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,295][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.2260, 0.0492, 0.0688, 0.0792, 0.0356, 0.0440, 0.0399, 0.0362, 0.0383,
        0.0307, 0.0222, 0.0658, 0.0570, 0.0340, 0.0285, 0.0447, 0.0588, 0.0410],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,298][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0871, 0.0248, 0.0286, 0.0192, 0.0134, 0.0468, 0.0224, 0.0166, 0.0293,
        0.0704, 0.0154, 0.1738, 0.1917, 0.0176, 0.0282, 0.0372, 0.1534, 0.0241],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,302][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0003, 0.0423, 0.1129, 0.0175, 0.0108, 0.0018, 0.0211, 0.0158, 0.0591,
        0.0332, 0.0109, 0.0281, 0.0492, 0.0563, 0.1881, 0.0987, 0.2357, 0.0182],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,306][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0107, 0.0650, 0.0568, 0.0549, 0.0551, 0.0593, 0.0577, 0.0784, 0.0576,
        0.0732, 0.0648, 0.0407, 0.0441, 0.0579, 0.0580, 0.0575, 0.0448, 0.0636],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,311][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.2901, 0.2608, 0.0127, 0.0226, 0.0067, 0.0059, 0.0017, 0.0025, 0.0022,
        0.0035, 0.0006, 0.0703, 0.0516, 0.0018, 0.0081, 0.0929, 0.0697, 0.0965],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:35,315][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.0696, 0.0457, 0.0266, 0.0370, 0.0618, 0.0217, 0.0696, 0.0587, 0.0323,
        0.0239, 0.0375, 0.0335, 0.0337, 0.0977, 0.1182, 0.0837, 0.0599, 0.0513,
        0.0377], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,320][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0049, 0.0180, 0.0347, 0.0419, 0.0464, 0.0283, 0.0299, 0.0296, 0.0968,
        0.0919, 0.0320, 0.0626, 0.0860, 0.0900, 0.0362, 0.0883, 0.0832, 0.0395,
        0.0599], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,324][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0813, 0.0459, 0.0494, 0.0571, 0.0472, 0.0594, 0.0521, 0.0544, 0.0531,
        0.0664, 0.0506, 0.0448, 0.0447, 0.0483, 0.0456, 0.0565, 0.0433, 0.0484,
        0.0514], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,325][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0527, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0527, 0.0526,
        0.0526, 0.0526, 0.0527, 0.0526, 0.0526, 0.0527, 0.0526, 0.0526, 0.0526,
        0.0526], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,326][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0917, 0.0331, 0.0601, 0.0656, 0.0204, 0.0389, 0.0231, 0.0272, 0.0348,
        0.0602, 0.0273, 0.0842, 0.0729, 0.0552, 0.0362, 0.0711, 0.0865, 0.0376,
        0.0738], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,327][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.3124, 0.0145, 0.0667, 0.0141, 0.0020, 0.0065, 0.0043, 0.0026, 0.0042,
        0.0130, 0.0015, 0.0682, 0.0965, 0.0155, 0.0265, 0.0321, 0.2630, 0.0044,
        0.0519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,329][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([1.5201e-01, 3.2719e-04, 3.6853e-04, 8.3502e-04, 3.0803e-04, 4.0234e-03,
        1.1405e-03, 2.7073e-04, 1.0710e-03, 1.3050e-02, 8.8032e-05, 2.1168e-02,
        2.3184e-01, 6.9045e-03, 1.3116e-03, 5.8170e-03, 5.5809e-01, 7.1836e-04,
        6.6103e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,332][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0907, 0.0816, 0.0961, 0.0772, 0.0467, 0.0492, 0.0416, 0.0409, 0.0434,
        0.0260, 0.0264, 0.0681, 0.0478, 0.0386, 0.0346, 0.0334, 0.0432, 0.0600,
        0.0545], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,336][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.1080, 0.0586, 0.0170, 0.0322, 0.0341, 0.0361, 0.0476, 0.0296, 0.0359,
        0.0684, 0.0222, 0.0696, 0.1521, 0.0226, 0.0368, 0.0528, 0.0868, 0.0575,
        0.0321], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,341][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0003, 0.0378, 0.0922, 0.0153, 0.0099, 0.0015, 0.0201, 0.0141, 0.0530,
        0.0291, 0.0094, 0.0280, 0.0480, 0.0526, 0.1904, 0.0965, 0.2340, 0.0174,
        0.0505], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,345][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0104, 0.0612, 0.0541, 0.0523, 0.0527, 0.0555, 0.0558, 0.0764, 0.0545,
        0.0666, 0.0625, 0.0377, 0.0410, 0.0541, 0.0551, 0.0548, 0.0434, 0.0609,
        0.0510], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,350][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.2362, 0.0232, 0.1550, 0.0305, 0.0107, 0.0084, 0.0026, 0.0024, 0.0035,
        0.0025, 0.0009, 0.0645, 0.0546, 0.0027, 0.0136, 0.1382, 0.0939, 0.0080,
        0.1485], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:35,355][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0568, 0.0406, 0.0225, 0.0247, 0.0434, 0.0173, 0.0739, 0.0703, 0.0324,
        0.0266, 0.0413, 0.0319, 0.0292, 0.0902, 0.1251, 0.0775, 0.0576, 0.0492,
        0.0429, 0.0466], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,356][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0058, 0.0196, 0.0444, 0.0266, 0.0367, 0.0256, 0.0307, 0.0242, 0.0828,
        0.0952, 0.0278, 0.0555, 0.0952, 0.0805, 0.0411, 0.0712, 0.0784, 0.0409,
        0.0769, 0.0408], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,357][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.0709, 0.0426, 0.0453, 0.0517, 0.0437, 0.0557, 0.0501, 0.0524, 0.0512,
        0.0638, 0.0491, 0.0420, 0.0437, 0.0462, 0.0452, 0.0537, 0.0422, 0.0474,
        0.0481, 0.0550], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,358][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0501, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,
        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,
        0.0500, 0.0500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,359][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0816, 0.0299, 0.0550, 0.0599, 0.0183, 0.0354, 0.0210, 0.0244, 0.0310,
        0.0565, 0.0248, 0.0792, 0.0685, 0.0510, 0.0331, 0.0669, 0.0818, 0.0345,
        0.0690, 0.0781], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,362][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.3078, 0.0167, 0.0454, 0.0286, 0.0029, 0.0045, 0.0024, 0.0022, 0.0041,
        0.0126, 0.0012, 0.0335, 0.1077, 0.0094, 0.0236, 0.0591, 0.2650, 0.0056,
        0.0394, 0.0284], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,364][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([1.9479e-01, 3.1935e-04, 2.3392e-03, 1.2592e-03, 2.5598e-03, 1.1055e-02,
        9.7354e-03, 1.0907e-03, 1.1257e-03, 7.1439e-02, 1.1089e-03, 7.9270e-02,
        1.7340e-01, 1.5817e-02, 1.9399e-03, 4.2853e-03, 4.2316e-01, 5.5741e-04,
        3.7244e-03, 1.0252e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,368][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0322, 0.1071, 0.1166, 0.0613, 0.0520, 0.0502, 0.0367, 0.0416, 0.0447,
        0.0211, 0.0285, 0.0673, 0.0388, 0.0409, 0.0403, 0.0247, 0.0307, 0.0766,
        0.0585, 0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,373][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0978, 0.0801, 0.0200, 0.0433, 0.0299, 0.0462, 0.0408, 0.0256, 0.0336,
        0.0483, 0.0171, 0.0391, 0.1394, 0.0204, 0.0390, 0.0489, 0.0746, 0.0598,
        0.0330, 0.0631], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,377][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0003, 0.0397, 0.1063, 0.0157, 0.0091, 0.0014, 0.0197, 0.0133, 0.0469,
        0.0273, 0.0092, 0.0255, 0.0433, 0.0501, 0.1820, 0.0844, 0.2090, 0.0163,
        0.0486, 0.0520], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,382][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0087, 0.0578, 0.0497, 0.0494, 0.0507, 0.0529, 0.0528, 0.0737, 0.0535,
        0.0633, 0.0593, 0.0355, 0.0381, 0.0518, 0.0536, 0.0534, 0.0405, 0.0587,
        0.0475, 0.0492], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,387][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.1591, 0.0099, 0.0087, 0.1634, 0.0201, 0.0041, 0.0018, 0.0021, 0.0065,
        0.0035, 0.0006, 0.0356, 0.0263, 0.0022, 0.0073, 0.1938, 0.0481, 0.0048,
        0.0093, 0.2926], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:35,388][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0384, 0.0289, 0.0244, 0.0202, 0.0317, 0.0180, 0.0670, 0.0759, 0.0465,
        0.0375, 0.0460, 0.0378, 0.0333, 0.0923, 0.1261, 0.0619, 0.0484, 0.0401,
        0.0534, 0.0390, 0.0332], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,389][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0049, 0.0166, 0.0470, 0.0455, 0.0331, 0.0205, 0.0278, 0.0229, 0.0840,
        0.0633, 0.0263, 0.0553, 0.0717, 0.0766, 0.0366, 0.0726, 0.0484, 0.0359,
        0.0850, 0.0716, 0.0544], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,390][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0653, 0.0417, 0.0431, 0.0477, 0.0421, 0.0530, 0.0479, 0.0503, 0.0491,
        0.0597, 0.0465, 0.0415, 0.0431, 0.0459, 0.0428, 0.0511, 0.0420, 0.0476,
        0.0462, 0.0509, 0.0424], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,392][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0477, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,
        0.0476, 0.0476, 0.0477, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476, 0.0476,
        0.0476, 0.0476, 0.0476], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,395][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0753, 0.0276, 0.0502, 0.0560, 0.0170, 0.0330, 0.0195, 0.0224, 0.0285,
        0.0525, 0.0232, 0.0744, 0.0645, 0.0467, 0.0302, 0.0613, 0.0764, 0.0318,
        0.0627, 0.0727, 0.0742], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,400][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2308, 0.0084, 0.0180, 0.0091, 0.0039, 0.0068, 0.0037, 0.0036, 0.0026,
        0.0121, 0.0018, 0.0332, 0.0934, 0.0109, 0.0182, 0.0253, 0.2268, 0.0027,
        0.0151, 0.0079, 0.2659], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,404][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1097, 0.0004, 0.0020, 0.0021, 0.0028, 0.0138, 0.0244, 0.0050, 0.0031,
        0.0575, 0.0017, 0.0566, 0.1222, 0.0121, 0.0050, 0.0044, 0.2791, 0.0008,
        0.0033, 0.0025, 0.2918], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,409][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0288, 0.0989, 0.1157, 0.0619, 0.0514, 0.0455, 0.0354, 0.0407, 0.0410,
        0.0189, 0.0273, 0.0663, 0.0361, 0.0399, 0.0394, 0.0245, 0.0285, 0.0767,
        0.0599, 0.0305, 0.0326], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,414][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0805, 0.0773, 0.0206, 0.0373, 0.0341, 0.0553, 0.0329, 0.0128, 0.0295,
        0.0572, 0.0120, 0.0113, 0.1551, 0.0172, 0.0491, 0.0344, 0.0639, 0.0530,
        0.0386, 0.0588, 0.0691], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,417][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.6221e-04, 2.5637e-02, 6.5942e-02, 1.1648e-02, 6.8526e-03, 9.7255e-04,
        1.4704e-02, 1.0031e-02, 4.2512e-02, 2.1215e-02, 7.1841e-03, 2.3096e-02,
        3.9691e-02, 4.3476e-02, 1.7611e-01, 7.7279e-02, 2.1051e-01, 1.4514e-02,
        4.1371e-02, 4.9498e-02, 1.1760e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,420][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0084, 0.0573, 0.0487, 0.0453, 0.0478, 0.0510, 0.0519, 0.0717, 0.0501,
        0.0608, 0.0568, 0.0342, 0.0374, 0.0512, 0.0508, 0.0481, 0.0395, 0.0580,
        0.0462, 0.0447, 0.0400], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,421][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2437, 0.0054, 0.0038, 0.0090, 0.0113, 0.0038, 0.0014, 0.0021, 0.0027,
        0.0049, 0.0008, 0.0223, 0.0492, 0.0014, 0.0080, 0.0545, 0.2642, 0.0032,
        0.0045, 0.0165, 0.2875], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:35,425][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:35,428][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 3781],
        [ 1745],
        [ 6425],
        [12312],
        [ 7268],
        [20697],
        [29553],
        [ 9903],
        [ 6168],
        [ 6342],
        [10894],
        [18956],
        [ 7627],
        [14722],
        [ 3776],
        [12730],
        [ 5373],
        [ 3504],
        [ 8276],
        [14241],
        [ 6439]], device='cuda:0')
[2024-07-23 21:06:35,431][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[13910],
        [ 2528],
        [ 4666],
        [ 7486],
        [ 4326],
        [12518],
        [31568],
        [10284],
        [ 3631],
        [ 3590],
        [18022],
        [19030],
        [ 7027],
        [20013],
        [ 3816],
        [ 5519],
        [ 2661],
        [ 1497],
        [ 3641],
        [ 7030],
        [ 2715]], device='cuda:0')
[2024-07-23 21:06:35,435][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[18160],
        [18180],
        [18181],
        [18172],
        [18157],
        [18163],
        [18177],
        [18173],
        [18180],
        [18179],
        [18181],
        [18178],
        [18179],
        [18170],
        [18174],
        [18173],
        [18177],
        [18175],
        [18182],
        [18181],
        [18180]], device='cuda:0')
[2024-07-23 21:06:35,438][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[40509],
        [ 9335],
        [ 7046],
        [ 6229],
        [ 6453],
        [ 7364],
        [ 8208],
        [ 8734],
        [ 9149],
        [10921],
        [11503],
        [12031],
        [12868],
        [14204],
        [14008],
        [12856],
        [13185],
        [13500],
        [13049],
        [13075],
        [12093]], device='cuda:0')
[2024-07-23 21:06:35,441][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[16843],
        [10978],
        [11876],
        [12852],
        [12146],
        [11400],
        [11371],
        [11487],
        [11736],
        [11841],
        [11732],
        [12569],
        [12328],
        [12279],
        [11916],
        [11907],
        [11945],
        [11822],
        [11895],
        [11936],
        [11920]], device='cuda:0')
[2024-07-23 21:06:35,444][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 5258],
        [16393],
        [15552],
        [16425],
        [17472],
        [19143],
        [20756],
        [22298],
        [22989],
        [20776],
        [21747],
        [21140],
        [19849],
        [21937],
        [23195],
        [22684],
        [21959],
        [22200],
        [22642],
        [22775],
        [22033]], device='cuda:0')
[2024-07-23 21:06:35,447][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[32773],
        [29543],
        [31898],
        [34251],
        [36120],
        [38991],
        [37779],
        [37535],
        [38925],
        [41708],
        [41656],
        [40897],
        [41004],
        [40996],
        [41776],
        [41876],
        [40414],
        [39418],
        [39917],
        [40504],
        [38807]], device='cuda:0')
[2024-07-23 21:06:35,450][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[38029],
        [35955],
        [35066],
        [47265],
        [49761],
        [47789],
        [48447],
        [49878],
        [48427],
        [27537],
        [49957],
        [42936],
        [39941],
        [39165],
        [42363],
        [43458],
        [43139],
        [37594],
        [28766],
        [44654],
        [42443]], device='cuda:0')
[2024-07-23 21:06:35,454][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[30136],
        [29758],
        [22461],
        [22346],
        [ 9545],
        [16303],
        [18314],
        [15584],
        [17559],
        [22420],
        [  594],
        [18133],
        [17341],
        [13214],
        [21319],
        [15449],
        [18807],
        [ 5856],
        [16886],
        [ 7523],
        [18794]], device='cuda:0')
[2024-07-23 21:06:35,457][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 3498],
        [ 4113],
        [15242],
        [14203],
        [16146],
        [ 9447],
        [12047],
        [ 1146],
        [  538],
        [ 1647],
        [  273],
        [ 6538],
        [ 1940],
        [ 9358],
        [ 1276],
        [11926],
        [ 4847],
        [10522],
        [29552],
        [10338],
        [ 6363]], device='cuda:0')
[2024-07-23 21:06:35,458][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[1153],
        [1269],
        [1301],
        [1375],
        [2232],
        [2146],
        [1571],
        [2485],
        [2347],
        [2348],
        [2716],
        [3479],
        [2118],
        [1682],
        [1356],
        [2281],
        [1936],
        [1669],
        [3136],
        [4130],
        [3639]], device='cuda:0')
[2024-07-23 21:06:35,460][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[20793],
        [21231],
        [25128],
        [24202],
        [25171],
        [25198],
        [25201],
        [24245],
        [25365],
        [25258],
        [25179],
        [25211],
        [25708],
        [27162],
        [29756],
        [27456],
        [27761],
        [28032],
        [30855],
        [28297],
        [28713]], device='cuda:0')
[2024-07-23 21:06:35,462][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[1543],
        [1951],
        [2120],
        [2216],
        [2309],
        [2329],
        [2279],
        [2276],
        [2276],
        [2316],
        [2320],
        [2333],
        [2338],
        [2347],
        [2360],
        [2371],
        [2391],
        [2408],
        [2433],
        [2448],
        [2464]], device='cuda:0')
[2024-07-23 21:06:35,465][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[3222],
        [3072],
        [2988],
        [2868],
        [2953],
        [2899],
        [2896],
        [2916],
        [2892],
        [2921],
        [2919],
        [2821],
        [2822],
        [2807],
        [2684],
        [2581],
        [2519],
        [2512],
        [2488],
        [2471],
        [2469]], device='cuda:0')
[2024-07-23 21:06:35,468][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 2562],
        [28147],
        [26723],
        [26623],
        [17334],
        [29226],
        [35348],
        [21679],
        [34296],
        [19674],
        [36389],
        [26478],
        [14254],
        [21649],
        [18496],
        [24916],
        [15689],
        [43226],
        [24535],
        [28950],
        [18353]], device='cuda:0')
[2024-07-23 21:06:35,471][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[133],
        [ 98],
        [113],
        [108],
        [121],
        [119],
        [ 92],
        [112],
        [115],
        [139],
        [146],
        [132],
        [130],
        [114],
        [135],
        [138],
        [142],
        [143],
        [141],
        [143],
        [152]], device='cuda:0')
[2024-07-23 21:06:35,474][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 4024],
        [10221],
        [17065],
        [20539],
        [22323],
        [21493],
        [19497],
        [18354],
        [17969],
        [15960],
        [14896],
        [14691],
        [15247],
        [14866],
        [14903],
        [14977],
        [14410],
        [13574],
        [13850],
        [14341],
        [14943]], device='cuda:0')
[2024-07-23 21:06:35,477][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[3633],
        [3357],
        [3778],
        [3890],
        [3914],
        [3979],
        [4078],
        [4172],
        [4446],
        [4507],
        [4465],
        [4346],
        [4332],
        [4345],
        [4407],
        [4400],
        [4328],
        [4236],
        [4255],
        [4243],
        [4191]], device='cuda:0')
[2024-07-23 21:06:35,481][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[3479],
        [3490],
        [3501],
        [3504],
        [3506],
        [3501],
        [3501],
        [3499],
        [3496],
        [3495],
        [3496],
        [3494],
        [3495],
        [3498],
        [3497],
        [3495],
        [3494],
        [3493],
        [3495],
        [3497],
        [3496]], device='cuda:0')
[2024-07-23 21:06:35,484][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[19615],
        [16899],
        [15050],
        [13603],
        [13650],
        [13567],
        [12794],
        [12117],
        [12033],
        [12441],
        [12493],
        [12651],
        [12916],
        [12554],
        [12275],
        [11661],
        [11737],
        [11507],
        [11236],
        [11071],
        [11105]], device='cuda:0')
[2024-07-23 21:06:35,487][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[8693],
        [8760],
        [9110],
        [8967],
        [8855],
        [8816],
        [8794],
        [8754],
        [8765],
        [8658],
        [8649],
        [9211],
        [8416],
        [8452],
        [8571],
        [8745],
        [8482],
        [8654],
        [8869],
        [8674],
        [8502]], device='cuda:0')
[2024-07-23 21:06:35,490][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 8798],
        [ 8621],
        [ 8597],
        [ 8018],
        [ 7199],
        [ 5848],
        [ 7206],
        [ 7350],
        [ 3897],
        [ 4334],
        [ 4743],
        [ 4464],
        [ 6031],
        [ 8450],
        [ 7221],
        [ 7411],
        [ 8962],
        [10135],
        [11347],
        [ 9731],
        [10183]], device='cuda:0')
[2024-07-23 21:06:35,493][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[29335],
        [31841],
        [32563],
        [32039],
        [32307],
        [31815],
        [32246],
        [32208],
        [31481],
        [30977],
        [31835],
        [29739],
        [29952],
        [30470],
        [30770],
        [29515],
        [29522],
        [30482],
        [29437],
        [28822],
        [28863]], device='cuda:0')
[2024-07-23 21:06:35,495][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[12982],
        [11821],
        [ 6941],
        [ 4797],
        [ 5238],
        [ 9544],
        [17508],
        [ 8788],
        [19700],
        [13314],
        [13928],
        [10738],
        [ 7165],
        [ 5966],
        [ 6449],
        [ 8445],
        [ 7185],
        [ 8209],
        [ 5387],
        [ 5403],
        [ 4712]], device='cuda:0')
[2024-07-23 21:06:35,496][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[  338],
        [  407],
        [  902],
        [  907],
        [  901],
        [  885],
        [ 1101],
        [ 1277],
        [ 2028],
        [ 2561],
        [ 3379],
        [ 3394],
        [ 4538],
        [ 5223],
        [ 6352],
        [ 6734],
        [ 8246],
        [ 7477],
        [ 8120],
        [ 8197],
        [10180]], device='cuda:0')
[2024-07-23 21:06:35,498][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[19643],
        [23869],
        [24025],
        [23717],
        [23390],
        [23718],
        [23780],
        [23634],
        [23647],
        [23611],
        [23527],
        [23639],
        [23672],
        [23738],
        [23811],
        [23707],
        [23675],
        [23594],
        [23604],
        [23571],
        [23559]], device='cuda:0')
[2024-07-23 21:06:35,501][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[3249],
        [3774],
        [3029],
        [3294],
        [2692],
        [3013],
        [3004],
        [3025],
        [2519],
        [2988],
        [2963],
        [3411],
        [3257],
        [3215],
        [3331],
        [3502],
        [3103],
        [3900],
        [3450],
        [3768],
        [3251]], device='cuda:0')
[2024-07-23 21:06:35,504][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[39446],
        [38981],
        [37019],
        [37124],
        [37328],
        [37783],
        [36082],
        [36929],
        [38503],
        [39470],
        [38770],
        [40393],
        [39861],
        [39387],
        [39090],
        [39240],
        [38875],
        [38186],
        [38999],
        [39474],
        [39270]], device='cuda:0')
[2024-07-23 21:06:35,507][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[13589],
        [ 6452],
        [ 4951],
        [ 5477],
        [ 8886],
        [ 5565],
        [ 3466],
        [ 7280],
        [ 2738],
        [ 5912],
        [ 1755],
        [ 2681],
        [ 5144],
        [ 3399],
        [ 3416],
        [ 2528],
        [ 3501],
        [  468],
        [ 1954],
        [ 2310],
        [ 2674]], device='cuda:0')
[2024-07-23 21:06:35,511][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148],
        [1148]], device='cuda:0')
[2024-07-23 21:06:35,550][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:35,555][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,558][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,562][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,566][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,567][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,567][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,568][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,569][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,569][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,571][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,574][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,577][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:35,582][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5265, 0.4735], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,587][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0166, 0.9834], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,591][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.7267, 0.2733], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,596][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0082, 0.9918], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,599][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0852, 0.9148], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,599][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8897, 0.1103], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,600][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.5437, 0.4563], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,601][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.3366, 0.6634], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,602][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0580, 0.9420], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,604][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.8041, 0.1959], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,608][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.1191, 0.8809], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,612][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.7617, 0.2383], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:35,617][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.1933, 0.4600, 0.3467], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,622][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.1161, 0.6087, 0.2752], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,626][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0134, 0.9853, 0.0013], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,631][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0034, 0.3077, 0.6889], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,632][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0421, 0.5215, 0.4364], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,632][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.1962, 0.7337, 0.0700], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,633][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.4483, 0.2922, 0.2595], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,634][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.2742, 0.4418, 0.2840], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,636][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0241, 0.5372, 0.4387], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,639][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0359, 0.9629, 0.0013], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,643][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0696, 0.4709, 0.4595], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,648][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.4137, 0.2805, 0.3057], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:35,653][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.1304, 0.2081, 0.3600, 0.3015], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,657][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0411, 0.3788, 0.5230, 0.0571], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,660][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ by] are: tensor([4.9479e-02, 4.2432e-01, 5.2614e-01, 5.9626e-05], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,663][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0020, 0.1291, 0.3204, 0.5485], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,664][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0210, 0.4665, 0.4062, 0.1063], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,665][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.1877, 0.2714, 0.4923, 0.0485], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,666][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.4101, 0.2833, 0.1970, 0.1096], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,666][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.2099, 0.3597, 0.2783, 0.1521], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,669][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0284, 0.3499, 0.3394, 0.2823], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,673][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0277, 0.7733, 0.1981, 0.0009], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,677][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0641, 0.3752, 0.3580, 0.2027], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,682][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.3376, 0.1284, 0.1518, 0.3822], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:35,687][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.0977, 0.1573, 0.2324, 0.3125, 0.2002], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,691][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0133, 0.6765, 0.1459, 0.0778, 0.0865], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,696][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0143, 0.4221, 0.5261, 0.0343, 0.0032], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,697][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0010, 0.0644, 0.1723, 0.3149, 0.4475], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,697][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.0203, 0.3879, 0.3257, 0.0954, 0.1708], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,698][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.1503, 0.2775, 0.5304, 0.0392, 0.0026], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,699][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.1868, 0.2933, 0.2372, 0.1028, 0.1799], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,701][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.1655, 0.3662, 0.2200, 0.1090, 0.1393], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,705][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.0209, 0.2592, 0.2477, 0.2086, 0.2635], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,710][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.4785, 0.3141, 0.1330, 0.0657, 0.0088], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,714][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.0479, 0.3150, 0.2971, 0.1606, 0.1794], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,719][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.2843, 0.1538, 0.2191, 0.2419, 0.1010], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:35,724][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.0847, 0.1177, 0.1462, 0.2114, 0.3111, 0.1289], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,728][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0043, 0.4267, 0.3441, 0.1006, 0.0858, 0.0384], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,729][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0074, 0.5799, 0.1644, 0.0087, 0.2239, 0.0158], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,730][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ B] are: tensor([4.0133e-04, 3.5213e-02, 8.9307e-02, 1.5515e-01, 2.7323e-01, 4.4670e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,730][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.0113, 0.3688, 0.3061, 0.0738, 0.1483, 0.0917], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,731][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.1424, 0.3855, 0.3341, 0.0919, 0.0294, 0.0166], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,734][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.3483, 0.1585, 0.2167, 0.0919, 0.1319, 0.0528], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,738][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.1172, 0.3297, 0.1886, 0.0905, 0.1177, 0.1563], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,742][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.0121, 0.2114, 0.2135, 0.1718, 0.2334, 0.1579], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,747][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0612, 0.1652, 0.0336, 0.0800, 0.6207, 0.0393], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,752][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0358, 0.2991, 0.2577, 0.1316, 0.1524, 0.1234], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,756][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.2637, 0.1107, 0.1695, 0.2524, 0.1199, 0.0838], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:35,760][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.0615, 0.0781, 0.1538, 0.2127, 0.2059, 0.1399, 0.1481],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,761][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0353, 0.3408, 0.1448, 0.0510, 0.2457, 0.0640, 0.1183],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,762][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [aut] are: tensor([4.4532e-03, 2.6359e-01, 1.3147e-01, 1.9331e-03, 1.1101e-01, 4.8748e-01,
        6.1629e-05], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,763][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [aut] are: tensor([3.1086e-04, 2.5573e-02, 6.2648e-02, 1.1790e-01, 2.0526e-01, 4.4133e-01,
        1.4697e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,763][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.0094, 0.3366, 0.2824, 0.0669, 0.1381, 0.0870, 0.0797],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,766][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.2081, 0.2082, 0.3603, 0.1000, 0.0369, 0.0752, 0.0113],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,770][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.2367, 0.2229, 0.1354, 0.0896, 0.1439, 0.0817, 0.0898],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,774][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.1296, 0.2515, 0.1558, 0.0884, 0.1063, 0.1290, 0.1393],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,779][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0109, 0.1833, 0.1852, 0.1519, 0.2046, 0.1414, 0.1227],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,784][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.2332, 0.2346, 0.0912, 0.0613, 0.0549, 0.3156, 0.0091],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,788][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0352, 0.2696, 0.2320, 0.1244, 0.1440, 0.1122, 0.0826],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,792][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.2404, 0.0873, 0.1230, 0.2607, 0.1116, 0.0782, 0.0988],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:35,793][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.0510, 0.0677, 0.1292, 0.1902, 0.1848, 0.1248, 0.1291, 0.1232],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,794][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0137, 0.2937, 0.0772, 0.0317, 0.2097, 0.0673, 0.0447, 0.2621],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,795][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0138, 0.2690, 0.1515, 0.0042, 0.3373, 0.2128, 0.0096, 0.0017],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,796][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ista] are: tensor([2.3305e-04, 2.2067e-02, 5.4176e-02, 1.0143e-01, 1.6202e-01, 3.2309e-01,
        1.2616e-01, 2.1083e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,799][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0075, 0.3198, 0.2699, 0.0619, 0.1316, 0.0906, 0.0847, 0.0341],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,802][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.2092, 0.1992, 0.2461, 0.0635, 0.0384, 0.1614, 0.0453, 0.0369],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,807][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.1928, 0.1993, 0.1680, 0.0674, 0.1210, 0.0615, 0.1094, 0.0806],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,811][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.1270, 0.1744, 0.1507, 0.0820, 0.1058, 0.1093, 0.1247, 0.1261],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,816][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0142, 0.1579, 0.1775, 0.1274, 0.1811, 0.1397, 0.1152, 0.0871],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,821][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.5265, 0.0466, 0.0325, 0.0547, 0.1115, 0.1098, 0.1150, 0.0036],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,824][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0383, 0.2247, 0.2020, 0.1204, 0.1369, 0.1119, 0.0845, 0.0813],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,825][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.2399, 0.0923, 0.1740, 0.1731, 0.0790, 0.0710, 0.1020, 0.0688],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:35,826][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0435, 0.0529, 0.1178, 0.1331, 0.1638, 0.0976, 0.1279, 0.1154, 0.1480],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,827][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0042, 0.1904, 0.0984, 0.0544, 0.1165, 0.0276, 0.0438, 0.3108, 0.1539],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,829][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0021, 0.2030, 0.0592, 0.0019, 0.0218, 0.6734, 0.0096, 0.0273, 0.0018],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,831][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ de] are: tensor([2.0534e-04, 1.5309e-02, 4.1909e-02, 7.3656e-02, 1.2396e-01, 2.3569e-01,
        9.6886e-02, 1.6213e-01, 2.5025e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,834][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0097, 0.2954, 0.2510, 0.0615, 0.1237, 0.0748, 0.0708, 0.0295, 0.0835],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,838][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0544, 0.4427, 0.2030, 0.0404, 0.0135, 0.0476, 0.0293, 0.1666, 0.0025],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,839][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.2020, 0.1638, 0.2010, 0.0667, 0.0872, 0.0761, 0.0969, 0.0897, 0.0166],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,843][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0812, 0.2320, 0.1240, 0.0635, 0.0708, 0.1106, 0.1280, 0.1178, 0.0720],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,847][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0069, 0.1494, 0.1478, 0.1258, 0.1694, 0.1190, 0.1039, 0.0879, 0.0899],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,852][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0708, 0.1732, 0.0194, 0.0273, 0.2162, 0.1638, 0.2334, 0.0918, 0.0042],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,857][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0264, 0.2389, 0.2029, 0.1014, 0.1191, 0.0952, 0.0689, 0.0613, 0.0858],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,858][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.1924, 0.1097, 0.1522, 0.1531, 0.0763, 0.0795, 0.0862, 0.0944, 0.0562],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:35,859][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0365, 0.0512, 0.1201, 0.1257, 0.1121, 0.0979, 0.1078, 0.1002, 0.1843,
        0.0642], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,859][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0085, 0.2046, 0.1343, 0.0302, 0.1641, 0.0245, 0.1102, 0.1558, 0.1444,
        0.0234], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,860][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0063, 0.1111, 0.1342, 0.0064, 0.0912, 0.1262, 0.0074, 0.1233, 0.3904,
        0.0035], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,862][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ An] are: tensor([1.0749e-04, 8.8963e-03, 2.3992e-02, 4.1048e-02, 7.0691e-02, 1.2715e-01,
        5.3093e-02, 9.7593e-02, 1.6842e-01, 4.0900e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,865][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.0076, 0.2786, 0.2387, 0.0552, 0.1104, 0.0698, 0.0651, 0.0263, 0.0815,
        0.0667], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,870][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0934, 0.2091, 0.1822, 0.0739, 0.0184, 0.0376, 0.0579, 0.2479, 0.0690,
        0.0105], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,874][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.1205, 0.1955, 0.1346, 0.0611, 0.1206, 0.0473, 0.1272, 0.1043, 0.0272,
        0.0616], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,879][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0761, 0.1871, 0.1183, 0.0680, 0.0711, 0.1022, 0.1073, 0.0981, 0.0755,
        0.0964], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,883][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0069, 0.1402, 0.1389, 0.1118, 0.1536, 0.1066, 0.0918, 0.0736, 0.0828,
        0.0939], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,888][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0604, 0.2565, 0.0982, 0.0446, 0.0369, 0.1935, 0.1061, 0.1644, 0.0370,
        0.0023], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,890][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0272, 0.2102, 0.1800, 0.0972, 0.1132, 0.0891, 0.0657, 0.0604, 0.0807,
        0.0765], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,890][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.1604, 0.0808, 0.1201, 0.1741, 0.0800, 0.0588, 0.1148, 0.0696, 0.0545,
        0.0870], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:35,891][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0333, 0.0482, 0.0818, 0.1206, 0.1006, 0.0795, 0.0855, 0.0935, 0.1830,
        0.0727, 0.1014], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,892][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0023, 0.1398, 0.1015, 0.0130, 0.3505, 0.0105, 0.0340, 0.1334, 0.0750,
        0.0127, 0.1274], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,894][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [za] are: tensor([9.2734e-03, 1.0654e-01, 1.5814e-01, 2.9089e-03, 7.8630e-02, 4.4830e-02,
        1.5108e-02, 4.1988e-02, 3.4642e-01, 1.9613e-01, 3.4683e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,896][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [za] are: tensor([1.1696e-04, 7.1969e-03, 1.8236e-02, 3.2052e-02, 4.9900e-02, 9.4721e-02,
        4.2131e-02, 7.0993e-02, 1.2051e-01, 3.2206e-01, 2.4209e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,899][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.0072, 0.2545, 0.2194, 0.0516, 0.1039, 0.0631, 0.0592, 0.0238, 0.0750,
        0.0633, 0.0790], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,904][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.1665, 0.1710, 0.1805, 0.0456, 0.0221, 0.0868, 0.0432, 0.1175, 0.0457,
        0.0878, 0.0333], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,909][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.1264, 0.1648, 0.1526, 0.0667, 0.0936, 0.0427, 0.0891, 0.0754, 0.0180,
        0.0802, 0.0906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,913][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0782, 0.1547, 0.1043, 0.0450, 0.0639, 0.0777, 0.0989, 0.0956, 0.0621,
        0.0887, 0.1308], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,918][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0084, 0.1314, 0.1353, 0.1015, 0.1413, 0.0939, 0.0792, 0.0617, 0.0720,
        0.0824, 0.0930], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,921][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.4871, 0.0371, 0.0336, 0.0470, 0.0234, 0.2429, 0.0196, 0.0255, 0.0702,
        0.0122, 0.0015], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,922][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0269, 0.1919, 0.1684, 0.0908, 0.1027, 0.0795, 0.0580, 0.0549, 0.0704,
        0.0682, 0.0883], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,923][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.1593, 0.0725, 0.1160, 0.0969, 0.0643, 0.0669, 0.1183, 0.0700, 0.0467,
        0.1255, 0.0634], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:35,924][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0354, 0.0468, 0.1044, 0.0941, 0.1044, 0.0743, 0.0952, 0.0784, 0.1493,
        0.0608, 0.1295, 0.0275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,926][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0061, 0.1700, 0.1337, 0.0194, 0.0784, 0.0202, 0.0758, 0.1205, 0.0825,
        0.0203, 0.2544, 0.0187], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,928][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ is] are: tensor([7.5086e-03, 3.4438e-01, 3.3773e-02, 3.7795e-03, 1.2736e-02, 6.8760e-02,
        5.4075e-03, 6.1236e-02, 4.1810e-01, 2.7403e-02, 1.6829e-02, 8.1975e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,931][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ is] are: tensor([6.3399e-05, 4.7037e-03, 1.2608e-02, 2.4493e-02, 3.6128e-02, 7.0248e-02,
        2.8130e-02, 4.9375e-02, 8.2650e-02, 2.2473e-01, 1.7761e-01, 2.8927e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,935][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0072, 0.2410, 0.2041, 0.0481, 0.0912, 0.0591, 0.0549, 0.0224, 0.0664,
        0.0584, 0.0767, 0.0707], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,940][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1678, 0.1542, 0.0689, 0.0407, 0.0400, 0.0526, 0.0807, 0.1234, 0.0342,
        0.0930, 0.1379, 0.0067], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,945][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.2728, 0.0883, 0.0602, 0.0498, 0.0705, 0.0250, 0.0836, 0.0623, 0.0091,
        0.0834, 0.1275, 0.0675], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,949][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0727, 0.1226, 0.0921, 0.0601, 0.0646, 0.0733, 0.0791, 0.0735, 0.0724,
        0.0928, 0.1244, 0.0724], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,953][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0072, 0.1168, 0.1220, 0.0877, 0.1260, 0.0839, 0.0700, 0.0536, 0.0655,
        0.0763, 0.0912, 0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,954][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0732, 0.0818, 0.0457, 0.0596, 0.0742, 0.2839, 0.1130, 0.0371, 0.0303,
        0.0695, 0.1259, 0.0057], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,955][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0302, 0.1404, 0.1339, 0.0809, 0.0890, 0.0764, 0.0578, 0.0562, 0.0702,
        0.0700, 0.0840, 0.1111], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,956][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.1659, 0.0596, 0.0804, 0.1422, 0.0748, 0.0515, 0.0777, 0.0611, 0.0385,
        0.1132, 0.0650, 0.0701], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:35,958][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0315, 0.0367, 0.0752, 0.0941, 0.1070, 0.0661, 0.0871, 0.0894, 0.1582,
        0.0688, 0.1241, 0.0270, 0.0349], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,961][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0058, 0.0741, 0.1365, 0.0237, 0.1074, 0.0207, 0.0715, 0.1164, 0.0800,
        0.0197, 0.3137, 0.0216, 0.0090], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,964][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ a] are: tensor([2.2686e-02, 1.0550e-01, 1.5945e-01, 1.1557e-02, 1.5323e-01, 1.0940e-01,
        6.3285e-03, 6.4941e-02, 2.5663e-01, 2.3554e-02, 5.9017e-02, 2.7615e-02,
        1.0235e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,966][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ a] are: tensor([5.1025e-05, 3.2873e-03, 8.3601e-03, 1.4921e-02, 2.5621e-02, 4.4240e-02,
        1.8458e-02, 3.4187e-02, 5.6494e-02, 1.4362e-01, 1.2287e-01, 1.8608e-01,
        3.4181e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,971][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0064, 0.2233, 0.1912, 0.0445, 0.0863, 0.0553, 0.0517, 0.0212, 0.0644,
        0.0551, 0.0726, 0.0680, 0.0601], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,975][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0605, 0.2257, 0.1302, 0.0450, 0.0208, 0.0181, 0.0294, 0.2587, 0.0471,
        0.0161, 0.0844, 0.0615, 0.0024], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,980][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1626, 0.0910, 0.0840, 0.0500, 0.0664, 0.0323, 0.0814, 0.0612, 0.0141,
        0.0694, 0.0884, 0.1836, 0.0156], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,985][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0609, 0.1502, 0.0931, 0.0526, 0.0566, 0.0668, 0.0818, 0.0768, 0.0562,
        0.0694, 0.1187, 0.0597, 0.0572], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,986][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0070, 0.1064, 0.1139, 0.0797, 0.1154, 0.0768, 0.0625, 0.0473, 0.0611,
        0.0714, 0.0837, 0.0971, 0.0777], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,987][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0208, 0.0990, 0.0316, 0.0386, 0.0696, 0.1696, 0.1473, 0.1522, 0.0252,
        0.0183, 0.1805, 0.0458, 0.0015], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,988][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0297, 0.1230, 0.1192, 0.0747, 0.0814, 0.0691, 0.0528, 0.0526, 0.0631,
        0.0638, 0.0752, 0.0991, 0.0962], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,989][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1348, 0.0571, 0.0919, 0.1442, 0.0706, 0.0410, 0.0821, 0.0571, 0.0353,
        0.0830, 0.0656, 0.0742, 0.0632], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:35,992][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0265, 0.0351, 0.0613, 0.0825, 0.1212, 0.0572, 0.0781, 0.0985, 0.1433,
        0.0616, 0.1088, 0.0234, 0.0319, 0.0706], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:35,996][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0282, 0.1040, 0.0764, 0.0300, 0.0870, 0.0332, 0.0188, 0.1545, 0.0941,
        0.0380, 0.2700, 0.0129, 0.0279, 0.0251], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,000][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0068, 0.3417, 0.0988, 0.0178, 0.1195, 0.0393, 0.0075, 0.0233, 0.1959,
        0.0829, 0.0463, 0.0034, 0.0157, 0.0011], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,003][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([2.8456e-05, 2.4240e-03, 6.3410e-03, 1.1300e-02, 1.7669e-02, 3.1234e-02,
        1.3618e-02, 2.4218e-02, 3.8088e-02, 8.4920e-02, 8.8044e-02, 1.3895e-01,
        2.6503e-01, 2.7813e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,007][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0060, 0.2067, 0.1755, 0.0419, 0.0823, 0.0521, 0.0472, 0.0200, 0.0616,
        0.0518, 0.0683, 0.0634, 0.0566, 0.0665], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,012][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.1209, 0.1615, 0.2334, 0.0324, 0.0130, 0.0515, 0.0320, 0.0961, 0.0418,
        0.0545, 0.0536, 0.0637, 0.0329, 0.0129], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,017][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.1716, 0.0780, 0.0915, 0.0580, 0.0506, 0.0388, 0.0718, 0.0417, 0.0203,
        0.0936, 0.0784, 0.1435, 0.0311, 0.0312], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,018][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0691, 0.1370, 0.0950, 0.0534, 0.0544, 0.0624, 0.0728, 0.0627, 0.0555,
        0.0701, 0.1034, 0.0620, 0.0513, 0.0509], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,019][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0054, 0.0981, 0.0985, 0.0794, 0.1082, 0.0737, 0.0629, 0.0518, 0.0589,
        0.0693, 0.0783, 0.0899, 0.0729, 0.0527], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,020][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.1175, 0.0900, 0.0474, 0.0355, 0.0437, 0.2310, 0.1254, 0.0499, 0.0545,
        0.0440, 0.0566, 0.0554, 0.0454, 0.0037], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,021][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0212, 0.1340, 0.1217, 0.0680, 0.0768, 0.0624, 0.0456, 0.0440, 0.0555,
        0.0550, 0.0679, 0.0902, 0.0872, 0.0703], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,024][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.1513, 0.0770, 0.0976, 0.1047, 0.0479, 0.0500, 0.0657, 0.0409, 0.0380,
        0.0809, 0.0589, 0.0595, 0.0606, 0.0669], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,028][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0226, 0.0406, 0.0639, 0.0869, 0.0920, 0.0556, 0.0693, 0.0829, 0.1130,
        0.0649, 0.1149, 0.0257, 0.0304, 0.0937, 0.0438], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,032][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0200, 0.0878, 0.0932, 0.0207, 0.0926, 0.0109, 0.0221, 0.1226, 0.1138,
        0.0230, 0.2706, 0.0135, 0.0141, 0.0131, 0.0820], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,037][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0322, 0.0894, 0.1815, 0.0092, 0.0695, 0.0588, 0.0157, 0.0145, 0.2761,
        0.1038, 0.0353, 0.0345, 0.0370, 0.0411, 0.0013], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,039][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ different] are: tensor([2.2491e-05, 1.4921e-03, 3.7073e-03, 6.2189e-03, 9.8488e-03, 1.8414e-02,
        8.2177e-03, 1.4588e-02, 2.2580e-02, 5.8245e-02, 5.4326e-02, 8.0354e-02,
        1.7722e-01, 2.1113e-01, 3.3363e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,044][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0060, 0.1958, 0.1659, 0.0400, 0.0761, 0.0486, 0.0452, 0.0189, 0.0562,
        0.0486, 0.0607, 0.0594, 0.0533, 0.0633, 0.0619], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,049][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.1002, 0.1082, 0.1551, 0.0292, 0.0141, 0.1023, 0.0269, 0.1078, 0.0638,
        0.0659, 0.0586, 0.0673, 0.0494, 0.0463, 0.0051], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,050][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.2078, 0.0596, 0.0870, 0.0460, 0.0542, 0.0273, 0.0707, 0.0351, 0.0189,
        0.0868, 0.0727, 0.1608, 0.0206, 0.0307, 0.0218], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,051][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0721, 0.1343, 0.0903, 0.0582, 0.0565, 0.0553, 0.0642, 0.0570, 0.0505,
        0.0640, 0.1037, 0.0532, 0.0466, 0.0542, 0.0399], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,052][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0053, 0.0956, 0.0938, 0.0761, 0.1038, 0.0698, 0.0601, 0.0496, 0.0552,
        0.0635, 0.0732, 0.0810, 0.0658, 0.0487, 0.0585], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,053][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0715, 0.3025, 0.0276, 0.0249, 0.0251, 0.0900, 0.0520, 0.0646, 0.0053,
        0.0357, 0.0689, 0.0262, 0.0147, 0.1816, 0.0096], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,057][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0214, 0.1229, 0.1121, 0.0647, 0.0727, 0.0585, 0.0434, 0.0422, 0.0524,
        0.0521, 0.0638, 0.0835, 0.0809, 0.0660, 0.0633], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,061][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.1039, 0.0821, 0.0966, 0.1057, 0.0642, 0.0363, 0.0615, 0.0469, 0.0332,
        0.0542, 0.0487, 0.0628, 0.0470, 0.0862, 0.0706], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,066][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0222, 0.0314, 0.0869, 0.0750, 0.0805, 0.0569, 0.0693, 0.0670, 0.1147,
        0.0490, 0.0988, 0.0264, 0.0251, 0.0819, 0.0513, 0.0636],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,070][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0331, 0.1883, 0.0397, 0.0239, 0.0422, 0.0192, 0.0183, 0.0884, 0.1669,
        0.0234, 0.2067, 0.0196, 0.0307, 0.0149, 0.0486, 0.0362],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,073][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ from] are: tensor([3.7934e-02, 2.7250e-01, 2.8884e-01, 3.1883e-03, 1.0415e-01, 5.6750e-02,
        2.3036e-02, 9.9659e-02, 2.2421e-02, 1.6117e-02, 2.2483e-02, 7.2407e-03,
        1.0164e-02, 2.3848e-02, 1.1544e-02, 1.1744e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,076][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ from] are: tensor([2.4070e-05, 1.0703e-03, 2.7658e-03, 4.5013e-03, 7.4988e-03, 1.4641e-02,
        5.8198e-03, 1.0455e-02, 1.7074e-02, 4.5681e-02, 3.7990e-02, 6.3883e-02,
        1.3127e-01, 1.5542e-01, 2.7851e-01, 2.2340e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,081][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0054, 0.1859, 0.1590, 0.0366, 0.0701, 0.0469, 0.0439, 0.0176, 0.0549,
        0.0468, 0.0607, 0.0572, 0.0509, 0.0602, 0.0626, 0.0413],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,082][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.1047, 0.1065, 0.2362, 0.0495, 0.0167, 0.0479, 0.0152, 0.1179, 0.0312,
        0.0396, 0.0511, 0.0662, 0.0394, 0.0473, 0.0269, 0.0038],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,083][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.1660, 0.1014, 0.0889, 0.0355, 0.0523, 0.0256, 0.0715, 0.0387, 0.0098,
        0.0783, 0.0943, 0.1534, 0.0188, 0.0291, 0.0196, 0.0169],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,083][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0575, 0.1071, 0.0783, 0.0505, 0.0478, 0.0545, 0.0652, 0.0630, 0.0473,
        0.0651, 0.1122, 0.0586, 0.0491, 0.0524, 0.0446, 0.0467],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,086][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0056, 0.0921, 0.0928, 0.0689, 0.0978, 0.0664, 0.0562, 0.0424, 0.0526,
        0.0590, 0.0700, 0.0770, 0.0622, 0.0461, 0.0559, 0.0551],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,089][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0258, 0.2947, 0.0648, 0.0013, 0.0309, 0.1531, 0.0528, 0.0421, 0.0085,
        0.0142, 0.0661, 0.0175, 0.0206, 0.0913, 0.1159, 0.0005],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,093][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0232, 0.1030, 0.0974, 0.0610, 0.0673, 0.0561, 0.0431, 0.0422, 0.0516,
        0.0512, 0.0598, 0.0785, 0.0765, 0.0629, 0.0607, 0.0656],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,098][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0966, 0.0473, 0.0634, 0.1681, 0.0482, 0.0336, 0.0613, 0.0318, 0.0311,
        0.0570, 0.0357, 0.0447, 0.0474, 0.0678, 0.0991, 0.0670],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,102][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0206, 0.0302, 0.0613, 0.0744, 0.0799, 0.0536, 0.0659, 0.0733, 0.1022,
        0.0534, 0.0887, 0.0245, 0.0274, 0.0902, 0.0530, 0.0763, 0.0251],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,107][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0066, 0.0787, 0.0764, 0.0196, 0.1305, 0.0260, 0.0476, 0.1109, 0.0938,
        0.0135, 0.2663, 0.0186, 0.0092, 0.0378, 0.0380, 0.0184, 0.0080],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,110][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([6.2969e-04, 6.5500e-02, 8.8310e-02, 1.6580e-02, 6.0078e-02, 1.1936e-01,
        3.6020e-03, 7.8550e-02, 3.5381e-01, 2.5730e-02, 7.5820e-02, 1.0390e-02,
        1.1680e-03, 7.4321e-02, 1.2293e-02, 1.3833e-02, 2.9027e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,113][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.1973e-05, 8.4150e-04, 2.0709e-03, 3.4518e-03, 5.6783e-03, 1.0705e-02,
        4.4299e-03, 7.9539e-03, 1.2788e-02, 3.3594e-02, 2.8511e-02, 4.2349e-02,
        9.2228e-02, 1.0849e-01, 1.9685e-01, 1.6563e-01, 2.8441e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,114][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0060, 0.1758, 0.1512, 0.0367, 0.0689, 0.0452, 0.0420, 0.0179, 0.0508,
        0.0450, 0.0584, 0.0548, 0.0487, 0.0586, 0.0585, 0.0402, 0.0415],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,115][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0366, 0.1458, 0.1439, 0.0409, 0.0197, 0.0363, 0.0330, 0.1430, 0.0834,
        0.0336, 0.0737, 0.0534, 0.0262, 0.0373, 0.0682, 0.0177, 0.0074],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,115][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1201, 0.0824, 0.0819, 0.0501, 0.0799, 0.0262, 0.0795, 0.0533, 0.0136,
        0.0469, 0.0838, 0.1351, 0.0143, 0.0560, 0.0252, 0.0351, 0.0167],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,118][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0578, 0.1236, 0.0764, 0.0471, 0.0497, 0.0572, 0.0647, 0.0605, 0.0496,
        0.0556, 0.0836, 0.0510, 0.0466, 0.0488, 0.0394, 0.0457, 0.0427],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,121][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0062, 0.0873, 0.0916, 0.0639, 0.0927, 0.0625, 0.0515, 0.0383, 0.0496,
        0.0555, 0.0655, 0.0729, 0.0589, 0.0430, 0.0522, 0.0520, 0.0566],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,124][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0143, 0.0884, 0.0302, 0.0373, 0.0435, 0.2042, 0.0847, 0.0578, 0.0147,
        0.0064, 0.0627, 0.0249, 0.0014, 0.1212, 0.1670, 0.0405, 0.0009],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,129][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0246, 0.0863, 0.0854, 0.0561, 0.0604, 0.0521, 0.0407, 0.0408, 0.0480,
        0.0489, 0.0569, 0.0742, 0.0722, 0.0592, 0.0570, 0.0624, 0.0748],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,134][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0978, 0.0376, 0.0578, 0.1101, 0.0452, 0.0337, 0.0528, 0.0293, 0.0348,
        0.0503, 0.0445, 0.0575, 0.0522, 0.0609, 0.0661, 0.1027, 0.0668],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,138][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0215, 0.0197, 0.0580, 0.0708, 0.0835, 0.0582, 0.0640, 0.0687, 0.1245,
        0.0461, 0.0925, 0.0238, 0.0246, 0.0609, 0.0620, 0.0773, 0.0271, 0.0167],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,143][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0043, 0.1748, 0.0349, 0.0177, 0.0458, 0.0108, 0.0117, 0.1410, 0.1695,
        0.0096, 0.1167, 0.0089, 0.0054, 0.0196, 0.0146, 0.0117, 0.0052, 0.1978],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,145][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ language] are: tensor([3.3808e-03, 9.5112e-04, 2.4582e-01, 5.4662e-02, 1.0926e-01, 3.0115e-01,
        7.8044e-03, 1.1100e-02, 6.6627e-02, 5.1576e-02, 6.9267e-02, 3.6205e-03,
        4.4966e-03, 4.1771e-02, 1.6199e-02, 7.8568e-03, 4.4015e-03, 5.1140e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,146][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ language] are: tensor([1.1478e-05, 6.4216e-04, 1.6366e-03, 2.8154e-03, 4.5780e-03, 8.3492e-03,
        3.5995e-03, 6.3293e-03, 1.0251e-02, 2.6389e-02, 2.4081e-02, 3.7091e-02,
        7.3065e-02, 9.0377e-02, 1.5456e-01, 1.3664e-01, 2.5492e-01, 1.6467e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,147][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0055, 0.1666, 0.1444, 0.0347, 0.0670, 0.0436, 0.0411, 0.0175, 0.0495,
        0.0426, 0.0545, 0.0523, 0.0462, 0.0539, 0.0549, 0.0381, 0.0391, 0.0486],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,148][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.1487, 0.0110, 0.1167, 0.0685, 0.0215, 0.0563, 0.0194, 0.0767, 0.0561,
        0.0701, 0.0727, 0.0479, 0.0714, 0.0426, 0.0234, 0.0440, 0.0504, 0.0027],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,151][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1263, 0.0816, 0.0901, 0.0421, 0.0490, 0.0489, 0.0582, 0.0468, 0.0139,
        0.1078, 0.0757, 0.0908, 0.0263, 0.0396, 0.0270, 0.0269, 0.0217, 0.0273],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,155][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0546, 0.1041, 0.0690, 0.0458, 0.0477, 0.0558, 0.0548, 0.0511, 0.0480,
        0.0645, 0.0911, 0.0489, 0.0462, 0.0473, 0.0322, 0.0484, 0.0354, 0.0551],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,159][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0052, 0.0801, 0.0823, 0.0631, 0.0875, 0.0591, 0.0504, 0.0402, 0.0479,
        0.0535, 0.0608, 0.0682, 0.0555, 0.0412, 0.0503, 0.0503, 0.0533, 0.0511],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,164][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0695, 0.0054, 0.0740, 0.0343, 0.0839, 0.0655, 0.0836, 0.0923, 0.0426,
        0.0366, 0.0496, 0.0702, 0.0189, 0.0892, 0.1100, 0.0338, 0.0343, 0.0063],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,168][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0198, 0.0894, 0.0846, 0.0526, 0.0583, 0.0486, 0.0370, 0.0369, 0.0441,
        0.0447, 0.0523, 0.0697, 0.0675, 0.0553, 0.0527, 0.0578, 0.0691, 0.0596],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,173][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.1040, 0.0406, 0.0642, 0.0820, 0.0480, 0.0366, 0.0410, 0.0277, 0.0371,
        0.0922, 0.0456, 0.0375, 0.0570, 0.0498, 0.0510, 0.0819, 0.0628, 0.0412],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,177][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0182, 0.0463, 0.0357, 0.0695, 0.0613, 0.0376, 0.0657, 0.0736, 0.0941,
        0.0476, 0.0819, 0.0224, 0.0242, 0.0865, 0.0635, 0.0720, 0.0247, 0.0426,
        0.0328], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,178][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0559, 0.1115, 0.0426, 0.0317, 0.0351, 0.0226, 0.0568, 0.0434, 0.1791,
        0.0510, 0.0225, 0.0375, 0.0319, 0.0193, 0.0325, 0.0361, 0.0142, 0.1453,
        0.0308], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,179][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ used] are: tensor([4.6881e-03, 2.0675e-01, 3.2274e-04, 4.6803e-03, 4.4579e-02, 2.1634e-01,
        9.0031e-03, 5.5972e-03, 1.9374e-01, 3.8628e-02, 4.3244e-02, 1.9404e-02,
        6.6941e-03, 1.2013e-01, 4.7185e-02, 4.5267e-04, 7.3441e-03, 3.1074e-02,
        1.4162e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,180][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ used] are: tensor([9.9649e-06, 5.5273e-04, 1.2395e-03, 2.5074e-03, 3.9066e-03, 7.3050e-03,
        3.0773e-03, 5.4691e-03, 8.9185e-03, 2.3652e-02, 2.0215e-02, 2.8423e-02,
        6.2559e-02, 7.5793e-02, 1.3761e-01, 1.2055e-01, 2.1734e-01, 1.4328e-01,
        1.3760e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,183][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0053, 0.1610, 0.1349, 0.0330, 0.0610, 0.0408, 0.0381, 0.0160, 0.0468,
        0.0412, 0.0517, 0.0503, 0.0447, 0.0527, 0.0522, 0.0367, 0.0380, 0.0478,
        0.0478], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,187][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0713, 0.1303, 0.0169, 0.0161, 0.0240, 0.0556, 0.0615, 0.0732, 0.0312,
        0.0775, 0.0960, 0.0261, 0.0506, 0.0414, 0.0585, 0.0243, 0.0815, 0.0526,
        0.0113], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,192][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.1326, 0.0801, 0.0687, 0.0449, 0.0674, 0.0229, 0.0670, 0.0509, 0.0139,
        0.0794, 0.0630, 0.1203, 0.0173, 0.0415, 0.0200, 0.0300, 0.0214, 0.0294,
        0.0293], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,196][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0586, 0.0860, 0.0611, 0.0428, 0.0411, 0.0422, 0.0572, 0.0486, 0.0380,
        0.0599, 0.0915, 0.0572, 0.0456, 0.0419, 0.0358, 0.0504, 0.0409, 0.0454,
        0.0556], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,201][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0046, 0.0799, 0.0795, 0.0604, 0.0848, 0.0565, 0.0487, 0.0376, 0.0453,
        0.0503, 0.0596, 0.0645, 0.0522, 0.0385, 0.0466, 0.0465, 0.0493, 0.0480,
        0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,206][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0774, 0.2788, 0.0007, 0.0419, 0.0428, 0.0531, 0.0416, 0.0180, 0.0086,
        0.0293, 0.0273, 0.0114, 0.0086, 0.0318, 0.0474, 0.0318, 0.0173, 0.2313,
        0.0010], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,210][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0207, 0.0776, 0.0757, 0.0495, 0.0537, 0.0458, 0.0354, 0.0356, 0.0418,
        0.0426, 0.0487, 0.0651, 0.0633, 0.0519, 0.0496, 0.0545, 0.0650, 0.0554,
        0.0680], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,212][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0730, 0.0477, 0.0555, 0.0590, 0.0458, 0.0248, 0.0477, 0.0416, 0.0245,
        0.0602, 0.0424, 0.0356, 0.0359, 0.0544, 0.0977, 0.0649, 0.0552, 0.0723,
        0.0619], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,213][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0186, 0.0292, 0.0548, 0.0432, 0.0751, 0.0453, 0.0673, 0.0715, 0.1024,
        0.0442, 0.0864, 0.0202, 0.0247, 0.0681, 0.0556, 0.0573, 0.0237, 0.0261,
        0.0497, 0.0368], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,214][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0291, 0.0799, 0.0950, 0.0149, 0.0452, 0.0188, 0.0206, 0.0942, 0.0877,
        0.0449, 0.1101, 0.0232, 0.0243, 0.0214, 0.0508, 0.0309, 0.0178, 0.0925,
        0.0849, 0.0139], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,215][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ by] are: tensor([2.3869e-02, 1.2788e-01, 2.5243e-01, 1.6746e-05, 2.5049e-02, 2.7185e-02,
        4.0337e-03, 9.8428e-03, 2.2420e-02, 2.4659e-02, 8.4434e-02, 1.0616e-02,
        1.7057e-02, 4.5877e-02, 2.4642e-02, 9.1744e-03, 4.6481e-02, 1.9188e-02,
        2.2514e-01, 6.1254e-06], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,217][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ by] are: tensor([1.0113e-05, 4.9221e-04, 1.2087e-03, 1.8192e-03, 3.3014e-03, 5.9821e-03,
        2.4938e-03, 4.3623e-03, 6.7860e-03, 1.8498e-02, 1.5897e-02, 2.4216e-02,
        5.1130e-02, 6.1136e-02, 1.0709e-01, 8.9675e-02, 1.7659e-01, 1.2130e-01,
        1.2788e-01, 1.8013e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,220][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0047, 0.1520, 0.1331, 0.0301, 0.0594, 0.0390, 0.0365, 0.0150, 0.0450,
        0.0397, 0.0514, 0.0489, 0.0432, 0.0512, 0.0514, 0.0350, 0.0367, 0.0458,
        0.0476, 0.0344], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,224][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0721, 0.0510, 0.0944, 0.0097, 0.0180, 0.0720, 0.0378, 0.1140, 0.0439,
        0.0544, 0.0545, 0.0471, 0.0609, 0.0473, 0.0353, 0.0149, 0.0674, 0.0240,
        0.0713, 0.0098], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,228][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.1205, 0.0925, 0.0555, 0.0313, 0.0693, 0.0325, 0.0669, 0.0438, 0.0159,
        0.0791, 0.0970, 0.1326, 0.0131, 0.0257, 0.0171, 0.0164, 0.0133, 0.0357,
        0.0239, 0.0179], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,233][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0473, 0.0880, 0.0662, 0.0363, 0.0406, 0.0457, 0.0565, 0.0516, 0.0394,
        0.0538, 0.0918, 0.0490, 0.0399, 0.0415, 0.0347, 0.0418, 0.0379, 0.0441,
        0.0614, 0.0323], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,238][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0046, 0.0743, 0.0758, 0.0549, 0.0790, 0.0545, 0.0461, 0.0349, 0.0444,
        0.0485, 0.0583, 0.0624, 0.0506, 0.0370, 0.0450, 0.0449, 0.0480, 0.0462,
        0.0464, 0.0443], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,242][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0157, 0.1079, 0.0395, 0.0004, 0.0273, 0.0773, 0.0514, 0.0263, 0.0065,
        0.0216, 0.0419, 0.0249, 0.0311, 0.0598, 0.0824, 0.0012, 0.0434, 0.2611,
        0.0795, 0.0007], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,244][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0188, 0.0774, 0.0737, 0.0475, 0.0523, 0.0433, 0.0336, 0.0333, 0.0397,
        0.0397, 0.0454, 0.0601, 0.0585, 0.0486, 0.0466, 0.0506, 0.0599, 0.0518,
        0.0630, 0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,245][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0832, 0.0326, 0.0409, 0.0715, 0.0432, 0.0249, 0.0416, 0.0268, 0.0309,
        0.0605, 0.0401, 0.0385, 0.0434, 0.0374, 0.0599, 0.0819, 0.0615, 0.0467,
        0.0447, 0.0897], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,246][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0172, 0.0264, 0.0522, 0.0638, 0.0678, 0.0462, 0.0562, 0.0629, 0.0864,
        0.0458, 0.0763, 0.0209, 0.0233, 0.0778, 0.0456, 0.0648, 0.0209, 0.0240,
        0.0476, 0.0547, 0.0193], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,247][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0053, 0.0659, 0.0723, 0.0180, 0.0971, 0.0198, 0.0392, 0.0886, 0.0726,
        0.0110, 0.2251, 0.0152, 0.0071, 0.0309, 0.0345, 0.0159, 0.0063, 0.0635,
        0.0877, 0.0187, 0.0054], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,249][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([6.4207e-04, 6.7493e-02, 8.9983e-02, 1.5837e-02, 5.8280e-02, 1.1145e-01,
        3.3168e-03, 7.7065e-02, 3.1794e-01, 2.4204e-02, 7.2479e-02, 8.9917e-03,
        9.9120e-04, 6.8201e-02, 1.0811e-02, 1.2483e-02, 2.3886e-05, 9.6910e-03,
        4.3859e-02, 6.2368e-03, 2.2507e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,250][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2855e-06, 4.1200e-04, 9.6741e-04, 1.6164e-03, 2.5347e-03, 4.8573e-03,
        1.9973e-03, 3.5173e-03, 5.5762e-03, 1.5008e-02, 1.2421e-02, 1.8177e-02,
        4.0976e-02, 4.7519e-02, 8.3177e-02, 7.0436e-02, 1.1848e-01, 9.6233e-02,
        9.8563e-02, 1.5279e-01, 2.2474e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,254][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0051, 0.1481, 0.1266, 0.0307, 0.0576, 0.0378, 0.0352, 0.0150, 0.0425,
        0.0379, 0.0492, 0.0463, 0.0410, 0.0493, 0.0493, 0.0340, 0.0350, 0.0440,
        0.0446, 0.0335, 0.0371], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,259][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0259, 0.1304, 0.1199, 0.0360, 0.0168, 0.0273, 0.0263, 0.1145, 0.0656,
        0.0242, 0.0628, 0.0418, 0.0171, 0.0293, 0.0508, 0.0148, 0.0051, 0.0624,
        0.0893, 0.0345, 0.0052], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,263][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1072, 0.0732, 0.0734, 0.0453, 0.0688, 0.0237, 0.0740, 0.0493, 0.0123,
        0.0419, 0.0766, 0.1229, 0.0123, 0.0511, 0.0217, 0.0318, 0.0141, 0.0269,
        0.0319, 0.0277, 0.0140], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,268][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0491, 0.1053, 0.0639, 0.0399, 0.0416, 0.0478, 0.0539, 0.0503, 0.0413,
        0.0459, 0.0690, 0.0424, 0.0385, 0.0405, 0.0327, 0.0382, 0.0355, 0.0485,
        0.0499, 0.0311, 0.0349], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,272][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0050, 0.0720, 0.0757, 0.0516, 0.0758, 0.0515, 0.0424, 0.0309, 0.0413,
        0.0455, 0.0546, 0.0599, 0.0483, 0.0352, 0.0428, 0.0424, 0.0462, 0.0439,
        0.0445, 0.0427, 0.0476], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,276][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0075, 0.0706, 0.0203, 0.0264, 0.0370, 0.1370, 0.0708, 0.0475, 0.0120,
        0.0050, 0.0550, 0.0161, 0.0010, 0.0904, 0.1117, 0.0307, 0.0007, 0.1804,
        0.0514, 0.0278, 0.0007], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,277][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0199, 0.0662, 0.0658, 0.0443, 0.0476, 0.0405, 0.0317, 0.0323, 0.0370,
        0.0380, 0.0434, 0.0572, 0.0557, 0.0460, 0.0440, 0.0483, 0.0572, 0.0496,
        0.0606, 0.0543, 0.0605], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,278][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0774, 0.0286, 0.0438, 0.0883, 0.0355, 0.0258, 0.0396, 0.0217, 0.0268,
        0.0393, 0.0335, 0.0438, 0.0399, 0.0458, 0.0504, 0.0798, 0.0503, 0.0362,
        0.0420, 0.0990, 0.0523], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:36,315][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:36,319][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,323][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,326][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,330][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,334][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,337][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,340][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,340][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,341][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,342][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,342][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,344][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:36,347][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.5425, 0.4575], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,351][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.4321, 0.5679], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,355][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0012, 0.9988], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,360][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.7141, 0.2859], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,364][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.7498, 0.2502], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,369][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.3701, 0.6299], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,372][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.7063, 0.2937], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,372][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.4548, 0.5452], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,373][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([9.9954e-01, 4.6341e-04], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,374][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.1472, 0.8528], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,375][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.9787, 0.0213], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,377][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.5342, 0.4658], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:36,381][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.1965, 0.4494, 0.3541], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,385][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.1637, 0.2350, 0.6013], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,388][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([1.2442e-05, 1.2234e-01, 8.7765e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,393][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.4836, 0.1358, 0.3805], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,398][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.2486, 0.7503, 0.0011], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,402][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.1635, 0.6963, 0.1402], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,404][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.7429, 0.1711, 0.0860], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,404][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.2229, 0.2594, 0.5177], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,405][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([9.9859e-01, 5.9718e-04, 8.1026e-04], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,406][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0479, 0.8671, 0.0850], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,407][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.9527, 0.0233, 0.0241], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,410][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.2860, 0.4029, 0.3111], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:36,413][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.1295, 0.1984, 0.3660, 0.3061], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,417][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.1191, 0.1796, 0.4017, 0.2997], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,420][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([1.6000e-06, 4.0686e-03, 8.4647e-01, 1.4946e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,425][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.2686, 0.0850, 0.2485, 0.3980], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,430][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0364, 0.1909, 0.7702, 0.0025], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,434][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.1116, 0.4028, 0.2640, 0.2217], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,436][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.6827, 0.2178, 0.0450, 0.0545], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,436][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0937, 0.1217, 0.2815, 0.5031], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,437][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.9920, 0.0013, 0.0016, 0.0051], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,438][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0465, 0.5403, 0.3286, 0.0846], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,440][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.9147, 0.0314, 0.0315, 0.0224], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,443][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.2169, 0.2247, 0.2147, 0.3437], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:36,447][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.0911, 0.1440, 0.2280, 0.3021, 0.2348], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,451][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.1131, 0.1406, 0.3459, 0.2339, 0.1665], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,454][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([5.5636e-07, 1.2259e-03, 6.7941e-02, 2.3059e-01, 7.0024e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,459][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.1966, 0.0532, 0.1803, 0.2819, 0.2880], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,463][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.4560, 0.1253, 0.0521, 0.1332, 0.2334], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,468][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.0702, 0.4270, 0.2557, 0.2069, 0.0403], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,468][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.2476, 0.2849, 0.1469, 0.0917, 0.2289], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,469][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0633, 0.1080, 0.1871, 0.3271, 0.3145], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,470][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.9899, 0.0010, 0.0014, 0.0047, 0.0029], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,471][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0392, 0.5077, 0.1873, 0.1230, 0.1428], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,474][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.9222, 0.0187, 0.0190, 0.0134, 0.0267], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,477][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.1588, 0.2030, 0.2310, 0.1782, 0.2291], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:36,481][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.0756, 0.1031, 0.1385, 0.1990, 0.3409, 0.1428], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,486][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0922, 0.1239, 0.2659, 0.2279, 0.1698, 0.1203], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,489][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([3.8952e-08, 2.4661e-05, 2.2335e-03, 2.9145e-03, 3.3198e-01, 6.6284e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,494][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.1252, 0.0311, 0.1097, 0.1641, 0.1927, 0.3771], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,498][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.0285, 0.0020, 0.0046, 0.0140, 0.8895, 0.0614], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,500][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.0583, 0.3395, 0.1810, 0.1911, 0.1096, 0.1206], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,501][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.7549, 0.0413, 0.0644, 0.0477, 0.0678, 0.0238], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,501][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0433, 0.0929, 0.1566, 0.2503, 0.2533, 0.2037], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,502][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.9794, 0.0018, 0.0023, 0.0071, 0.0045, 0.0048], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,504][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0188, 0.2769, 0.1311, 0.1209, 0.3524, 0.0999], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,507][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.8709, 0.0252, 0.0254, 0.0179, 0.0354, 0.0252], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,511][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.1247, 0.1256, 0.1491, 0.1603, 0.2208, 0.2195], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:36,515][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.0551, 0.0684, 0.1456, 0.1988, 0.2287, 0.1512, 0.1521],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,520][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0801, 0.1099, 0.2711, 0.1900, 0.1333, 0.1126, 0.1030],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,523][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([4.2715e-10, 5.0836e-07, 1.9954e-05, 4.3851e-05, 3.0780e-03, 9.8363e-01,
        1.3232e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,527][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.1316, 0.0270, 0.0850, 0.1305, 0.1498, 0.2958, 0.1802],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,532][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.1038, 0.0091, 0.0319, 0.0185, 0.2939, 0.5420, 0.0009],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,532][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.0623, 0.2731, 0.1525, 0.1795, 0.1274, 0.1649, 0.0403],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,533][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.4861, 0.1216, 0.0398, 0.0704, 0.1403, 0.0996, 0.0421],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,534][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0454, 0.0696, 0.1134, 0.2261, 0.1984, 0.1586, 0.1884],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,536][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.9810, 0.0012, 0.0018, 0.0055, 0.0036, 0.0044, 0.0025],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,539][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0198, 0.3064, 0.1185, 0.0845, 0.2122, 0.1678, 0.0908],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,543][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.8724, 0.0197, 0.0201, 0.0143, 0.0280, 0.0202, 0.0252],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,547][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.1175, 0.1052, 0.1149, 0.1584, 0.2051, 0.1930, 0.1060],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:36,552][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.0456, 0.0585, 0.1205, 0.1736, 0.2048, 0.1341, 0.1309, 0.1319],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,556][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0707, 0.1024, 0.2147, 0.1708, 0.1304, 0.0958, 0.0857, 0.1295],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,559][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([1.5378e-10, 2.7882e-07, 8.7307e-06, 1.5285e-05, 3.2904e-03, 3.7111e-01,
        6.4164e-02, 5.6141e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,564][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.1008, 0.0162, 0.0614, 0.0935, 0.1114, 0.2398, 0.1503, 0.2267],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,564][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([7.5564e-03, 2.5091e-03, 8.0738e-04, 4.1431e-03, 7.9058e-01, 1.8069e-01,
        1.3687e-02, 2.7388e-05], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,565][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.0704, 0.1964, 0.1502, 0.1874, 0.1080, 0.1785, 0.0719, 0.0373],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,566][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.3372, 0.1370, 0.0895, 0.0461, 0.1229, 0.0985, 0.1076, 0.0611],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,568][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0354, 0.0524, 0.1032, 0.1942, 0.1693, 0.1323, 0.1589, 0.1543],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,571][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.9588, 0.0021, 0.0029, 0.0082, 0.0053, 0.0066, 0.0038, 0.0124],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,575][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0182, 0.2348, 0.0834, 0.0699, 0.2152, 0.0985, 0.1826, 0.0974],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,579][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.8748, 0.0158, 0.0160, 0.0110, 0.0227, 0.0161, 0.0204, 0.0233],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,584][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.1203, 0.1049, 0.1361, 0.1026, 0.1392, 0.1947, 0.1056, 0.0966],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:36,588][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0386, 0.0453, 0.1074, 0.1229, 0.1784, 0.1048, 0.1275, 0.1200, 0.1552],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,593][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0646, 0.0886, 0.1904, 0.1943, 0.1347, 0.0836, 0.0727, 0.1205, 0.0505],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,596][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([6.6014e-10, 1.2625e-07, 1.0042e-05, 3.0731e-05, 7.4544e-04, 5.4681e-02,
        2.3293e-02, 8.4073e-01, 8.0506e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,597][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0851, 0.0161, 0.0556, 0.0805, 0.0922, 0.1735, 0.1229, 0.1727, 0.2013],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,597][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([9.3283e-04, 1.0990e-03, 1.3878e-02, 9.2487e-04, 9.2678e-01, 2.0115e-02,
        2.5998e-02, 9.5724e-03, 6.9988e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,598][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.0341, 0.3203, 0.1402, 0.1358, 0.0639, 0.1270, 0.0656, 0.0698, 0.0432],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,600][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.3947, 0.0883, 0.1231, 0.0491, 0.0677, 0.1176, 0.0662, 0.0684, 0.0250],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,603][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0278, 0.0578, 0.0829, 0.1390, 0.1304, 0.1158, 0.1464, 0.1387, 0.1612],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,607][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.9573, 0.0020, 0.0027, 0.0078, 0.0050, 0.0056, 0.0035, 0.0118, 0.0042],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,611][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0112, 0.1788, 0.0651, 0.0655, 0.1637, 0.0940, 0.1652, 0.1679, 0.0887],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,616][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.8158, 0.0191, 0.0197, 0.0138, 0.0275, 0.0199, 0.0247, 0.0279, 0.0317],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,621][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0808, 0.0987, 0.1029, 0.0942, 0.1285, 0.1651, 0.0875, 0.1196, 0.1227],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:36,625][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0326, 0.0443, 0.1105, 0.1154, 0.1228, 0.1026, 0.1061, 0.1035, 0.1876,
        0.0745], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,628][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0630, 0.0826, 0.1726, 0.2010, 0.1306, 0.0771, 0.0693, 0.1166, 0.0433,
        0.0439], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,629][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([3.5554e-11, 1.3837e-09, 1.5821e-07, 2.0220e-07, 1.6324e-05, 4.0388e-04,
        2.5634e-04, 1.2975e-02, 1.5393e-02, 9.7096e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,629][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0737, 0.0110, 0.0397, 0.0567, 0.0654, 0.1255, 0.0867, 0.1260, 0.1519,
        0.2634], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,630][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.0363, 0.0104, 0.0916, 0.0169, 0.4399, 0.2998, 0.0056, 0.0106, 0.0144,
        0.0747], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,633][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.0374, 0.2074, 0.1341, 0.1566, 0.0624, 0.1044, 0.0617, 0.0716, 0.1096,
        0.0547], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,636][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.1347, 0.1920, 0.0784, 0.0460, 0.1503, 0.0593, 0.1222, 0.1002, 0.0693,
        0.0476], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,640][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0219, 0.0455, 0.0774, 0.1413, 0.1220, 0.1036, 0.1250, 0.1165, 0.1566,
        0.0903], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,644][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.9496, 0.0019, 0.0026, 0.0077, 0.0048, 0.0054, 0.0035, 0.0125, 0.0043,
        0.0076], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,649][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0102, 0.1871, 0.0848, 0.0527, 0.0880, 0.0778, 0.0952, 0.1824, 0.1601,
        0.0619], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,654][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.7963, 0.0186, 0.0189, 0.0132, 0.0267, 0.0191, 0.0239, 0.0274, 0.0306,
        0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,658][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.0804, 0.0883, 0.0939, 0.1181, 0.1315, 0.1338, 0.0982, 0.0790, 0.1383,
        0.0386], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:36,660][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0289, 0.0401, 0.0735, 0.1071, 0.1103, 0.0832, 0.0846, 0.0956, 0.1836,
        0.0829, 0.1103], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,661][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0617, 0.0731, 0.1722, 0.1719, 0.1113, 0.0762, 0.0634, 0.1135, 0.0525,
        0.0504, 0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,662][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([1.5875e-13, 1.1020e-11, 1.1520e-09, 6.5921e-10, 9.3906e-08, 3.1902e-05,
        1.0122e-05, 1.9612e-04, 6.4292e-04, 9.5239e-01, 4.6733e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,662][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0514, 0.0068, 0.0253, 0.0373, 0.0428, 0.0852, 0.0591, 0.0845, 0.1024,
        0.1969, 0.3083], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,664][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([2.8775e-02, 6.9653e-02, 1.7171e-03, 1.3880e-02, 1.9949e-01, 8.6455e-02,
        5.7317e-02, 7.2809e-02, 3.2568e-01, 1.4406e-01, 1.5822e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,667][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.0313, 0.2248, 0.1039, 0.1105, 0.0698, 0.1221, 0.0711, 0.0565, 0.0714,
        0.0828, 0.0558], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,672][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.2193, 0.0876, 0.0764, 0.0550, 0.0754, 0.0547, 0.0737, 0.0875, 0.0389,
        0.1063, 0.1253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,676][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0225, 0.0393, 0.0697, 0.1185, 0.1106, 0.0924, 0.1165, 0.1132, 0.1422,
        0.0930, 0.0821], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,681][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.9586, 0.0012, 0.0018, 0.0057, 0.0034, 0.0043, 0.0025, 0.0088, 0.0032,
        0.0060, 0.0045], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,686][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0088, 0.1397, 0.0541, 0.0405, 0.0926, 0.0761, 0.0748, 0.1769, 0.1958,
        0.1018, 0.0388], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,690][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.8379, 0.0126, 0.0131, 0.0089, 0.0184, 0.0133, 0.0167, 0.0187, 0.0213,
        0.0175, 0.0217], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,692][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.0733, 0.0726, 0.0830, 0.0603, 0.1047, 0.1399, 0.1098, 0.0901, 0.1055,
        0.0484, 0.1122], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:36,693][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0313, 0.0397, 0.0957, 0.0858, 0.1125, 0.0768, 0.0925, 0.0800, 0.1500,
        0.0693, 0.1376, 0.0288], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,693][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0482, 0.0673, 0.1434, 0.1553, 0.1101, 0.0654, 0.0572, 0.0921, 0.0355,
        0.0371, 0.0471, 0.1414], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,694][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([4.5136e-12, 1.5797e-10, 1.0161e-08, 2.0543e-08, 3.8308e-07, 8.6499e-06,
        1.0398e-05, 3.1299e-04, 7.8656e-04, 1.8394e-01, 7.5790e-01, 5.7041e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,697][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0432, 0.0045, 0.0177, 0.0265, 0.0302, 0.0637, 0.0425, 0.0640, 0.0762,
        0.1504, 0.2548, 0.2263], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,700][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0508, 0.0322, 0.0904, 0.0444, 0.0945, 0.1306, 0.0368, 0.0234, 0.0052,
        0.0901, 0.3906, 0.0111], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,704][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0360, 0.1859, 0.0822, 0.1145, 0.0862, 0.0852, 0.0599, 0.0631, 0.0740,
        0.0844, 0.1124, 0.0163], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,708][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.6560, 0.0173, 0.0065, 0.0175, 0.0250, 0.0067, 0.0236, 0.0221, 0.0056,
        0.0532, 0.1545, 0.0119], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,713][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0183, 0.0295, 0.0632, 0.1310, 0.1048, 0.0768, 0.0940, 0.0902, 0.1365,
        0.0822, 0.0711, 0.1023], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,718][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.9514, 0.0015, 0.0018, 0.0059, 0.0037, 0.0037, 0.0024, 0.0097, 0.0031,
        0.0060, 0.0051, 0.0056], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,722][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0083, 0.1266, 0.0771, 0.0480, 0.0978, 0.0585, 0.0935, 0.1186, 0.1463,
        0.1100, 0.1061, 0.0090], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,724][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.6873, 0.0239, 0.0236, 0.0165, 0.0330, 0.0235, 0.0295, 0.0350, 0.0377,
        0.0310, 0.0400, 0.0190], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,725][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0813, 0.0546, 0.0557, 0.0931, 0.1166, 0.1034, 0.0574, 0.0544, 0.0987,
        0.0408, 0.0770, 0.1671], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:36,725][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0281, 0.0315, 0.0696, 0.0857, 0.1145, 0.0680, 0.0840, 0.0885, 0.1564,
        0.0769, 0.1290, 0.0279, 0.0398], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,726][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0445, 0.0613, 0.1334, 0.1425, 0.0979, 0.0600, 0.0542, 0.0867, 0.0344,
        0.0359, 0.0433, 0.1306, 0.0751], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,728][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([3.5678e-12, 5.8069e-11, 6.2039e-09, 2.3199e-08, 2.4637e-07, 6.1577e-06,
        6.1925e-06, 9.8251e-05, 3.5785e-04, 2.5752e-02, 2.6331e-01, 4.5252e-01,
        2.5795e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,731][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0331, 0.0032, 0.0125, 0.0184, 0.0221, 0.0461, 0.0303, 0.0470, 0.0571,
        0.1149, 0.1943, 0.1687, 0.2524], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,735][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0456, 0.0011, 0.0679, 0.0895, 0.3931, 0.1160, 0.0096, 0.0124, 0.0109,
        0.0520, 0.1056, 0.0854, 0.0109], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,739][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0262, 0.1938, 0.1121, 0.1121, 0.0656, 0.0726, 0.0516, 0.0676, 0.0978,
        0.0553, 0.0922, 0.0400, 0.0133], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,744][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.4099, 0.0366, 0.0225, 0.0392, 0.0475, 0.0240, 0.0462, 0.0376, 0.0225,
        0.0558, 0.1106, 0.1289, 0.0187], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,748][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0160, 0.0316, 0.0608, 0.1133, 0.0952, 0.0695, 0.0931, 0.0884, 0.1173,
        0.0673, 0.0686, 0.0898, 0.0891], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,753][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.9439, 0.0014, 0.0018, 0.0059, 0.0036, 0.0038, 0.0025, 0.0099, 0.0031,
        0.0060, 0.0052, 0.0059, 0.0070], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,756][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0079, 0.1128, 0.0570, 0.0407, 0.0927, 0.0623, 0.0905, 0.1597, 0.1570,
        0.0834, 0.0989, 0.0143, 0.0229], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,757][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.6705, 0.0235, 0.0231, 0.0161, 0.0323, 0.0231, 0.0291, 0.0342, 0.0373,
        0.0305, 0.0392, 0.0185, 0.0226], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,757][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0535, 0.0488, 0.0554, 0.0850, 0.0964, 0.0841, 0.0546, 0.0554, 0.0849,
        0.0303, 0.0805, 0.1794, 0.0917], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:36,758][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0233, 0.0294, 0.0554, 0.0747, 0.1275, 0.0596, 0.0754, 0.0953, 0.1417,
        0.0688, 0.1136, 0.0240, 0.0361, 0.0752], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,761][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0364, 0.0508, 0.1303, 0.1012, 0.0732, 0.0533, 0.0463, 0.0736, 0.0426,
        0.0402, 0.0417, 0.1192, 0.0747, 0.1165], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,763][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([1.4977e-12, 7.1869e-12, 6.8289e-10, 3.5119e-09, 2.9894e-08, 2.9945e-07,
        1.1029e-06, 1.6623e-05, 1.3434e-05, 6.3238e-03, 6.4215e-02, 2.5511e-02,
        2.6563e-01, 6.3829e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,767][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0269, 0.0041, 0.0138, 0.0202, 0.0226, 0.0424, 0.0297, 0.0416, 0.0488,
        0.0881, 0.1511, 0.1295, 0.1862, 0.1950], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,771][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0659, 0.0017, 0.0035, 0.0633, 0.0966, 0.2357, 0.0015, 0.0134, 0.0670,
        0.1030, 0.1953, 0.0470, 0.1041, 0.0019], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,776][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.0302, 0.1896, 0.0995, 0.0868, 0.0520, 0.0931, 0.0519, 0.0538, 0.0735,
        0.0707, 0.0957, 0.0354, 0.0280, 0.0398], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,780][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.4115, 0.0227, 0.0300, 0.0456, 0.0229, 0.0306, 0.0321, 0.0159, 0.0314,
        0.1040, 0.0748, 0.1220, 0.0469, 0.0098], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,785][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0187, 0.0295, 0.0528, 0.1025, 0.0855, 0.0614, 0.0800, 0.0716, 0.1030,
        0.0599, 0.0555, 0.0767, 0.0734, 0.1296], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,788][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.9277, 0.0017, 0.0024, 0.0076, 0.0047, 0.0051, 0.0031, 0.0107, 0.0039,
        0.0073, 0.0056, 0.0078, 0.0089, 0.0036], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,789][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0071, 0.1098, 0.0543, 0.0366, 0.0828, 0.0608, 0.0872, 0.1304, 0.1663,
        0.1070, 0.0784, 0.0114, 0.0457, 0.0224], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,789][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.7072, 0.0182, 0.0185, 0.0131, 0.0259, 0.0185, 0.0235, 0.0264, 0.0294,
        0.0241, 0.0301, 0.0144, 0.0179, 0.0328], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,790][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0550, 0.0662, 0.0647, 0.0633, 0.0678, 0.1015, 0.0494, 0.0448, 0.0810,
        0.0302, 0.0900, 0.1640, 0.0817, 0.0404], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:36,793][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0197, 0.0338, 0.0573, 0.0779, 0.0973, 0.0570, 0.0661, 0.0809, 0.1115,
        0.0715, 0.1185, 0.0261, 0.0345, 0.0975, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,797][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0286, 0.0423, 0.1206, 0.0818, 0.0579, 0.0446, 0.0395, 0.0613, 0.0403,
        0.0369, 0.0365, 0.1015, 0.0658, 0.1047, 0.1377], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,800][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([1.4897e-13, 3.3247e-13, 4.0727e-11, 1.2222e-10, 7.8931e-10, 2.1423e-08,
        2.3186e-08, 9.2423e-07, 1.3346e-06, 5.1148e-04, 4.4922e-03, 3.6695e-03,
        2.2814e-02, 3.2181e-01, 6.4670e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,804][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0243, 0.0033, 0.0108, 0.0161, 0.0184, 0.0334, 0.0240, 0.0327, 0.0385,
        0.0712, 0.1200, 0.1010, 0.1460, 0.1608, 0.1995], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,809][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.2376, 0.0508, 0.0144, 0.0603, 0.1224, 0.0329, 0.0276, 0.0082, 0.0550,
        0.1882, 0.0014, 0.0152, 0.1389, 0.0469, 0.0002], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,814][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.0316, 0.1652, 0.0861, 0.0949, 0.0497, 0.0959, 0.0412, 0.0520, 0.0775,
        0.0599, 0.0845, 0.0357, 0.0293, 0.0742, 0.0223], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,818][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.5917, 0.0145, 0.0221, 0.0226, 0.0258, 0.0126, 0.0215, 0.0100, 0.0220,
        0.0709, 0.0550, 0.0986, 0.0185, 0.0074, 0.0069], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,820][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0161, 0.0249, 0.0430, 0.0973, 0.0779, 0.0511, 0.0675, 0.0618, 0.0814,
        0.0495, 0.0506, 0.0652, 0.0661, 0.1184, 0.1294], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,820][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.9269, 0.0015, 0.0021, 0.0069, 0.0042, 0.0050, 0.0030, 0.0101, 0.0037,
        0.0069, 0.0053, 0.0078, 0.0085, 0.0039, 0.0041], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,821][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0067, 0.1484, 0.0418, 0.0288, 0.0659, 0.0527, 0.0594, 0.1511, 0.1023,
        0.1060, 0.0835, 0.0108, 0.0452, 0.0831, 0.0142], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,822][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.6787, 0.0182, 0.0185, 0.0131, 0.0254, 0.0184, 0.0231, 0.0257, 0.0294,
        0.0239, 0.0295, 0.0143, 0.0178, 0.0327, 0.0314], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,825][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0470, 0.0796, 0.0617, 0.0710, 0.0987, 0.0764, 0.0496, 0.0492, 0.0784,
        0.0227, 0.0680, 0.1444, 0.0681, 0.0528, 0.0323], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:36,829][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.0198, 0.0265, 0.0778, 0.0666, 0.0852, 0.0574, 0.0648, 0.0660, 0.1118,
        0.0539, 0.1011, 0.0258, 0.0276, 0.0837, 0.0570, 0.0749],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,833][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0310, 0.0457, 0.0990, 0.0978, 0.0696, 0.0444, 0.0402, 0.0606, 0.0266,
        0.0273, 0.0331, 0.0968, 0.0567, 0.0907, 0.1272, 0.0532],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,836][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([4.1543e-13, 1.3937e-13, 2.5486e-11, 1.9025e-11, 2.8568e-10, 5.2433e-09,
        6.7805e-09, 1.2013e-07, 1.3032e-07, 4.1209e-05, 1.9791e-04, 3.2951e-04,
        1.9884e-03, 3.3831e-02, 4.9227e-01, 4.7134e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,841][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0233, 0.0024, 0.0083, 0.0118, 0.0139, 0.0256, 0.0183, 0.0253, 0.0311,
        0.0592, 0.0961, 0.0830, 0.1218, 0.1365, 0.1733, 0.1702],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,843][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([5.0356e-02, 1.6568e-02, 4.0628e-02, 1.1114e-02, 5.0307e-02, 7.8281e-02,
        1.3025e-02, 6.8230e-05, 4.4136e-02, 1.8002e-01, 2.9993e-01, 2.1111e-02,
        2.2779e-02, 2.0875e-03, 1.6560e-01, 3.9864e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,848][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0291, 0.1305, 0.0917, 0.1024, 0.0603, 0.0756, 0.0346, 0.0561, 0.0675,
        0.0523, 0.0867, 0.0376, 0.0314, 0.0766, 0.0455, 0.0222],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,851][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.4570, 0.0449, 0.0261, 0.0193, 0.0348, 0.0130, 0.0365, 0.0147, 0.0108,
        0.0662, 0.1391, 0.0907, 0.0198, 0.0091, 0.0078, 0.0103],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,852][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0155, 0.0207, 0.0414, 0.0846, 0.0642, 0.0466, 0.0603, 0.0600, 0.0727,
        0.0486, 0.0489, 0.0660, 0.0623, 0.1056, 0.1223, 0.0803],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,853][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.9324, 0.0014, 0.0016, 0.0052, 0.0034, 0.0036, 0.0024, 0.0091, 0.0031,
        0.0060, 0.0050, 0.0055, 0.0068, 0.0029, 0.0034, 0.0081],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,854][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0086, 0.1297, 0.0519, 0.0194, 0.0713, 0.0635, 0.0681, 0.1191, 0.1269,
        0.0931, 0.0800, 0.0118, 0.0502, 0.0666, 0.0291, 0.0107],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,857][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.6272, 0.0198, 0.0197, 0.0139, 0.0276, 0.0202, 0.0251, 0.0291, 0.0323,
        0.0264, 0.0332, 0.0160, 0.0195, 0.0346, 0.0341, 0.0213],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,861][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0396, 0.0483, 0.0457, 0.0978, 0.0762, 0.0752, 0.0515, 0.0457, 0.0657,
        0.0235, 0.0657, 0.1371, 0.0710, 0.0426, 0.0443, 0.0700],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:36,865][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0179, 0.0248, 0.0541, 0.0646, 0.0841, 0.0535, 0.0607, 0.0701, 0.0983,
        0.0582, 0.0902, 0.0237, 0.0299, 0.0909, 0.0587, 0.0884, 0.0318],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,870][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0302, 0.0421, 0.0912, 0.0973, 0.0679, 0.0418, 0.0387, 0.0601, 0.0238,
        0.0251, 0.0306, 0.0901, 0.0524, 0.0841, 0.1175, 0.0510, 0.0561],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,873][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([5.9142e-15, 3.3729e-15, 5.5361e-13, 2.1185e-12, 1.0971e-11, 4.0029e-10,
        1.7700e-10, 7.8345e-09, 1.8222e-08, 3.9302e-06, 1.8033e-05, 1.9623e-05,
        4.7851e-05, 3.9717e-03, 1.9714e-02, 8.5351e-01, 1.2271e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,877][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0170, 0.0014, 0.0052, 0.0072, 0.0088, 0.0170, 0.0122, 0.0174, 0.0215,
        0.0428, 0.0727, 0.0607, 0.0915, 0.1049, 0.1337, 0.1338, 0.2523],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,882][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0076, 0.0011, 0.0074, 0.0231, 0.3779, 0.0592, 0.0166, 0.0018, 0.0021,
        0.0775, 0.3537, 0.0085, 0.0092, 0.0121, 0.0015, 0.0392, 0.0015],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,883][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0188, 0.1587, 0.0795, 0.0866, 0.0591, 0.0711, 0.0470, 0.0510, 0.0907,
        0.0508, 0.0858, 0.0322, 0.0234, 0.0583, 0.0416, 0.0301, 0.0153],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,884][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2541, 0.0427, 0.0310, 0.0536, 0.1068, 0.0227, 0.0515, 0.0383, 0.0286,
        0.0271, 0.1212, 0.0791, 0.0136, 0.0380, 0.0167, 0.0683, 0.0067],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,885][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0118, 0.0192, 0.0378, 0.0738, 0.0608, 0.0446, 0.0590, 0.0543, 0.0738,
        0.0422, 0.0418, 0.0587, 0.0563, 0.1032, 0.1192, 0.0795, 0.0642],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,886][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.9258, 0.0011, 0.0015, 0.0046, 0.0029, 0.0031, 0.0021, 0.0086, 0.0027,
        0.0052, 0.0046, 0.0052, 0.0063, 0.0028, 0.0032, 0.0081, 0.0123],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,889][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0069, 0.1111, 0.0543, 0.0353, 0.0841, 0.0631, 0.0778, 0.1213, 0.1352,
        0.0622, 0.0767, 0.0125, 0.0202, 0.0674, 0.0318, 0.0264, 0.0138],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,893][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.6137, 0.0190, 0.0190, 0.0134, 0.0269, 0.0195, 0.0244, 0.0281, 0.0314,
        0.0256, 0.0323, 0.0154, 0.0188, 0.0340, 0.0332, 0.0207, 0.0249],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,897][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0379, 0.0384, 0.0395, 0.0655, 0.0651, 0.0733, 0.0386, 0.0331, 0.0781,
        0.0194, 0.0554, 0.1526, 0.0779, 0.0365, 0.0277, 0.0908, 0.0703],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:36,902][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0185, 0.0163, 0.0511, 0.0611, 0.0874, 0.0574, 0.0584, 0.0663, 0.1178,
        0.0508, 0.0933, 0.0228, 0.0267, 0.0622, 0.0674, 0.0881, 0.0338, 0.0205],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,907][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0265, 0.0350, 0.0910, 0.0629, 0.0482, 0.0393, 0.0336, 0.0521, 0.0356,
        0.0316, 0.0309, 0.0801, 0.0534, 0.0814, 0.1028, 0.0523, 0.0559, 0.0874],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,909][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([2.2249e-14, 5.8149e-15, 7.2517e-13, 2.1910e-12, 3.1322e-11, 8.7118e-10,
        6.0897e-10, 1.5626e-08, 1.4016e-08, 3.9791e-06, 1.4614e-05, 2.1475e-05,
        7.9033e-05, 1.2909e-03, 1.0708e-02, 9.8814e-02, 3.6435e-01, 5.2471e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,914][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0174, 0.0019, 0.0064, 0.0088, 0.0101, 0.0177, 0.0135, 0.0177, 0.0212,
        0.0397, 0.0656, 0.0549, 0.0784, 0.0896, 0.1118, 0.1091, 0.1906, 0.1454],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,915][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0061, 0.0030, 0.0595, 0.0213, 0.4792, 0.0576, 0.2267, 0.0647, 0.0217,
        0.0225, 0.0068, 0.0030, 0.0045, 0.0029, 0.0044, 0.0135, 0.0018, 0.0009],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,916][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0338, 0.0525, 0.0675, 0.1082, 0.0630, 0.0874, 0.0435, 0.0518, 0.0693,
        0.0608, 0.0872, 0.0278, 0.0336, 0.0600, 0.0435, 0.0544, 0.0342, 0.0215],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,917][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.2026, 0.0476, 0.0382, 0.0284, 0.0317, 0.0742, 0.0311, 0.0315, 0.0212,
        0.1619, 0.0944, 0.0461, 0.0560, 0.0250, 0.0217, 0.0423, 0.0205, 0.0254],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,918][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0151, 0.0203, 0.0352, 0.0760, 0.0647, 0.0479, 0.0528, 0.0496, 0.0716,
        0.0456, 0.0395, 0.0531, 0.0551, 0.0889, 0.0899, 0.0769, 0.0571, 0.0607],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,921][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9026, 0.0013, 0.0020, 0.0059, 0.0036, 0.0043, 0.0026, 0.0090, 0.0033,
        0.0063, 0.0051, 0.0068, 0.0079, 0.0033, 0.0038, 0.0099, 0.0145, 0.0080],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,925][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0047, 0.0360, 0.0513, 0.0295, 0.0971, 0.0450, 0.0872, 0.1547, 0.1219,
        0.0872, 0.0834, 0.0107, 0.0354, 0.0633, 0.0286, 0.0220, 0.0283, 0.0136],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,929][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.7066, 0.0129, 0.0129, 0.0087, 0.0184, 0.0129, 0.0165, 0.0186, 0.0212,
        0.0171, 0.0218, 0.0097, 0.0122, 0.0247, 0.0233, 0.0138, 0.0168, 0.0320],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,934][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0492, 0.0436, 0.0465, 0.0475, 0.0743, 0.0738, 0.0352, 0.0367, 0.0797,
        0.0324, 0.0699, 0.0962, 0.0748, 0.0319, 0.0257, 0.0772, 0.0652, 0.0402],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:36,939][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.0160, 0.0380, 0.0324, 0.0610, 0.0647, 0.0381, 0.0601, 0.0701, 0.0902,
        0.0510, 0.0827, 0.0214, 0.0257, 0.0855, 0.0674, 0.0804, 0.0300, 0.0472,
        0.0380], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,943][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0212, 0.0322, 0.0830, 0.0655, 0.0472, 0.0317, 0.0274, 0.0443, 0.0228,
        0.0211, 0.0236, 0.0699, 0.0410, 0.0686, 0.0980, 0.0386, 0.0417, 0.0843,
        0.1380], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,946][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([1.0538e-15, 4.7018e-16, 2.5005e-15, 2.3553e-14, 1.7629e-13, 4.6348e-12,
        3.4900e-12, 6.0847e-11, 2.1359e-10, 2.8369e-08, 1.2491e-07, 8.2021e-07,
        1.1781e-06, 1.1675e-04, 3.3574e-04, 1.5096e-03, 1.7032e-02, 1.8117e-01,
        7.9983e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,947][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0168, 0.0015, 0.0049, 0.0071, 0.0082, 0.0145, 0.0109, 0.0147, 0.0171,
        0.0329, 0.0560, 0.0443, 0.0640, 0.0743, 0.0913, 0.0891, 0.1570, 0.1225,
        0.1728], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,948][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([3.9045e-02, 1.7181e-01, 2.7034e-04, 2.1309e-02, 6.0668e-03, 2.2270e-02,
        7.5512e-03, 3.0635e-04, 3.2820e-02, 4.9004e-01, 6.9245e-03, 4.2086e-02,
        2.2486e-02, 2.2868e-03, 1.5342e-03, 1.5871e-02, 3.6966e-02, 8.0176e-02,
        1.8611e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,949][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0289, 0.1198, 0.0272, 0.0749, 0.0531, 0.0765, 0.0484, 0.0468, 0.0518,
        0.0650, 0.0821, 0.0273, 0.0355, 0.0516, 0.0485, 0.0421, 0.0494, 0.0593,
        0.0119], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,950][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.2347, 0.0436, 0.0267, 0.0332, 0.0664, 0.0168, 0.0422, 0.0443, 0.0265,
        0.1006, 0.0903, 0.1016, 0.0223, 0.0267, 0.0113, 0.0504, 0.0178, 0.0275,
        0.0169], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,953][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0136, 0.0180, 0.0325, 0.0672, 0.0577, 0.0388, 0.0552, 0.0479, 0.0599,
        0.0419, 0.0392, 0.0537, 0.0522, 0.0863, 0.0994, 0.0709, 0.0568, 0.0550,
        0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,957][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.9058, 0.0012, 0.0016, 0.0052, 0.0033, 0.0037, 0.0023, 0.0088, 0.0030,
        0.0057, 0.0047, 0.0057, 0.0068, 0.0030, 0.0032, 0.0085, 0.0128, 0.0084,
        0.0063], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,962][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0064, 0.1427, 0.0116, 0.0325, 0.0722, 0.0413, 0.0574, 0.0930, 0.1034,
        0.1007, 0.0595, 0.0081, 0.0425, 0.0524, 0.0237, 0.0240, 0.0334, 0.0831,
        0.0120], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,966][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.6230, 0.0156, 0.0158, 0.0110, 0.0220, 0.0157, 0.0200, 0.0228, 0.0255,
        0.0207, 0.0260, 0.0122, 0.0152, 0.0282, 0.0273, 0.0169, 0.0201, 0.0379,
        0.0240], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,971][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0392, 0.0552, 0.0383, 0.0463, 0.0747, 0.0556, 0.0366, 0.0443, 0.0632,
        0.0243, 0.0649, 0.0923, 0.0588, 0.0320, 0.0333, 0.0740, 0.0637, 0.0574,
        0.0456], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:36,976][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0163, 0.0240, 0.0483, 0.0380, 0.0764, 0.0442, 0.0605, 0.0671, 0.0963,
        0.0471, 0.0856, 0.0193, 0.0260, 0.0681, 0.0593, 0.0651, 0.0287, 0.0299,
        0.0556, 0.0442], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,979][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0222, 0.0320, 0.0686, 0.0710, 0.0496, 0.0316, 0.0286, 0.0439, 0.0182,
        0.0190, 0.0233, 0.0686, 0.0401, 0.0633, 0.0895, 0.0371, 0.0420, 0.0847,
        0.1134, 0.0533], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,980][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([8.6365e-16, 5.7345e-17, 6.4252e-15, 3.8600e-16, 4.9527e-14, 7.2637e-13,
        6.7677e-13, 9.5431e-12, 8.7192e-12, 5.6066e-09, 1.7418e-08, 3.3114e-08,
        1.2457e-07, 2.5692e-06, 2.3978e-05, 2.7071e-04, 1.7558e-03, 8.3482e-03,
        9.2022e-01, 6.9383e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,981][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0130, 0.0013, 0.0043, 0.0058, 0.0070, 0.0120, 0.0092, 0.0121, 0.0140,
        0.0267, 0.0451, 0.0358, 0.0510, 0.0599, 0.0741, 0.0707, 0.1245, 0.1000,
        0.1417, 0.1920], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,982][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([1.4335e-03, 8.2471e-03, 4.3187e-02, 1.5537e-04, 8.6343e-01, 3.0866e-02,
        2.7972e-04, 5.8645e-05, 8.9103e-04, 1.6743e-02, 5.9770e-03, 1.9638e-03,
        4.2543e-03, 7.7142e-04, 2.9685e-04, 7.4482e-04, 1.3779e-03, 2.6613e-03,
        1.6598e-02, 6.0476e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,985][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0253, 0.1085, 0.0702, 0.0523, 0.0487, 0.0736, 0.0408, 0.0590, 0.0582,
        0.0545, 0.0708, 0.0327, 0.0341, 0.0601, 0.0384, 0.0295, 0.0366, 0.0503,
        0.0295, 0.0269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,988][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.2040, 0.0677, 0.0148, 0.0183, 0.0837, 0.0336, 0.0383, 0.0231, 0.0363,
        0.0818, 0.2119, 0.0778, 0.0121, 0.0088, 0.0087, 0.0112, 0.0051, 0.0425,
        0.0083, 0.0120], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,993][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0112, 0.0154, 0.0323, 0.0585, 0.0498, 0.0354, 0.0477, 0.0436, 0.0544,
        0.0376, 0.0360, 0.0519, 0.0467, 0.0796, 0.0907, 0.0644, 0.0535, 0.0498,
        0.0565, 0.0848], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:36,998][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.8903, 0.0014, 0.0018, 0.0052, 0.0034, 0.0036, 0.0024, 0.0091, 0.0030,
        0.0058, 0.0050, 0.0055, 0.0067, 0.0029, 0.0034, 0.0084, 0.0129, 0.0096,
        0.0068, 0.0129], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,002][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0073, 0.1010, 0.0563, 0.0128, 0.0589, 0.0458, 0.0602, 0.1093, 0.0986,
        0.0825, 0.0691, 0.0117, 0.0400, 0.0556, 0.0251, 0.0121, 0.0275, 0.0552,
        0.0637, 0.0072], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,007][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.5534, 0.0176, 0.0175, 0.0124, 0.0247, 0.0180, 0.0225, 0.0260, 0.0285,
        0.0234, 0.0294, 0.0142, 0.0173, 0.0310, 0.0301, 0.0189, 0.0225, 0.0428,
        0.0271, 0.0226], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,011][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0358, 0.0349, 0.0304, 0.0469, 0.0658, 0.0565, 0.0318, 0.0341, 0.0688,
        0.0242, 0.0609, 0.1129, 0.0725, 0.0221, 0.0253, 0.0776, 0.0715, 0.0403,
        0.0386, 0.0492], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,012][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0146, 0.0210, 0.0445, 0.0537, 0.0698, 0.0447, 0.0503, 0.0586, 0.0804,
        0.0487, 0.0754, 0.0195, 0.0245, 0.0761, 0.0489, 0.0727, 0.0255, 0.0278,
        0.0537, 0.0642, 0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,013][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0222, 0.0304, 0.0632, 0.0743, 0.0508, 0.0301, 0.0276, 0.0443, 0.0157,
        0.0169, 0.0214, 0.0648, 0.0363, 0.0587, 0.0835, 0.0347, 0.0385, 0.0827,
        0.1059, 0.0518, 0.0461], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,014][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([6.6959e-17, 6.5964e-18, 6.2381e-16, 1.6921e-15, 9.1516e-15, 2.2362e-13,
        1.0751e-13, 3.3417e-12, 6.6737e-12, 1.1682e-09, 5.5766e-09, 6.3348e-09,
        1.2088e-08, 9.1002e-07, 3.9532e-06, 1.5569e-04, 2.5152e-05, 1.2986e-03,
        1.5161e-01, 7.4877e-01, 9.8140e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,017][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0093, 0.0007, 0.0026, 0.0036, 0.0043, 0.0079, 0.0059, 0.0080, 0.0096,
        0.0192, 0.0333, 0.0265, 0.0391, 0.0457, 0.0578, 0.0564, 0.1021, 0.0824,
        0.1200, 0.1677, 0.1978], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,020][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([7.9573e-03, 1.0036e-03, 6.9896e-03, 2.2820e-02, 3.7726e-01, 5.6850e-02,
        1.5563e-02, 1.5920e-03, 1.8721e-03, 7.8776e-02, 3.4513e-01, 8.2674e-03,
        9.0615e-03, 1.1154e-02, 1.5666e-03, 3.8505e-02, 1.4708e-03, 3.7518e-04,
        2.9650e-03, 9.2454e-03, 1.5834e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,024][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0166, 0.1362, 0.0666, 0.0733, 0.0508, 0.0606, 0.0402, 0.0444, 0.0767,
        0.0439, 0.0738, 0.0268, 0.0201, 0.0493, 0.0358, 0.0257, 0.0132, 0.0691,
        0.0280, 0.0368, 0.0120], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,029][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2312, 0.0449, 0.0309, 0.0474, 0.0927, 0.0204, 0.0504, 0.0387, 0.0237,
        0.0232, 0.1267, 0.0639, 0.0107, 0.0373, 0.0139, 0.0607, 0.0052, 0.0244,
        0.0180, 0.0305, 0.0052], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,034][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0094, 0.0154, 0.0294, 0.0568, 0.0466, 0.0342, 0.0453, 0.0417, 0.0555,
        0.0322, 0.0322, 0.0449, 0.0424, 0.0783, 0.0881, 0.0596, 0.0480, 0.0523,
        0.0525, 0.0823, 0.0529], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,038][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.8789, 0.0012, 0.0015, 0.0046, 0.0030, 0.0031, 0.0021, 0.0086, 0.0027,
        0.0052, 0.0047, 0.0051, 0.0062, 0.0028, 0.0033, 0.0081, 0.0122, 0.0094,
        0.0067, 0.0128, 0.0180], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,043][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0063, 0.0994, 0.0486, 0.0301, 0.0719, 0.0553, 0.0666, 0.1065, 0.1201,
        0.0536, 0.0683, 0.0109, 0.0174, 0.0610, 0.0286, 0.0229, 0.0115, 0.0466,
        0.0486, 0.0168, 0.0092], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,044][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.5394, 0.0170, 0.0171, 0.0121, 0.0241, 0.0175, 0.0219, 0.0253, 0.0280,
        0.0228, 0.0288, 0.0137, 0.0168, 0.0302, 0.0295, 0.0185, 0.0220, 0.0421,
        0.0266, 0.0222, 0.0244], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,045][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0314, 0.0322, 0.0331, 0.0546, 0.0543, 0.0604, 0.0316, 0.0277, 0.0649,
        0.0161, 0.0462, 0.1238, 0.0633, 0.0303, 0.0228, 0.0750, 0.0565, 0.0338,
        0.0362, 0.0515, 0.0543], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,048][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:37,051][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 3336],
        [ 3317],
        [ 7166],
        [ 7649],
        [ 6868],
        [ 8810],
        [23386],
        [ 8706],
        [ 7855],
        [ 4955],
        [20768],
        [ 8701],
        [ 7364],
        [ 6703],
        [ 2641],
        [ 6510],
        [ 3428],
        [ 1618],
        [ 3894],
        [ 8755],
        [ 3522]], device='cuda:0')
[2024-07-23 21:06:37,054][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 4123],
        [ 2501],
        [ 8131],
        [15217],
        [ 8954],
        [25226],
        [33347],
        [16761],
        [ 8671],
        [ 8225],
        [32065],
        [25894],
        [10798],
        [18274],
        [ 6549],
        [15405],
        [ 6322],
        [ 4613],
        [ 7962],
        [18918],
        [ 7297]], device='cuda:0')
[2024-07-23 21:06:37,058][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[41721],
        [39417],
        [35771],
        [34324],
        [34942],
        [36603],
        [38110],
        [37018],
        [37690],
        [38356],
        [38320],
        [38561],
        [38953],
        [39368],
        [39725],
        [39490],
        [39798],
        [39662],
        [39597],
        [39302],
        [39206]], device='cuda:0')
[2024-07-23 21:06:37,061][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[39633],
        [ 7853],
        [ 6917],
        [ 5013],
        [ 5758],
        [ 4345],
        [ 5542],
        [ 6668],
        [ 7164],
        [ 6155],
        [ 4104],
        [ 8795],
        [ 9708],
        [10576],
        [ 8970],
        [ 8353],
        [ 9264],
        [ 6093],
        [ 6899],
        [ 6541],
        [ 7017]], device='cuda:0')
[2024-07-23 21:06:37,064][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 5241],
        [12156],
        [42362],
        [43871],
        [44149],
        [43600],
        [44005],
        [41853],
        [39966],
        [38529],
        [31904],
        [43686],
        [38471],
        [41692],
        [34504],
        [35530],
        [33917],
        [34445],
        [36980],
        [37342],
        [35244]], device='cuda:0')
[2024-07-23 21:06:37,067][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[11317],
        [14645],
        [15983],
        [19062],
        [16716],
        [14391],
        [13762],
        [13614],
        [13817],
        [14946],
        [14837],
        [15810],
        [17153],
        [16274],
        [16309],
        [17035],
        [16575],
        [16658],
        [16665],
        [17624],
        [17244]], device='cuda:0')
[2024-07-23 21:06:37,070][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[16547],
        [11261],
        [13179],
        [13322],
        [13059],
        [13020],
        [12834],
        [12814],
        [12888],
        [12955],
        [12613],
        [12571],
        [12550],
        [12571],
        [12642],
        [12566],
        [12582],
        [12577],
        [12716],
        [12732],
        [12774]], device='cuda:0')
[2024-07-23 21:06:37,073][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[21205],
        [22967],
        [32817],
        [34310],
        [34518],
        [33019],
        [31838],
        [31586],
        [34894],
        [35627],
        [32189],
        [33584],
        [37080],
        [34392],
        [32862],
        [35523],
        [37757],
        [29784],
        [33602],
        [33703],
        [36381]], device='cuda:0')
[2024-07-23 21:06:37,077][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[41293],
        [40349],
        [42224],
        [40654],
        [33820],
        [38344],
        [31931],
        [30226],
        [30921],
        [26091],
        [24368],
        [29841],
        [32271],
        [30905],
        [33379],
        [30952],
        [28291],
        [25821],
        [28619],
        [27550],
        [27872]], device='cuda:0')
[2024-07-23 21:06:37,080][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[4245],
        [7138],
        [6588],
        [6670],
        [7417],
        [7485],
        [6835],
        [7240],
        [7592],
        [7367],
        [7676],
        [7877],
        [7759],
        [7448],
        [7357],
        [7282],
        [7095],
        [7212],
        [7026],
        [7119],
        [6989]], device='cuda:0')
[2024-07-23 21:06:37,081][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[10516],
        [10671],
        [12186],
        [13131],
        [13436],
        [13332],
        [13413],
        [13323],
        [13165],
        [13267],
        [12868],
        [12791],
        [12707],
        [12613],
        [12552],
        [12443],
        [12324],
        [12134],
        [12125],
        [12057],
        [11979]], device='cuda:0')
[2024-07-23 21:06:37,083][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[34328],
        [25390],
        [ 1101],
        [ 1344],
        [12768],
        [ 3637],
        [ 8405],
        [21024],
        [ 5279],
        [ 5292],
        [20655],
        [ 7484],
        [ 6532],
        [ 9441],
        [ 2828],
        [ 2261],
        [ 4034],
        [ 7458],
        [ 1929],
        [ 2308],
        [ 2758]], device='cuda:0')
[2024-07-23 21:06:37,085][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[5570],
        [ 831],
        [ 801],
        [ 827],
        [ 901],
        [ 940],
        [1036],
        [1152],
        [1165],
        [1233],
        [1294],
        [1335],
        [1270],
        [1167],
        [1144],
        [1134],
        [1111],
        [1082],
        [1087],
        [1058],
        [1043]], device='cuda:0')
[2024-07-23 21:06:37,088][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[27977],
        [32251],
        [33392],
        [27332],
        [27458],
        [24598],
        [24868],
        [27713],
        [25512],
        [23568],
        [24498],
        [24418],
        [25333],
        [26538],
        [26875],
        [24976],
        [25871],
        [25834],
        [28495],
        [26821],
        [27336]], device='cuda:0')
[2024-07-23 21:06:37,091][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 2934],
        [17513],
        [27512],
        [33585],
        [16098],
        [ 6343],
        [27601],
        [21189],
        [21267],
        [25724],
        [19175],
        [ 5798],
        [29161],
        [13602],
        [14339],
        [ 2434],
        [14429],
        [15511],
        [13515],
        [12838],
        [15227]], device='cuda:0')
[2024-07-23 21:06:37,094][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 5937],
        [15629],
        [15502],
        [ 9823],
        [ 9474],
        [ 9894],
        [10704],
        [10839],
        [ 9247],
        [ 8814],
        [ 9310],
        [ 9215],
        [ 9044],
        [ 9317],
        [ 9467],
        [ 9454],
        [ 9701],
        [ 9598],
        [10096],
        [ 9438],
        [ 9489]], device='cuda:0')
[2024-07-23 21:06:37,097][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[8503],
        [6073],
        [6663],
        [6725],
        [6578],
        [6585],
        [6623],
        [6906],
        [6804],
        [6867],
        [6851],
        [6858],
        [7050],
        [7379],
        [7845],
        [8096],
        [8281],
        [8102],
        [8182],
        [8438],
        [8605]], device='cuda:0')
[2024-07-23 21:06:37,100][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 7399],
        [ 6724],
        [16496],
        [19595],
        [15501],
        [ 5904],
        [13196],
        [ 3188],
        [ 5765],
        [ 6428],
        [ 6633],
        [ 8706],
        [ 5971],
        [  655],
        [  641],
        [ 4559],
        [10835],
        [ 3996],
        [10335],
        [14008],
        [17279]], device='cuda:0')
[2024-07-23 21:06:37,104][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[6830],
        [6432],
        [5787],
        [5768],
        [6081],
        [5582],
        [5729],
        [5961],
        [5606],
        [5532],
        [5675],
        [5434],
        [5374],
        [5616],
        [5522],
        [5434],
        [5435],
        [5419],
        [5318],
        [5282],
        [5263]], device='cuda:0')
[2024-07-23 21:06:37,107][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 4797],
        [ 3931],
        [ 7539],
        [ 5091],
        [10137],
        [ 7514],
        [13754],
        [ 7736],
        [ 6791],
        [10878],
        [ 5474],
        [ 4376],
        [11818],
        [ 8939],
        [ 9917],
        [ 2606],
        [ 7571],
        [ 7590],
        [10970],
        [ 7337],
        [ 7821]], device='cuda:0')
[2024-07-23 21:06:37,110][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[770],
        [305],
        [271],
        [474],
        [476],
        [524],
        [539],
        [555],
        [464],
        [514],
        [454],
        [455],
        [448],
        [409],
        [435],
        [448],
        [427],
        [469],
        [389],
        [400],
        [421]], device='cuda:0')
[2024-07-23 21:06:37,113][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[18187],
        [19061],
        [20374],
        [18873],
        [ 2039],
        [21698],
        [15459],
        [ 5606],
        [ 7546],
        [ 1926],
        [ 4449],
        [22378],
        [14652],
        [15214],
        [23470],
        [18152],
        [ 4137],
        [ 3888],
        [ 3171],
        [ 3113],
        [ 2928]], device='cuda:0')
[2024-07-23 21:06:37,116][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[10793],
        [ 8737],
        [ 7780],
        [ 7356],
        [ 6950],
        [ 7087],
        [ 7292],
        [ 7317],
        [ 7465],
        [ 7599],
        [ 7426],
        [ 7731],
        [ 7883],
        [ 7814],
        [ 7860],
        [ 7821],
        [ 8019],
        [ 7894],
        [ 7846],
        [ 7796],
        [ 7864]], device='cuda:0')
[2024-07-23 21:06:37,118][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[21416],
        [21414],
        [21417],
        [21412],
        [21409],
        [21392],
        [21393],
        [21394],
        [21391],
        [21406],
        [21422],
        [21434],
        [21456],
        [21464],
        [21467],
        [21482],
        [21533],
        [21543],
        [21542],
        [21545],
        [21612]], device='cuda:0')
[2024-07-23 21:06:37,119][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 6314],
        [10550],
        [10657],
        [ 7904],
        [ 9462],
        [ 9805],
        [ 9047],
        [ 9384],
        [ 9272],
        [ 8984],
        [ 9050],
        [ 8988],
        [ 9163],
        [ 8784],
        [ 8938],
        [ 8637],
        [ 8652],
        [ 8619],
        [ 8960],
        [ 8218],
        [ 8404]], device='cuda:0')
[2024-07-23 21:06:37,122][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[9669],
        [8979],
        [8233],
        [7345],
        [7663],
        [6685],
        [6690],
        [6717],
        [5835],
        [5563],
        [6125],
        [4595],
        [4473],
        [4603],
        [4406],
        [4118],
        [4060],
        [4567],
        [4128],
        [3902],
        [3872]], device='cuda:0')
[2024-07-23 21:06:37,124][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[2124],
        [1944],
        [2147],
        [2466],
        [2160],
        [1816],
        [1801],
        [1799],
        [1857],
        [1964],
        [1898],
        [1783],
        [1691],
        [1668],
        [1691],
        [1760],
        [1716],
        [1770],
        [1782],
        [1798],
        [1776]], device='cuda:0')
[2024-07-23 21:06:37,127][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[10475],
        [10070],
        [ 7084],
        [ 7284],
        [ 8950],
        [ 8173],
        [ 8166],
        [10570],
        [10260],
        [ 9642],
        [ 9461],
        [ 9023],
        [ 9950],
        [13479],
        [11759],
        [10491],
        [10939],
        [12575],
        [10925],
        [10846],
        [10668]], device='cuda:0')
[2024-07-23 21:06:37,130][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[32839],
        [17289],
        [15415],
        [ 9642],
        [26744],
        [28497],
        [ 7913],
        [11065],
        [19109],
        [19747],
        [23875],
        [29629],
        [13222],
        [16561],
        [23214],
        [36366],
        [20833],
        [26427],
        [24500],
        [25902],
        [22969]], device='cuda:0')
[2024-07-23 21:06:37,134][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049],
        [8049]], device='cuda:0')
[2024-07-23 21:06:37,175][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:37,178][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,179][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,180][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,180][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,181][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,182][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,182][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,183][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,184][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,187][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,190][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,190][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,194][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.4877, 0.5123], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,199][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.2730, 0.7270], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,202][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ language] are: tensor([1.5608e-07, 1.0000e+00], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,202][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0548, 0.9452], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,203][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.6732, 0.3268], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,204][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7184, 0.2816], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,205][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0671, 0.9329], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,207][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4354, 0.5646], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,211][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.1697, 0.8303], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,215][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.6713, 0.3287], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,220][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.3610, 0.6390], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,225][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.5477, 0.4523], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,229][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.3271, 0.3360, 0.3370], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,234][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.1104, 0.4219, 0.4678], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,234][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ used] are: tensor([7.5468e-09, 5.8816e-03, 9.9412e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,235][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0299, 0.9286, 0.0415], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,236][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.6544, 0.1998, 0.1458], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,237][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.5257, 0.3993, 0.0750], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,239][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0248, 0.2895, 0.6857], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,242][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.2333, 0.7524, 0.0142], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,246][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0525, 0.6754, 0.2721], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,250][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.3274, 0.4850, 0.1876], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,255][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.2377, 0.3883, 0.3741], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,260][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.2304, 0.3266, 0.4430], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:37,264][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.2392, 0.2482, 0.2519, 0.2607], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,266][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0881, 0.3034, 0.4259, 0.1827], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,267][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ by] are: tensor([1.2601e-08, 8.6101e-05, 4.8984e-02, 9.5093e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,268][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0436, 0.7633, 0.1008, 0.0924], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,268][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.3349, 0.2540, 0.2022, 0.2089], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,269][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.4363, 0.3742, 0.1843, 0.0051], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,272][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0126, 0.1113, 0.2194, 0.6567], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,275][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0723, 0.8605, 0.0431, 0.0241], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,280][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0199, 0.3829, 0.3439, 0.2533], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,281][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0678, 0.5452, 0.3792, 0.0078], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,284][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.1570, 0.2882, 0.2720, 0.2828], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,289][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.1428, 0.0943, 0.6417, 0.1212], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:37,294][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.1906, 0.1945, 0.1943, 0.1991, 0.2215], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,298][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0426, 0.2522, 0.3513, 0.1884, 0.1654], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,300][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([7.0140e-09, 6.5236e-05, 3.8596e-02, 9.5197e-01, 9.3690e-03],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,301][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0051, 0.0923, 0.0114, 0.1325, 0.7587], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,302][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.3521, 0.1844, 0.1400, 0.1833, 0.1403], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,302][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.2529, 0.4147, 0.2386, 0.0361, 0.0577], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,305][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0108, 0.0953, 0.2087, 0.5923, 0.0929], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,308][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0895, 0.6407, 0.0889, 0.0956, 0.0854], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,312][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.0069, 0.3626, 0.1518, 0.4183, 0.0604], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,316][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.2994, 0.3932, 0.0747, 0.0781, 0.1546], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,321][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.1315, 0.2316, 0.2388, 0.2110, 0.1871], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,326][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.0635, 0.2526, 0.3741, 0.2515, 0.0582], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:37,330][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1575, 0.1604, 0.1599, 0.1649, 0.1824, 0.1749], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,332][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0428, 0.1878, 0.2485, 0.1556, 0.1906, 0.1746], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,333][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ B] are: tensor([1.0342e-09, 1.8073e-05, 1.1046e-02, 9.1532e-01, 1.5924e-02, 5.7693e-02],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,334][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0016, 0.0780, 0.0047, 0.0472, 0.8033, 0.0653], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,334][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.2263, 0.2192, 0.1551, 0.1620, 0.1232, 0.1141], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,335][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.4685, 0.1564, 0.2287, 0.0159, 0.1135, 0.0170], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,338][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0049, 0.0730, 0.1775, 0.5566, 0.0841, 0.1038], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,342][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0502, 0.7163, 0.0321, 0.0461, 0.1326, 0.0226], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,346][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.0070, 0.3207, 0.1725, 0.3150, 0.1323, 0.0525], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,351][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.2318, 0.3189, 0.1626, 0.0826, 0.1438, 0.0603], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,356][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0956, 0.2207, 0.1756, 0.1722, 0.1785, 0.1574], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,361][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.0668, 0.1378, 0.4099, 0.2106, 0.1178, 0.0571], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:37,366][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.1360, 0.1345, 0.1354, 0.1409, 0.1525, 0.1472, 0.1534],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,366][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0312, 0.1786, 0.2122, 0.1256, 0.1425, 0.1722, 0.1376],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,367][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [aut] are: tensor([1.4969e-09, 1.7033e-05, 4.8649e-03, 8.4577e-01, 6.2647e-03, 9.7290e-02,
        4.5789e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,368][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.0033, 0.1055, 0.0099, 0.0840, 0.5721, 0.2196, 0.0056],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,369][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.1856, 0.2025, 0.1578, 0.1430, 0.1222, 0.1036, 0.0853],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,371][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.2707, 0.2965, 0.0961, 0.0772, 0.1264, 0.0547, 0.0784],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,375][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.0088, 0.0701, 0.1540, 0.4674, 0.0760, 0.0933, 0.1303],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,380][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0681, 0.6383, 0.0523, 0.0561, 0.1270, 0.0457, 0.0126],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,385][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0078, 0.2881, 0.2060, 0.3003, 0.1171, 0.0598, 0.0210],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,390][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.1445, 0.4634, 0.0761, 0.1237, 0.1223, 0.0556, 0.0144],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,395][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0992, 0.1896, 0.1633, 0.1625, 0.1391, 0.1212, 0.1252],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,399][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0768, 0.0941, 0.3592, 0.1994, 0.1041, 0.0595, 0.1068],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:37,400][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.1154, 0.1166, 0.1170, 0.1202, 0.1324, 0.1279, 0.1342, 0.1363],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,401][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0214, 0.1209, 0.2397, 0.0988, 0.1349, 0.1647, 0.1383, 0.0814],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,402][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ista] are: tensor([9.1067e-10, 6.9821e-08, 1.6989e-05, 1.4664e-03, 7.4617e-05, 2.3000e-04,
        4.1671e-04, 9.9780e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,403][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.0030, 0.0958, 0.0068, 0.0582, 0.5817, 0.2239, 0.0125, 0.0181],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,406][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.2029, 0.1326, 0.1553, 0.1286, 0.1018, 0.0714, 0.0742, 0.1331],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,409][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.2085, 0.1734, 0.1140, 0.0261, 0.1282, 0.1025, 0.1654, 0.0819],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,414][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.0047, 0.0589, 0.1462, 0.4443, 0.0639, 0.0995, 0.1244, 0.0581],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,419][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.1182, 0.3744, 0.0631, 0.0676, 0.1577, 0.1063, 0.0502, 0.0626],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,424][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0041, 0.2990, 0.2211, 0.2418, 0.1304, 0.0482, 0.0342, 0.0212],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,430][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.2467, 0.1223, 0.1434, 0.1736, 0.1948, 0.0333, 0.0160, 0.0699],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,433][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0851, 0.1666, 0.1483, 0.1416, 0.1366, 0.1154, 0.1122, 0.0942],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,434][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.0538, 0.1060, 0.3328, 0.1907, 0.0537, 0.0517, 0.1701, 0.0412],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:37,435][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.1006, 0.1028, 0.1032, 0.1061, 0.1178, 0.1127, 0.1186, 0.1192, 0.1188],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,435][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0215, 0.1272, 0.2239, 0.0951, 0.1174, 0.1377, 0.1241, 0.0826, 0.0704],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,436][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ de] are: tensor([1.1363e-10, 5.8976e-08, 6.4457e-05, 3.0094e-03, 3.0578e-05, 1.8932e-04,
        2.1586e-04, 9.9519e-01, 1.2980e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,439][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0015, 0.1158, 0.0049, 0.0368, 0.6090, 0.1038, 0.0217, 0.0597, 0.0468],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,444][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.2307, 0.1124, 0.1187, 0.1229, 0.0877, 0.0721, 0.0815, 0.1220, 0.0520],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,449][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.2143, 0.1883, 0.2577, 0.0189, 0.0829, 0.0436, 0.0794, 0.0840, 0.0309],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,453][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.0031, 0.0446, 0.1205, 0.4424, 0.0591, 0.0780, 0.1109, 0.0632, 0.0783],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,458][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0636, 0.5470, 0.0249, 0.0484, 0.1110, 0.0389, 0.0268, 0.1320, 0.0073],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,463][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0073, 0.2892, 0.1352, 0.2962, 0.1023, 0.0496, 0.0227, 0.0473, 0.0504],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,467][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.2123, 0.2583, 0.0350, 0.0637, 0.1898, 0.0242, 0.0358, 0.1270, 0.0539],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,468][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0721, 0.1650, 0.1241, 0.1383, 0.1243, 0.1015, 0.1080, 0.0803, 0.0863],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,468][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0508, 0.1016, 0.3107, 0.1647, 0.0548, 0.0667, 0.1383, 0.0661, 0.0462],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:37,469][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0901, 0.0929, 0.0927, 0.0946, 0.1055, 0.1004, 0.1064, 0.1076, 0.1058,
        0.1041], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,471][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0153, 0.1127, 0.1493, 0.0759, 0.0898, 0.1018, 0.1082, 0.1064, 0.0930,
        0.1476], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,473][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ An] are: tensor([9.7253e-11, 2.2839e-08, 1.1525e-05, 7.4746e-04, 4.0686e-05, 1.2045e-04,
        1.9475e-04, 9.9488e-01, 2.8387e-03, 1.1629e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,478][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0011, 0.0620, 0.0068, 0.0649, 0.5501, 0.1009, 0.0067, 0.0373, 0.1185,
        0.0517], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,483][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.1930, 0.1196, 0.0993, 0.1212, 0.1028, 0.0738, 0.0829, 0.0988, 0.0532,
        0.0555], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,488][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0959, 0.1662, 0.1672, 0.0097, 0.1813, 0.0564, 0.1252, 0.1086, 0.0372,
        0.0522], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,493][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0026, 0.0433, 0.1174, 0.4061, 0.0526, 0.0709, 0.1066, 0.0563, 0.0728,
        0.0716], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,498][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0347, 0.5840, 0.0301, 0.0362, 0.1045, 0.0432, 0.0266, 0.0874, 0.0156,
        0.0377], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,500][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0030, 0.3323, 0.1521, 0.2062, 0.0927, 0.0483, 0.0243, 0.0262, 0.0931,
        0.0218], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,501][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0849, 0.3347, 0.1256, 0.0493, 0.0934, 0.0250, 0.0327, 0.1467, 0.0895,
        0.0182], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,502][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0727, 0.1424, 0.1281, 0.1243, 0.1015, 0.0895, 0.0917, 0.0769, 0.0752,
        0.0976], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,503][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0343, 0.1938, 0.1686, 0.1906, 0.0495, 0.0310, 0.0932, 0.0555, 0.1451,
        0.0385], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:37,505][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0832, 0.0845, 0.0843, 0.0863, 0.0947, 0.0915, 0.0957, 0.0965, 0.0958,
        0.0941, 0.0935], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,508][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0147, 0.0819, 0.1589, 0.0512, 0.0751, 0.1209, 0.0918, 0.0786, 0.0773,
        0.1874, 0.0623], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,511][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [za] are: tensor([4.8395e-11, 4.5630e-08, 2.4208e-05, 9.3007e-04, 2.3134e-05, 1.8754e-04,
        4.1737e-04, 9.7773e-01, 5.6861e-03, 1.0996e-02, 4.0097e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,516][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.0013, 0.0708, 0.0041, 0.0589, 0.4169, 0.0867, 0.0068, 0.0546, 0.0999,
        0.1804, 0.0195], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,522][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.2431, 0.0975, 0.1009, 0.1182, 0.0925, 0.0683, 0.0705, 0.0857, 0.0422,
        0.0481, 0.0331], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,526][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.1165, 0.1241, 0.1247, 0.0303, 0.0918, 0.0577, 0.0897, 0.0379, 0.0444,
        0.2486, 0.0342], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,531][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0030, 0.0417, 0.1156, 0.3725, 0.0483, 0.0767, 0.0996, 0.0512, 0.0759,
        0.0844, 0.0312], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,534][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0948, 0.3111, 0.0319, 0.0619, 0.0726, 0.0702, 0.0485, 0.0847, 0.0233,
        0.1829, 0.0180], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,534][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0040, 0.2837, 0.1318, 0.2188, 0.0953, 0.0469, 0.0193, 0.0414, 0.0848,
        0.0520, 0.0220], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,535][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.1328, 0.0832, 0.0531, 0.0733, 0.0689, 0.0570, 0.0182, 0.1780, 0.1491,
        0.1181, 0.0683], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,536][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0625, 0.1236, 0.1075, 0.1131, 0.1025, 0.0879, 0.0761, 0.0667, 0.0776,
        0.0991, 0.0836], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,538][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0480, 0.0717, 0.1282, 0.1900, 0.0292, 0.0316, 0.0930, 0.0562, 0.2037,
        0.1048, 0.0435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:37,542][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0735, 0.0763, 0.0768, 0.0794, 0.0888, 0.0835, 0.0895, 0.0896, 0.0881,
        0.0864, 0.0865, 0.0816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,547][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0167, 0.0820, 0.0870, 0.0434, 0.0719, 0.0937, 0.0948, 0.0894, 0.0792,
        0.2236, 0.0736, 0.0447], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,550][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ is] are: tensor([5.5730e-11, 9.0368e-08, 3.3772e-05, 2.4667e-03, 1.9873e-05, 2.7592e-04,
        3.2735e-04, 9.7366e-01, 2.8619e-03, 7.5266e-03, 4.4227e-03, 8.4042e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,555][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0024, 0.0613, 0.0094, 0.0365, 0.4105, 0.0975, 0.0131, 0.0310, 0.0740,
        0.1954, 0.0580, 0.0110], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,559][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1349, 0.1142, 0.1351, 0.1074, 0.0816, 0.0663, 0.0657, 0.0884, 0.0608,
        0.0523, 0.0331, 0.0601], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,564][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1317, 0.0464, 0.2117, 0.0127, 0.0743, 0.0309, 0.0633, 0.1227, 0.0554,
        0.1600, 0.0769, 0.0140], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,567][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0017, 0.0387, 0.1152, 0.3607, 0.0463, 0.0698, 0.1027, 0.0489, 0.0712,
        0.0799, 0.0290, 0.0360], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,568][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0463, 0.4267, 0.0175, 0.0304, 0.1211, 0.0455, 0.0335, 0.0936, 0.0228,
        0.0952, 0.0607, 0.0067], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,568][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0051, 0.3170, 0.1043, 0.1801, 0.0842, 0.0466, 0.0338, 0.0349, 0.0917,
        0.0444, 0.0373, 0.0207], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,569][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0703, 0.1606, 0.0375, 0.1112, 0.0210, 0.0250, 0.0586, 0.0945, 0.0402,
        0.1254, 0.1724, 0.0832], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,572][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0575, 0.1143, 0.1038, 0.1002, 0.0875, 0.0802, 0.0782, 0.0640, 0.0741,
        0.0835, 0.0768, 0.0799], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,575][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0511, 0.0668, 0.2358, 0.1661, 0.0318, 0.0308, 0.0865, 0.0385, 0.0730,
        0.0888, 0.1027, 0.0279], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:37,580][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0672, 0.0698, 0.0703, 0.0726, 0.0827, 0.0774, 0.0830, 0.0837, 0.0820,
        0.0801, 0.0804, 0.0749, 0.0758], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,585][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0140, 0.0908, 0.1170, 0.0582, 0.0668, 0.0945, 0.0843, 0.0836, 0.0645,
        0.1568, 0.0674, 0.0534, 0.0487], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,588][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ a] are: tensor([8.1130e-11, 1.9168e-08, 1.8817e-05, 1.5951e-03, 1.8927e-05, 1.0827e-04,
        1.8658e-04, 9.4860e-01, 1.5770e-03, 1.8202e-03, 3.2548e-03, 3.1918e-02,
        1.0906e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,592][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0023, 0.0764, 0.0124, 0.0651, 0.3932, 0.0795, 0.0129, 0.0582, 0.0521,
        0.1078, 0.0857, 0.0217, 0.0326], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,597][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1388, 0.1012, 0.1080, 0.0973, 0.0762, 0.0633, 0.0639, 0.0937, 0.0599,
        0.0527, 0.0317, 0.0567, 0.0566], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,600][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1256, 0.1125, 0.1260, 0.0147, 0.1192, 0.0228, 0.0643, 0.0620, 0.0412,
        0.1141, 0.1415, 0.0245, 0.0316], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,601][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0014, 0.0356, 0.1057, 0.3582, 0.0462, 0.0622, 0.1005, 0.0420, 0.0628,
        0.0656, 0.0244, 0.0306, 0.0648], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,602][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0199, 0.5903, 0.0238, 0.0230, 0.0909, 0.0273, 0.0255, 0.0956, 0.0121,
        0.0360, 0.0290, 0.0153, 0.0113], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,602][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0034, 0.2342, 0.1363, 0.1853, 0.1078, 0.0610, 0.0253, 0.0370, 0.0791,
        0.0309, 0.0299, 0.0582, 0.0115], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,605][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0777, 0.1354, 0.1556, 0.0353, 0.0213, 0.0218, 0.0317, 0.0669, 0.0468,
        0.0182, 0.1095, 0.2762, 0.0036], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,608][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0502, 0.1052, 0.0907, 0.0899, 0.0855, 0.0705, 0.0734, 0.0645, 0.0605,
        0.0849, 0.0648, 0.0684, 0.0916], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,613][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0396, 0.1215, 0.1716, 0.1784, 0.0569, 0.0275, 0.1089, 0.0460, 0.0572,
        0.0448, 0.0891, 0.0328, 0.0258], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:37,618][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0645, 0.0663, 0.0664, 0.0682, 0.0753, 0.0719, 0.0755, 0.0760, 0.0753,
        0.0740, 0.0740, 0.0706, 0.0706, 0.0713], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,622][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0113, 0.0613, 0.0980, 0.0501, 0.0731, 0.0924, 0.0788, 0.0737, 0.0680,
        0.1679, 0.0709, 0.0491, 0.0500, 0.0554], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,625][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([3.4131e-12, 2.1757e-09, 6.7556e-07, 1.3982e-04, 1.0100e-06, 3.9250e-06,
        6.0358e-06, 4.5531e-02, 1.0078e-04, 2.3699e-04, 3.3230e-04, 2.7480e-03,
        3.6115e-03, 9.4729e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,630][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0008, 0.0444, 0.0055, 0.0388, 0.4274, 0.0658, 0.0051, 0.0348, 0.0533,
        0.1685, 0.0359, 0.0095, 0.0874, 0.0227], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,633][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.1296, 0.1293, 0.1036, 0.0930, 0.0683, 0.0584, 0.0631, 0.0714, 0.0546,
        0.0488, 0.0331, 0.0508, 0.0510, 0.0451], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,634][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.1789, 0.0258, 0.0946, 0.0077, 0.0342, 0.0367, 0.0690, 0.0690, 0.0495,
        0.1625, 0.0586, 0.1090, 0.0918, 0.0128], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,635][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.0027, 0.0329, 0.0809, 0.3187, 0.0461, 0.0574, 0.0836, 0.0416, 0.0516,
        0.0543, 0.0228, 0.0287, 0.0616, 0.1172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,635][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0568, 0.3338, 0.0229, 0.0434, 0.1208, 0.0362, 0.0251, 0.1347, 0.0187,
        0.1004, 0.0557, 0.0171, 0.0247, 0.0098], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,638][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0041, 0.1519, 0.1223, 0.2180, 0.0853, 0.0529, 0.0227, 0.0329, 0.0967,
        0.0562, 0.0280, 0.0382, 0.0185, 0.0721], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,641][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0919, 0.1465, 0.1449, 0.0385, 0.0223, 0.0331, 0.0122, 0.0400, 0.0477,
        0.0699, 0.0632, 0.2105, 0.0165, 0.0628], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,646][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0492, 0.1013, 0.0876, 0.0963, 0.0755, 0.0698, 0.0614, 0.0537, 0.0525,
        0.0765, 0.0637, 0.0755, 0.0734, 0.0635], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,651][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0275, 0.0456, 0.1281, 0.0977, 0.0568, 0.0192, 0.0943, 0.0417, 0.1335,
        0.0813, 0.0925, 0.0351, 0.0900, 0.0568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:37,655][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0603, 0.0619, 0.0620, 0.0634, 0.0703, 0.0672, 0.0705, 0.0710, 0.0705,
        0.0693, 0.0691, 0.0659, 0.0659, 0.0667, 0.0658], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,660][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0115, 0.0540, 0.0989, 0.0417, 0.0549, 0.0813, 0.0831, 0.0655, 0.0613,
        0.1810, 0.0695, 0.0501, 0.0541, 0.0696, 0.0233], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,663][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ different] are: tensor([2.0705e-11, 7.1205e-11, 1.2736e-07, 1.0369e-05, 2.6726e-08, 1.7846e-07,
        8.6749e-07, 4.8375e-03, 5.4390e-06, 2.1735e-05, 1.6869e-05, 1.2134e-04,
        3.1442e-04, 1.9828e-01, 7.9639e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,666][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0010, 0.0524, 0.0053, 0.0339, 0.4562, 0.0586, 0.0051, 0.0336, 0.0832,
        0.1150, 0.0667, 0.0126, 0.0304, 0.0429, 0.0030], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,667][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0964, 0.1174, 0.0945, 0.0870, 0.0676, 0.0607, 0.0584, 0.0720, 0.0579,
        0.0423, 0.0282, 0.0518, 0.0524, 0.0489, 0.0645], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,668][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.2054, 0.0272, 0.0683, 0.0053, 0.0187, 0.0158, 0.0279, 0.0564, 0.0236,
        0.1309, 0.0443, 0.0237, 0.0628, 0.0296, 0.2602], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,668][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0022, 0.0285, 0.0753, 0.2993, 0.0399, 0.0547, 0.0727, 0.0366, 0.0469,
        0.0528, 0.0203, 0.0270, 0.0611, 0.1118, 0.0708], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,671][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0232, 0.5562, 0.0228, 0.0209, 0.0908, 0.0215, 0.0096, 0.0919, 0.0095,
        0.0476, 0.0426, 0.0116, 0.0142, 0.0243, 0.0133], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,674][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0034, 0.1162, 0.1014, 0.1763, 0.0923, 0.0546, 0.0227, 0.0330, 0.1006,
        0.0428, 0.0257, 0.0532, 0.0147, 0.1398, 0.0234], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,679][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.1049, 0.0883, 0.0887, 0.0181, 0.0461, 0.0206, 0.0105, 0.0377, 0.0402,
        0.0609, 0.1080, 0.2061, 0.0078, 0.0551, 0.1069], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,684][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0416, 0.0979, 0.0849, 0.0955, 0.0680, 0.0638, 0.0634, 0.0502, 0.0516,
        0.0666, 0.0552, 0.0655, 0.0690, 0.0603, 0.0666], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,688][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0152, 0.0438, 0.1237, 0.0574, 0.0365, 0.0206, 0.0644, 0.0372, 0.1531,
        0.0838, 0.0794, 0.0239, 0.0771, 0.0730, 0.1109], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:37,693][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0555, 0.0574, 0.0580, 0.0599, 0.0681, 0.0632, 0.0683, 0.0684, 0.0669,
        0.0651, 0.0656, 0.0614, 0.0617, 0.0624, 0.0610, 0.0571],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,698][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0155, 0.0579, 0.0897, 0.0446, 0.0567, 0.0736, 0.0713, 0.0711, 0.0556,
        0.1866, 0.0508, 0.0562, 0.0458, 0.0612, 0.0274, 0.0360],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,699][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ from] are: tensor([2.1392e-12, 2.0910e-10, 8.7425e-08, 4.6758e-06, 6.2519e-08, 2.9871e-07,
        4.8359e-07, 2.6484e-03, 5.1740e-06, 1.1923e-05, 1.1030e-05, 1.1704e-04,
        1.6144e-04, 3.2469e-02, 9.6422e-01, 3.4599e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,700][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0012, 0.0222, 0.0030, 0.0228, 0.3779, 0.0570, 0.0152, 0.0151, 0.0334,
        0.2082, 0.0668, 0.0120, 0.0768, 0.0178, 0.0313, 0.0394],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,701][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.1004, 0.0919, 0.0697, 0.0778, 0.0625, 0.0558, 0.0559, 0.0856, 0.0584,
        0.0460, 0.0286, 0.0508, 0.0491, 0.0407, 0.0608, 0.0660],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,703][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.1012, 0.0496, 0.0595, 0.0074, 0.0615, 0.0141, 0.0488, 0.0619, 0.0381,
        0.0503, 0.1472, 0.0200, 0.0510, 0.0334, 0.2343, 0.0219],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,707][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0025, 0.0297, 0.0738, 0.2508, 0.0387, 0.0453, 0.0718, 0.0313, 0.0389,
        0.0433, 0.0174, 0.0216, 0.0467, 0.0962, 0.0705, 0.1217],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,712][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0273, 0.5084, 0.0216, 0.0144, 0.0670, 0.0368, 0.0194, 0.0609, 0.0115,
        0.0636, 0.0250, 0.0140, 0.0347, 0.0450, 0.0447, 0.0057],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,717][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0031, 0.1825, 0.1702, 0.1193, 0.0518, 0.0311, 0.0182, 0.0230, 0.0845,
        0.0337, 0.0216, 0.0431, 0.0160, 0.1127, 0.0456, 0.0437],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,721][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0437, 0.0358, 0.0226, 0.0025, 0.0058, 0.0057, 0.0271, 0.0100, 0.0146,
        0.0172, 0.0130, 0.0949, 0.0060, 0.0386, 0.6563, 0.0062],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,726][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0446, 0.0793, 0.0769, 0.0819, 0.0682, 0.0584, 0.0528, 0.0446, 0.0499,
        0.0696, 0.0490, 0.0640, 0.0624, 0.0525, 0.0605, 0.0854],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,731][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0215, 0.0578, 0.1189, 0.0443, 0.0236, 0.0128, 0.0409, 0.0348, 0.0468,
        0.0732, 0.0738, 0.0240, 0.0347, 0.0807, 0.2804, 0.0316],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:37,732][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0518, 0.0541, 0.0546, 0.0563, 0.0639, 0.0596, 0.0642, 0.0646, 0.0632,
        0.0613, 0.0617, 0.0578, 0.0584, 0.0593, 0.0580, 0.0539, 0.0572],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,733][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0097, 0.0832, 0.0812, 0.0389, 0.0553, 0.0804, 0.0745, 0.0707, 0.0595,
        0.1197, 0.0508, 0.0397, 0.0372, 0.0740, 0.0242, 0.0402, 0.0609],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,734][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.8066e-12, 1.9006e-10, 8.5314e-08, 6.0338e-06, 6.2719e-08, 5.8923e-07,
        6.8702e-07, 3.1261e-03, 7.1873e-06, 1.3479e-05, 1.7969e-05, 1.4177e-04,
        1.4486e-04, 7.2795e-02, 9.1845e-01, 2.9219e-03, 2.3734e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,736][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0013, 0.0456, 0.0108, 0.0513, 0.4042, 0.0629, 0.0110, 0.0305, 0.0329,
        0.0859, 0.0524, 0.0124, 0.0233, 0.0647, 0.0129, 0.0715, 0.0264],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,739][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0976, 0.0857, 0.0657, 0.0726, 0.0598, 0.0507, 0.0493, 0.0720, 0.0485,
        0.0427, 0.0263, 0.0453, 0.0452, 0.0360, 0.0554, 0.0634, 0.0839],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,744][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0699, 0.1157, 0.1194, 0.0055, 0.0960, 0.0205, 0.0426, 0.0605, 0.0342,
        0.0420, 0.0896, 0.0142, 0.0113, 0.0471, 0.1976, 0.0173, 0.0166],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,748][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0012, 0.0245, 0.0645, 0.2324, 0.0324, 0.0398, 0.0652, 0.0255, 0.0371,
        0.0384, 0.0149, 0.0185, 0.0375, 0.0889, 0.0603, 0.1095, 0.1093],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,753][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0094, 0.5601, 0.0214, 0.0115, 0.0617, 0.0338, 0.0206, 0.0723, 0.0124,
        0.0379, 0.0204, 0.0099, 0.0168, 0.0361, 0.0335, 0.0068, 0.0353],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,758][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0017, 0.1941, 0.0819, 0.1218, 0.0720, 0.0397, 0.0181, 0.0203, 0.0615,
        0.0265, 0.0196, 0.0305, 0.0109, 0.1277, 0.0370, 0.1255, 0.0109],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,762][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0438, 0.0779, 0.0707, 0.0170, 0.0166, 0.0131, 0.0254, 0.0587, 0.0324,
        0.0134, 0.0675, 0.0906, 0.0022, 0.1464, 0.2938, 0.0257, 0.0047],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,764][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0373, 0.0768, 0.0737, 0.0652, 0.0613, 0.0556, 0.0551, 0.0477, 0.0476,
        0.0586, 0.0494, 0.0532, 0.0624, 0.0512, 0.0565, 0.0676, 0.0808],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,765][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0118, 0.0708, 0.0937, 0.0611, 0.0338, 0.0225, 0.0372, 0.0280, 0.0585,
        0.0543, 0.0531, 0.0185, 0.0466, 0.1173, 0.1628, 0.0941, 0.0357],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:37,766][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0484, 0.0513, 0.0513, 0.0524, 0.0602, 0.0568, 0.0609, 0.0613, 0.0600,
        0.0583, 0.0590, 0.0546, 0.0547, 0.0562, 0.0549, 0.0508, 0.0535, 0.0554],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,767][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0094, 0.0376, 0.0747, 0.0472, 0.0510, 0.0768, 0.0556, 0.0597, 0.0652,
        0.1246, 0.0526, 0.0465, 0.0479, 0.0603, 0.0208, 0.0595, 0.0898, 0.0210],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,769][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ language] are: tensor([2.3594e-12, 9.7131e-11, 1.5293e-07, 8.6291e-06, 2.3326e-08, 2.7429e-07,
        5.7390e-07, 2.2038e-03, 5.9301e-06, 1.5447e-05, 2.1299e-05, 1.9467e-04,
        3.5705e-04, 1.5376e-01, 8.1284e-01, 4.4473e-03, 2.1048e-02, 5.0893e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,771][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0006, 0.0229, 0.0042, 0.0388, 0.3236, 0.0528, 0.0169, 0.0210, 0.0835,
        0.1178, 0.0295, 0.0116, 0.0579, 0.0572, 0.0199, 0.0741, 0.0524, 0.0153],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,776][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0740, 0.0777, 0.0564, 0.0597, 0.0503, 0.0487, 0.0468, 0.0676, 0.0528,
        0.0394, 0.0259, 0.0434, 0.0405, 0.0336, 0.0504, 0.0605, 0.0774, 0.0951],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,781][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0851, 0.0460, 0.0368, 0.0157, 0.0618, 0.0108, 0.0370, 0.0781, 0.0378,
        0.1085, 0.0964, 0.0051, 0.0497, 0.0684, 0.0345, 0.0484, 0.0834, 0.0965],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,785][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0013, 0.0189, 0.0555, 0.2114, 0.0263, 0.0394, 0.0506, 0.0230, 0.0352,
        0.0418, 0.0129, 0.0206, 0.0469, 0.0767, 0.0527, 0.1238, 0.1338, 0.0292],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,790][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0381, 0.0671, 0.0280, 0.0327, 0.0974, 0.0561, 0.0253, 0.0899, 0.0307,
        0.1376, 0.0485, 0.0174, 0.0507, 0.0526, 0.0527, 0.0198, 0.1266, 0.0288],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,794][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0028, 0.0240, 0.0564, 0.1708, 0.0649, 0.0408, 0.0135, 0.0225, 0.0823,
        0.0661, 0.0300, 0.0330, 0.0194, 0.1229, 0.0380, 0.1850, 0.0212, 0.0062],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,796][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0428, 0.0264, 0.3817, 0.0396, 0.0155, 0.0223, 0.0176, 0.0335, 0.0363,
        0.0269, 0.0206, 0.0710, 0.0111, 0.0439, 0.1194, 0.0272, 0.0220, 0.0424],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,797][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0343, 0.0737, 0.0616, 0.0731, 0.0529, 0.0519, 0.0450, 0.0370, 0.0407,
        0.0611, 0.0442, 0.0486, 0.0649, 0.0435, 0.0525, 0.0696, 0.0755, 0.0701],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,798][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0119, 0.0092, 0.0770, 0.0512, 0.0243, 0.0108, 0.0820, 0.0149, 0.0568,
        0.0284, 0.0500, 0.0163, 0.0212, 0.0744, 0.2934, 0.1219, 0.0456, 0.0106],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:37,799][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0465, 0.0483, 0.0489, 0.0503, 0.0568, 0.0538, 0.0574, 0.0577, 0.0567,
        0.0552, 0.0556, 0.0520, 0.0523, 0.0532, 0.0524, 0.0488, 0.0514, 0.0524,
        0.0503], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,802][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0102, 0.0482, 0.0517, 0.0285, 0.0521, 0.0562, 0.0686, 0.0624, 0.0488,
        0.1356, 0.0612, 0.0437, 0.0467, 0.0623, 0.0265, 0.0453, 0.0931, 0.0261,
        0.0327], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,804][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ used] are: tensor([7.9218e-12, 2.5003e-10, 1.9538e-08, 4.6883e-06, 6.7231e-08, 1.4146e-07,
        3.7796e-07, 1.5691e-03, 8.5651e-06, 8.9176e-06, 1.3754e-05, 1.2586e-04,
        1.8710e-04, 4.6138e-02, 8.2702e-01, 2.4936e-03, 1.2371e-02, 6.7976e-03,
        1.0326e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,808][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0010, 0.0463, 0.0017, 0.0343, 0.2458, 0.0626, 0.0058, 0.0187, 0.0494,
        0.1642, 0.0236, 0.0118, 0.0761, 0.0444, 0.0050, 0.0811, 0.0629, 0.0598,
        0.0053], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,813][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.1086, 0.0681, 0.0588, 0.0655, 0.0527, 0.0383, 0.0387, 0.0697, 0.0395,
        0.0320, 0.0231, 0.0371, 0.0376, 0.0297, 0.0449, 0.0534, 0.0622, 0.0802,
        0.0598], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,817][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0586, 0.0477, 0.0135, 0.0111, 0.0352, 0.0141, 0.0279, 0.0302, 0.0613,
        0.1372, 0.0566, 0.0493, 0.0343, 0.0203, 0.1281, 0.0578, 0.1126, 0.0870,
        0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,822][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0014, 0.0191, 0.0518, 0.2168, 0.0267, 0.0392, 0.0511, 0.0240, 0.0328,
        0.0378, 0.0130, 0.0170, 0.0411, 0.0774, 0.0505, 0.1077, 0.1141, 0.0251,
        0.0536], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,827][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0410, 0.1874, 0.0027, 0.0206, 0.0849, 0.0160, 0.0168, 0.0635, 0.0069,
        0.1052, 0.0415, 0.0067, 0.0544, 0.0158, 0.0388, 0.0157, 0.1785, 0.1004,
        0.0030], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,828][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0040, 0.0835, 0.0282, 0.1636, 0.0590, 0.0402, 0.0197, 0.0327, 0.0657,
        0.0559, 0.0261, 0.0365, 0.0223, 0.0857, 0.0298, 0.1869, 0.0216, 0.0206,
        0.0181], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,829][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0463, 0.0633, 0.0156, 0.0661, 0.0164, 0.0263, 0.0285, 0.0457, 0.0462,
        0.0976, 0.0548, 0.1438, 0.0346, 0.0455, 0.0321, 0.0295, 0.0804, 0.1080,
        0.0192], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,830][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0376, 0.0659, 0.0633, 0.0690, 0.0484, 0.0459, 0.0441, 0.0351, 0.0383,
        0.0562, 0.0421, 0.0492, 0.0556, 0.0407, 0.0509, 0.0608, 0.0688, 0.0592,
        0.0689], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,831][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0183, 0.0312, 0.0398, 0.0713, 0.0216, 0.0087, 0.0489, 0.0163, 0.0841,
        0.0372, 0.0431, 0.0123, 0.0343, 0.0551, 0.2394, 0.0862, 0.0501, 0.0428,
        0.0591], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:37,834][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0434, 0.0458, 0.0465, 0.0478, 0.0551, 0.0512, 0.0557, 0.0559, 0.0545,
        0.0527, 0.0532, 0.0493, 0.0497, 0.0508, 0.0497, 0.0458, 0.0485, 0.0497,
        0.0477, 0.0470], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,838][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0119, 0.0502, 0.0715, 0.0259, 0.0536, 0.0616, 0.0555, 0.0638, 0.0422,
        0.1499, 0.0552, 0.0430, 0.0427, 0.0554, 0.0249, 0.0396, 0.0726, 0.0249,
        0.0439, 0.0117], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,841][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ by] are: tensor([5.0953e-10, 5.7062e-10, 1.3904e-07, 1.1502e-06, 8.1722e-08, 1.6859e-07,
        6.7050e-07, 9.8383e-04, 6.1004e-06, 9.1846e-06, 5.9871e-06, 7.6296e-05,
        1.0020e-04, 1.5903e-02, 3.9231e-01, 7.1632e-04, 4.2555e-03, 2.1645e-03,
        9.7683e-02, 4.8578e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,845][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0026, 0.0591, 0.0088, 0.0094, 0.3738, 0.0916, 0.0083, 0.0092, 0.0214,
        0.1098, 0.0286, 0.0119, 0.0817, 0.0456, 0.0085, 0.0231, 0.0446, 0.0316,
        0.0185, 0.0118], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,850][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0720, 0.0578, 0.0479, 0.0556, 0.0490, 0.0404, 0.0395, 0.0644, 0.0431,
        0.0338, 0.0212, 0.0389, 0.0368, 0.0292, 0.0452, 0.0513, 0.0670, 0.0873,
        0.0630, 0.0565], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,854][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0706, 0.0379, 0.0406, 0.0013, 0.0332, 0.0136, 0.0462, 0.0526, 0.0360,
        0.0734, 0.1107, 0.0169, 0.0405, 0.0393, 0.1242, 0.0167, 0.0787, 0.0969,
        0.0683, 0.0024], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,859][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0016, 0.0221, 0.0520, 0.1776, 0.0282, 0.0321, 0.0575, 0.0224, 0.0281,
        0.0314, 0.0126, 0.0148, 0.0302, 0.0703, 0.0507, 0.0843, 0.0911, 0.0275,
        0.0522, 0.1131], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,861][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0208, 0.3472, 0.0173, 0.0084, 0.0476, 0.0172, 0.0195, 0.0440, 0.0066,
        0.0549, 0.0195, 0.0090, 0.0346, 0.0235, 0.0447, 0.0067, 0.1045, 0.1486,
        0.0172, 0.0083], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,862][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0037, 0.1159, 0.0937, 0.0701, 0.0648, 0.0310, 0.0238, 0.0239, 0.0695,
        0.0359, 0.0254, 0.0340, 0.0174, 0.0837, 0.0436, 0.1009, 0.0222, 0.0263,
        0.0655, 0.0487], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,862][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0171, 0.1566, 0.0806, 0.0018, 0.0426, 0.0139, 0.0098, 0.0079, 0.0199,
        0.0097, 0.0439, 0.0746, 0.0075, 0.1313, 0.0931, 0.0112, 0.0094, 0.1985,
        0.0682, 0.0023], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,863][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0307, 0.0634, 0.0574, 0.0593, 0.0495, 0.0410, 0.0419, 0.0347, 0.0351,
        0.0477, 0.0370, 0.0449, 0.0528, 0.0390, 0.0453, 0.0617, 0.0634, 0.0585,
        0.0628, 0.0740], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,866][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0172, 0.0123, 0.0962, 0.0168, 0.0344, 0.0152, 0.0309, 0.0273, 0.0398,
        0.0548, 0.0702, 0.0204, 0.0409, 0.0454, 0.1382, 0.0863, 0.0659, 0.0148,
        0.1536, 0.0194], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:37,870][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0418, 0.0440, 0.0445, 0.0457, 0.0523, 0.0487, 0.0528, 0.0532, 0.0519,
        0.0502, 0.0506, 0.0470, 0.0476, 0.0485, 0.0475, 0.0438, 0.0464, 0.0475,
        0.0455, 0.0449, 0.0455], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,874][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0081, 0.0715, 0.0688, 0.0323, 0.0474, 0.0694, 0.0640, 0.0610, 0.0524,
        0.1046, 0.0449, 0.0339, 0.0324, 0.0648, 0.0209, 0.0344, 0.0526, 0.0350,
        0.0395, 0.0146, 0.0475], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,877][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([2.8599e-12, 7.4690e-11, 2.2871e-08, 1.2479e-06, 1.1567e-08, 1.0192e-07,
        1.3694e-07, 4.4814e-04, 1.1887e-06, 2.2949e-06, 2.7341e-06, 2.7918e-05,
        2.3192e-05, 1.0236e-02, 1.3101e-01, 4.0369e-04, 3.1582e-04, 7.2133e-04,
        5.9792e-02, 7.9522e-01, 1.7900e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,882][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0009, 0.0367, 0.0097, 0.0421, 0.3558, 0.0546, 0.0106, 0.0288, 0.0297,
        0.0736, 0.0469, 0.0113, 0.0185, 0.0582, 0.0115, 0.0634, 0.0212, 0.0235,
        0.0205, 0.0647, 0.0175], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,887][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0770, 0.0562, 0.0466, 0.0521, 0.0438, 0.0355, 0.0361, 0.0587, 0.0361,
        0.0331, 0.0201, 0.0347, 0.0336, 0.0257, 0.0424, 0.0477, 0.0633, 0.0829,
        0.0627, 0.0520, 0.0597], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,891][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0521, 0.0755, 0.0809, 0.0036, 0.0650, 0.0138, 0.0290, 0.0406, 0.0245,
        0.0286, 0.0606, 0.0096, 0.0075, 0.0317, 0.1284, 0.0110, 0.0108, 0.1754,
        0.1325, 0.0053, 0.0135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,893][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0007, 0.0181, 0.0488, 0.1770, 0.0239, 0.0299, 0.0494, 0.0187, 0.0277,
        0.0279, 0.0107, 0.0129, 0.0258, 0.0664, 0.0439, 0.0764, 0.0752, 0.0229,
        0.0467, 0.1058, 0.0912], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,894][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0074, 0.4075, 0.0165, 0.0087, 0.0475, 0.0263, 0.0160, 0.0579, 0.0099,
        0.0301, 0.0165, 0.0076, 0.0134, 0.0292, 0.0268, 0.0053, 0.0270, 0.1916,
        0.0191, 0.0098, 0.0260], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,895][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0013, 0.1563, 0.0657, 0.1023, 0.0587, 0.0342, 0.0151, 0.0173, 0.0522,
        0.0225, 0.0169, 0.0256, 0.0090, 0.1077, 0.0308, 0.1091, 0.0088, 0.0387,
        0.0468, 0.0725, 0.0085], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,896][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0369, 0.0714, 0.0580, 0.0139, 0.0141, 0.0108, 0.0210, 0.0492, 0.0268,
        0.0119, 0.0559, 0.0723, 0.0019, 0.1200, 0.2293, 0.0222, 0.0039, 0.1047,
        0.0525, 0.0188, 0.0045], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,899][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0285, 0.0582, 0.0569, 0.0499, 0.0470, 0.0426, 0.0428, 0.0368, 0.0370,
        0.0454, 0.0384, 0.0404, 0.0479, 0.0394, 0.0433, 0.0513, 0.0612, 0.0526,
        0.0604, 0.0576, 0.0623], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,904][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0089, 0.0493, 0.0695, 0.0455, 0.0255, 0.0167, 0.0270, 0.0216, 0.0442,
        0.0402, 0.0417, 0.0143, 0.0340, 0.0855, 0.1243, 0.0705, 0.0269, 0.0620,
        0.1085, 0.0551, 0.0285], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:37,947][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:37,950][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,950][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,951][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,952][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,952][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,953][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,954][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,955][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,955][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,957][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,961][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,962][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:37,962][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.4951, 0.5049], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,966][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.6353, 0.3647], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,970][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.9911, 0.0089], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,974][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.4526, 0.5474], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,975][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.5553, 0.4447], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,976][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.4660, 0.5340], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,976][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0827, 0.9173], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,977][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.7369, 0.2631], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,979][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.5839, 0.4161], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,982][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.5772, 0.4228], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,987][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.4464, 0.5536], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,992][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.4550, 0.5450], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:37,997][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.2920, 0.3336, 0.3745], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,001][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.4272, 0.2754, 0.2974], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,006][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.8057, 0.1882, 0.0061], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,007][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.5342, 0.4463, 0.0195], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,008][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.2564, 0.2841, 0.4595], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,008][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.3107, 0.3609, 0.3284], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,009][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0601, 0.5012, 0.4387], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,011][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.5251, 0.4456, 0.0293], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,014][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.3955, 0.3510, 0.2535], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,019][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.4296, 0.4632, 0.1071], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,023][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.2925, 0.3663, 0.3412], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,028][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.2850, 0.3458, 0.3692], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,033][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.2220, 0.2518, 0.2760, 0.2502], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,037][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.3331, 0.2147, 0.2467, 0.2055], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,039][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.1757, 0.4617, 0.3584, 0.0042], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,040][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.1045, 0.5599, 0.3083, 0.0273], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,041][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.1421, 0.2188, 0.2975, 0.3416], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,041][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.2398, 0.2761, 0.2663, 0.2178], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,042][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0324, 0.3582, 0.3135, 0.2959], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,046][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.3214, 0.5201, 0.0687, 0.0899], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,050][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.2658, 0.2554, 0.2331, 0.2456], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,055][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0032, 0.3579, 0.6351, 0.0038], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,060][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.2211, 0.2748, 0.2565, 0.2476], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,064][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.2093, 0.2477, 0.2630, 0.2801], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,069][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.1781, 0.1993, 0.2225, 0.1994, 0.2007], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,072][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.2635, 0.1714, 0.2006, 0.1746, 0.1900], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,072][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.3288, 0.0393, 0.4587, 0.0781, 0.0951], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,073][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.0862, 0.0545, 0.0650, 0.7784, 0.0159], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,074][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.2044, 0.1798, 0.2342, 0.2382, 0.1433], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,075][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.1803, 0.2208, 0.2127, 0.1856, 0.2006], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,077][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.0264, 0.2227, 0.2010, 0.2059, 0.3441], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,081][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.3306, 0.3416, 0.0656, 0.1564, 0.1057], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,085][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.2158, 0.2151, 0.1687, 0.2358, 0.1645], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,090][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0406, 0.5464, 0.0706, 0.2418, 0.1005], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,095][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.1771, 0.2154, 0.2022, 0.1946, 0.2108], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,099][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.1589, 0.1952, 0.2090, 0.2217, 0.2152], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,104][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.1564, 0.1721, 0.1886, 0.1643, 0.1564, 0.1623], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,105][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.2230, 0.1420, 0.1612, 0.1469, 0.1672, 0.1597], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,106][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.2556, 0.0339, 0.1410, 0.0190, 0.4683, 0.0822], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,106][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0094, 0.3533, 0.0175, 0.1969, 0.3708, 0.0520], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,107][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.1215, 0.1561, 0.2047, 0.2177, 0.0993, 0.2008], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,110][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.1541, 0.1796, 0.1786, 0.1499, 0.1747, 0.1631], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,114][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0185, 0.1679, 0.1539, 0.1556, 0.2715, 0.2326], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,119][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.3310, 0.3281, 0.0394, 0.1102, 0.1485, 0.0428], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,123][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.1771, 0.1793, 0.1507, 0.1914, 0.1770, 0.1245], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,128][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0128, 0.4251, 0.2539, 0.2399, 0.0472, 0.0211], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,133][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.1466, 0.1781, 0.1661, 0.1602, 0.1737, 0.1755], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,137][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.1271, 0.1565, 0.1677, 0.1794, 0.1742, 0.1951], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,139][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.1284, 0.1448, 0.1554, 0.1435, 0.1405, 0.1360, 0.1513],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,140][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.1863, 0.1228, 0.1406, 0.1268, 0.1444, 0.1432, 0.1360],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,141][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.1090, 0.0204, 0.0496, 0.0682, 0.3169, 0.4287, 0.0073],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,142][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.0378, 0.0490, 0.0236, 0.4535, 0.0118, 0.4228, 0.0014],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,144][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.1701, 0.1285, 0.1771, 0.1735, 0.0829, 0.1265, 0.1415],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,147][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.1321, 0.1577, 0.1502, 0.1339, 0.1539, 0.1465, 0.1256],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,151][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.0189, 0.1310, 0.1192, 0.1170, 0.1894, 0.1711, 0.2534],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,156][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.2203, 0.3709, 0.0495, 0.1250, 0.1444, 0.0688, 0.0210],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,161][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.1496, 0.1600, 0.1425, 0.1795, 0.1588, 0.1210, 0.0885],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,166][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0084, 0.5048, 0.0758, 0.3499, 0.0484, 0.0106, 0.0021],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,170][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.1240, 0.1518, 0.1417, 0.1365, 0.1480, 0.1490, 0.1490],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,172][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.1092, 0.1318, 0.1411, 0.1502, 0.1458, 0.1652, 0.1567],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:38,173][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.1083, 0.1197, 0.1325, 0.1235, 0.1215, 0.1143, 0.1190, 0.1612],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,173][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.1548, 0.1045, 0.1309, 0.1116, 0.1324, 0.1301, 0.1234, 0.1123],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,174][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.0820, 0.0033, 0.0102, 0.0053, 0.7476, 0.1277, 0.0192, 0.0047],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,176][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.0196, 0.0016, 0.0046, 0.1331, 0.0014, 0.8223, 0.0151, 0.0023],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,179][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.1243, 0.1083, 0.1694, 0.1354, 0.0736, 0.1291, 0.1468, 0.1131],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,184][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.1111, 0.1317, 0.1298, 0.1114, 0.1326, 0.1329, 0.1162, 0.1342],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,189][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.0129, 0.0989, 0.0901, 0.0895, 0.1561, 0.1382, 0.2050, 0.2093],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,193][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.1728, 0.2943, 0.0568, 0.1191, 0.1572, 0.1070, 0.0473, 0.0455],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,198][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.1300, 0.1378, 0.1302, 0.1497, 0.1494, 0.1059, 0.0930, 0.1041],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,203][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0227, 0.0319, 0.0995, 0.7534, 0.0846, 0.0028, 0.0024, 0.0027],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,205][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.1078, 0.1321, 0.1230, 0.1186, 0.1292, 0.1299, 0.1298, 0.1296],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,205][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.0927, 0.1140, 0.1228, 0.1306, 0.1269, 0.1442, 0.1369, 0.1321],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:38,206][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0951, 0.1041, 0.1143, 0.1066, 0.1012, 0.0982, 0.1047, 0.1356, 0.1402],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,207][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.1507, 0.0968, 0.1174, 0.1003, 0.1146, 0.1138, 0.1105, 0.1025, 0.0934],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,210][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.1438, 0.0568, 0.2647, 0.0179, 0.2455, 0.1829, 0.0118, 0.0546, 0.0218],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,213][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0096, 0.2465, 0.0045, 0.0509, 0.0475, 0.1588, 0.0978, 0.3813, 0.0031],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,218][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0941, 0.0866, 0.1219, 0.1420, 0.0925, 0.1057, 0.1387, 0.1215, 0.0971],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,223][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.1028, 0.1235, 0.1237, 0.0973, 0.1207, 0.1157, 0.1018, 0.1204, 0.0941],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,227][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.0125, 0.0867, 0.0777, 0.0772, 0.1263, 0.1107, 0.1762, 0.1750, 0.1576],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,232][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.2484, 0.2906, 0.0357, 0.1162, 0.1250, 0.0606, 0.0365, 0.0657, 0.0214],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,237][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.1323, 0.1332, 0.1049, 0.1413, 0.1254, 0.0914, 0.0741, 0.1140, 0.0833],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,238][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0115, 0.4251, 0.0259, 0.1905, 0.2282, 0.0067, 0.0288, 0.0613, 0.0221],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,238][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.0956, 0.1165, 0.1087, 0.1050, 0.1144, 0.1151, 0.1149, 0.1144, 0.1154],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,239][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0806, 0.1002, 0.1075, 0.1155, 0.1116, 0.1256, 0.1202, 0.1154, 0.1235],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:38,240][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0945, 0.0961, 0.1023, 0.0965, 0.0932, 0.0875, 0.0910, 0.1177, 0.1185,
        0.1027], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,242][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.1239, 0.0863, 0.1002, 0.0879, 0.1024, 0.0987, 0.1000, 0.0997, 0.0934,
        0.1074], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,246][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.0335, 0.0308, 0.0621, 0.0349, 0.3131, 0.1185, 0.0081, 0.2916, 0.0817,
        0.0255], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,250][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0039, 0.0382, 0.0130, 0.6886, 0.0355, 0.1093, 0.0020, 0.0748, 0.0330,
        0.0016], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,255][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.1521, 0.0876, 0.1455, 0.1174, 0.0532, 0.1054, 0.1116, 0.0860, 0.0787,
        0.0625], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,260][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.0856, 0.1059, 0.1063, 0.0894, 0.1092, 0.1051, 0.0934, 0.1055, 0.0881,
        0.1116], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,265][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.0110, 0.0731, 0.0676, 0.0680, 0.1149, 0.1000, 0.1575, 0.1605, 0.1418,
        0.1057], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,270][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.2380, 0.2814, 0.0357, 0.0844, 0.1047, 0.0565, 0.0304, 0.0420, 0.0341,
        0.0929], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,270][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.1113, 0.1258, 0.1036, 0.1242, 0.1151, 0.0871, 0.0720, 0.0937, 0.0940,
        0.0733], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,271][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0039, 0.4808, 0.2254, 0.1595, 0.0353, 0.0048, 0.0217, 0.0368, 0.0286,
        0.0033], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,272][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.0861, 0.1049, 0.0980, 0.0941, 0.1023, 0.1025, 0.1021, 0.1024, 0.1027,
        0.1049], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,273][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.0716, 0.0896, 0.0962, 0.1028, 0.0996, 0.1122, 0.1073, 0.1035, 0.1110,
        0.1063], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:38,276][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0905, 0.0904, 0.0967, 0.0896, 0.0852, 0.0772, 0.0794, 0.1049, 0.1064,
        0.0888, 0.0908], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,279][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.1151, 0.0762, 0.0939, 0.0768, 0.0930, 0.0951, 0.0895, 0.0870, 0.0842,
        0.1028, 0.0863], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,284][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.0405, 0.0019, 0.0118, 0.0305, 0.1177, 0.1183, 0.0134, 0.0339, 0.4480,
        0.1819, 0.0019], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,289][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0185, 0.0102, 0.0075, 0.6712, 0.0012, 0.1204, 0.0206, 0.0744, 0.0197,
        0.0473, 0.0092], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,294][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.1005, 0.0688, 0.1157, 0.1334, 0.0630, 0.0823, 0.1247, 0.0990, 0.0592,
        0.0629, 0.0903], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,298][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.0807, 0.0975, 0.0961, 0.0828, 0.0975, 0.0966, 0.0823, 0.0971, 0.0816,
        0.1086, 0.0792], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,303][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0105, 0.0627, 0.0574, 0.0569, 0.0977, 0.0847, 0.1305, 0.1284, 0.1262,
        0.0980, 0.1471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,303][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.1110, 0.2135, 0.0334, 0.0982, 0.0826, 0.0716, 0.0374, 0.0413, 0.0408,
        0.2432, 0.0269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,304][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.1015, 0.1064, 0.0885, 0.1157, 0.1017, 0.0812, 0.0640, 0.0917, 0.0809,
        0.0814, 0.0870], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,305][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0623, 0.0633, 0.0432, 0.5380, 0.0226, 0.0177, 0.0072, 0.0899, 0.0809,
        0.0535, 0.0213], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,308][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.0774, 0.0946, 0.0885, 0.0852, 0.0926, 0.0930, 0.0927, 0.0924, 0.0929,
        0.0952, 0.0956], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,311][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.0637, 0.0807, 0.0870, 0.0926, 0.0904, 0.1027, 0.0976, 0.0938, 0.1023,
        0.0976, 0.0916], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:38,315][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0685, 0.0790, 0.0867, 0.0796, 0.0727, 0.0684, 0.0779, 0.0950, 0.1008,
        0.0812, 0.0821, 0.1079], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,320][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.1111, 0.0720, 0.0782, 0.0680, 0.0836, 0.0839, 0.0834, 0.0827, 0.0763,
        0.0978, 0.0824, 0.0806], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,322][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([3.2270e-02, 1.1657e-02, 1.4722e-02, 2.2641e-03, 4.4346e-02, 1.4799e-01,
        5.6265e-03, 2.1819e-02, 4.4660e-02, 6.6760e-01, 6.8521e-03, 1.9130e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,327][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0210, 0.0563, 0.0270, 0.3728, 0.0037, 0.0850, 0.0126, 0.0524, 0.0031,
        0.0273, 0.3365, 0.0023], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,332][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0480, 0.0557, 0.0903, 0.0987, 0.0361, 0.0857, 0.0942, 0.0765, 0.0621,
        0.0566, 0.0857, 0.2103], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,336][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0735, 0.0854, 0.0862, 0.0752, 0.0884, 0.0882, 0.0747, 0.0872, 0.0758,
        0.0985, 0.0743, 0.0925], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,336][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0102, 0.0539, 0.0488, 0.0451, 0.0791, 0.0686, 0.1142, 0.1134, 0.1019,
        0.0814, 0.1296, 0.1538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,337][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.1622, 0.2122, 0.0244, 0.0752, 0.1174, 0.0549, 0.0311, 0.0413, 0.0399,
        0.1618, 0.0595, 0.0201], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,338][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0948, 0.0994, 0.0796, 0.1007, 0.0931, 0.0738, 0.0656, 0.0794, 0.0762,
        0.0734, 0.0902, 0.0738], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,341][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0436, 0.0665, 0.0168, 0.6868, 0.0020, 0.0030, 0.0386, 0.0075, 0.0056,
        0.0613, 0.0419, 0.0265], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,344][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0704, 0.0865, 0.0807, 0.0777, 0.0846, 0.0854, 0.0851, 0.0847, 0.0850,
        0.0871, 0.0875, 0.0852], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,348][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0600, 0.0750, 0.0798, 0.0859, 0.0829, 0.0919, 0.0890, 0.0859, 0.0903,
        0.0876, 0.0830, 0.0885], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:38,352][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0603, 0.0654, 0.0709, 0.0691, 0.0698, 0.0671, 0.0725, 0.0875, 0.0942,
        0.0771, 0.0790, 0.0995, 0.0876], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,357][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0979, 0.0684, 0.0780, 0.0673, 0.0772, 0.0787, 0.0766, 0.0769, 0.0691,
        0.0851, 0.0758, 0.0766, 0.0724], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,362][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0223, 0.0182, 0.0821, 0.0210, 0.1436, 0.1295, 0.0138, 0.2799, 0.0368,
        0.0773, 0.0347, 0.1403, 0.0005], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,365][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([4.8397e-03, 3.5567e-02, 7.8769e-03, 6.5329e-01, 1.8345e-03, 1.0935e-02,
        2.1671e-03, 4.3922e-02, 4.3567e-04, 2.1221e-03, 2.3107e-01, 3.2180e-03,
        2.7314e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,368][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0540, 0.0483, 0.0825, 0.0758, 0.0239, 0.0642, 0.0780, 0.0584, 0.0506,
        0.0391, 0.0745, 0.1948, 0.1560], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,369][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0677, 0.0812, 0.0781, 0.0681, 0.0820, 0.0801, 0.0698, 0.0796, 0.0696,
        0.0892, 0.0703, 0.0858, 0.0784], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,370][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0089, 0.0453, 0.0423, 0.0391, 0.0693, 0.0581, 0.1005, 0.0947, 0.0870,
        0.0694, 0.1124, 0.1363, 0.1365], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,371][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.2252, 0.2549, 0.0301, 0.0638, 0.0913, 0.0401, 0.0278, 0.0394, 0.0269,
        0.0891, 0.0349, 0.0355, 0.0409], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,374][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0846, 0.0910, 0.0784, 0.0929, 0.0963, 0.0722, 0.0573, 0.0775, 0.0707,
        0.0621, 0.0812, 0.0858, 0.0500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,376][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([2.9777e-03, 8.3491e-02, 3.4584e-01, 3.8614e-02, 2.8654e-03, 2.3838e-03,
        9.8463e-03, 7.4648e-03, 8.1702e-03, 1.8236e-03, 4.5139e-02, 4.5110e-01,
        2.9290e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,379][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0638, 0.0790, 0.0741, 0.0715, 0.0780, 0.0783, 0.0786, 0.0781, 0.0786,
        0.0809, 0.0813, 0.0782, 0.0797], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,384][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0556, 0.0687, 0.0733, 0.0791, 0.0761, 0.0840, 0.0816, 0.0788, 0.0823,
        0.0799, 0.0757, 0.0808, 0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:38,389][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0594, 0.0646, 0.0691, 0.0667, 0.0643, 0.0599, 0.0642, 0.0792, 0.0845,
        0.0746, 0.0738, 0.0860, 0.0760, 0.0779], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,393][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0945, 0.0603, 0.0710, 0.0615, 0.0728, 0.0736, 0.0707, 0.0703, 0.0653,
        0.0811, 0.0713, 0.0709, 0.0682, 0.0684], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,398][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.0199, 0.0012, 0.0042, 0.0077, 0.1086, 0.0462, 0.0010, 0.0033, 0.0652,
        0.6656, 0.0050, 0.0197, 0.0447, 0.0078], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,400][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0083, 0.0303, 0.0158, 0.3814, 0.0087, 0.0711, 0.0032, 0.0389, 0.0029,
        0.0338, 0.1109, 0.0067, 0.2840, 0.0039], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,401][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0566, 0.0430, 0.0801, 0.0668, 0.0174, 0.0451, 0.0559, 0.0426, 0.0334,
        0.0248, 0.0565, 0.1625, 0.1135, 0.2017], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,402][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.0640, 0.0718, 0.0738, 0.0624, 0.0755, 0.0758, 0.0643, 0.0758, 0.0642,
        0.0864, 0.0639, 0.0862, 0.0784, 0.0574], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,403][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.0086, 0.0463, 0.0425, 0.0403, 0.0631, 0.0547, 0.0896, 0.0839, 0.0745,
        0.0597, 0.0964, 0.1210, 0.1209, 0.0985], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,406][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.1432, 0.1651, 0.0235, 0.0887, 0.1074, 0.0445, 0.0254, 0.0447, 0.0315,
        0.1559, 0.0475, 0.0324, 0.0600, 0.0301], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,409][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0785, 0.0742, 0.0693, 0.0898, 0.0850, 0.0689, 0.0516, 0.0732, 0.0728,
        0.0687, 0.0792, 0.0771, 0.0524, 0.0592], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,413][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0211, 0.1091, 0.2777, 0.0824, 0.0082, 0.0164, 0.0057, 0.0079, 0.0192,
        0.0341, 0.0504, 0.3426, 0.0169, 0.0084], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,417][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0602, 0.0738, 0.0686, 0.0661, 0.0714, 0.0721, 0.0722, 0.0710, 0.0717,
        0.0735, 0.0741, 0.0720, 0.0733, 0.0800], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,422][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0505, 0.0621, 0.0662, 0.0722, 0.0690, 0.0775, 0.0744, 0.0709, 0.0765,
        0.0734, 0.0684, 0.0738, 0.0769, 0.0882], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:38,427][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0556, 0.0598, 0.0635, 0.0596, 0.0587, 0.0568, 0.0631, 0.0748, 0.0777,
        0.0658, 0.0660, 0.0810, 0.0718, 0.0704, 0.0753], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,431][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0844, 0.0559, 0.0670, 0.0569, 0.0667, 0.0691, 0.0679, 0.0655, 0.0617,
        0.0799, 0.0676, 0.0679, 0.0662, 0.0691, 0.0542], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,433][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.0374, 0.0036, 0.0057, 0.0060, 0.0248, 0.0222, 0.0020, 0.0129, 0.0533,
        0.2373, 0.0068, 0.0508, 0.0360, 0.2585, 0.2427], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,434][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([1.5890e-02, 5.4911e-02, 1.2102e-02, 3.4557e-01, 1.8520e-02, 3.3799e-02,
        1.7921e-03, 3.0521e-02, 1.4147e-02, 8.7434e-03, 4.2641e-01, 9.3999e-03,
        1.3705e-02, 1.4259e-02, 2.3010e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,435][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0336, 0.0394, 0.0609, 0.0604, 0.0167, 0.0462, 0.0503, 0.0408, 0.0342,
        0.0237, 0.0474, 0.1436, 0.1178, 0.1982, 0.0866], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,436][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.0615, 0.0687, 0.0685, 0.0585, 0.0695, 0.0694, 0.0594, 0.0713, 0.0607,
        0.0826, 0.0612, 0.0781, 0.0738, 0.0580, 0.0589], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,438][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0087, 0.0418, 0.0385, 0.0356, 0.0553, 0.0486, 0.0803, 0.0753, 0.0684,
        0.0546, 0.0859, 0.1107, 0.1101, 0.0916, 0.0946], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,442][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.1029, 0.2534, 0.0264, 0.0606, 0.0980, 0.0391, 0.0148, 0.0405, 0.0256,
        0.1170, 0.0463, 0.0277, 0.0512, 0.0584, 0.0381], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,447][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0707, 0.0697, 0.0643, 0.0825, 0.0824, 0.0673, 0.0496, 0.0693, 0.0738,
        0.0624, 0.0712, 0.0788, 0.0478, 0.0682, 0.0420], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,451][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0080, 0.0925, 0.1420, 0.0167, 0.0285, 0.0044, 0.0034, 0.0091, 0.0161,
        0.0221, 0.1353, 0.5041, 0.0037, 0.0072, 0.0070], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,456][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.0559, 0.0683, 0.0637, 0.0612, 0.0661, 0.0668, 0.0668, 0.0658, 0.0664,
        0.0680, 0.0685, 0.0668, 0.0677, 0.0737, 0.0743], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,460][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0464, 0.0573, 0.0611, 0.0662, 0.0633, 0.0714, 0.0682, 0.0649, 0.0703,
        0.0675, 0.0628, 0.0679, 0.0706, 0.0810, 0.0811], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:38,465][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.0535, 0.0641, 0.0680, 0.0574, 0.0541, 0.0491, 0.0549, 0.0692, 0.0734,
        0.0573, 0.0586, 0.0744, 0.0637, 0.0656, 0.0682, 0.0684],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,466][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0834, 0.0538, 0.0625, 0.0542, 0.0638, 0.0631, 0.0625, 0.0624, 0.0569,
        0.0747, 0.0606, 0.0650, 0.0607, 0.0633, 0.0532, 0.0597],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,467][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([5.5747e-03, 4.1676e-03, 6.0882e-03, 1.9494e-03, 6.3401e-02, 1.4188e-02,
        2.1146e-03, 8.1017e-03, 7.0401e-03, 1.4592e-01, 4.2268e-03, 3.6983e-02,
        2.0613e-02, 3.3251e-02, 6.4593e-01, 4.4876e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,468][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0090, 0.0059, 0.0015, 0.0260, 0.0044, 0.0149, 0.0202, 0.0027, 0.0003,
        0.0433, 0.3383, 0.0081, 0.1879, 0.0014, 0.3286, 0.0072],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,471][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0275, 0.0319, 0.0540, 0.0517, 0.0194, 0.0383, 0.0508, 0.0413, 0.0321,
        0.0289, 0.0453, 0.1003, 0.0932, 0.1724, 0.0668, 0.1460],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,474][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0562, 0.0662, 0.0636, 0.0562, 0.0684, 0.0666, 0.0576, 0.0663, 0.0581,
        0.0743, 0.0572, 0.0717, 0.0671, 0.0538, 0.0544, 0.0622],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,479][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0074, 0.0361, 0.0328, 0.0316, 0.0530, 0.0456, 0.0739, 0.0694, 0.0640,
        0.0534, 0.0814, 0.1006, 0.1008, 0.0834, 0.0873, 0.0791],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,483][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.2335, 0.1931, 0.0215, 0.0422, 0.0626, 0.0379, 0.0173, 0.0259, 0.0194,
        0.0908, 0.0242, 0.0248, 0.0673, 0.0603, 0.0553, 0.0241],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,488][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0665, 0.0701, 0.0710, 0.0766, 0.0689, 0.0565, 0.0479, 0.0613, 0.0652,
        0.0561, 0.0652, 0.0734, 0.0470, 0.0623, 0.0483, 0.0638],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,492][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0035, 0.0223, 0.0352, 0.0026, 0.0021, 0.0017, 0.0461, 0.0094, 0.0052,
        0.0100, 0.0129, 0.6314, 0.0058, 0.0051, 0.2026, 0.0039],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,497][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.0518, 0.0631, 0.0590, 0.0570, 0.0617, 0.0620, 0.0621, 0.0615, 0.0620,
        0.0636, 0.0639, 0.0620, 0.0629, 0.0683, 0.0690, 0.0701],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,498][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0450, 0.0539, 0.0571, 0.0618, 0.0591, 0.0652, 0.0634, 0.0608, 0.0639,
        0.0619, 0.0581, 0.0625, 0.0650, 0.0738, 0.0741, 0.0743],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:38,499][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0431, 0.0558, 0.0605, 0.0508, 0.0493, 0.0453, 0.0502, 0.0629, 0.0711,
        0.0555, 0.0568, 0.0744, 0.0626, 0.0632, 0.0674, 0.0664, 0.0646],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,500][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0756, 0.0532, 0.0577, 0.0493, 0.0602, 0.0604, 0.0598, 0.0600, 0.0547,
        0.0653, 0.0579, 0.0580, 0.0555, 0.0612, 0.0486, 0.0572, 0.0654],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,503][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0023, 0.0107, 0.0063, 0.0018, 0.0164, 0.0277, 0.0023, 0.0339, 0.0146,
        0.0096, 0.0047, 0.0251, 0.0012, 0.5479, 0.2717, 0.0231, 0.0009],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,504][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.8213e-03, 2.0053e-02, 2.9324e-02, 5.6303e-01, 2.3836e-03, 1.3576e-02,
        3.4472e-03, 2.8120e-02, 1.2592e-04, 2.3567e-03, 1.6598e-01, 2.7585e-03,
        5.4314e-03, 2.5323e-02, 4.7054e-02, 8.1299e-02, 7.9193e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,509][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0411, 0.0372, 0.0700, 0.0476, 0.0217, 0.0336, 0.0512, 0.0380, 0.0348,
        0.0262, 0.0527, 0.0978, 0.0726, 0.1322, 0.0531, 0.1316, 0.0586],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,513][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0514, 0.0634, 0.0607, 0.0528, 0.0648, 0.0635, 0.0548, 0.0623, 0.0541,
        0.0688, 0.0532, 0.0665, 0.0608, 0.0499, 0.0491, 0.0583, 0.0655],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,518][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0070, 0.0295, 0.0280, 0.0256, 0.0440, 0.0378, 0.0638, 0.0604, 0.0566,
        0.0467, 0.0738, 0.0893, 0.0904, 0.0755, 0.0804, 0.0731, 0.1182],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,523][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1794, 0.1824, 0.0221, 0.0369, 0.0607, 0.0397, 0.0190, 0.0268, 0.0237,
        0.0763, 0.0219, 0.0226, 0.0454, 0.0547, 0.0486, 0.0309, 0.1089],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,527][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0702, 0.0714, 0.0591, 0.0736, 0.0723, 0.0562, 0.0446, 0.0590, 0.0554,
        0.0511, 0.0643, 0.0643, 0.0422, 0.0573, 0.0419, 0.0739, 0.0432],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,529][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([2.7768e-03, 8.8728e-02, 3.5653e-01, 5.3531e-02, 4.7892e-03, 2.4076e-03,
        1.9972e-02, 2.3739e-02, 1.0571e-02, 2.0945e-03, 6.0479e-02, 2.6855e-01,
        2.1999e-04, 4.1884e-02, 4.7181e-02, 1.6333e-02, 2.1648e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,530][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0481, 0.0587, 0.0550, 0.0533, 0.0579, 0.0580, 0.0583, 0.0578, 0.0583,
        0.0598, 0.0601, 0.0581, 0.0592, 0.0638, 0.0648, 0.0661, 0.0629],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,531][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0408, 0.0499, 0.0530, 0.0575, 0.0549, 0.0605, 0.0589, 0.0566, 0.0593,
        0.0574, 0.0541, 0.0582, 0.0606, 0.0691, 0.0694, 0.0699, 0.0699],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:38,532][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0466, 0.0503, 0.0542, 0.0519, 0.0470, 0.0480, 0.0511, 0.0610, 0.0645,
        0.0554, 0.0548, 0.0682, 0.0591, 0.0570, 0.0596, 0.0591, 0.0575, 0.0550],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,535][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0711, 0.0444, 0.0547, 0.0499, 0.0569, 0.0583, 0.0543, 0.0541, 0.0540,
        0.0635, 0.0548, 0.0575, 0.0555, 0.0574, 0.0459, 0.0588, 0.0671, 0.0419],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,537][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([1.2609e-02, 3.5108e-05, 3.2926e-03, 5.5199e-03, 5.7293e-03, 5.7895e-03,
        1.3192e-03, 1.8960e-03, 1.4297e-02, 4.6267e-02, 2.0620e-03, 2.2148e-03,
        2.4888e-02, 5.9476e-02, 6.5922e-01, 2.8547e-02, 1.2619e-01, 6.4693e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,541][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0042, 0.0020, 0.0183, 0.2256, 0.0011, 0.0246, 0.0682, 0.0238, 0.0146,
        0.0129, 0.0490, 0.0206, 0.1607, 0.0219, 0.1462, 0.0601, 0.1444, 0.0018],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,546][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0780, 0.0342, 0.0727, 0.0563, 0.0221, 0.0336, 0.0518, 0.0363, 0.0293,
        0.0234, 0.0455, 0.0947, 0.0632, 0.1046, 0.0506, 0.1301, 0.0491, 0.0243],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,550][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0498, 0.0580, 0.0562, 0.0493, 0.0593, 0.0573, 0.0503, 0.0602, 0.0510,
        0.0668, 0.0517, 0.0600, 0.0590, 0.0481, 0.0449, 0.0568, 0.0636, 0.0576],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,555][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0069, 0.0298, 0.0279, 0.0257, 0.0426, 0.0357, 0.0587, 0.0564, 0.0512,
        0.0427, 0.0658, 0.0818, 0.0815, 0.0676, 0.0694, 0.0651, 0.1044, 0.0868],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,559][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0945, 0.0405, 0.0179, 0.0404, 0.0692, 0.0423, 0.0176, 0.0285, 0.0315,
        0.1360, 0.0341, 0.0197, 0.0598, 0.0626, 0.0612, 0.0381, 0.1731, 0.0331],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,561][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0612, 0.0448, 0.0505, 0.0720, 0.0687, 0.0545, 0.0430, 0.0562, 0.0581,
        0.0608, 0.0649, 0.0609, 0.0459, 0.0577, 0.0428, 0.0732, 0.0483, 0.0366],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,562][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0088, 0.0047, 0.8128, 0.0831, 0.0038, 0.0036, 0.0062, 0.0039, 0.0086,
        0.0033, 0.0033, 0.0300, 0.0049, 0.0019, 0.0048, 0.0093, 0.0035, 0.0036],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,563][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0465, 0.0562, 0.0524, 0.0503, 0.0543, 0.0548, 0.0548, 0.0540, 0.0543,
        0.0559, 0.0563, 0.0548, 0.0554, 0.0604, 0.0611, 0.0615, 0.0583, 0.0588],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,564][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0373, 0.0458, 0.0491, 0.0532, 0.0507, 0.0570, 0.0549, 0.0521, 0.0562,
        0.0541, 0.0502, 0.0547, 0.0569, 0.0649, 0.0653, 0.0656, 0.0655, 0.0664],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:38,567][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.0336, 0.0450, 0.0525, 0.0451, 0.0411, 0.0441, 0.0478, 0.0587, 0.0618,
        0.0553, 0.0571, 0.0681, 0.0549, 0.0533, 0.0604, 0.0618, 0.0552, 0.0498,
        0.0544], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,572][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0656, 0.0441, 0.0481, 0.0432, 0.0550, 0.0522, 0.0534, 0.0526, 0.0494,
        0.0623, 0.0540, 0.0540, 0.0533, 0.0554, 0.0460, 0.0540, 0.0648, 0.0427,
        0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,575][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([2.2900e-03, 3.1060e-04, 6.8981e-06, 4.7837e-04, 9.3707e-02, 5.5437e-03,
        4.9156e-04, 1.5233e-03, 6.9617e-02, 4.1912e-02, 2.6618e-03, 3.3040e-03,
        2.3464e-02, 3.2203e-02, 3.8326e-01, 2.4577e-02, 2.9848e-01, 1.6136e-02,
        3.5609e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,577][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([5.8724e-03, 1.9459e-03, 1.7838e-04, 4.1778e-01, 7.8472e-04, 5.8482e-02,
        2.4841e-03, 5.5251e-03, 3.4309e-03, 7.3831e-02, 1.7599e-02, 4.1817e-03,
        2.2695e-01, 8.1429e-03, 1.1979e-03, 7.8159e-02, 9.1110e-02, 2.2206e-03,
        1.1989e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,582][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0840, 0.0395, 0.0781, 0.0530, 0.0213, 0.0323, 0.0472, 0.0340, 0.0287,
        0.0224, 0.0422, 0.0916, 0.0528, 0.1037, 0.0482, 0.1110, 0.0384, 0.0234,
        0.0481], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,587][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0474, 0.0555, 0.0507, 0.0469, 0.0567, 0.0552, 0.0480, 0.0549, 0.0480,
        0.0638, 0.0476, 0.0607, 0.0547, 0.0436, 0.0437, 0.0540, 0.0624, 0.0534,
        0.0529], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,591][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0065, 0.0267, 0.0242, 0.0233, 0.0392, 0.0339, 0.0546, 0.0534, 0.0478,
        0.0398, 0.0612, 0.0745, 0.0742, 0.0611, 0.0640, 0.0590, 0.0956, 0.0793,
        0.0816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,593][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.1354, 0.0826, 0.0043, 0.0439, 0.0564, 0.0184, 0.0120, 0.0214, 0.0114,
        0.1061, 0.0269, 0.0125, 0.0662, 0.0262, 0.0471, 0.0382, 0.2193, 0.0626,
        0.0092], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,594][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0620, 0.0560, 0.0408, 0.0664, 0.0624, 0.0524, 0.0423, 0.0584, 0.0511,
        0.0545, 0.0595, 0.0599, 0.0448, 0.0497, 0.0373, 0.0708, 0.0465, 0.0452,
        0.0401], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,595][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([2.8278e-02, 1.7812e-02, 4.5392e-03, 4.2099e-01, 5.1165e-03, 7.3287e-03,
        1.2550e-02, 4.0631e-03, 1.1594e-02, 4.7175e-02, 4.0915e-02, 2.1052e-01,
        5.4074e-02, 3.1566e-03, 3.7394e-04, 1.3775e-02, 1.0167e-01, 1.1806e-02,
        4.2595e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,596][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0436, 0.0527, 0.0494, 0.0475, 0.0514, 0.0517, 0.0516, 0.0511, 0.0515,
        0.0529, 0.0532, 0.0518, 0.0525, 0.0567, 0.0576, 0.0583, 0.0555, 0.0555,
        0.0556], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,599][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0353, 0.0432, 0.0458, 0.0502, 0.0476, 0.0535, 0.0515, 0.0490, 0.0523,
        0.0505, 0.0471, 0.0511, 0.0532, 0.0608, 0.0607, 0.0612, 0.0612, 0.0624,
        0.0632], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:38,604][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0342, 0.0502, 0.0557, 0.0432, 0.0398, 0.0404, 0.0416, 0.0560, 0.0591,
        0.0494, 0.0516, 0.0614, 0.0507, 0.0522, 0.0590, 0.0579, 0.0509, 0.0508,
        0.0527, 0.0431], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,609][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0658, 0.0430, 0.0498, 0.0412, 0.0534, 0.0512, 0.0505, 0.0510, 0.0461,
        0.0605, 0.0512, 0.0520, 0.0500, 0.0519, 0.0434, 0.0504, 0.0591, 0.0406,
        0.0506, 0.0382], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,611][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([6.5330e-04, 1.3160e-03, 7.2910e-04, 6.8654e-06, 1.7173e-02, 9.7233e-03,
        1.3189e-03, 1.1001e-03, 3.6694e-03, 8.6112e-02, 1.3264e-03, 5.1063e-03,
        2.8116e-02, 2.1480e-02, 4.9949e-01, 3.2254e-03, 2.8653e-01, 2.9090e-02,
        3.8032e-03, 2.5856e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,614][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([1.0330e-02, 1.6862e-02, 2.4196e-02, 5.3730e-03, 1.3312e-03, 1.1892e-01,
        4.0505e-03, 5.8848e-04, 1.5107e-04, 1.0709e-02, 2.5636e-02, 5.3025e-03,
        5.9455e-01, 8.8288e-03, 9.0047e-03, 3.2497e-03, 1.2321e-01, 1.4707e-02,
        1.8075e-02, 4.9245e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,619][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0420, 0.0319, 0.0628, 0.0478, 0.0223, 0.0286, 0.0481, 0.0365, 0.0308,
        0.0267, 0.0445, 0.0792, 0.0600, 0.1070, 0.0453, 0.1039, 0.0480, 0.0286,
        0.0552, 0.0510], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,623][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0456, 0.0528, 0.0504, 0.0423, 0.0538, 0.0541, 0.0476, 0.0535, 0.0452,
        0.0598, 0.0452, 0.0574, 0.0534, 0.0427, 0.0424, 0.0493, 0.0587, 0.0516,
        0.0532, 0.0412], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,625][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0059, 0.0253, 0.0232, 0.0219, 0.0371, 0.0318, 0.0513, 0.0490, 0.0447,
        0.0375, 0.0565, 0.0697, 0.0692, 0.0575, 0.0607, 0.0547, 0.0888, 0.0748,
        0.0763, 0.0640], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,626][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.1091, 0.1332, 0.0179, 0.0251, 0.0445, 0.0205, 0.0167, 0.0205, 0.0119,
        0.0753, 0.0186, 0.0168, 0.0584, 0.0363, 0.0500, 0.0234, 0.1585, 0.0931,
        0.0343, 0.0360], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,627][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0577, 0.0552, 0.0513, 0.0543, 0.0591, 0.0467, 0.0417, 0.0496, 0.0509,
        0.0474, 0.0540, 0.0571, 0.0393, 0.0473, 0.0383, 0.0598, 0.0434, 0.0430,
        0.0486, 0.0553], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,628][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0007, 0.1070, 0.2154, 0.0007, 0.0208, 0.0047, 0.0039, 0.0006, 0.0031,
        0.0015, 0.0118, 0.2547, 0.0027, 0.0160, 0.0046, 0.0023, 0.0016, 0.0614,
        0.2859, 0.0007], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,631][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0410, 0.0496, 0.0466, 0.0451, 0.0488, 0.0491, 0.0491, 0.0485, 0.0489,
        0.0503, 0.0506, 0.0490, 0.0498, 0.0537, 0.0546, 0.0554, 0.0528, 0.0525,
        0.0527, 0.0521], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,632][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0348, 0.0415, 0.0438, 0.0474, 0.0453, 0.0498, 0.0485, 0.0467, 0.0488,
        0.0474, 0.0446, 0.0479, 0.0498, 0.0563, 0.0568, 0.0571, 0.0573, 0.0583,
        0.0589, 0.0590], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:38,634][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0326, 0.0463, 0.0508, 0.0400, 0.0395, 0.0383, 0.0407, 0.0526, 0.0591,
        0.0469, 0.0482, 0.0594, 0.0501, 0.0521, 0.0580, 0.0559, 0.0503, 0.0483,
        0.0487, 0.0408, 0.0414], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,638][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0612, 0.0436, 0.0473, 0.0403, 0.0497, 0.0495, 0.0492, 0.0491, 0.0452,
        0.0536, 0.0477, 0.0474, 0.0456, 0.0502, 0.0401, 0.0470, 0.0535, 0.0410,
        0.0477, 0.0377, 0.0531], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,643][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0023, 0.0089, 0.0051, 0.0015, 0.0141, 0.0225, 0.0018, 0.0239, 0.0127,
        0.0079, 0.0034, 0.0202, 0.0009, 0.4135, 0.1954, 0.0169, 0.0006, 0.2153,
        0.0264, 0.0059, 0.0009], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,646][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([8.8273e-04, 8.7370e-03, 1.1021e-02, 3.4044e-01, 1.2331e-03, 8.0845e-03,
        1.7336e-03, 1.3833e-02, 5.6433e-05, 1.2494e-03, 9.6722e-02, 1.3196e-03,
        3.4530e-03, 1.2156e-02, 2.6000e-02, 4.6205e-02, 4.8456e-03, 1.1382e-02,
        1.8229e-02, 3.8824e-01, 4.1834e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,650][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0369, 0.0329, 0.0624, 0.0419, 0.0174, 0.0297, 0.0446, 0.0316, 0.0284,
        0.0209, 0.0411, 0.0873, 0.0575, 0.1094, 0.0436, 0.1094, 0.0420, 0.0257,
        0.0470, 0.0448, 0.0456], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,655][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0412, 0.0509, 0.0492, 0.0425, 0.0519, 0.0516, 0.0445, 0.0507, 0.0435,
        0.0554, 0.0427, 0.0538, 0.0485, 0.0402, 0.0398, 0.0469, 0.0526, 0.0501,
        0.0521, 0.0412, 0.0507], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,658][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0055, 0.0219, 0.0207, 0.0188, 0.0319, 0.0273, 0.0459, 0.0434, 0.0393,
        0.0331, 0.0521, 0.0622, 0.0626, 0.0529, 0.0562, 0.0508, 0.0818, 0.0699,
        0.0726, 0.0604, 0.0908], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,659][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1436, 0.1221, 0.0157, 0.0264, 0.0444, 0.0298, 0.0139, 0.0201, 0.0181,
        0.0559, 0.0163, 0.0160, 0.0334, 0.0408, 0.0346, 0.0227, 0.0753, 0.1028,
        0.0356, 0.0473, 0.0852], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,660][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0565, 0.0580, 0.0480, 0.0590, 0.0590, 0.0459, 0.0368, 0.0476, 0.0457,
        0.0416, 0.0521, 0.0529, 0.0341, 0.0469, 0.0343, 0.0604, 0.0352, 0.0459,
        0.0461, 0.0601, 0.0339], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,661][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.3444e-03, 4.6541e-02, 1.8756e-01, 2.7884e-02, 2.5072e-03, 1.2103e-03,
        1.0174e-02, 1.2978e-02, 5.5820e-03, 1.0598e-03, 2.9164e-02, 1.4342e-01,
        1.0822e-04, 2.0793e-02, 2.2846e-02, 8.2948e-03, 1.0364e-04, 3.5165e-02,
        3.8521e-01, 5.7951e-02, 1.1061e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,665][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0381, 0.0466, 0.0439, 0.0425, 0.0463, 0.0462, 0.0466, 0.0463, 0.0466,
        0.0479, 0.0482, 0.0464, 0.0473, 0.0508, 0.0517, 0.0528, 0.0503, 0.0497,
        0.0501, 0.0496, 0.0519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,669][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0319, 0.0387, 0.0411, 0.0447, 0.0428, 0.0470, 0.0458, 0.0440, 0.0460,
        0.0446, 0.0421, 0.0453, 0.0471, 0.0535, 0.0538, 0.0542, 0.0542, 0.0553,
        0.0560, 0.0561, 0.0555], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:38,673][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:38,676][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 3305],
        [  499],
        [ 1373],
        [ 3317],
        [ 3726],
        [ 6151],
        [18955],
        [11745],
        [16745],
        [ 6438],
        [ 9306],
        [ 4542],
        [ 5611],
        [ 2080],
        [ 1578],
        [ 1812],
        [ 1529],
        [ 1162],
        [ 1209],
        [ 3602],
        [ 1737]], device='cuda:0')
[2024-07-23 21:06:38,679][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 3329],
        [ 3448],
        [ 3474],
        [ 5082],
        [ 7383],
        [ 7600],
        [20465],
        [12601],
        [12844],
        [ 5643],
        [34435],
        [ 7340],
        [ 8395],
        [ 5584],
        [ 3438],
        [ 5824],
        [ 3537],
        [ 2854],
        [ 2479],
        [ 9391],
        [ 3370]], device='cuda:0')
[2024-07-23 21:06:38,682][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[20152],
        [18867],
        [18126],
        [17931],
        [17413],
        [17080],
        [16455],
        [16329],
        [16471],
        [16159],
        [15792],
        [15767],
        [15926],
        [16109],
        [16323],
        [16382],
        [16429],
        [16704],
        [16806],
        [16848],
        [16903]], device='cuda:0')
[2024-07-23 21:06:38,685][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[45645],
        [41285],
        [45925],
        [45669],
        [45026],
        [44591],
        [44831],
        [45024],
        [45025],
        [44752],
        [45069],
        [44881],
        [44968],
        [45399],
        [45657],
        [45318],
        [45047],
        [44787],
        [45022],
        [45226],
        [45111]], device='cuda:0')
[2024-07-23 21:06:38,689][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[30543],
        [22842],
        [15766],
        [26625],
        [26701],
        [26841],
        [26575],
        [25018],
        [25029],
        [25043],
        [25107],
        [25047],
        [24983],
        [11005],
        [12087],
        [12695],
        [12520],
        [12287],
        [12916],
        [18374],
        [21530]], device='cuda:0')
[2024-07-23 21:06:38,692][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[22254],
        [12154],
        [12000],
        [10894],
        [17090],
        [19335],
        [18760],
        [19256],
        [19023],
        [20106],
        [20301],
        [21300],
        [20253],
        [21369],
        [20973],
        [20317],
        [17233],
        [18098],
        [18023],
        [19656],
        [15573]], device='cuda:0')
[2024-07-23 21:06:38,695][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[16856],
        [18063],
        [18280],
        [17737],
        [15902],
        [16928],
        [17353],
        [19874],
        [20563],
        [20122],
        [19863],
        [20350],
        [19745],
        [19168],
        [18220],
        [17471],
        [15779],
        [15358],
        [15263],
        [14401],
        [13705]], device='cuda:0')
[2024-07-23 21:06:38,696][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 1722],
        [ 7931],
        [17028],
        [21675],
        [27059],
        [14075],
        [17438],
        [18561],
        [25224],
        [22682],
        [21695],
        [25122],
        [21201],
        [18549],
        [11529],
        [13968],
        [16992],
        [20133],
        [16825],
        [20468],
        [27586]], device='cuda:0')
[2024-07-23 21:06:38,698][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[12584],
        [18318],
        [18790],
        [15629],
        [15852],
        [15855],
        [16129],
        [16005],
        [15854],
        [15861],
        [15823],
        [15700],
        [15544],
        [15487],
        [15910],
        [15819],
        [15758],
        [15673],
        [15677],
        [15413],
        [15295]], device='cuda:0')
[2024-07-23 21:06:38,700][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[27201],
        [16363],
        [13130],
        [12170],
        [17691],
        [14331],
        [16521],
        [23380],
        [18830],
        [17822],
        [24648],
        [21119],
        [17726],
        [23970],
        [18756],
        [20754],
        [19250],
        [30534],
        [24216],
        [20468],
        [18062]], device='cuda:0')
[2024-07-23 21:06:38,703][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 5410],
        [10820],
        [ 8158],
        [ 5564],
        [ 6459],
        [ 6690],
        [ 6318],
        [ 6562],
        [ 6466],
        [ 6653],
        [ 6595],
        [ 6995],
        [ 6611],
        [ 5983],
        [ 6027],
        [ 5740],
        [ 5854],
        [ 5095],
        [ 5381],
        [ 5439],
        [ 5627]], device='cuda:0')
[2024-07-23 21:06:38,706][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[  95],
        [ 426],
        [1361],
        [2160],
        [ 849],
        [ 526],
        [ 604],
        [ 369],
        [ 718],
        [ 728],
        [ 348],
        [ 460],
        [ 927],
        [ 609],
        [ 771],
        [1147],
        [ 703],
        [ 713],
        [ 377],
        [1232],
        [ 671]], device='cuda:0')
[2024-07-23 21:06:38,709][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[33221],
        [42008],
        [43236],
        [43817],
        [43501],
        [43726],
        [43361],
        [43009],
        [42352],
        [42090],
        [41916],
        [41614],
        [42008],
        [42097],
        [42111],
        [42557],
        [42483],
        [42782],
        [42891],
        [42981],
        [42772]], device='cuda:0')
[2024-07-23 21:06:38,712][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[28198],
        [17474],
        [11555],
        [10130],
        [ 8174],
        [ 8329],
        [ 8352],
        [ 8102],
        [ 7874],
        [ 6984],
        [ 7080],
        [ 8023],
        [ 7700],
        [ 7174],
        [ 8245],
        [11796],
        [ 9562],
        [11825],
        [10718],
        [10412],
        [ 9060]], device='cuda:0')
[2024-07-23 21:06:38,716][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 4485],
        [  633],
        [11010],
        [ 5809],
        [ 6384],
        [ 6663],
        [14188],
        [20202],
        [37746],
        [20289],
        [ 6895],
        [11086],
        [12795],
        [ 8204],
        [13231],
        [ 5682],
        [14034],
        [ 2267],
        [12225],
        [ 5859],
        [13185]], device='cuda:0')
[2024-07-23 21:06:38,719][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[11786],
        [16140],
        [16858],
        [14592],
        [13389],
        [12502],
        [11630],
        [11377],
        [11281],
        [11091],
        [10983],
        [11047],
        [10885],
        [11526],
        [11741],
        [11671],
        [11403],
        [11504],
        [11341],
        [11179],
        [11064]], device='cuda:0')
[2024-07-23 21:06:38,722][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[9270],
        [7532],
        [6523],
        [6917],
        [6745],
        [5974],
        [6098],
        [6242],
        [6415],
        [6547],
        [6552],
        [6672],
        [6849],
        [6794],
        [7102],
        [7473],
        [7676],
        [7681],
        [7484],
        [7559],
        [7677]], device='cuda:0')
[2024-07-23 21:06:38,725][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1895],
        [1872],
        [1787],
        [5954],
        [5373],
        [3780],
        [7436],
        [4917],
        [5993],
        [4097],
        [4438],
        [4725],
        [6462],
        [4342],
        [7589],
        [6641],
        [6848],
        [5277],
        [3405],
        [4312],
        [5527]], device='cuda:0')
[2024-07-23 21:06:38,728][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 4812],
        [ 2861],
        [ 3042],
        [ 1566],
        [ 9147],
        [11267],
        [ 7672],
        [ 1671],
        [ 5931],
        [11893],
        [11783],
        [ 6878],
        [ 8892],
        [ 8372],
        [ 3989],
        [ 9032],
        [11161],
        [10182],
        [10684],
        [10272],
        [ 9388]], device='cuda:0')
[2024-07-23 21:06:38,731][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[11168],
        [23077],
        [22744],
        [18342],
        [21687],
        [20626],
        [24525],
        [25945],
        [27050],
        [27484],
        [29991],
        [28153],
        [29064],
        [30045],
        [29808],
        [30577],
        [30901],
        [30568],
        [31039],
        [32091],
        [32091]], device='cuda:0')
[2024-07-23 21:06:38,733][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[5213],
        [3052],
        [2971],
        [2520],
        [2563],
        [2517],
        [2377],
        [2516],
        [2528],
        [2659],
        [2833],
        [2801],
        [2796],
        [2804],
        [2874],
        [2798],
        [2757],
        [2733],
        [2684],
        [2608],
        [2582]], device='cuda:0')
[2024-07-23 21:06:38,735][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[9828],
        [8754],
        [9361],
        [8620],
        [8565],
        [8463],
        [8270],
        [8302],
        [8025],
        [8117],
        [8182],
        [7999],
        [7906],
        [7978],
        [7906],
        [7863],
        [7894],
        [7986],
        [8047],
        [8016],
        [8030]], device='cuda:0')
[2024-07-23 21:06:38,737][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 953],
        [ 972],
        [ 931],
        [ 846],
        [ 886],
        [ 982],
        [ 988],
        [1071],
        [1066],
        [1076],
        [1092],
        [1175],
        [1157],
        [1205],
        [1244],
        [1231],
        [1221],
        [1264],
        [1225],
        [1196],
        [1162]], device='cuda:0')
[2024-07-23 21:06:38,739][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[12498],
        [18029],
        [15665],
        [14831],
        [14405],
        [13909],
        [14560],
        [13925],
        [13752],
        [13295],
        [13018],
        [12623],
        [12098],
        [11428],
        [11322],
        [11147],
        [10719],
        [10722],
        [11061],
        [11284],
        [11080]], device='cuda:0')
[2024-07-23 21:06:38,743][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 4083],
        [ 7159],
        [ 8670],
        [14982],
        [18631],
        [20634],
        [20496],
        [22398],
        [23060],
        [17216],
        [21574],
        [21460],
        [19697],
        [17750],
        [15646],
        [14690],
        [23535],
        [15107],
        [14317],
        [21142],
        [21216]], device='cuda:0')
[2024-07-23 21:06:38,746][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[2840],
        [1794],
        [1923],
        [2011],
        [2266],
        [2294],
        [2444],
        [2646],
        [2748],
        [2972],
        [3097],
        [3061],
        [3089],
        [3018],
        [2930],
        [2902],
        [2951],
        [2869],
        [2948],
        [3062],
        [3141]], device='cuda:0')
[2024-07-23 21:06:38,749][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[13283],
        [12653],
        [12136],
        [11947],
        [12002],
        [11894],
        [11790],
        [11957],
        [12007],
        [12006],
        [12037],
        [11984],
        [11918],
        [11861],
        [11808],
        [11755],
        [11719],
        [11721],
        [11660],
        [11633],
        [11598]], device='cuda:0')
[2024-07-23 21:06:38,752][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[23116],
        [23116],
        [24334],
        [24977],
        [22480],
        [24270],
        [22103],
        [24939],
        [22303],
        [23873],
        [21108],
        [22230],
        [22933],
        [22695],
        [22771],
        [21902],
        [21541],
        [22637],
        [22501],
        [22540],
        [23490]], device='cuda:0')
[2024-07-23 21:06:38,755][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[38478],
        [47497],
        [25438],
        [35799],
        [36211],
        [36597],
        [43331],
        [23830],
        [30489],
        [31180],
        [23771],
        [26720],
        [22445],
        [30567],
        [27802],
        [33918],
        [23593],
        [42352],
        [29473],
        [35393],
        [22486]], device='cuda:0')
[2024-07-23 21:06:38,759][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801],
        [6801]], device='cuda:0')
[2024-07-23 21:06:38,811][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:38,812][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,814][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,816][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,819][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,822][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,826][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,830][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,833][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,837][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,841][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,842][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,842][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:38,843][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.4366, 0.5634], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,844][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9785, 0.0215], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,846][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1656, 0.8344], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,849][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8012, 0.1988], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,854][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.5595, 0.4405], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,858][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7695, 0.2305], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,863][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.7023, 0.2977], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,867][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.1341, 0.8659], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,872][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.2206, 0.7794], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,874][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.3903, 0.6097], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,874][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.4097, 0.5903], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,875][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ language] are: tensor([8.4767e-05, 9.9992e-01], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:38,876][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.3197, 0.4333, 0.2470], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,876][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.9486, 0.0315, 0.0199], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,879][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0876, 0.4489, 0.4635], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,883][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.6532, 0.1680, 0.1788], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,887][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.4498, 0.3232, 0.2270], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,892][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.6748, 0.1551, 0.1701], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,897][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.4830, 0.2279, 0.2890], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,901][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0091, 0.0351, 0.9558], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,906][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.3094, 0.5048, 0.1859], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,906][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.1748, 0.2825, 0.5427], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,907][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.2604, 0.3739, 0.3657], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,908][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ used] are: tensor([9.5303e-21, 1.1570e-20, 1.0000e+00], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:38,908][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.2285, 0.3379, 0.2455, 0.1881], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,911][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.9587, 0.0086, 0.0052, 0.0275], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,915][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0648, 0.3039, 0.3072, 0.3241], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,919][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.5536, 0.1277, 0.1462, 0.1725], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,924][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.3096, 0.2709, 0.1885, 0.2311], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,929][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.6569, 0.1431, 0.0972, 0.1028], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,933][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.3275, 0.1865, 0.2198, 0.2662], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,938][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0037, 0.0133, 0.2420, 0.7410], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,938][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.1817, 0.5834, 0.1101, 0.1247], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,939][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.1253, 0.1893, 0.3467, 0.3386], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,940][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.1988, 0.2760, 0.2604, 0.2647], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,941][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ by] are: tensor([6.7510e-12, 1.8332e-20, 6.0514e-05, 9.9994e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:38,943][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.1802, 0.2875, 0.1819, 0.1901, 0.1604], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,947][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.9344, 0.0069, 0.0036, 0.0257, 0.0294], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,952][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0407, 0.2335, 0.2389, 0.2461, 0.2408], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,956][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.4573, 0.1160, 0.1330, 0.1631, 0.1306], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,961][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.3319, 0.2222, 0.1570, 0.1621, 0.1268], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,966][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.4947, 0.1369, 0.1115, 0.1182, 0.1387], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,970][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.2825, 0.1280, 0.1632, 0.2997, 0.1265], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,971][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0062, 0.0157, 0.2190, 0.6198, 0.1392], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,972][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.1789, 0.3980, 0.1511, 0.0787, 0.1934], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,972][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0707, 0.1319, 0.2334, 0.3940, 0.1700], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,973][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.1640, 0.2300, 0.2140, 0.2077, 0.1843], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,975][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([8.2856e-10, 1.4282e-23, 1.0413e-05, 9.9999e-01, 7.4824e-08],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:38,978][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1450, 0.2239, 0.1519, 0.1476, 0.1567, 0.1750], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,983][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.8162, 0.0128, 0.0075, 0.0612, 0.0770, 0.0253], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,987][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0380, 0.1849, 0.1822, 0.2035, 0.2036, 0.1879], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,992][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.3870, 0.1087, 0.1245, 0.1573, 0.1233, 0.0991], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:38,997][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.2777, 0.1864, 0.1346, 0.1536, 0.0900, 0.1578], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,001][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.4987, 0.1161, 0.0931, 0.0915, 0.1169, 0.0837], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,003][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.2629, 0.1186, 0.1387, 0.2145, 0.1124, 0.1529], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,004][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0015, 0.0171, 0.1662, 0.5661, 0.1278, 0.1212], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,005][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.1620, 0.5703, 0.0511, 0.0365, 0.1382, 0.0419], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,005][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0655, 0.1283, 0.1843, 0.3028, 0.2008, 0.1183], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,007][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.1378, 0.1845, 0.1793, 0.1830, 0.1593, 0.1561], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,010][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ B] are: tensor([1.1604e-14, 1.0456e-17, 2.2027e-05, 9.4935e-01, 5.0555e-02, 7.1404e-05],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,014][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.1347, 0.1634, 0.1154, 0.1305, 0.1038, 0.1553, 0.1968],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,019][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.8383, 0.0091, 0.0047, 0.0424, 0.0482, 0.0170, 0.0403],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,024][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0303, 0.1494, 0.1565, 0.1668, 0.1683, 0.1610, 0.1678],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,028][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.3558, 0.0981, 0.1113, 0.1422, 0.1110, 0.0904, 0.0912],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,033][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.2117, 0.1729, 0.1190, 0.1425, 0.0777, 0.1250, 0.1512],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,035][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.4108, 0.1063, 0.0903, 0.0862, 0.1125, 0.0754, 0.1184],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,036][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.2030, 0.0957, 0.1185, 0.2180, 0.1012, 0.1574, 0.1061],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,037][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0035, 0.0100, 0.0965, 0.5531, 0.1278, 0.1362, 0.0730],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,038][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.1105, 0.4217, 0.0345, 0.0562, 0.2300, 0.0216, 0.1255],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,040][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0484, 0.1173, 0.1710, 0.2755, 0.1448, 0.1186, 0.1244],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,043][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.1141, 0.1589, 0.1588, 0.1506, 0.1335, 0.1318, 0.1522],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,046][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [aut] are: tensor([6.9291e-10, 4.6756e-23, 3.2224e-15, 7.3432e-06, 5.7275e-08, 1.8019e-05,
        9.9997e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,051][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.1058, 0.1322, 0.0983, 0.1109, 0.0850, 0.1159, 0.1707, 0.1812],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,055][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.6966, 0.0094, 0.0057, 0.0463, 0.0697, 0.0245, 0.0601, 0.0878],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,060][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0232, 0.1297, 0.1333, 0.1432, 0.1427, 0.1426, 0.1424, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,065][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.3180, 0.0885, 0.0988, 0.1266, 0.0964, 0.0808, 0.0802, 0.1108],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,068][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.1861, 0.1507, 0.1018, 0.1137, 0.0633, 0.0919, 0.1245, 0.1681],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,069][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.3829, 0.0974, 0.0838, 0.0866, 0.1060, 0.0723, 0.1145, 0.0566],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,070][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.1591, 0.0937, 0.1338, 0.2123, 0.0899, 0.1607, 0.1026, 0.0479],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,070][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0035, 0.0064, 0.0455, 0.1573, 0.0551, 0.0502, 0.0423, 0.6398],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,072][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.1235, 0.3288, 0.1047, 0.0381, 0.1542, 0.0347, 0.1493, 0.0667],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,076][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.0494, 0.0883, 0.1674, 0.2235, 0.1544, 0.1080, 0.1229, 0.0861],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,079][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.1004, 0.1367, 0.1320, 0.1326, 0.1205, 0.1153, 0.1362, 0.1262],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,082][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ista] are: tensor([2.3102e-21, 2.5377e-38, 3.8923e-31, 2.5601e-19, 2.6453e-20, 9.4774e-12,
        9.9931e-01, 6.9327e-04], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,087][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0808, 0.1209, 0.0878, 0.0777, 0.0818, 0.1091, 0.1587, 0.1767, 0.1066],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,092][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.8050, 0.0077, 0.0041, 0.0283, 0.0355, 0.0102, 0.0381, 0.0570, 0.0142],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,096][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0209, 0.1183, 0.1194, 0.1299, 0.1289, 0.1275, 0.1278, 0.1267, 0.1006],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,101][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.2882, 0.0778, 0.0911, 0.1144, 0.0881, 0.0719, 0.0743, 0.1020, 0.0921],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,104][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.1498, 0.1371, 0.0920, 0.0970, 0.0529, 0.0812, 0.0983, 0.1244, 0.1673],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,104][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.3292, 0.0907, 0.0731, 0.0760, 0.0893, 0.0637, 0.1024, 0.0512, 0.1244],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,105][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.1562, 0.0854, 0.1127, 0.1793, 0.0843, 0.1407, 0.1031, 0.0569, 0.0814],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,106][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0014, 0.0038, 0.0584, 0.1590, 0.0378, 0.0339, 0.0298, 0.5324, 0.1435],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,108][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.1042, 0.1220, 0.0566, 0.0122, 0.1827, 0.0296, 0.1051, 0.0911, 0.2965],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,111][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0460, 0.1043, 0.1518, 0.2339, 0.1247, 0.0857, 0.1299, 0.0748, 0.0489],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,116][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0926, 0.1239, 0.1221, 0.1184, 0.1051, 0.1008, 0.1223, 0.1094, 0.1054],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,119][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ de] are: tensor([1.6330e-17, 5.7923e-39, 8.2496e-29, 1.4946e-19, 2.3158e-21, 3.6272e-19,
        3.0564e-06, 1.1766e-06, 1.0000e+00], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,123][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0801, 0.1171, 0.0813, 0.0858, 0.0650, 0.0861, 0.1271, 0.1466, 0.1233,
        0.0876], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,128][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.6712, 0.0105, 0.0054, 0.0406, 0.0560, 0.0182, 0.0544, 0.0988, 0.0270,
        0.0180], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,133][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0223, 0.1027, 0.1030, 0.1112, 0.1132, 0.1093, 0.1136, 0.1118, 0.0939,
        0.1190], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,136][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.2642, 0.0715, 0.0821, 0.1030, 0.0811, 0.0650, 0.0690, 0.0941, 0.0857,
        0.0842], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,137][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.1501, 0.1072, 0.0774, 0.0813, 0.0557, 0.0770, 0.0923, 0.1098, 0.1387,
        0.1105], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,138][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.2222, 0.0899, 0.0751, 0.0771, 0.0921, 0.0606, 0.0907, 0.0637, 0.1168,
        0.1117], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,139][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.1661, 0.0804, 0.0971, 0.1314, 0.0758, 0.1090, 0.0893, 0.0676, 0.0771,
        0.1063], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,141][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0007, 0.0034, 0.0440, 0.1145, 0.0337, 0.0265, 0.0291, 0.4201, 0.0944,
        0.2336], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,144][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0665, 0.2242, 0.0610, 0.0196, 0.0553, 0.0200, 0.0682, 0.0675, 0.4091,
        0.0086], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,149][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0530, 0.0859, 0.1493, 0.2101, 0.1233, 0.0831, 0.0908, 0.0771, 0.0533,
        0.0743], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,154][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0825, 0.1093, 0.1082, 0.1042, 0.0952, 0.0954, 0.1096, 0.1003, 0.0903,
        0.1051], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,157][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ An] are: tensor([1.0876e-27, 9.9927e-42, 1.1909e-32, 2.4792e-23, 3.7508e-25, 8.5708e-20,
        1.3692e-07, 5.6967e-08, 1.0000e+00, 1.1704e-11], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,161][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0713, 0.0930, 0.0648, 0.0716, 0.0607, 0.0847, 0.1118, 0.1295, 0.1090,
        0.0828, 0.1208], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,166][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.6420, 0.0112, 0.0065, 0.0539, 0.0649, 0.0206, 0.0486, 0.0747, 0.0278,
        0.0210, 0.0289], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,168][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.0191, 0.0907, 0.0976, 0.1006, 0.1013, 0.0997, 0.1001, 0.1002, 0.0812,
        0.1112, 0.0983], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,169][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.2404, 0.0664, 0.0742, 0.0937, 0.0757, 0.0617, 0.0639, 0.0862, 0.0802,
        0.0833, 0.0743], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,170][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.1404, 0.0948, 0.0658, 0.0689, 0.0518, 0.0699, 0.0870, 0.0979, 0.1264,
        0.1038, 0.0932], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,171][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.2122, 0.0769, 0.0691, 0.0743, 0.0813, 0.0507, 0.0789, 0.0452, 0.1041,
        0.0907, 0.1166], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,174][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.1208, 0.0689, 0.0952, 0.1581, 0.0629, 0.1194, 0.0834, 0.0420, 0.0781,
        0.1272, 0.0439], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,177][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0013, 0.0041, 0.0304, 0.1156, 0.0441, 0.0373, 0.0157, 0.2991, 0.0957,
        0.2189, 0.1379], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,181][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0504, 0.1418, 0.0665, 0.0183, 0.0566, 0.0099, 0.0631, 0.0439, 0.4823,
        0.0189, 0.0482], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,185][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0459, 0.0789, 0.1544, 0.1613, 0.0961, 0.0926, 0.0948, 0.0684, 0.0598,
        0.0809, 0.0669], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,190][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0743, 0.0991, 0.0962, 0.0966, 0.0828, 0.0854, 0.0974, 0.0918, 0.0792,
        0.0947, 0.1024], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,193][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [za] are: tensor([9.2947e-13, 5.6047e-23, 1.3531e-18, 7.7105e-13, 1.9175e-12, 1.8814e-15,
        3.5698e-08, 1.1522e-10, 3.3025e-01, 3.4554e-08, 6.6975e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,197][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0675, 0.0860, 0.0581, 0.0652, 0.0593, 0.0710, 0.1116, 0.1139, 0.1071,
        0.0772, 0.1079, 0.0753], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,201][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.7689, 0.0060, 0.0032, 0.0253, 0.0339, 0.0129, 0.0335, 0.0531, 0.0167,
        0.0127, 0.0224, 0.0114], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,202][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0176, 0.0846, 0.0843, 0.0920, 0.0921, 0.0879, 0.0925, 0.0904, 0.0739,
        0.0961, 0.0870, 0.1017], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,202][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.2020, 0.0628, 0.0708, 0.0892, 0.0684, 0.0605, 0.0625, 0.0857, 0.0799,
        0.0780, 0.0680, 0.0721], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,203][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1156, 0.0905, 0.0638, 0.0665, 0.0417, 0.0615, 0.0729, 0.0894, 0.1159,
        0.0856, 0.0723, 0.1244], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,206][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.3437, 0.0588, 0.0482, 0.0545, 0.0683, 0.0412, 0.0524, 0.0318, 0.1006,
        0.0849, 0.0990, 0.0165], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,209][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.1187, 0.0663, 0.0908, 0.1375, 0.0610, 0.1037, 0.0734, 0.0413, 0.0686,
        0.1076, 0.0471, 0.0840], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,213][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0004, 0.0018, 0.0337, 0.0834, 0.0144, 0.0212, 0.0154, 0.1897, 0.0660,
        0.1680, 0.1137, 0.2925], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,217][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0522, 0.1724, 0.0191, 0.0188, 0.0431, 0.0101, 0.0402, 0.0324, 0.4706,
        0.0138, 0.1206, 0.0068], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,222][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0403, 0.0553, 0.0994, 0.1722, 0.1078, 0.0630, 0.0950, 0.0739, 0.0441,
        0.0860, 0.0629, 0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,227][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0664, 0.0870, 0.0852, 0.0878, 0.0808, 0.0754, 0.0885, 0.0821, 0.0764,
        0.0880, 0.0934, 0.0889], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,230][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ is] are: tensor([1.6816e-43, 0.0000e+00, 7.9533e-40, 2.9967e-30, 2.7797e-31, 8.1662e-22,
        5.3923e-09, 1.4382e-10, 9.4674e-01, 1.3576e-11, 5.3264e-02, 1.3433e-20],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,233][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0548, 0.0849, 0.0613, 0.0572, 0.0498, 0.0705, 0.1023, 0.1113, 0.0824,
        0.0641, 0.1080, 0.0825, 0.0707], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,234][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.7203, 0.0078, 0.0041, 0.0310, 0.0451, 0.0130, 0.0387, 0.0602, 0.0179,
        0.0129, 0.0215, 0.0125, 0.0151], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,235][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0165, 0.0771, 0.0779, 0.0836, 0.0847, 0.0805, 0.0837, 0.0827, 0.0676,
        0.0891, 0.0815, 0.0928, 0.0821], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,236][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.1986, 0.0562, 0.0650, 0.0822, 0.0648, 0.0527, 0.0557, 0.0750, 0.0698,
        0.0692, 0.0631, 0.0652, 0.0825], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,238][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1089, 0.0794, 0.0554, 0.0550, 0.0405, 0.0565, 0.0656, 0.0763, 0.1013,
        0.0810, 0.0708, 0.1034, 0.1061], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,241][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.1886, 0.0609, 0.0564, 0.0596, 0.0686, 0.0464, 0.0752, 0.0459, 0.1004,
        0.0835, 0.1107, 0.0204, 0.0835], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,245][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.1248, 0.0640, 0.0785, 0.1089, 0.0539, 0.0838, 0.0659, 0.0427, 0.0553,
        0.0829, 0.0467, 0.0901, 0.1026], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,248][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ a] are: tensor([2.8209e-04, 1.3717e-03, 2.9162e-02, 5.0617e-02, 8.5452e-03, 1.0809e-02,
        1.0558e-02, 1.3923e-01, 3.4595e-02, 1.0018e-01, 6.2025e-02, 2.4568e-01,
        3.0694e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,252][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0345, 0.0844, 0.0390, 0.0182, 0.0421, 0.0124, 0.0445, 0.0485, 0.5949,
        0.0119, 0.0379, 0.0062, 0.0254], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,257][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0359, 0.0599, 0.1108, 0.1392, 0.1004, 0.0700, 0.0730, 0.0611, 0.0499,
        0.0607, 0.0591, 0.1149, 0.0650], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,262][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0616, 0.0816, 0.0806, 0.0792, 0.0717, 0.0709, 0.0814, 0.0756, 0.0686,
        0.0795, 0.0844, 0.0816, 0.0833], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,265][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ a] are: tensor([6.1106e-31, 8.8154e-40, 3.9217e-30, 2.5664e-20, 8.2445e-24, 3.3609e-19,
        8.8218e-06, 3.6452e-04, 5.5361e-01, 1.6004e-09, 4.4602e-01, 2.1743e-12,
        1.5122e-12], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:39,266][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0557, 0.0626, 0.0493, 0.0486, 0.0475, 0.0640, 0.0890, 0.0917, 0.0850,
        0.0665, 0.0938, 0.0759, 0.0731, 0.0975], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,267][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.8337, 0.0038, 0.0018, 0.0152, 0.0240, 0.0072, 0.0195, 0.0286, 0.0094,
        0.0078, 0.0132, 0.0073, 0.0082, 0.0203], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,268][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0125, 0.0703, 0.0708, 0.0781, 0.0795, 0.0728, 0.0789, 0.0793, 0.0599,
        0.0839, 0.0764, 0.0864, 0.0771, 0.0741], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,269][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.1587, 0.0535, 0.0616, 0.0774, 0.0603, 0.0511, 0.0522, 0.0739, 0.0684,
        0.0672, 0.0585, 0.0617, 0.0782, 0.0775], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,273][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.1088, 0.0708, 0.0500, 0.0512, 0.0329, 0.0488, 0.0596, 0.0746, 0.0894,
        0.0654, 0.0561, 0.0978, 0.0871, 0.1076], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,277][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.1662, 0.0528, 0.0548, 0.0684, 0.0754, 0.0433, 0.0650, 0.0414, 0.1032,
        0.0752, 0.1043, 0.0233, 0.0804, 0.0462], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,282][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.1112, 0.0570, 0.0686, 0.1009, 0.0500, 0.0796, 0.0617, 0.0376, 0.0511,
        0.0771, 0.0410, 0.0799, 0.0974, 0.0869], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,285][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([2.3499e-04, 1.2670e-03, 1.3153e-02, 2.7672e-02, 6.5057e-03, 7.0079e-03,
        6.0549e-03, 6.7019e-02, 2.3639e-02, 7.2439e-02, 4.0079e-02, 1.4378e-01,
        1.9832e-01, 3.9283e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,289][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0456, 0.0923, 0.0153, 0.0104, 0.0384, 0.0047, 0.0322, 0.0335, 0.3503,
        0.0157, 0.2577, 0.0094, 0.0611, 0.0333], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,294][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0308, 0.0442, 0.0954, 0.1151, 0.0801, 0.0678, 0.0811, 0.0553, 0.0382,
        0.0676, 0.0431, 0.1246, 0.0749, 0.0818], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,297][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0555, 0.0746, 0.0734, 0.0754, 0.0627, 0.0658, 0.0749, 0.0707, 0.0642,
        0.0758, 0.0763, 0.0802, 0.0780, 0.0725], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,298][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([2.7363e-29, 0.0000e+00, 4.4186e-39, 5.5721e-26, 2.1628e-29, 1.0929e-20,
        9.3922e-09, 9.0968e-06, 4.3711e-02, 1.4445e-07, 9.5621e-01, 1.0835e-12,
        4.5375e-08, 7.4031e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:39,299][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0442, 0.0563, 0.0411, 0.0423, 0.0379, 0.0541, 0.0700, 0.0811, 0.0744,
        0.0550, 0.0860, 0.0662, 0.0597, 0.0916, 0.1401], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,300][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.7403, 0.0053, 0.0032, 0.0234, 0.0290, 0.0101, 0.0310, 0.0463, 0.0145,
        0.0092, 0.0146, 0.0083, 0.0099, 0.0266, 0.0282], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,302][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0132, 0.0660, 0.0660, 0.0734, 0.0727, 0.0692, 0.0722, 0.0726, 0.0558,
        0.0766, 0.0709, 0.0804, 0.0725, 0.0680, 0.0706], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,306][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.1867, 0.0471, 0.0547, 0.0688, 0.0528, 0.0423, 0.0445, 0.0622, 0.0578,
        0.0570, 0.0516, 0.0541, 0.0689, 0.0702, 0.0815], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,310][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0925, 0.0661, 0.0457, 0.0465, 0.0327, 0.0434, 0.0540, 0.0633, 0.0797,
        0.0660, 0.0567, 0.0890, 0.0871, 0.0921, 0.0851], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,314][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.1086, 0.0569, 0.0528, 0.0567, 0.0672, 0.0413, 0.0663, 0.0466, 0.0932,
        0.0677, 0.1081, 0.0233, 0.0751, 0.0545, 0.0818], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,319][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0882, 0.0502, 0.0611, 0.0933, 0.0446, 0.0764, 0.0590, 0.0366, 0.0469,
        0.0734, 0.0361, 0.0761, 0.0940, 0.0830, 0.0809], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,322][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ different] are: tensor([1.8494e-04, 4.7654e-04, 9.1974e-03, 2.1226e-02, 2.8327e-03, 3.2228e-03,
        3.5000e-03, 4.1021e-02, 1.0263e-02, 3.0968e-02, 2.3141e-02, 7.2382e-02,
        1.1578e-01, 2.5158e-01, 4.1422e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,326][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0392, 0.1076, 0.0250, 0.0240, 0.0416, 0.0068, 0.0395, 0.0178, 0.4375,
        0.0191, 0.0673, 0.0107, 0.0448, 0.0500, 0.0691], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,330][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0190, 0.0612, 0.1104, 0.1017, 0.0633, 0.0570, 0.0625, 0.0445, 0.0467,
        0.0584, 0.0456, 0.1082, 0.0587, 0.0880, 0.0748], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,331][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0511, 0.0720, 0.0706, 0.0697, 0.0607, 0.0625, 0.0691, 0.0685, 0.0611,
        0.0715, 0.0723, 0.0729, 0.0728, 0.0690, 0.0562], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,331][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ different] are: tensor([3.9210e-30, 0.0000e+00, 0.0000e+00, 5.4674e-31, 4.4393e-39, 1.9245e-27,
        5.9707e-12, 1.3516e-07, 4.5854e-04, 7.1261e-11, 6.9385e-01, 5.5798e-17,
        1.5816e-10, 1.8367e-01, 1.2202e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:39,332][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0429, 0.0481, 0.0417, 0.0357, 0.0299, 0.0468, 0.0678, 0.0684, 0.0589,
        0.0494, 0.0724, 0.0665, 0.0607, 0.0854, 0.1524, 0.0727],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,335][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.7004, 0.0077, 0.0036, 0.0258, 0.0279, 0.0105, 0.0292, 0.0538, 0.0161,
        0.0109, 0.0170, 0.0081, 0.0112, 0.0234, 0.0317, 0.0228],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,339][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0131, 0.0630, 0.0618, 0.0668, 0.0666, 0.0631, 0.0656, 0.0658, 0.0529,
        0.0706, 0.0643, 0.0766, 0.0654, 0.0641, 0.0665, 0.0737],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,343][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.1580, 0.0454, 0.0528, 0.0635, 0.0488, 0.0410, 0.0432, 0.0593, 0.0548,
        0.0529, 0.0484, 0.0516, 0.0627, 0.0661, 0.0747, 0.0770],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,348][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0794, 0.0588, 0.0420, 0.0440, 0.0231, 0.0362, 0.0451, 0.0638, 0.0693,
        0.0484, 0.0404, 0.0787, 0.0659, 0.0809, 0.0735, 0.1503],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,353][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.2044, 0.0486, 0.0485, 0.0518, 0.0640, 0.0428, 0.0582, 0.0321, 0.0858,
        0.0736, 0.0851, 0.0146, 0.0655, 0.0357, 0.0673, 0.0221],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,357][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0804, 0.0465, 0.0604, 0.0899, 0.0456, 0.0768, 0.0573, 0.0352, 0.0433,
        0.0712, 0.0353, 0.0670, 0.0813, 0.0769, 0.0769, 0.0561],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,360][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ from] are: tensor([6.7856e-05, 3.3732e-04, 8.9313e-03, 1.8234e-02, 2.2175e-03, 2.9438e-03,
        2.6190e-03, 2.8191e-02, 9.6228e-03, 2.6501e-02, 1.4871e-02, 5.0643e-02,
        9.0735e-02, 1.2703e-01, 1.8124e-01, 4.3581e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,362][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0221, 0.1006, 0.0225, 0.0189, 0.0258, 0.0092, 0.0324, 0.0291, 0.3442,
        0.0121, 0.0701, 0.0062, 0.0356, 0.0383, 0.2161, 0.0165],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,363][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0271, 0.0505, 0.0798, 0.0946, 0.0758, 0.0531, 0.0508, 0.0399, 0.0360,
        0.0626, 0.0502, 0.1035, 0.0655, 0.0763, 0.0722, 0.0621],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,364][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0498, 0.0657, 0.0658, 0.0648, 0.0588, 0.0564, 0.0663, 0.0610, 0.0569,
        0.0637, 0.0670, 0.0659, 0.0675, 0.0645, 0.0532, 0.0728],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,365][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ from] are: tensor([6.5280e-37, 0.0000e+00, 0.0000e+00, 1.4799e-33, 7.4192e-37, 5.6201e-26,
        1.6793e-09, 4.4323e-09, 4.4577e-04, 9.9782e-12, 9.4468e-01, 1.1000e-22,
        1.1701e-15, 6.0107e-06, 5.4864e-02, 6.6805e-08], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:39,367][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0357, 0.0533, 0.0376, 0.0357, 0.0320, 0.0446, 0.0685, 0.0749, 0.0551,
        0.0420, 0.0739, 0.0547, 0.0456, 0.0828, 0.1230, 0.0766, 0.0641],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,371][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5627, 0.0088, 0.0052, 0.0341, 0.0459, 0.0145, 0.0411, 0.0654, 0.0204,
        0.0141, 0.0214, 0.0113, 0.0148, 0.0319, 0.0399, 0.0384, 0.0302],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,376][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0119, 0.0575, 0.0573, 0.0618, 0.0621, 0.0591, 0.0608, 0.0616, 0.0492,
        0.0659, 0.0593, 0.0698, 0.0611, 0.0596, 0.0615, 0.0694, 0.0721],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,380][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1544, 0.0413, 0.0472, 0.0592, 0.0467, 0.0381, 0.0401, 0.0535, 0.0498,
        0.0503, 0.0460, 0.0474, 0.0601, 0.0603, 0.0681, 0.0698, 0.0675],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,385][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0744, 0.0526, 0.0371, 0.0374, 0.0270, 0.0397, 0.0460, 0.0536, 0.0689,
        0.0539, 0.0473, 0.0708, 0.0697, 0.0752, 0.0657, 0.0990, 0.0819],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,389][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1990, 0.0574, 0.0475, 0.0474, 0.0594, 0.0399, 0.0613, 0.0287, 0.0724,
        0.0679, 0.0873, 0.0121, 0.0661, 0.0361, 0.0764, 0.0229, 0.0182],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,394][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0924, 0.0434, 0.0520, 0.0731, 0.0369, 0.0556, 0.0459, 0.0288, 0.0391,
        0.0570, 0.0327, 0.0656, 0.0765, 0.0730, 0.0841, 0.0667, 0.0772],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,395][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.2276e-05, 2.8960e-04, 5.1952e-03, 1.4538e-02, 2.3373e-03, 2.3039e-03,
        2.8726e-03, 2.7987e-02, 7.4010e-03, 2.1980e-02, 1.3934e-02, 4.8632e-02,
        6.4725e-02, 9.5941e-02, 1.4417e-01, 3.1949e-01, 2.2811e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,396][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0270, 0.0797, 0.0198, 0.0172, 0.0419, 0.0074, 0.0293, 0.0366, 0.5549,
        0.0097, 0.0388, 0.0033, 0.0206, 0.0293, 0.0598, 0.0195, 0.0053],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,397][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0203, 0.0444, 0.0708, 0.0959, 0.0610, 0.0514, 0.0493, 0.0441, 0.0367,
        0.0474, 0.0488, 0.0841, 0.0509, 0.0873, 0.0621, 0.0766, 0.0688],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,400][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0457, 0.0619, 0.0618, 0.0611, 0.0540, 0.0538, 0.0613, 0.0584, 0.0540,
        0.0607, 0.0645, 0.0625, 0.0630, 0.0598, 0.0486, 0.0682, 0.0607],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,401][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.1944e-27, 0.0000e+00, 0.0000e+00, 2.9516e-30, 5.1913e-36, 1.4232e-28,
        8.6270e-13, 6.4622e-10, 3.0206e-06, 1.0889e-14, 9.9608e-01, 2.8349e-22,
        6.3445e-15, 4.3153e-05, 3.8156e-03, 4.4394e-05, 1.1763e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:39,406][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0399, 0.0413, 0.0310, 0.0318, 0.0288, 0.0380, 0.0621, 0.0617, 0.0527,
        0.0419, 0.0589, 0.0485, 0.0479, 0.0698, 0.1142, 0.0656, 0.0648, 0.1011],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,410][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.5139, 0.0088, 0.0048, 0.0404, 0.0453, 0.0174, 0.0372, 0.0755, 0.0242,
        0.0165, 0.0273, 0.0104, 0.0155, 0.0290, 0.0417, 0.0332, 0.0315, 0.0271],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,415][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0106, 0.0510, 0.0529, 0.0564, 0.0585, 0.0569, 0.0578, 0.0579, 0.0459,
        0.0625, 0.0568, 0.0675, 0.0584, 0.0568, 0.0570, 0.0657, 0.0709, 0.0565],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,420][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.1404, 0.0397, 0.0441, 0.0553, 0.0434, 0.0353, 0.0372, 0.0504, 0.0468,
        0.0473, 0.0430, 0.0442, 0.0557, 0.0559, 0.0649, 0.0664, 0.0639, 0.0660],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,424][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0643, 0.0500, 0.0334, 0.0327, 0.0216, 0.0337, 0.0402, 0.0513, 0.0580,
        0.0439, 0.0368, 0.0652, 0.0599, 0.0701, 0.0657, 0.1122, 0.0718, 0.0891],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,426][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.1171, 0.0425, 0.0463, 0.0557, 0.0645, 0.0424, 0.0642, 0.0377, 0.0766,
        0.0673, 0.0904, 0.0188, 0.0698, 0.0406, 0.0661, 0.0292, 0.0289, 0.0421],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,427][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0860, 0.0354, 0.0436, 0.0630, 0.0356, 0.0459, 0.0416, 0.0259, 0.0372,
        0.0546, 0.0321, 0.0616, 0.0708, 0.0717, 0.0797, 0.0615, 0.0763, 0.0773],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,428][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ language] are: tensor([1.0307e-04, 4.1809e-04, 2.0852e-03, 9.0126e-03, 2.0659e-03, 2.1863e-03,
        2.2510e-03, 2.1333e-02, 5.3277e-03, 1.6211e-02, 1.6873e-02, 3.4056e-02,
        4.8691e-02, 8.7086e-02, 9.4109e-02, 2.1872e-01, 1.5603e-01, 2.8344e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,429][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0249, 0.0923, 0.0286, 0.0121, 0.0536, 0.0107, 0.0678, 0.0267, 0.1772,
        0.0079, 0.1331, 0.0055, 0.0186, 0.0487, 0.1179, 0.0428, 0.0131, 0.1185],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,432][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0223, 0.0397, 0.0853, 0.0758, 0.0561, 0.0525, 0.0451, 0.0371, 0.0349,
        0.0487, 0.0406, 0.0823, 0.0640, 0.0615, 0.0548, 0.0742, 0.0865, 0.0384],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,435][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0420, 0.0584, 0.0551, 0.0580, 0.0505, 0.0495, 0.0577, 0.0545, 0.0547,
        0.0596, 0.0612, 0.0574, 0.0597, 0.0546, 0.0466, 0.0656, 0.0565, 0.0584],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,438][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ language] are: tensor([5.4589e-22, 0.0000e+00, 6.3199e-43, 6.2661e-27, 1.7643e-31, 2.3290e-24,
        5.7696e-11, 1.4182e-09, 1.8853e-05, 3.7140e-10, 4.9719e-03, 6.8919e-17,
        2.2894e-10, 1.1459e-03, 3.5494e-01, 2.8084e-03, 6.1920e-01, 1.6906e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:39,443][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0354, 0.0411, 0.0251, 0.0299, 0.0281, 0.0401, 0.0526, 0.0594, 0.0523,
        0.0411, 0.0589, 0.0431, 0.0464, 0.0667, 0.1001, 0.0636, 0.0623, 0.1044,
        0.0492], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,447][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.4788, 0.0136, 0.0059, 0.0412, 0.0487, 0.0158, 0.0374, 0.0634, 0.0223,
        0.0161, 0.0231, 0.0117, 0.0189, 0.0307, 0.0439, 0.0363, 0.0350, 0.0319,
        0.0256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,452][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0102, 0.0486, 0.0488, 0.0547, 0.0559, 0.0534, 0.0567, 0.0557, 0.0422,
        0.0605, 0.0525, 0.0646, 0.0554, 0.0516, 0.0535, 0.0622, 0.0674, 0.0547,
        0.0513], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,456][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.1190, 0.0374, 0.0412, 0.0522, 0.0406, 0.0335, 0.0349, 0.0483, 0.0444,
        0.0451, 0.0407, 0.0419, 0.0529, 0.0537, 0.0609, 0.0624, 0.0605, 0.0622,
        0.0682], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,458][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0694, 0.0440, 0.0320, 0.0337, 0.0201, 0.0312, 0.0366, 0.0467, 0.0589,
        0.0408, 0.0337, 0.0606, 0.0544, 0.0657, 0.0554, 0.0983, 0.0661, 0.0723,
        0.0800], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,459][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.1385, 0.0435, 0.0459, 0.0561, 0.0675, 0.0379, 0.0558, 0.0258, 0.0796,
        0.0661, 0.0896, 0.0144, 0.0691, 0.0364, 0.0627, 0.0238, 0.0221, 0.0406,
        0.0245], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,460][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0625, 0.0319, 0.0469, 0.0709, 0.0329, 0.0583, 0.0409, 0.0256, 0.0372,
        0.0574, 0.0278, 0.0494, 0.0751, 0.0701, 0.0667, 0.0488, 0.0783, 0.0789,
        0.0404], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,461][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ used] are: tensor([1.0467e-04, 1.4367e-04, 2.6733e-03, 7.0212e-03, 9.7801e-04, 1.1565e-03,
        1.2054e-03, 1.4179e-02, 2.9352e-03, 8.9190e-03, 6.9841e-03, 2.2494e-02,
        3.1993e-02, 5.6692e-02, 7.3587e-02, 1.4916e-01, 1.1501e-01, 1.2169e-01,
        3.8307e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,464][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0371, 0.0557, 0.0122, 0.0198, 0.0370, 0.0044, 0.0512, 0.0195, 0.3197,
        0.0305, 0.0610, 0.0115, 0.0403, 0.0368, 0.0807, 0.0557, 0.0189, 0.0953,
        0.0127], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,468][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0186, 0.0295, 0.0619, 0.0788, 0.0566, 0.0435, 0.0531, 0.0364, 0.0289,
        0.0477, 0.0326, 0.0776, 0.0469, 0.0668, 0.0649, 0.0592, 0.0766, 0.0298,
        0.0905], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,472][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0398, 0.0563, 0.0557, 0.0560, 0.0463, 0.0473, 0.0554, 0.0514, 0.0483,
        0.0547, 0.0564, 0.0556, 0.0553, 0.0529, 0.0446, 0.0604, 0.0537, 0.0554,
        0.0548], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,475][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ used] are: tensor([7.1827e-35, 0.0000e+00, 0.0000e+00, 1.9820e-39, 1.4013e-45, 5.8304e-35,
        2.9385e-18, 2.5973e-13, 2.6244e-13, 1.7255e-18, 2.1461e-07, 9.3238e-26,
        9.2132e-19, 8.7594e-08, 4.7656e-06, 2.9436e-09, 1.5389e-07, 1.4077e-03,
        9.9859e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:39,480][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0298, 0.0414, 0.0315, 0.0233, 0.0264, 0.0374, 0.0497, 0.0556, 0.0429,
        0.0350, 0.0563, 0.0449, 0.0435, 0.0619, 0.1003, 0.0569, 0.0560, 0.1030,
        0.0594, 0.0450], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,484][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.7570, 0.0059, 0.0025, 0.0155, 0.0182, 0.0065, 0.0173, 0.0365, 0.0094,
        0.0075, 0.0119, 0.0048, 0.0068, 0.0130, 0.0167, 0.0121, 0.0127, 0.0118,
        0.0117, 0.0221], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,489][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0103, 0.0474, 0.0480, 0.0518, 0.0518, 0.0499, 0.0514, 0.0525, 0.0400,
        0.0565, 0.0505, 0.0605, 0.0524, 0.0501, 0.0526, 0.0588, 0.0610, 0.0526,
        0.0500, 0.0520], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,490][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.1177, 0.0347, 0.0393, 0.0479, 0.0367, 0.0308, 0.0318, 0.0436, 0.0408,
        0.0402, 0.0364, 0.0382, 0.0470, 0.0482, 0.0558, 0.0579, 0.0541, 0.0567,
        0.0625, 0.0797], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,491][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0696, 0.0448, 0.0326, 0.0326, 0.0176, 0.0250, 0.0338, 0.0477, 0.0518,
        0.0343, 0.0294, 0.0562, 0.0475, 0.0594, 0.0521, 0.0938, 0.0561, 0.0690,
        0.0717, 0.0750], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,492][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.1681, 0.0423, 0.0381, 0.0476, 0.0674, 0.0414, 0.0507, 0.0280, 0.0817,
        0.0681, 0.0765, 0.0120, 0.0601, 0.0304, 0.0626, 0.0178, 0.0180, 0.0402,
        0.0168, 0.0321], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,493][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0824, 0.0345, 0.0435, 0.0600, 0.0307, 0.0440, 0.0376, 0.0259, 0.0321,
        0.0459, 0.0307, 0.0509, 0.0619, 0.0601, 0.0724, 0.0552, 0.0661, 0.0688,
        0.0452, 0.0521], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,495][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ by] are: tensor([1.1930e-04, 1.3278e-04, 2.2559e-03, 5.1238e-03, 7.5128e-04, 6.5298e-04,
        7.9624e-04, 7.8861e-03, 2.2812e-03, 6.8901e-03, 4.4725e-03, 1.2681e-02,
        2.3864e-02, 3.7490e-02, 4.5365e-02, 9.4951e-02, 7.0521e-02, 8.9391e-02,
        2.2821e-01, 3.6616e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,498][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0220, 0.0839, 0.0087, 0.0151, 0.0201, 0.0057, 0.0219, 0.0237, 0.3597,
        0.0098, 0.0604, 0.0106, 0.0413, 0.0204, 0.1086, 0.0226, 0.0080, 0.1307,
        0.0096, 0.0173], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,503][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0210, 0.0322, 0.0619, 0.0582, 0.0456, 0.0485, 0.0562, 0.0342, 0.0259,
        0.0469, 0.0289, 0.0796, 0.0440, 0.0616, 0.0548, 0.0647, 0.0632, 0.0294,
        0.0782, 0.0649], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,507][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0384, 0.0536, 0.0510, 0.0515, 0.0442, 0.0453, 0.0528, 0.0501, 0.0447,
        0.0516, 0.0534, 0.0526, 0.0519, 0.0508, 0.0422, 0.0582, 0.0509, 0.0528,
        0.0504, 0.0535], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,510][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ by] are: tensor([1.4619e-35, 0.0000e+00, 0.0000e+00, 1.4071e-40, 0.0000e+00, 1.9303e-34,
        2.6508e-16, 5.0964e-13, 5.3882e-14, 3.2379e-18, 2.9697e-08, 4.2717e-26,
        5.7873e-19, 3.1384e-09, 4.4662e-08, 8.5113e-12, 3.9774e-11, 1.9726e-06,
        9.9999e-01, 1.0133e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:39,515][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0265, 0.0407, 0.0280, 0.0266, 0.0237, 0.0332, 0.0509, 0.0559, 0.0407,
        0.0308, 0.0552, 0.0407, 0.0332, 0.0618, 0.0903, 0.0573, 0.0471, 0.1029,
        0.0546, 0.0516, 0.0482], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,519][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.5363, 0.0070, 0.0040, 0.0249, 0.0360, 0.0107, 0.0306, 0.0536, 0.0149,
        0.0107, 0.0176, 0.0092, 0.0112, 0.0245, 0.0303, 0.0267, 0.0227, 0.0225,
        0.0265, 0.0504, 0.0296], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,523][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0096, 0.0457, 0.0455, 0.0491, 0.0493, 0.0472, 0.0487, 0.0487, 0.0393,
        0.0527, 0.0472, 0.0556, 0.0487, 0.0476, 0.0491, 0.0553, 0.0576, 0.0502,
        0.0473, 0.0495, 0.0563], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,524][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1170, 0.0316, 0.0361, 0.0454, 0.0356, 0.0289, 0.0308, 0.0410, 0.0380,
        0.0380, 0.0348, 0.0361, 0.0454, 0.0459, 0.0510, 0.0529, 0.0507, 0.0514,
        0.0599, 0.0758, 0.0537], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,525][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0557, 0.0406, 0.0289, 0.0288, 0.0203, 0.0303, 0.0352, 0.0410, 0.0528,
        0.0405, 0.0355, 0.0531, 0.0527, 0.0566, 0.0492, 0.0739, 0.0621, 0.0599,
        0.0678, 0.0610, 0.0541], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,526][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1753, 0.0514, 0.0413, 0.0433, 0.0549, 0.0367, 0.0547, 0.0251, 0.0644,
        0.0636, 0.0821, 0.0109, 0.0600, 0.0324, 0.0683, 0.0190, 0.0160, 0.0359,
        0.0206, 0.0286, 0.0155], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,529][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0707, 0.0318, 0.0388, 0.0528, 0.0291, 0.0392, 0.0333, 0.0235, 0.0302,
        0.0427, 0.0292, 0.0520, 0.0605, 0.0603, 0.0693, 0.0541, 0.0632, 0.0629,
        0.0440, 0.0507, 0.0618], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,532][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([5.0104e-05, 1.0597e-04, 1.7312e-03, 4.4832e-03, 6.8207e-04, 6.5650e-04,
        7.6580e-04, 6.5605e-03, 1.7780e-03, 6.1391e-03, 3.4205e-03, 1.2813e-02,
        1.6920e-02, 2.3533e-02, 3.4150e-02, 7.8323e-02, 5.3609e-02, 8.1250e-02,
        2.0861e-01, 3.0806e-01, 1.5635e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,536][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0254, 0.0751, 0.0188, 0.0168, 0.0363, 0.0073, 0.0264, 0.0341, 0.4409,
        0.0104, 0.0348, 0.0035, 0.0202, 0.0283, 0.0516, 0.0186, 0.0055, 0.1037,
        0.0200, 0.0142, 0.0082], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,541][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0151, 0.0332, 0.0530, 0.0714, 0.0452, 0.0386, 0.0364, 0.0344, 0.0279,
        0.0352, 0.0372, 0.0630, 0.0374, 0.0665, 0.0460, 0.0579, 0.0505, 0.0347,
        0.0729, 0.0885, 0.0548], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,546][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0365, 0.0501, 0.0495, 0.0490, 0.0434, 0.0430, 0.0495, 0.0472, 0.0436,
        0.0488, 0.0520, 0.0501, 0.0503, 0.0482, 0.0389, 0.0545, 0.0486, 0.0492,
        0.0489, 0.0511, 0.0476], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,549][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([5.0981e-30, 0.0000e+00, 0.0000e+00, 1.8334e-32, 5.0361e-40, 1.1260e-34,
        1.1830e-19, 7.4859e-14, 4.5786e-15, 1.6862e-21, 4.5650e-10, 3.9665e-25,
        1.6532e-19, 5.1214e-09, 2.1053e-09, 1.6424e-10, 6.2690e-14, 2.8410e-07,
        8.6584e-01, 1.3416e-01, 1.4212e-09], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:39,609][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:39,614][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,617][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,620][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,620][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,621][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,622][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,622][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,624][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,624][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,625][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,626][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,626][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:39,627][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.4089, 0.5911], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,628][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.6969, 0.3031], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,629][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.9546, 0.0454], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,629][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.4828, 0.5172], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,630][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.1978, 0.8022], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,633][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.7839, 0.2161], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,636][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.6680, 0.3320], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,640][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.4596, 0.5404], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,645][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.4587, 0.5413], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,650][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.8443, 0.1557], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,654][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.9292, 0.0708], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,659][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.9893, 0.0107], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:39,659][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.4761, 0.3579, 0.1660], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,660][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.3552, 0.5333, 0.1115], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,661][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.9115, 0.0492, 0.0394], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,662][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.2783, 0.3281, 0.3936], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,664][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0374, 0.0345, 0.9281], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,668][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.5354, 0.1351, 0.3295], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,672][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.4958, 0.1699, 0.3343], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,677][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0619, 0.0593, 0.8788], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,682][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.3940, 0.2692, 0.3368], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,686][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.7600, 0.0903, 0.1496], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,691][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.9312, 0.0323, 0.0364], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,692][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0572, 0.2564, 0.6864], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:39,692][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.4139, 0.0504, 0.1194, 0.4162], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,693][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.3746, 0.4450, 0.1077, 0.0726], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,694][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.9052, 0.0350, 0.0222, 0.0375], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,696][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.1646, 0.1333, 0.2448, 0.4573], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,700][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.1535, 0.0682, 0.5731, 0.2052], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,705][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.6391, 0.0581, 0.0937, 0.2091], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,709][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.4814, 0.0766, 0.1413, 0.3007], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,714][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0181, 0.0183, 0.1851, 0.7785], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,718][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.2672, 0.1463, 0.1814, 0.4052], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,723][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.7691, 0.0362, 0.0627, 0.1321], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,724][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.9188, 0.0218, 0.0231, 0.0364], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,724][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0233, 0.0017, 0.8005, 0.1745], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:39,725][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.0143, 0.0149, 0.0294, 0.9030, 0.0385], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,726][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.2171, 0.4956, 0.1148, 0.1077, 0.0647], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,729][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.7566, 0.0391, 0.0400, 0.1070, 0.0574], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,733][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.1451, 0.0786, 0.0666, 0.6406, 0.0692], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,737][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.2336, 0.0654, 0.2167, 0.0354, 0.4489], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,742][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.3569, 0.0537, 0.1408, 0.3534, 0.0952], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,746][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.1766, 0.0582, 0.1569, 0.5601, 0.0483], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,751][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0166, 0.0155, 0.1435, 0.7284, 0.0961], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,755][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.1676, 0.0910, 0.1870, 0.4179, 0.1365], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,756][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.3782, 0.0346, 0.0937, 0.4060, 0.0875], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,757][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.7449, 0.0788, 0.0321, 0.0416, 0.1027], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,758][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([1.1249e-04, 9.3894e-07, 5.8134e-03, 9.9407e-01, 1.7903e-07],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:39,760][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.0069, 0.0093, 0.0141, 0.9003, 0.0517, 0.0178], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,763][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.2144, 0.3947, 0.1047, 0.1149, 0.0914, 0.0799], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,768][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.5854, 0.0541, 0.0614, 0.1562, 0.0621, 0.0808], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,773][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0155, 0.0499, 0.0828, 0.6797, 0.1123, 0.0598], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,777][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.2544, 0.0961, 0.2935, 0.0659, 0.1665, 0.1235], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,782][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.1690, 0.0317, 0.1174, 0.4985, 0.1203, 0.0630], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,787][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0906, 0.0335, 0.1200, 0.5837, 0.0719, 0.1004], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,788][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0094, 0.0180, 0.1198, 0.6633, 0.0933, 0.0962], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,789][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.2648, 0.0959, 0.1041, 0.2424, 0.2518, 0.0410], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,789][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.2554, 0.0452, 0.0688, 0.4376, 0.1230, 0.0700], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,790][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.8007, 0.0330, 0.0209, 0.0663, 0.0468, 0.0324], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,791][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([3.1544e-05, 1.7898e-05, 6.3204e-04, 9.9922e-01, 9.5164e-05, 2.5638e-09],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:39,794][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.0105, 0.0084, 0.0081, 0.7061, 0.0435, 0.1063, 0.1171],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,797][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.1361, 0.2646, 0.1101, 0.1634, 0.1069, 0.1486, 0.0703],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,802][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.6614, 0.0254, 0.0329, 0.1245, 0.0676, 0.0699, 0.0181],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,807][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.0110, 0.0340, 0.0433, 0.7036, 0.0703, 0.0441, 0.0938],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,811][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.1078, 0.0485, 0.4036, 0.0355, 0.1281, 0.0435, 0.2330],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,816][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.2286, 0.0157, 0.0636, 0.4034, 0.1093, 0.0873, 0.0921],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,820][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.2117, 0.0523, 0.1051, 0.3697, 0.0592, 0.1439, 0.0580],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,821][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0137, 0.0107, 0.0679, 0.6654, 0.0855, 0.1057, 0.0510],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,822][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.0909, 0.0661, 0.0854, 0.2530, 0.2525, 0.0373, 0.2147],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,823][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.5075, 0.0330, 0.0271, 0.2757, 0.0613, 0.0708, 0.0245],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,824][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.8418, 0.0151, 0.0196, 0.0530, 0.0226, 0.0193, 0.0286],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,827][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([2.2914e-04, 2.0922e-06, 3.3618e-05, 9.9971e-01, 2.5374e-05, 2.5949e-07,
        7.9541e-07], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:39,831][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.0570, 0.0058, 0.0079, 0.3479, 0.0039, 0.0060, 0.0152, 0.5562],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,836][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.2114, 0.2563, 0.0882, 0.1028, 0.1329, 0.0987, 0.0737, 0.0359],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,841][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.6836, 0.0227, 0.0267, 0.1164, 0.0349, 0.0282, 0.0171, 0.0704],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,845][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.1012, 0.0338, 0.0209, 0.4029, 0.0392, 0.0221, 0.0527, 0.3272],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,850][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0582, 0.0811, 0.4901, 0.0253, 0.0530, 0.0357, 0.0819, 0.1747],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,853][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.3464, 0.0044, 0.0094, 0.0534, 0.0148, 0.0064, 0.0161, 0.5491],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,853][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.2228, 0.0362, 0.0732, 0.3613, 0.0392, 0.0963, 0.0614, 0.1096],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,854][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0133, 0.0056, 0.0255, 0.1539, 0.0265, 0.0274, 0.0190, 0.7290],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,855][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.2139, 0.0775, 0.0931, 0.1775, 0.1141, 0.0343, 0.1499, 0.1397],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,857][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.6166, 0.0082, 0.0176, 0.1285, 0.0190, 0.0112, 0.0089, 0.1900],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,860][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.7539, 0.0334, 0.0143, 0.0387, 0.0396, 0.0170, 0.0458, 0.0573],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,863][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([1.5699e-02, 2.1756e-08, 2.8679e-07, 2.6818e-03, 2.4131e-05, 9.4051e-10,
        1.2899e-05, 9.8158e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:39,868][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0048, 0.0050, 0.0061, 0.1872, 0.0049, 0.0102, 0.0109, 0.7457, 0.0251],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,872][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.2073, 0.3163, 0.0937, 0.0867, 0.0594, 0.0687, 0.1053, 0.0420, 0.0205],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,877][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.5313, 0.0291, 0.0381, 0.1420, 0.0532, 0.0477, 0.0192, 0.1037, 0.0357],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,882][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0021, 0.0118, 0.0095, 0.1593, 0.0226, 0.0116, 0.0488, 0.7137, 0.0207],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,885][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.2050, 0.0557, 0.1991, 0.0572, 0.1263, 0.0356, 0.2299, 0.0568, 0.0344],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,886][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.0520, 0.0052, 0.0169, 0.0742, 0.0220, 0.0091, 0.0162, 0.7402, 0.0642],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,887][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.0898, 0.0216, 0.0676, 0.3757, 0.0286, 0.0732, 0.0566, 0.1756, 0.1111],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,888][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0040, 0.0039, 0.0330, 0.1608, 0.0206, 0.0211, 0.0161, 0.6445, 0.0961],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,890][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.0658, 0.0406, 0.0436, 0.1111, 0.1539, 0.0260, 0.1222, 0.2012, 0.2356],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,893][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.2535, 0.0214, 0.0251, 0.1951, 0.0331, 0.0255, 0.0263, 0.3788, 0.0411],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,897][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.7523, 0.0358, 0.0291, 0.0394, 0.0251, 0.0133, 0.0392, 0.0356, 0.0301],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,900][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([2.8808e-07, 1.5042e-09, 3.9970e-07, 1.7590e-05, 4.8879e-09, 7.7697e-12,
        5.3592e-07, 9.9998e-01, 6.7248e-07], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:39,905][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0100, 0.0046, 0.0038, 0.2459, 0.0033, 0.0039, 0.0100, 0.6288, 0.0506,
        0.0392], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,909][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.1045, 0.3762, 0.0709, 0.0842, 0.0673, 0.0723, 0.0668, 0.0453, 0.0293,
        0.0831], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,914][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.4632, 0.0290, 0.0281, 0.1019, 0.0487, 0.0428, 0.0162, 0.1035, 0.0668,
        0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,918][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0080, 0.0162, 0.0084, 0.1919, 0.0249, 0.0098, 0.0414, 0.6598, 0.0218,
        0.0179], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,918][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.1283, 0.0735, 0.1510, 0.0605, 0.0722, 0.0591, 0.1258, 0.1183, 0.0074,
        0.2040], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,919][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.0943, 0.0070, 0.0146, 0.0501, 0.0169, 0.0094, 0.0121, 0.6701, 0.0488,
        0.0766], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,920][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.1428, 0.0225, 0.0677, 0.2537, 0.0291, 0.0522, 0.0499, 0.1626, 0.1165,
        0.1030], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,923][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0037, 0.0041, 0.0278, 0.1246, 0.0237, 0.0228, 0.0206, 0.5873, 0.0835,
        0.1020], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,926][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.0868, 0.0772, 0.0888, 0.1121, 0.1304, 0.0341, 0.1086, 0.0968, 0.2178,
        0.0474], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,931][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.2164, 0.0150, 0.0196, 0.1845, 0.0284, 0.0235, 0.0134, 0.3452, 0.0631,
        0.0910], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,936][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.6907, 0.0350, 0.0174, 0.0375, 0.0363, 0.0247, 0.0324, 0.0275, 0.0379,
        0.0606], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,938][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([2.1426e-06, 3.6359e-09, 8.9906e-08, 7.3680e-05, 2.3311e-08, 2.0147e-11,
        4.4521e-07, 9.9989e-01, 3.4530e-05, 9.8055e-07], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:39,943][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0050, 0.0027, 0.0050, 0.1117, 0.0037, 0.0067, 0.0110, 0.4969, 0.0658,
        0.0912, 0.2003], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,948][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.1492, 0.2070, 0.0814, 0.0920, 0.0760, 0.0888, 0.0611, 0.0411, 0.0516,
        0.1025, 0.0491], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,950][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.5017, 0.0127, 0.0288, 0.0968, 0.0371, 0.0388, 0.0140, 0.1058, 0.0358,
        0.1030, 0.0255], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,951][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0515, 0.0241, 0.0138, 0.1676, 0.0345, 0.0181, 0.0598, 0.3010, 0.0600,
        0.1596, 0.1100], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,952][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.1278, 0.1067, 0.3015, 0.0393, 0.0651, 0.0368, 0.0447, 0.0627, 0.0026,
        0.1030, 0.1099], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,953][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.0745, 0.0038, 0.0131, 0.0504, 0.0121, 0.0085, 0.0187, 0.5775, 0.0569,
        0.1317, 0.0528], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,955][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0903, 0.0224, 0.0564, 0.2504, 0.0266, 0.0861, 0.0444, 0.0982, 0.1316,
        0.1564, 0.0373], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,958][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0046, 0.0038, 0.0175, 0.1201, 0.0261, 0.0294, 0.0107, 0.4964, 0.0864,
        0.1068, 0.0981], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,962][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.0590, 0.0528, 0.0588, 0.0972, 0.1293, 0.0197, 0.1349, 0.1101, 0.2512,
        0.0351, 0.0519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,967][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.2601, 0.0117, 0.0145, 0.1068, 0.0204, 0.0201, 0.0121, 0.2791, 0.0694,
        0.0983, 0.1075], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,972][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.6363, 0.0388, 0.0203, 0.0489, 0.0253, 0.0179, 0.0390, 0.0506, 0.0323,
        0.0454, 0.0452], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,974][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([4.0593e-03, 3.8556e-07, 2.6282e-07, 4.3777e-03, 6.5475e-06, 8.2050e-09,
        2.2667e-05, 9.7465e-01, 4.5339e-03, 1.2306e-02, 3.9027e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:39,979][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0862, 0.0194, 0.0093, 0.3722, 0.0019, 0.0026, 0.0094, 0.2047, 0.0485,
        0.0478, 0.1137, 0.0844], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,982][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.1230, 0.2304, 0.0592, 0.0711, 0.0443, 0.0972, 0.0872, 0.0355, 0.0240,
        0.1181, 0.0629, 0.0470], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,983][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.4016, 0.0220, 0.0252, 0.0808, 0.0495, 0.0435, 0.0131, 0.1076, 0.0527,
        0.0973, 0.0404, 0.0663], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,984][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0122, 0.0100, 0.0130, 0.1528, 0.0142, 0.0073, 0.0830, 0.2845, 0.0613,
        0.0959, 0.0988, 0.1670], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,985][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0629, 0.0612, 0.1961, 0.0449, 0.0237, 0.0334, 0.0563, 0.1205, 0.0061,
        0.0550, 0.1053, 0.2347], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,987][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.1451, 0.0076, 0.0111, 0.0397, 0.0131, 0.0107, 0.0126, 0.4571, 0.0324,
        0.1376, 0.0714, 0.0616], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,991][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0864, 0.0119, 0.0387, 0.2140, 0.0261, 0.0338, 0.0431, 0.1205, 0.0741,
        0.1082, 0.0375, 0.2058], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,995][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0031, 0.0032, 0.0310, 0.1201, 0.0147, 0.0251, 0.0160, 0.3937, 0.0796,
        0.1055, 0.0981, 0.1099], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:39,999][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0344, 0.0400, 0.0571, 0.1159, 0.1243, 0.0201, 0.1250, 0.1209, 0.1913,
        0.0325, 0.0639, 0.0748], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,004][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.3304, 0.0157, 0.0118, 0.1498, 0.0200, 0.0217, 0.0108, 0.1856, 0.0378,
        0.0721, 0.0887, 0.0556], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,009][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.5821, 0.0237, 0.0158, 0.0688, 0.0434, 0.0212, 0.0323, 0.0234, 0.0392,
        0.0565, 0.0290, 0.0645], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,011][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([1.4148e-06, 3.6325e-09, 4.0097e-08, 2.9370e-05, 4.5374e-08, 9.2076e-11,
        1.3500e-06, 9.9982e-01, 5.6076e-05, 1.8940e-05, 3.0325e-05, 4.4346e-05],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,015][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0167, 0.0036, 0.0075, 0.1915, 0.0009, 0.0022, 0.0054, 0.2164, 0.0202,
        0.0219, 0.0874, 0.1808, 0.2454], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,016][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0934, 0.2447, 0.0532, 0.0609, 0.0657, 0.0722, 0.0600, 0.0396, 0.0347,
        0.0759, 0.0809, 0.0881, 0.0307], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,016][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.4943, 0.0175, 0.0179, 0.0594, 0.0265, 0.0327, 0.0086, 0.0685, 0.0316,
        0.0836, 0.0205, 0.0535, 0.0854], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,017][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0224, 0.0081, 0.0113, 0.1106, 0.0104, 0.0053, 0.0449, 0.1860, 0.0223,
        0.0351, 0.0570, 0.2760, 0.2106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,020][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0272, 0.0272, 0.0982, 0.0264, 0.0278, 0.0202, 0.0439, 0.0695, 0.0039,
        0.0694, 0.0828, 0.0546, 0.4489], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,023][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1386, 0.0047, 0.0096, 0.0334, 0.0095, 0.0051, 0.0069, 0.3579, 0.0190,
        0.0587, 0.0455, 0.1321, 0.1790], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,028][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0772, 0.0121, 0.0366, 0.1434, 0.0174, 0.0215, 0.0266, 0.0814, 0.0461,
        0.0629, 0.0250, 0.2309, 0.2188], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,032][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0022, 0.0028, 0.0325, 0.0976, 0.0108, 0.0163, 0.0136, 0.3732, 0.0525,
        0.0780, 0.0681, 0.1061, 0.1464], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,037][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0613, 0.0418, 0.0704, 0.1156, 0.1038, 0.0153, 0.0859, 0.0986, 0.1861,
        0.0279, 0.0461, 0.0718, 0.0754], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,041][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.2050, 0.0105, 0.0140, 0.1366, 0.0155, 0.0140, 0.0071, 0.1890, 0.0398,
        0.0585, 0.0895, 0.0943, 0.1262], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,046][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.5582, 0.0247, 0.0190, 0.0509, 0.0325, 0.0156, 0.0246, 0.0198, 0.0340,
        0.0517, 0.0312, 0.0768, 0.0611], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,049][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([7.9466e-06, 6.8493e-09, 1.3641e-07, 8.8262e-05, 5.8260e-08, 5.3021e-12,
        1.1762e-06, 9.9929e-01, 2.4136e-05, 7.7986e-06, 1.3366e-05, 4.8970e-04,
        7.7740e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,050][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0234, 0.0008, 0.0020, 0.0701, 0.0006, 0.0008, 0.0025, 0.1078, 0.0138,
        0.0197, 0.0411, 0.1193, 0.1789, 0.4193], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,051][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.1129, 0.1391, 0.0504, 0.0761, 0.0546, 0.0795, 0.0418, 0.0247, 0.0385,
        0.1122, 0.0540, 0.0594, 0.0495, 0.1073], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,052][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.5478, 0.0123, 0.0099, 0.0554, 0.0187, 0.0156, 0.0087, 0.0529, 0.0225,
        0.0583, 0.0105, 0.0399, 0.0709, 0.0765], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,054][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0196, 0.0020, 0.0021, 0.0359, 0.0030, 0.0009, 0.0038, 0.0978, 0.0038,
        0.0097, 0.0207, 0.0735, 0.0725, 0.6547], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,057][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0220, 0.0150, 0.1376, 0.0147, 0.0100, 0.0180, 0.0247, 0.0114, 0.0016,
        0.0250, 0.0286, 0.0488, 0.1168, 0.5258], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,062][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.0706, 0.0013, 0.0020, 0.0185, 0.0026, 0.0023, 0.0022, 0.2170, 0.0122,
        0.0467, 0.0205, 0.0811, 0.1647, 0.3582], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,066][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.0729, 0.0112, 0.0241, 0.1160, 0.0177, 0.0184, 0.0237, 0.0557, 0.0448,
        0.0646, 0.0190, 0.1711, 0.2022, 0.1586], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,071][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0023, 0.0026, 0.0166, 0.0620, 0.0081, 0.0101, 0.0075, 0.2069, 0.0362,
        0.0577, 0.0416, 0.0629, 0.0936, 0.3919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,076][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0551, 0.0385, 0.0603, 0.1217, 0.0949, 0.0108, 0.0662, 0.0930, 0.1413,
        0.0288, 0.0537, 0.0746, 0.0913, 0.0698], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,080][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.2310, 0.0034, 0.0058, 0.0657, 0.0086, 0.0063, 0.0030, 0.1181, 0.0200,
        0.0352, 0.0361, 0.0532, 0.0867, 0.3269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,082][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.7037, 0.0247, 0.0187, 0.0349, 0.0123, 0.0113, 0.0134, 0.0193, 0.0153,
        0.0270, 0.0131, 0.0498, 0.0462, 0.0104], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,083][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([3.0170e-07, 5.9918e-11, 2.5544e-10, 3.9785e-07, 8.4917e-10, 2.6561e-13,
        2.2821e-09, 1.1112e-02, 1.4254e-06, 3.3385e-06, 8.4408e-07, 2.1292e-05,
        3.5643e-04, 9.8850e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,083][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([7.9579e-03, 4.0738e-04, 1.1931e-03, 3.4142e-02, 1.0800e-04, 2.6493e-04,
        6.2085e-04, 3.6323e-02, 3.9454e-03, 4.7028e-03, 1.3996e-02, 2.5427e-02,
        5.2449e-02, 5.2457e-01, 2.9389e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,084][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0622, 0.1247, 0.0424, 0.0611, 0.0482, 0.0523, 0.0579, 0.0218, 0.0322,
        0.0807, 0.0614, 0.0742, 0.0460, 0.2210, 0.0139], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,087][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.3957, 0.0093, 0.0081, 0.0637, 0.0273, 0.0152, 0.0077, 0.0623, 0.0237,
        0.0529, 0.0189, 0.0362, 0.0595, 0.0821, 0.1372], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,089][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([6.0179e-03, 2.0345e-04, 2.0141e-04, 8.2180e-03, 1.2616e-04, 4.0526e-05,
        1.4054e-04, 6.8121e-03, 1.9218e-04, 2.9493e-04, 8.9282e-04, 4.4214e-03,
        4.6304e-03, 1.6477e-01, 8.0304e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,094][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0134, 0.0138, 0.0798, 0.0255, 0.0120, 0.0138, 0.0272, 0.0118, 0.0012,
        0.0200, 0.0309, 0.0362, 0.0464, 0.0582, 0.6098], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,097][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([2.4334e-02, 4.3567e-04, 1.1713e-03, 8.4266e-03, 1.4245e-03, 8.8864e-04,
        1.1420e-03, 1.0151e-01, 5.4481e-03, 1.4427e-02, 7.3816e-03, 2.2679e-02,
        5.6801e-02, 2.7962e-01, 4.7430e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,101][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0475, 0.0070, 0.0192, 0.1033, 0.0124, 0.0150, 0.0194, 0.0424, 0.0344,
        0.0469, 0.0140, 0.1866, 0.1957, 0.1640, 0.0923], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,106][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0007, 0.0007, 0.0091, 0.0386, 0.0027, 0.0033, 0.0032, 0.1069, 0.0124,
        0.0186, 0.0194, 0.0246, 0.0416, 0.2042, 0.5138], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,110][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0530, 0.0402, 0.0492, 0.1123, 0.0758, 0.0126, 0.0829, 0.0786, 0.1504,
        0.0236, 0.0402, 0.0554, 0.0753, 0.0775, 0.0730], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,114][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.1020, 0.0024, 0.0035, 0.0430, 0.0046, 0.0033, 0.0033, 0.0708, 0.0154,
        0.0240, 0.0219, 0.0318, 0.0453, 0.2848, 0.3439], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,115][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.5634, 0.0347, 0.0357, 0.0501, 0.0229, 0.0159, 0.0143, 0.0184, 0.0209,
        0.0364, 0.0186, 0.0589, 0.0670, 0.0220, 0.0210], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,115][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([4.4837e-11, 1.1630e-14, 5.9655e-13, 5.5984e-08, 2.6606e-13, 2.8016e-16,
        6.2704e-12, 4.0400e-04, 1.4536e-08, 4.5858e-09, 3.2311e-09, 2.4686e-07,
        6.8304e-07, 8.8242e-01, 1.1717e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,116][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([8.4373e-03, 2.8615e-04, 5.6592e-04, 6.4019e-03, 5.5542e-05, 1.0436e-04,
        5.5361e-04, 1.0884e-02, 3.0040e-03, 2.4424e-03, 7.1503e-03, 1.6193e-02,
        3.1660e-02, 1.0155e-01, 7.5760e-01, 5.3110e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,119][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.1021, 0.1664, 0.0679, 0.0476, 0.0464, 0.0558, 0.0426, 0.0242, 0.0160,
        0.0984, 0.0418, 0.0743, 0.0475, 0.1222, 0.0299, 0.0167],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,123][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.5017, 0.0102, 0.0084, 0.0263, 0.0136, 0.0184, 0.0045, 0.0342, 0.0142,
        0.0426, 0.0137, 0.0267, 0.0422, 0.0550, 0.1296, 0.0584],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,126][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([6.9860e-04, 2.5112e-04, 6.8535e-04, 4.0596e-03, 4.6792e-04, 3.2033e-04,
        4.3557e-03, 5.9370e-03, 1.1131e-03, 1.3092e-03, 2.0010e-03, 1.1262e-02,
        7.2226e-03, 2.1474e-01, 7.0535e-01, 4.0221e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,130][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0196, 0.0076, 0.0782, 0.0122, 0.0151, 0.0067, 0.0212, 0.0144, 0.0014,
        0.0315, 0.0247, 0.0286, 0.1049, 0.1089, 0.4780, 0.0469],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,135][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0414, 0.0014, 0.0026, 0.0057, 0.0025, 0.0016, 0.0018, 0.0490, 0.0048,
        0.0171, 0.0097, 0.0226, 0.0600, 0.1588, 0.5339, 0.0871],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,140][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0880, 0.0103, 0.0232, 0.0700, 0.0120, 0.0139, 0.0176, 0.0440, 0.0238,
        0.0405, 0.0140, 0.1043, 0.1159, 0.1486, 0.0990, 0.1749],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,144][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0009, 0.0012, 0.0164, 0.0504, 0.0046, 0.0069, 0.0051, 0.1152, 0.0209,
        0.0289, 0.0224, 0.0312, 0.0540, 0.1613, 0.3408, 0.1397],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,146][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0689, 0.0346, 0.0430, 0.0862, 0.0828, 0.0124, 0.0648, 0.0678, 0.1311,
        0.0252, 0.0374, 0.0553, 0.0612, 0.0638, 0.0836, 0.0817],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,147][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0987, 0.0027, 0.0030, 0.0242, 0.0060, 0.0054, 0.0033, 0.0363, 0.0160,
        0.0212, 0.0243, 0.0339, 0.0449, 0.2092, 0.3992, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,148][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.5771, 0.0197, 0.0167, 0.0486, 0.0295, 0.0128, 0.0174, 0.0137, 0.0238,
        0.0363, 0.0229, 0.0553, 0.0529, 0.0181, 0.0199, 0.0354],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,148][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([1.6552e-08, 1.6213e-10, 2.4909e-09, 7.1536e-09, 2.0840e-11, 1.0991e-14,
        9.0469e-10, 3.7002e-04, 1.6904e-08, 2.4491e-08, 9.5037e-09, 5.0134e-08,
        1.7594e-07, 1.1715e-01, 8.8237e-01, 1.0470e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:40,150][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([4.4310e-03, 4.3582e-04, 5.8711e-04, 1.9742e-02, 1.2270e-04, 2.9635e-04,
        7.6486e-04, 2.3777e-02, 3.1375e-03, 3.0823e-03, 1.3305e-02, 1.7804e-02,
        2.2994e-02, 1.8986e-01, 2.8359e-01, 2.8003e-01, 1.3604e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,153][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0812, 0.2196, 0.0440, 0.0419, 0.0473, 0.0515, 0.0517, 0.0294, 0.0219,
        0.0693, 0.0538, 0.0678, 0.0249, 0.1272, 0.0244, 0.0228, 0.0215],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,158][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3864, 0.0111, 0.0096, 0.0349, 0.0158, 0.0195, 0.0058, 0.0414, 0.0181,
        0.0510, 0.0125, 0.0318, 0.0510, 0.0600, 0.1180, 0.0681, 0.0649],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,161][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([9.3320e-04, 2.8472e-04, 5.2867e-04, 4.8392e-03, 3.3024e-04, 1.5541e-04,
        1.3961e-03, 4.8217e-03, 5.4242e-04, 1.0191e-03, 1.4743e-03, 9.2874e-03,
        6.5851e-03, 1.3875e-01, 7.4936e-01, 5.4355e-02, 2.5337e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,165][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0207, 0.0177, 0.0534, 0.0188, 0.0233, 0.0135, 0.0380, 0.0531, 0.0036,
        0.0334, 0.0783, 0.0362, 0.0764, 0.1271, 0.1919, 0.0319, 0.1828],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,170][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0628, 0.0009, 0.0021, 0.0064, 0.0017, 0.0015, 0.0010, 0.0433, 0.0038,
        0.0138, 0.0072, 0.0238, 0.0553, 0.1265, 0.4213, 0.1200, 0.1085],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,175][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0463, 0.0062, 0.0173, 0.0604, 0.0083, 0.0101, 0.0132, 0.0385, 0.0208,
        0.0326, 0.0115, 0.0979, 0.1039, 0.1311, 0.0881, 0.1888, 0.1252],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,178][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0011, 0.0012, 0.0108, 0.0451, 0.0052, 0.0060, 0.0057, 0.1198, 0.0179,
        0.0272, 0.0230, 0.0322, 0.0443, 0.1382, 0.3110, 0.1166, 0.0946],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,179][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0541, 0.0344, 0.0495, 0.0838, 0.0708, 0.0114, 0.0601, 0.0677, 0.1048,
        0.0213, 0.0359, 0.0511, 0.0585, 0.0632, 0.0746, 0.0887, 0.0702],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,180][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1027, 0.0035, 0.0042, 0.0369, 0.0049, 0.0039, 0.0019, 0.0406, 0.0122,
        0.0174, 0.0253, 0.0298, 0.0365, 0.1869, 0.2204, 0.1757, 0.0973],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,181][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.5288, 0.0245, 0.0171, 0.0502, 0.0301, 0.0133, 0.0184, 0.0198, 0.0299,
        0.0321, 0.0198, 0.0574, 0.0448, 0.0192, 0.0152, 0.0379, 0.0416],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,182][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([4.5573e-06, 1.6607e-10, 2.3126e-09, 3.8323e-06, 6.3977e-10, 1.0335e-13,
        2.4999e-09, 4.0027e-03, 3.5456e-07, 6.4433e-08, 1.0961e-07, 2.1808e-06,
        9.4324e-07, 2.1938e-01, 7.1973e-01, 5.6074e-02, 8.0304e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:40,185][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([2.5111e-03, 1.7929e-04, 3.9743e-04, 8.4499e-03, 5.3541e-05, 7.3967e-05,
        8.3917e-04, 1.0231e-02, 2.6798e-03, 2.5108e-03, 7.1115e-03, 1.5323e-02,
        2.3760e-02, 1.0701e-01, 2.3751e-01, 1.5449e-01, 1.5022e-01, 2.7665e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,189][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0820, 0.0569, 0.0432, 0.0733, 0.0543, 0.0540, 0.0398, 0.0214, 0.0304,
        0.0790, 0.0456, 0.0427, 0.0518, 0.1700, 0.0284, 0.0437, 0.0464, 0.0371],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,194][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.3470, 0.0065, 0.0082, 0.0343, 0.0187, 0.0179, 0.0083, 0.0387, 0.0205,
        0.0412, 0.0131, 0.0282, 0.0454, 0.0508, 0.1472, 0.0720, 0.0705, 0.0316],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,198][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0054, 0.0012, 0.0014, 0.0099, 0.0022, 0.0010, 0.0031, 0.0139, 0.0017,
        0.0055, 0.0045, 0.0233, 0.0192, 0.1167, 0.5758, 0.0737, 0.0882, 0.0532],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,201][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([9.0884e-03, 5.4860e-02, 2.8635e-02, 3.3070e-03, 3.5100e-03, 2.0315e-03,
        1.0065e-02, 4.4488e-03, 5.7668e-04, 1.0293e-02, 5.4797e-03, 9.4325e-03,
        1.3411e-02, 2.1392e-02, 6.9037e-01, 6.6989e-03, 2.5611e-02, 1.0079e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,205][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0424, 0.0004, 0.0014, 0.0069, 0.0011, 0.0008, 0.0009, 0.0351, 0.0028,
        0.0117, 0.0049, 0.0245, 0.0424, 0.1416, 0.3019, 0.1327, 0.1411, 0.1073],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,210][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0122, 0.0028, 0.0093, 0.0694, 0.0055, 0.0108, 0.0106, 0.0199, 0.0201,
        0.0292, 0.0073, 0.1119, 0.1279, 0.0905, 0.0483, 0.2337, 0.1645, 0.0262],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,211][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0012, 0.0013, 0.0048, 0.0292, 0.0043, 0.0055, 0.0043, 0.0957, 0.0137,
        0.0237, 0.0297, 0.0271, 0.0390, 0.1469, 0.2483, 0.0989, 0.0779, 0.1485],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,212][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0381, 0.0189, 0.0453, 0.0920, 0.0513, 0.0119, 0.0703, 0.0599, 0.1076,
        0.0160, 0.0263, 0.0407, 0.0517, 0.0616, 0.0909, 0.1069, 0.0663, 0.0442],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,213][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0649, 0.0013, 0.0030, 0.0260, 0.0037, 0.0036, 0.0015, 0.0380, 0.0118,
        0.0158, 0.0198, 0.0253, 0.0326, 0.1712, 0.2753, 0.1222, 0.1070, 0.0771],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,216][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.5041, 0.0299, 0.0270, 0.0446, 0.0283, 0.0147, 0.0197, 0.0255, 0.0314,
        0.0290, 0.0220, 0.0396, 0.0525, 0.0170, 0.0197, 0.0291, 0.0381, 0.0278],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,217][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([2.4002e-05, 1.7021e-11, 8.5762e-11, 3.5398e-07, 3.5085e-10, 2.2095e-13,
        8.9679e-10, 7.1774e-05, 6.5695e-08, 8.7572e-07, 1.5839e-08, 3.1236e-06,
        1.3367e-05, 1.9222e-01, 6.6112e-01, 1.5287e-02, 2.9638e-02, 1.0161e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:40,220][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([8.9302e-03, 5.0278e-04, 1.6805e-04, 9.9540e-03, 5.7856e-05, 1.0870e-04,
        3.0699e-04, 1.4195e-02, 1.8678e-03, 1.4671e-03, 8.2118e-03, 7.8827e-03,
        1.6763e-02, 5.5131e-02, 1.1058e-01, 1.1619e-01, 1.4092e-01, 2.8292e-01,
        2.2384e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,225][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0863, 0.1449, 0.0240, 0.0327, 0.0386, 0.0408, 0.0334, 0.0236, 0.0182,
        0.0888, 0.0437, 0.0500, 0.0555, 0.1162, 0.0156, 0.0228, 0.0410, 0.0921,
        0.0318], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,229][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.4139, 0.0067, 0.0052, 0.0242, 0.0137, 0.0127, 0.0035, 0.0403, 0.0174,
        0.0413, 0.0084, 0.0251, 0.0415, 0.0477, 0.1210, 0.0628, 0.0554, 0.0229,
        0.0361], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,232][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([1.0488e-03, 1.7610e-04, 1.6276e-04, 4.7314e-03, 1.6934e-04, 6.0645e-05,
        3.4311e-04, 5.2876e-03, 1.6306e-04, 5.9639e-04, 1.2205e-03, 6.5192e-03,
        4.8942e-03, 8.7740e-02, 5.8309e-01, 5.6400e-02, 3.7216e-02, 7.5153e-02,
        1.3502e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,237][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0079, 0.0058, 0.2693, 0.0389, 0.0042, 0.0085, 0.0116, 0.0069, 0.0017,
        0.0093, 0.0135, 0.0344, 0.0161, 0.0434, 0.0737, 0.0194, 0.0563, 0.0083,
        0.3708], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,242][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0443, 0.0005, 0.0006, 0.0046, 0.0008, 0.0006, 0.0007, 0.0265, 0.0024,
        0.0084, 0.0035, 0.0165, 0.0261, 0.0665, 0.2755, 0.0919, 0.1132, 0.0948,
        0.2224], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,243][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0424, 0.0046, 0.0090, 0.0580, 0.0076, 0.0092, 0.0108, 0.0242, 0.0192,
        0.0306, 0.0082, 0.0784, 0.1015, 0.0836, 0.0549, 0.1753, 0.1436, 0.0375,
        0.1014], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,244][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0008, 0.0006, 0.0062, 0.0261, 0.0021, 0.0029, 0.0025, 0.0710, 0.0082,
        0.0134, 0.0133, 0.0187, 0.0281, 0.1068, 0.2079, 0.0804, 0.0695, 0.0811,
        0.2602], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,245][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0696, 0.0306, 0.0322, 0.0815, 0.0635, 0.0098, 0.0678, 0.0382, 0.0861,
        0.0213, 0.0186, 0.0410, 0.0567, 0.0483, 0.0781, 0.0921, 0.0654, 0.0563,
        0.0428], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,247][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.1052, 0.0015, 0.0013, 0.0210, 0.0041, 0.0034, 0.0014, 0.0286, 0.0111,
        0.0130, 0.0133, 0.0207, 0.0231, 0.1285, 0.2547, 0.0861, 0.0778, 0.0717,
        0.1335], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,250][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.5225, 0.0301, 0.0272, 0.0391, 0.0212, 0.0107, 0.0193, 0.0232, 0.0190,
        0.0282, 0.0161, 0.0556, 0.0478, 0.0163, 0.0134, 0.0283, 0.0356, 0.0226,
        0.0237], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,252][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([4.8201e-07, 4.4803e-11, 3.1509e-12, 1.2236e-08, 2.1205e-12, 9.1012e-16,
        1.6864e-11, 4.6356e-05, 7.9789e-10, 1.7589e-09, 1.5688e-09, 1.3460e-08,
        2.5395e-08, 9.7313e-03, 1.1270e-02, 1.6844e-04, 7.7801e-05, 1.5826e-01,
        8.2045e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:40,255][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([3.2386e-02, 4.6715e-04, 7.3076e-04, 2.3255e-03, 4.9356e-05, 3.5175e-05,
        1.6425e-04, 4.1681e-03, 3.7221e-04, 6.0803e-04, 4.6256e-03, 5.7515e-03,
        9.2553e-03, 3.1452e-02, 5.9417e-02, 3.9596e-02, 4.4643e-02, 5.2945e-02,
        1.8912e-01, 5.2189e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,260][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.1125, 0.1677, 0.0370, 0.0247, 0.0357, 0.0491, 0.0385, 0.0173, 0.0129,
        0.0731, 0.0284, 0.0486, 0.0480, 0.0899, 0.0210, 0.0150, 0.0339, 0.0878,
        0.0398, 0.0189], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,264][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.5854, 0.0085, 0.0050, 0.0131, 0.0068, 0.0087, 0.0025, 0.0167, 0.0078,
        0.0239, 0.0058, 0.0186, 0.0245, 0.0267, 0.0637, 0.0450, 0.0340, 0.0227,
        0.0256, 0.0550], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,267][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([1.9980e-03, 3.3384e-04, 3.9407e-04, 2.0000e-03, 1.0532e-04, 3.0758e-05,
        2.5913e-04, 2.0017e-03, 1.3882e-04, 1.4485e-04, 4.2755e-04, 2.1029e-03,
        1.6258e-03, 3.7080e-02, 9.5965e-02, 1.8177e-02, 1.0587e-02, 3.1430e-02,
        1.3217e-01, 6.6302e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,272][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0272, 0.0136, 0.1131, 0.0442, 0.0070, 0.0100, 0.0164, 0.0109, 0.0007,
        0.0139, 0.0151, 0.0556, 0.0700, 0.1085, 0.1206, 0.0169, 0.1504, 0.0181,
        0.1300, 0.0577], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,274][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.1876, 0.0015, 0.0014, 0.0023, 0.0008, 0.0005, 0.0004, 0.0128, 0.0016,
        0.0066, 0.0025, 0.0123, 0.0239, 0.0405, 0.1017, 0.0501, 0.0762, 0.0809,
        0.1324, 0.2638], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,275][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.1261, 0.0053, 0.0092, 0.0228, 0.0050, 0.0037, 0.0068, 0.0169, 0.0066,
        0.0135, 0.0050, 0.0267, 0.0315, 0.0456, 0.0351, 0.0568, 0.0475, 0.0317,
        0.0746, 0.4296], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,276][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0011, 0.0008, 0.0070, 0.0230, 0.0021, 0.0020, 0.0020, 0.0442, 0.0072,
        0.0111, 0.0094, 0.0125, 0.0226, 0.0711, 0.1263, 0.0563, 0.0460, 0.0607,
        0.1601, 0.3345], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,277][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0923, 0.0278, 0.0336, 0.0653, 0.0691, 0.0093, 0.0533, 0.0408, 0.0911,
        0.0186, 0.0209, 0.0425, 0.0512, 0.0422, 0.0513, 0.0783, 0.0580, 0.0452,
        0.0413, 0.0678], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,281][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.2312, 0.0019, 0.0017, 0.0059, 0.0019, 0.0014, 0.0008, 0.0082, 0.0040,
        0.0091, 0.0086, 0.0136, 0.0149, 0.0505, 0.0866, 0.0662, 0.0450, 0.0471,
        0.1077, 0.2938], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,285][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.5832, 0.0240, 0.0165, 0.0343, 0.0150, 0.0099, 0.0124, 0.0128, 0.0145,
        0.0278, 0.0123, 0.0530, 0.0351, 0.0126, 0.0164, 0.0272, 0.0291, 0.0152,
        0.0158, 0.0331], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,287][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([1.7073e-06, 3.0590e-11, 1.1186e-10, 7.6665e-11, 9.4058e-14, 1.1039e-17,
        7.1325e-13, 8.5490e-08, 1.1850e-11, 4.3398e-11, 5.5362e-12, 3.4946e-10,
        5.5615e-10, 3.7960e-05, 1.2372e-04, 1.8731e-06, 6.9979e-07, 1.6118e-03,
        3.5344e-01, 6.4478e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:40,290][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([9.8126e-04, 6.2212e-05, 6.5491e-05, 2.1210e-03, 1.1297e-05, 2.4518e-05,
        7.8975e-05, 2.0452e-03, 2.4508e-04, 2.9048e-04, 1.3844e-03, 1.7690e-03,
        2.1030e-03, 1.4654e-02, 2.3017e-02, 2.6032e-02, 1.2133e-02, 3.6255e-02,
        6.2050e-02, 7.8400e-01, 3.0682e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,295][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0706, 0.1800, 0.0362, 0.0339, 0.0382, 0.0427, 0.0431, 0.0238, 0.0180,
        0.0575, 0.0449, 0.0557, 0.0196, 0.1056, 0.0203, 0.0189, 0.0169, 0.0880,
        0.0405, 0.0299, 0.0157], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,299][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3986, 0.0082, 0.0057, 0.0210, 0.0106, 0.0136, 0.0035, 0.0284, 0.0113,
        0.0363, 0.0080, 0.0223, 0.0350, 0.0375, 0.0821, 0.0480, 0.0446, 0.0222,
        0.0316, 0.0847, 0.0469], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,302][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([3.8851e-04, 1.0724e-04, 1.6760e-04, 1.4745e-03, 7.0665e-05, 3.1504e-05,
        2.3372e-04, 8.5596e-04, 8.2461e-05, 1.5024e-04, 2.7346e-04, 1.7650e-03,
        1.0827e-03, 2.1287e-02, 1.0954e-01, 9.9920e-03, 4.0595e-03, 1.1487e-02,
        4.8951e-02, 7.7323e-01, 1.4770e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,307][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0172, 0.0133, 0.0424, 0.0166, 0.0184, 0.0097, 0.0287, 0.0386, 0.0026,
        0.0244, 0.0576, 0.0280, 0.0559, 0.0998, 0.1499, 0.0252, 0.1364, 0.0202,
        0.0524, 0.0228, 0.1400], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,307][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([6.2764e-02, 5.6010e-04, 9.7534e-04, 2.4515e-03, 6.1981e-04, 5.5467e-04,
        3.0345e-04, 1.2165e-02, 1.0985e-03, 5.0690e-03, 2.1754e-03, 9.8357e-03,
        2.0601e-02, 3.6047e-02, 1.2416e-01, 4.3026e-02, 3.7982e-02, 5.0136e-02,
        1.7915e-01, 3.3132e-01, 7.9008e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,308][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0236, 0.0023, 0.0060, 0.0201, 0.0027, 0.0029, 0.0045, 0.0129, 0.0060,
        0.0103, 0.0036, 0.0269, 0.0298, 0.0392, 0.0256, 0.0555, 0.0361, 0.0228,
        0.0710, 0.5127, 0.0853], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,309][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0007, 0.0007, 0.0059, 0.0229, 0.0024, 0.0027, 0.0024, 0.0458, 0.0071,
        0.0122, 0.0093, 0.0144, 0.0197, 0.0576, 0.1262, 0.0503, 0.0400, 0.0608,
        0.1613, 0.2931, 0.0645], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,312][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0674, 0.0295, 0.0397, 0.0646, 0.0555, 0.0090, 0.0452, 0.0449, 0.0765,
        0.0164, 0.0246, 0.0387, 0.0463, 0.0463, 0.0543, 0.0700, 0.0555, 0.0452,
        0.0458, 0.0685, 0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,316][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0635, 0.0015, 0.0015, 0.0109, 0.0016, 0.0013, 0.0006, 0.0102, 0.0037,
        0.0057, 0.0075, 0.0105, 0.0118, 0.0506, 0.0673, 0.0591, 0.0318, 0.0431,
        0.0984, 0.4643, 0.0550], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,320][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.5492, 0.0213, 0.0143, 0.0354, 0.0226, 0.0105, 0.0125, 0.0150, 0.0218,
        0.0238, 0.0138, 0.0472, 0.0346, 0.0145, 0.0127, 0.0276, 0.0299, 0.0161,
        0.0154, 0.0355, 0.0263], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,323][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([6.6315e-09, 5.6469e-14, 2.9340e-13, 1.8226e-10, 7.3365e-15, 6.6863e-19,
        1.6691e-14, 9.6725e-09, 1.3740e-12, 3.9719e-13, 3.1929e-13, 4.2223e-11,
        9.1814e-12, 1.0930e-06, 1.5083e-06, 8.3402e-07, 7.6121e-09, 1.2532e-05,
        6.3934e-03, 9.9359e-01, 1.0086e-06], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:40,327][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:40,330][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 3175],
        [  956],
        [  630],
        [ 1918],
        [ 1784],
        [ 4704],
        [12026],
        [ 9569],
        [17927],
        [ 7002],
        [11202],
        [ 4432],
        [ 3921],
        [ 1335],
        [ 1735],
        [ 1975],
        [  846],
        [ 1436],
        [ 1559],
        [ 3403],
        [  988]], device='cuda:0')
[2024-07-23 21:06:40,333][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 3055],
        [   77],
        [  113],
        [  570],
        [  689],
        [ 2569],
        [ 8054],
        [ 8169],
        [15450],
        [ 5236],
        [12629],
        [ 3238],
        [ 3942],
        [  831],
        [  791],
        [  539],
        [  532],
        [  557],
        [  716],
        [ 2022],
        [  500]], device='cuda:0')
[2024-07-23 21:06:40,336][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[15070],
        [25444],
        [20480],
        [19864],
        [16373],
        [13553],
        [13664],
        [11415],
        [ 8384],
        [ 8583],
        [ 9706],
        [ 9569],
        [10176],
        [10617],
        [11382],
        [11110],
        [10851],
        [12528],
        [12179],
        [12423],
        [12433]], device='cuda:0')
[2024-07-23 21:06:40,339][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[37110],
        [36810],
        [36306],
        [35835],
        [35787],
        [33551],
        [34357],
        [33521],
        [34605],
        [32894],
        [31777],
        [33390],
        [32453],
        [33564],
        [31776],
        [31027],
        [28958],
        [28451],
        [27945],
        [31903],
        [28551]], device='cuda:0')
[2024-07-23 21:06:40,342][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[27813],
        [24989],
        [28073],
        [28323],
        [30111],
        [28922],
        [28882],
        [28411],
        [28869],
        [29088],
        [29139],
        [28878],
        [28955],
        [28830],
        [28950],
        [28390],
        [28564],
        [28199],
        [28268],
        [28179],
        [28400]], device='cuda:0')
[2024-07-23 21:06:40,344][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[15144],
        [15160],
        [14182],
        [11923],
        [11367],
        [10561],
        [10549],
        [10268],
        [ 9902],
        [ 9816],
        [ 9812],
        [ 9946],
        [ 9732],
        [10255],
        [10425],
        [10580],
        [10346],
        [10632],
        [10738],
        [10773],
        [10566]], device='cuda:0')
[2024-07-23 21:06:40,346][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[27134],
        [22215],
        [22885],
        [22267],
        [21293],
        [20297],
        [20454],
        [21489],
        [21446],
        [21116],
        [19637],
        [18576],
        [17969],
        [18766],
        [18327],
        [21299],
        [19940],
        [20793],
        [21044],
        [22696],
        [21566]], device='cuda:0')
[2024-07-23 21:06:40,348][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[29950],
        [30148],
        [30602],
        [33581],
        [32516],
        [32609],
        [33677],
        [33506],
        [31743],
        [32525],
        [33203],
        [32974],
        [33213],
        [33486],
        [33457],
        [33924],
        [33986],
        [34273],
        [34199],
        [34370],
        [34553]], device='cuda:0')
[2024-07-23 21:06:40,350][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[6389],
        [8134],
        [8793],
        [7158],
        [6564],
        [6498],
        [5982],
        [6314],
        [6315],
        [6639],
        [6397],
        [6434],
        [6571],
        [6325],
        [6299],
        [6346],
        [6371],
        [6315],
        [6368],
        [6307],
        [6409]], device='cuda:0')
[2024-07-23 21:06:40,354][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 2333],
        [ 6528],
        [ 5170],
        [ 7458],
        [ 7190],
        [ 6775],
        [ 6892],
        [ 4696],
        [ 4572],
        [ 5130],
        [ 5768],
        [ 6832],
        [ 8103],
        [ 9291],
        [ 9287],
        [10552],
        [10002],
        [10630],
        [ 8406],
        [ 8918],
        [ 8840]], device='cuda:0')
[2024-07-23 21:06:40,357][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[26284],
        [31182],
        [28780],
        [28735],
        [26729],
        [29363],
        [29508],
        [30361],
        [24889],
        [23558],
        [21711],
        [22621],
        [19837],
        [24695],
        [22424],
        [25231],
        [20674],
        [28555],
        [25072],
        [25003],
        [22955]], device='cuda:0')
[2024-07-23 21:06:40,360][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[23537],
        [22061],
        [15183],
        [16917],
        [18015],
        [18143],
        [17557],
        [17996],
        [18409],
        [18541],
        [18766],
        [19458],
        [19303],
        [18657],
        [18609],
        [19564],
        [20090],
        [20107],
        [19205],
        [19288],
        [19819]], device='cuda:0')
[2024-07-23 21:06:40,363][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[17645],
        [18387],
        [19636],
        [18315],
        [18012],
        [17838],
        [17854],
        [17547],
        [18222],
        [17671],
        [17106],
        [17069],
        [17211],
        [17352],
        [17600],
        [17581],
        [17619],
        [18056],
        [18514],
        [18225],
        [18244]], device='cuda:0')
[2024-07-23 21:06:40,366][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[24556],
        [39297],
        [27604],
        [33319],
        [33317],
        [34024],
        [32788],
        [32784],
        [33558],
        [33558],
        [33112],
        [33643],
        [33587],
        [32093],
        [35431],
        [32948],
        [31994],
        [35977],
        [27503],
        [27491],
        [27331]], device='cuda:0')
[2024-07-23 21:06:40,370][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 4478],
        [26565],
        [11668],
        [14388],
        [11072],
        [11225],
        [16294],
        [ 1431],
        [15573],
        [ 6101],
        [ 3102],
        [ 7223],
        [ 4132],
        [ 4217],
        [14806],
        [11366],
        [ 6521],
        [ 6884],
        [ 7857],
        [ 5280],
        [ 6838]], device='cuda:0')
[2024-07-23 21:06:40,373][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 759],
        [ 197],
        [  52],
        [  60],
        [ 272],
        [ 312],
        [ 284],
        [ 545],
        [1169],
        [ 781],
        [ 826],
        [ 226],
        [ 357],
        [1097],
        [1301],
        [ 407],
        [ 678],
        [ 572],
        [ 187],
        [ 124],
        [ 302]], device='cuda:0')
[2024-07-23 21:06:40,376][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[833],
        [499],
        [505],
        [503],
        [502],
        [485],
        [477],
        [456],
        [481],
        [439],
        [437],
        [427],
        [452],
        [409],
        [424],
        [389],
        [403],
        [373],
        [396],
        [424],
        [441]], device='cuda:0')
[2024-07-23 21:06:40,379][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 370],
        [ 689],
        [1040],
        [ 827],
        [ 966],
        [1136],
        [ 825],
        [ 868],
        [1185],
        [1909],
        [1464],
        [1785],
        [1318],
        [ 832],
        [1164],
        [1089],
        [1292],
        [1592],
        [1513],
        [1056],
        [1477]], device='cuda:0')
[2024-07-23 21:06:40,381][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 3122],
        [12251],
        [13020],
        [17018],
        [16714],
        [15454],
        [16508],
        [11784],
        [ 7762],
        [ 8193],
        [ 9594],
        [ 9733],
        [10336],
        [ 4352],
        [ 4453],
        [ 4487],
        [ 4894],
        [ 5850],
        [ 5999],
        [12026],
        [12721]], device='cuda:0')
[2024-07-23 21:06:40,382][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[15505],
        [ 5872],
        [10335],
        [11791],
        [13336],
        [12707],
        [12145],
        [10434],
        [11896],
        [12218],
        [12029],
        [11322],
        [11923],
        [11962],
        [10774],
        [10563],
        [10322],
        [10488],
        [12767],
        [12117],
        [10509]], device='cuda:0')
[2024-07-23 21:06:40,384][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 1866],
        [ 6610],
        [20568],
        [13556],
        [14940],
        [15113],
        [13798],
        [17078],
        [14598],
        [15108],
        [14932],
        [16112],
        [14741],
        [20053],
        [ 8856],
        [ 6742],
        [ 8530],
        [12999],
        [14613],
        [19492],
        [18643]], device='cuda:0')
[2024-07-23 21:06:40,387][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[7596],
        [4157],
        [3086],
        [4704],
        [3596],
        [3529],
        [3294],
        [2937],
        [2442],
        [2456],
        [2563],
        [2999],
        [3737],
        [3551],
        [3439],
        [3316],
        [3562],
        [3844],
        [3585],
        [5304],
        [5624]], device='cuda:0')
[2024-07-23 21:06:40,390][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[30108],
        [25846],
        [29426],
        [30252],
        [29800],
        [29874],
        [30013],
        [24364],
        [24560],
        [24461],
        [23959],
        [24134],
        [24039],
        [24292],
        [23602],
        [24234],
        [24257],
        [23743],
        [24693],
        [25989],
        [25942]], device='cuda:0')
[2024-07-23 21:06:40,393][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[12438],
        [ 5543],
        [ 8394],
        [12055],
        [10680],
        [ 8190],
        [ 7346],
        [ 7779],
        [ 5006],
        [ 5194],
        [ 4645],
        [ 4994],
        [ 5370],
        [ 5481],
        [ 5655],
        [ 5798],
        [ 6030],
        [ 5901],
        [ 5838],
        [ 5996],
        [ 6119]], device='cuda:0')
[2024-07-23 21:06:40,397][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[14388],
        [22984],
        [15351],
        [ 9042],
        [ 4067],
        [ 4665],
        [ 5214],
        [ 5728],
        [ 5661],
        [ 5992],
        [ 7926],
        [ 6827],
        [ 6902],
        [ 4587],
        [ 9473],
        [11232],
        [10191],
        [11872],
        [12786],
        [ 6442],
        [ 4904]], device='cuda:0')
[2024-07-23 21:06:40,400][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[3899],
        [2322],
        [2425],
        [2419],
        [1447],
        [1676],
        [2245],
        [1879],
        [1436],
        [1166],
        [ 932],
        [1281],
        [1276],
        [1605],
        [1286],
        [1282],
        [1203],
        [ 995],
        [1295],
        [1534],
        [1319]], device='cuda:0')
[2024-07-23 21:06:40,403][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[11200],
        [11050],
        [ 4931],
        [ 2148],
        [  385],
        [  383],
        [  383],
        [ 1768],
        [ 1734],
        [ 1734],
        [ 1746],
        [ 1734],
        [ 1734],
        [ 1362],
        [ 1035],
        [  361],
        [  347],
        [  390],
        [ 1353],
        [  244],
        [  192]], device='cuda:0')
[2024-07-23 21:06:40,406][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[39367],
        [43031],
        [40427],
        [40543],
        [40485],
        [40061],
        [40474],
        [40780],
        [40388],
        [40583],
        [40575],
        [41052],
        [40782],
        [41812],
        [42863],
        [43899],
        [42896],
        [42519],
        [41027],
        [41582],
        [41314]], device='cuda:0')
[2024-07-23 21:06:40,410][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[36995],
        [38235],
        [38712],
        [36292],
        [43161],
        [42696],
        [39276],
        [47728],
        [40387],
        [44247],
        [46366],
        [44367],
        [45479],
        [45409],
        [40828],
        [43616],
        [44221],
        [45147],
        [43992],
        [43979],
        [42337]], device='cuda:0')
[2024-07-23 21:06:40,413][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446],
        [5446]], device='cuda:0')
[2024-07-23 21:06:40,471][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:40,473][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,473][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,474][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,475][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,476][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,477][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,477][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,478][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,478][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,479][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,480][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,483][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:40,486][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0051, 0.9949], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,486][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.6800, 0.3200], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,490][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.4023, 0.5977], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,495][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.2484, 0.7516], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,498][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.6828, 0.3172], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,498][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.6349, 0.3651], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,499][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0615, 0.9385], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,500][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.6481, 0.3519], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,501][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.3113, 0.6887], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,503][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.5933, 0.4067], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,507][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.3158, 0.6842], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,511][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0049, 0.9951], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:40,516][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0025, 0.4860, 0.5115], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,521][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0016, 0.0034, 0.9950], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,525][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.2480, 0.2286, 0.5234], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,530][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.2040, 0.5516, 0.2444], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,531][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.5235, 0.1560, 0.3206], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,531][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.5152, 0.2183, 0.2665], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,532][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0378, 0.4628, 0.4994], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,533][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.6053, 0.1408, 0.2540], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,535][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.3462, 0.3128, 0.3410], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,539][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.4311, 0.2741, 0.2948], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,543][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.4488, 0.3307, 0.2206], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,548][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0103, 0.8047, 0.1850], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:40,554][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0018, 0.3402, 0.3751, 0.2829], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,559][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0038, 0.0103, 0.5149, 0.4710], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,563][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.4553, 0.0786, 0.1318, 0.3343], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,563][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.1677, 0.2899, 0.2459, 0.2966], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,564][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.5279, 0.0486, 0.1010, 0.3225], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,565][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.1743, 0.2444, 0.4058, 0.1755], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,566][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0240, 0.3045, 0.3289, 0.3425], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,569][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.5532, 0.0700, 0.1981, 0.1787], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,573][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.2195, 0.2367, 0.4101, 0.1337], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,578][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.2389, 0.2264, 0.3105, 0.2242], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,584][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.5856, 0.1951, 0.1251, 0.0943], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,588][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0139, 0.7641, 0.1326, 0.0894], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:40,594][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.0015, 0.2334, 0.2399, 0.1877, 0.3376], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,596][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([5.8291e-04, 7.5229e-03, 5.9053e-01, 2.0783e-01, 1.9354e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,597][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.2280, 0.0565, 0.1855, 0.4394, 0.0906], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,598][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.1074, 0.1675, 0.2006, 0.3516, 0.1728], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,598][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.2430, 0.0607, 0.1208, 0.5281, 0.0475], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,599][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.0773, 0.2299, 0.4125, 0.2394, 0.0409], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,602][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0227, 0.2273, 0.2490, 0.2592, 0.2418], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,604][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.4184, 0.1141, 0.1453, 0.2179, 0.1044], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,605][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.0908, 0.2489, 0.3139, 0.1259, 0.2204], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,609][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.2393, 0.2170, 0.2521, 0.1725, 0.1191], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,614][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.4594, 0.1961, 0.1042, 0.0736, 0.1667], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,619][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.0457, 0.6290, 0.0993, 0.0711, 0.1549], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:40,624][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.0013, 0.1618, 0.1742, 0.1533, 0.2758, 0.2336], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,629][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0025, 0.0060, 0.4406, 0.2483, 0.2115, 0.0911], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,631][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.1286, 0.0448, 0.1226, 0.5812, 0.0780, 0.0447], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,632][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0824, 0.1715, 0.1356, 0.2331, 0.2170, 0.1604], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,633][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.1756, 0.0455, 0.0913, 0.6082, 0.0478, 0.0317], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,634][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.1346, 0.1319, 0.2181, 0.2887, 0.1366, 0.0900], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,635][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0144, 0.1767, 0.1929, 0.2028, 0.1915, 0.2216], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,638][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.3212, 0.0990, 0.1131, 0.3068, 0.0839, 0.0760], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,643][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.1444, 0.1314, 0.1939, 0.0788, 0.1882, 0.2633], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,648][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.1892, 0.1652, 0.1833, 0.1475, 0.1867, 0.1281], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,653][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.3823, 0.1551, 0.1153, 0.0755, 0.1316, 0.1402], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,658][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.0481, 0.4910, 0.1188, 0.0720, 0.0970, 0.1732], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:40,664][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.0007, 0.1308, 0.1401, 0.1132, 0.2180, 0.1913, 0.2058],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,665][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [aut] are: tensor([2.8895e-04, 2.9729e-03, 4.8126e-01, 2.1301e-01, 1.5442e-01, 6.1177e-02,
        8.6874e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,666][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.1807, 0.0458, 0.1090, 0.4733, 0.0918, 0.0568, 0.0426],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,667][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.0997, 0.0843, 0.0799, 0.1639, 0.1323, 0.1825, 0.2574],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,668][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.2692, 0.0290, 0.0579, 0.5553, 0.0263, 0.0264, 0.0359],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,671][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.1365, 0.1494, 0.1341, 0.2143, 0.1278, 0.1450, 0.0929],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,675][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.0139, 0.1452, 0.1565, 0.1651, 0.1573, 0.1842, 0.1778],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,680][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.5382, 0.0717, 0.0469, 0.1980, 0.0426, 0.0458, 0.0569],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,686][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0877, 0.0881, 0.1106, 0.0494, 0.1168, 0.1510, 0.3964],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,690][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.1641, 0.1426, 0.1391, 0.1524, 0.1950, 0.1289, 0.0780],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,696][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.3294, 0.1567, 0.1068, 0.0690, 0.1195, 0.1083, 0.1102],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,698][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0351, 0.4089, 0.0997, 0.0766, 0.1302, 0.1307, 0.1189],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:40,699][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.0005, 0.1116, 0.1250, 0.1011, 0.1917, 0.1665, 0.1775, 0.1260],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,700][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0012, 0.0091, 0.1795, 0.1345, 0.1666, 0.0852, 0.0834, 0.3405],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,701][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.3306, 0.0190, 0.0455, 0.2703, 0.0477, 0.0211, 0.0238, 0.2419],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,703][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.0633, 0.0727, 0.0541, 0.1748, 0.0989, 0.1304, 0.2613, 0.1446],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,706][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.4303, 0.0180, 0.0276, 0.1888, 0.0125, 0.0070, 0.0145, 0.3014],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,712][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.0999, 0.1202, 0.1244, 0.1637, 0.1233, 0.1544, 0.1168, 0.0974],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,717][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.0107, 0.1236, 0.1345, 0.1416, 0.1342, 0.1595, 0.1529, 0.1430],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,721][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.5623, 0.0465, 0.0403, 0.1248, 0.0381, 0.0189, 0.0209, 0.1482],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,726][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0603, 0.0783, 0.1097, 0.0403, 0.0601, 0.1336, 0.3796, 0.1380],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,732][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.1123, 0.1351, 0.1570, 0.1183, 0.1234, 0.1389, 0.1605, 0.0545],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,733][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.2567, 0.1328, 0.0844, 0.0531, 0.1058, 0.0932, 0.0933, 0.1807],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,733][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.0162, 0.1660, 0.0419, 0.0343, 0.0970, 0.0503, 0.0548, 0.5394],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:40,734][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0006, 0.1012, 0.1043, 0.0863, 0.1446, 0.1385, 0.1468, 0.1154, 0.1623],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,735][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0019, 0.0281, 0.2448, 0.1064, 0.2146, 0.0817, 0.1253, 0.1067, 0.0906],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,738][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0927, 0.0188, 0.0408, 0.2693, 0.0430, 0.0225, 0.0296, 0.3500, 0.1331],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,743][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0432, 0.0915, 0.0653, 0.0989, 0.0761, 0.0814, 0.2323, 0.1776, 0.1337],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,748][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.1617, 0.0164, 0.0267, 0.1833, 0.0143, 0.0108, 0.0207, 0.4916, 0.0745],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,753][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0740, 0.1092, 0.2507, 0.1696, 0.0462, 0.1084, 0.1001, 0.1214, 0.0204],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,758][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.0088, 0.1073, 0.1168, 0.1221, 0.1160, 0.1357, 0.1323, 0.1255, 0.1355],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,763][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.4397, 0.0505, 0.0336, 0.0869, 0.0394, 0.0268, 0.0434, 0.1831, 0.0966],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,765][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0652, 0.0474, 0.0545, 0.0177, 0.0271, 0.0806, 0.3101, 0.1384, 0.2589],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,766][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.1498, 0.1042, 0.1229, 0.1190, 0.1212, 0.1433, 0.1031, 0.0670, 0.0695],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,767][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.1996, 0.1007, 0.0803, 0.0380, 0.0879, 0.0735, 0.0837, 0.1323, 0.2039],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,767][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0110, 0.0606, 0.0256, 0.0347, 0.0413, 0.0682, 0.0756, 0.2695, 0.4135],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:40,770][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0008, 0.0857, 0.0926, 0.0801, 0.1354, 0.1143, 0.1329, 0.0955, 0.1483,
        0.1144], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,773][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0011, 0.0263, 0.1929, 0.1170, 0.1832, 0.0769, 0.1188, 0.1140, 0.0651,
        0.1046], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,778][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.1486, 0.0191, 0.0449, 0.2475, 0.0420, 0.0206, 0.0322, 0.2313, 0.1335,
        0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,783][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0241, 0.0538, 0.0428, 0.0745, 0.0718, 0.0911, 0.2312, 0.1854, 0.1898,
        0.0357], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,788][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.1692, 0.0178, 0.0265, 0.1827, 0.0172, 0.0108, 0.0275, 0.3580, 0.0941,
        0.0963], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,793][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0711, 0.1053, 0.1612, 0.1545, 0.0429, 0.0863, 0.0862, 0.1234, 0.1396,
        0.0295], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,798][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0090, 0.0956, 0.1028, 0.1073, 0.1024, 0.1187, 0.1172, 0.1120, 0.1194,
        0.1157], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,799][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.4437, 0.0498, 0.0434, 0.0960, 0.0337, 0.0238, 0.0303, 0.1471, 0.0601,
        0.0723], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,800][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0429, 0.0490, 0.0609, 0.0398, 0.0687, 0.0730, 0.1959, 0.1165, 0.3168,
        0.0365], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,800][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.1069, 0.1063, 0.1266, 0.0843, 0.1036, 0.1016, 0.1063, 0.0612, 0.1210,
        0.0821], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,801][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.2246, 0.0944, 0.0685, 0.0458, 0.0763, 0.0711, 0.0724, 0.1220, 0.1316,
        0.0935], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,804][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0054, 0.0747, 0.0147, 0.0387, 0.0493, 0.0501, 0.0608, 0.2456, 0.3110,
        0.1497], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:40,808][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0005, 0.0787, 0.0835, 0.0661, 0.1238, 0.1052, 0.1172, 0.0864, 0.1409,
        0.1104, 0.0872], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,812][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0017, 0.0151, 0.0993, 0.0742, 0.2016, 0.0602, 0.0990, 0.1325, 0.0879,
        0.0839, 0.1448], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,817][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.1149, 0.0198, 0.0361, 0.1950, 0.0394, 0.0222, 0.0265, 0.2339, 0.1658,
        0.1027, 0.0437], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,822][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.0162, 0.0477, 0.0427, 0.0977, 0.0600, 0.0882, 0.1854, 0.1488, 0.2045,
        0.0380, 0.0709], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,827][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.1554, 0.0166, 0.0301, 0.1595, 0.0131, 0.0085, 0.0224, 0.3820, 0.0848,
        0.0874, 0.0403], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,831][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.0470, 0.0836, 0.1269, 0.1315, 0.0628, 0.0997, 0.0919, 0.0858, 0.1279,
        0.0864, 0.0565], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,832][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0082, 0.0853, 0.0922, 0.0961, 0.0920, 0.1082, 0.1054, 0.0989, 0.1081,
        0.1053, 0.1004], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,833][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.3472, 0.0484, 0.0392, 0.0885, 0.0294, 0.0242, 0.0241, 0.1690, 0.0852,
        0.0979, 0.0468], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,834][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0312, 0.0465, 0.0527, 0.0266, 0.0317, 0.0657, 0.2288, 0.0834, 0.3631,
        0.0356, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,837][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.1022, 0.0882, 0.1076, 0.0909, 0.0775, 0.1054, 0.1092, 0.0561, 0.1024,
        0.0992, 0.0612], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,841][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.1865, 0.1138, 0.0647, 0.0481, 0.0719, 0.0656, 0.0727, 0.1167, 0.1178,
        0.1022, 0.0399], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,845][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0083, 0.0960, 0.0295, 0.0263, 0.0555, 0.0386, 0.0463, 0.1765, 0.2031,
        0.0795, 0.2404], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:40,850][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0005, 0.0696, 0.0754, 0.0635, 0.1125, 0.1007, 0.1100, 0.0804, 0.1242,
        0.1072, 0.0821, 0.0741], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,856][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0019, 0.0147, 0.1765, 0.1342, 0.1594, 0.0609, 0.1410, 0.1141, 0.0530,
        0.0630, 0.0662, 0.0151], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,860][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.1201, 0.0194, 0.0373, 0.2381, 0.0399, 0.0162, 0.0200, 0.1844, 0.1222,
        0.0919, 0.0404, 0.0701], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,865][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0246, 0.0594, 0.0260, 0.0664, 0.0624, 0.0763, 0.1976, 0.1465, 0.1644,
        0.0484, 0.1083, 0.0196], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,865][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1603, 0.0166, 0.0260, 0.1324, 0.0139, 0.0087, 0.0231, 0.3037, 0.0701,
        0.0838, 0.0488, 0.1125], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,866][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1054, 0.0598, 0.1303, 0.1204, 0.0468, 0.0687, 0.0863, 0.0931, 0.0761,
        0.0629, 0.1022, 0.0481], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,867][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0059, 0.0771, 0.0831, 0.0877, 0.0829, 0.0982, 0.0955, 0.0917, 0.0989,
        0.0956, 0.0921, 0.0913], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,870][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.3282, 0.0632, 0.0497, 0.0970, 0.0205, 0.0218, 0.0308, 0.1152, 0.0978,
        0.0930, 0.0374, 0.0453], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,874][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0379, 0.0251, 0.0335, 0.0080, 0.0218, 0.0520, 0.2160, 0.0928, 0.3710,
        0.0204, 0.0459, 0.0757], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,879][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0932, 0.0691, 0.1020, 0.0858, 0.0737, 0.0819, 0.0852, 0.0486, 0.0917,
        0.1008, 0.0877, 0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,884][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1500, 0.0833, 0.0698, 0.0496, 0.0719, 0.0748, 0.0694, 0.1046, 0.1358,
        0.0784, 0.0386, 0.0737], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,889][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0179, 0.1529, 0.0356, 0.0373, 0.0445, 0.0468, 0.0474, 0.1516, 0.1446,
        0.0593, 0.1321, 0.1301], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:40,893][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0005, 0.0682, 0.0734, 0.0663, 0.1044, 0.0916, 0.1055, 0.0753, 0.1123,
        0.0901, 0.0744, 0.0748, 0.0631], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,898][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0023, 0.0172, 0.1577, 0.1179, 0.1584, 0.0722, 0.1372, 0.1068, 0.0526,
        0.0624, 0.0767, 0.0189, 0.0197], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,899][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.1166, 0.0122, 0.0286, 0.2362, 0.0305, 0.0126, 0.0167, 0.1823, 0.0898,
        0.0697, 0.0317, 0.0635, 0.1096], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,899][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0170, 0.0446, 0.0341, 0.0523, 0.0606, 0.0636, 0.2124, 0.1676, 0.1291,
        0.0337, 0.1005, 0.0429, 0.0416], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,900][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.1392, 0.0134, 0.0238, 0.1303, 0.0136, 0.0068, 0.0173, 0.2622, 0.0513,
        0.0636, 0.0306, 0.1337, 0.1139], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,903][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0590, 0.0512, 0.1116, 0.1518, 0.0447, 0.0695, 0.0675, 0.0866, 0.0840,
        0.0597, 0.0638, 0.0924, 0.0580], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,907][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0045, 0.0699, 0.0759, 0.0803, 0.0762, 0.0900, 0.0879, 0.0835, 0.0901,
        0.0883, 0.0850, 0.0844, 0.0840], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,912][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.3916, 0.0372, 0.0468, 0.0932, 0.0161, 0.0144, 0.0157, 0.0939, 0.0545,
        0.0618, 0.0232, 0.0424, 0.1092], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,917][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0667, 0.0381, 0.0351, 0.0217, 0.0518, 0.0497, 0.1868, 0.0719, 0.2550,
        0.0270, 0.0496, 0.0766, 0.0701], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,922][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0980, 0.0749, 0.0873, 0.0809, 0.0655, 0.0778, 0.0783, 0.0393, 0.0789,
        0.0783, 0.0871, 0.0723, 0.0813], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,926][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1391, 0.0762, 0.0602, 0.0389, 0.0680, 0.0647, 0.0565, 0.0952, 0.1419,
        0.0738, 0.0368, 0.0568, 0.0919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,931][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0298, 0.1220, 0.0322, 0.0443, 0.0249, 0.0334, 0.0387, 0.1097, 0.0893,
        0.0717, 0.1303, 0.0697, 0.2041], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:40,932][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0004, 0.0611, 0.0693, 0.0560, 0.0929, 0.0832, 0.0932, 0.0694, 0.1037,
        0.0840, 0.0691, 0.0658, 0.0619, 0.0899], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,933][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0006, 0.0016, 0.1423, 0.0881, 0.0723, 0.0127, 0.0449, 0.2279, 0.0881,
        0.0989, 0.1388, 0.0209, 0.0164, 0.0464], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,933][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.1625, 0.0079, 0.0170, 0.1528, 0.0183, 0.0061, 0.0106, 0.1648, 0.0704,
        0.0594, 0.0248, 0.0473, 0.1076, 0.1504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,936][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0326, 0.0348, 0.0244, 0.0485, 0.0483, 0.0613, 0.1578, 0.1165, 0.1494,
        0.0392, 0.0805, 0.0320, 0.0540, 0.1209], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,940][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.1449, 0.0062, 0.0080, 0.0684, 0.0064, 0.0031, 0.0053, 0.1783, 0.0255,
        0.0443, 0.0191, 0.0651, 0.0806, 0.3448], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,945][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.0436, 0.0380, 0.1326, 0.0777, 0.0852, 0.0667, 0.0590, 0.0799, 0.1043,
        0.0727, 0.0869, 0.0687, 0.0656, 0.0191], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,950][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.0051, 0.0637, 0.0695, 0.0739, 0.0697, 0.0821, 0.0805, 0.0761, 0.0832,
        0.0807, 0.0770, 0.0773, 0.0771, 0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,955][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.4794, 0.0140, 0.0269, 0.0496, 0.0108, 0.0077, 0.0113, 0.0796, 0.0435,
        0.0419, 0.0255, 0.0265, 0.0695, 0.1137], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,959][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0563, 0.0394, 0.0529, 0.0183, 0.0201, 0.0402, 0.1407, 0.0672, 0.2706,
        0.0206, 0.0277, 0.0796, 0.0516, 0.1148], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,963][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0899, 0.0671, 0.0813, 0.0696, 0.0787, 0.0608, 0.0744, 0.0369, 0.0817,
        0.0727, 0.0820, 0.0720, 0.0747, 0.0582], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,964][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0855, 0.0795, 0.0531, 0.0384, 0.0688, 0.0569, 0.0485, 0.0849, 0.1388,
        0.0720, 0.0342, 0.0727, 0.1128, 0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,965][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0275, 0.0936, 0.0176, 0.0322, 0.0306, 0.0378, 0.0426, 0.0757, 0.0822,
        0.0645, 0.0753, 0.0681, 0.2075, 0.1449], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:40,966][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0004, 0.0545, 0.0619, 0.0507, 0.0903, 0.0770, 0.0820, 0.0618, 0.0977,
        0.0797, 0.0648, 0.0612, 0.0573, 0.0939, 0.0668], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,969][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0013, 0.0009, 0.0951, 0.0775, 0.0490, 0.0121, 0.0419, 0.2087, 0.1355,
        0.1220, 0.1581, 0.0256, 0.0176, 0.0333, 0.0213], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,972][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0786, 0.0052, 0.0091, 0.1378, 0.0132, 0.0041, 0.0073, 0.1677, 0.0562,
        0.0421, 0.0178, 0.0337, 0.0810, 0.1867, 0.1596], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,977][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0390, 0.0310, 0.0131, 0.0356, 0.0494, 0.0482, 0.1264, 0.0901, 0.0835,
        0.0229, 0.0517, 0.0223, 0.0266, 0.1023, 0.2580], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,981][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0773, 0.0033, 0.0049, 0.0487, 0.0053, 0.0020, 0.0042, 0.1407, 0.0230,
        0.0365, 0.0140, 0.0440, 0.0583, 0.3714, 0.1665], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,986][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.0342, 0.0433, 0.0863, 0.0918, 0.0650, 0.0773, 0.0491, 0.0574, 0.0808,
        0.0671, 0.0785, 0.1172, 0.0830, 0.0441, 0.0249], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,991][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0053, 0.0589, 0.0640, 0.0681, 0.0646, 0.0750, 0.0738, 0.0703, 0.0763,
        0.0741, 0.0711, 0.0711, 0.0707, 0.0775, 0.0790], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,996][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.1668, 0.0115, 0.0143, 0.0629, 0.0115, 0.0050, 0.0093, 0.0979, 0.0451,
        0.0370, 0.0164, 0.0267, 0.0644, 0.1525, 0.2787], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,996][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0413, 0.0283, 0.0235, 0.0141, 0.0181, 0.0347, 0.1382, 0.0626, 0.2418,
        0.0171, 0.0245, 0.0548, 0.0478, 0.1243, 0.1288], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,997][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0581, 0.0589, 0.0626, 0.0678, 0.0678, 0.0690, 0.0699, 0.0419, 0.0783,
        0.0788, 0.0858, 0.0716, 0.0815, 0.0716, 0.0364], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,998][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0642, 0.0825, 0.0536, 0.0366, 0.0639, 0.0518, 0.0524, 0.0852, 0.1117,
        0.0719, 0.0375, 0.0610, 0.0893, 0.0480, 0.0905], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:40,999][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0371, 0.0556, 0.0114, 0.0177, 0.0195, 0.0256, 0.0245, 0.0508, 0.0527,
        0.0365, 0.0572, 0.0474, 0.1393, 0.0873, 0.3374], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,002][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0004, 0.0544, 0.0581, 0.0466, 0.0753, 0.0749, 0.0814, 0.0585, 0.0864,
        0.0729, 0.0594, 0.0572, 0.0520, 0.0859, 0.0692, 0.0673],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,006][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0013, 0.0025, 0.1655, 0.1004, 0.0613, 0.0109, 0.0360, 0.1597, 0.0954,
        0.1121, 0.1288, 0.0259, 0.0173, 0.0444, 0.0229, 0.0157],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,011][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.1754, 0.0141, 0.0184, 0.1048, 0.0155, 0.0064, 0.0116, 0.0867, 0.0486,
        0.0348, 0.0183, 0.0314, 0.0663, 0.1100, 0.1148, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,015][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0151, 0.0249, 0.0218, 0.0330, 0.0316, 0.0390, 0.1051, 0.0848, 0.1075,
        0.0238, 0.0418, 0.0224, 0.0373, 0.1104, 0.2523, 0.0491],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,020][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.1166, 0.0099, 0.0143, 0.0420, 0.0086, 0.0040, 0.0080, 0.1052, 0.0207,
        0.0366, 0.0161, 0.0465, 0.0588, 0.2559, 0.1555, 0.1014],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,025][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0395, 0.0587, 0.1090, 0.0586, 0.0228, 0.0721, 0.0517, 0.0752, 0.0749,
        0.0492, 0.0718, 0.0771, 0.0854, 0.0681, 0.0666, 0.0194],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,029][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0051, 0.0566, 0.0599, 0.0627, 0.0599, 0.0695, 0.0684, 0.0649, 0.0692,
        0.0677, 0.0655, 0.0649, 0.0652, 0.0716, 0.0729, 0.0761],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,031][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.1490, 0.0230, 0.0235, 0.0421, 0.0131, 0.0120, 0.0241, 0.0461, 0.0442,
        0.0409, 0.0174, 0.0201, 0.0444, 0.0916, 0.3356, 0.0728],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,032][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0398, 0.0242, 0.0155, 0.0051, 0.0142, 0.0299, 0.1243, 0.0525, 0.1521,
        0.0091, 0.0194, 0.0516, 0.0240, 0.1316, 0.1425, 0.1643],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,033][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0711, 0.0589, 0.0681, 0.0605, 0.0607, 0.0604, 0.0689, 0.0347, 0.0702,
        0.0663, 0.0634, 0.0639, 0.0650, 0.0645, 0.0640, 0.0594],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,034][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.1163, 0.0744, 0.0521, 0.0324, 0.0479, 0.0439, 0.0412, 0.0561, 0.1100,
        0.0565, 0.0252, 0.0486, 0.0752, 0.0356, 0.0716, 0.1129],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,037][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0516, 0.1132, 0.0281, 0.0334, 0.0235, 0.0186, 0.0157, 0.0450, 0.0320,
        0.0388, 0.0777, 0.0523, 0.1518, 0.0687, 0.0968, 0.1528],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,041][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0003, 0.0505, 0.0544, 0.0485, 0.0777, 0.0679, 0.0754, 0.0546, 0.0853,
        0.0695, 0.0538, 0.0555, 0.0492, 0.0796, 0.0646, 0.0690, 0.0442],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,045][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0007, 0.0018, 0.1371, 0.0867, 0.0389, 0.0083, 0.0320, 0.1678, 0.1141,
        0.1441, 0.1072, 0.0293, 0.0234, 0.0376, 0.0247, 0.0145, 0.0317],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,050][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1087, 0.0081, 0.0147, 0.0957, 0.0138, 0.0057, 0.0080, 0.0778, 0.0399,
        0.0296, 0.0160, 0.0273, 0.0572, 0.1021, 0.1003, 0.1566, 0.1384],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,055][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0126, 0.0275, 0.0175, 0.0274, 0.0338, 0.0410, 0.1050, 0.0829, 0.0801,
        0.0179, 0.0492, 0.0226, 0.0283, 0.1246, 0.2503, 0.0553, 0.0240],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,059][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0920, 0.0063, 0.0089, 0.0477, 0.0052, 0.0027, 0.0053, 0.0799, 0.0184,
        0.0244, 0.0125, 0.0466, 0.0495, 0.2145, 0.1500, 0.1479, 0.0881],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,064][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0445, 0.0522, 0.0859, 0.0945, 0.0361, 0.0642, 0.0518, 0.0584, 0.0568,
        0.0439, 0.0580, 0.0820, 0.0612, 0.0560, 0.0515, 0.0729, 0.0302],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,065][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0036, 0.0515, 0.0555, 0.0583, 0.0555, 0.0651, 0.0640, 0.0608, 0.0647,
        0.0634, 0.0615, 0.0609, 0.0608, 0.0668, 0.0683, 0.0715, 0.0677],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,066][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2201, 0.0212, 0.0221, 0.0484, 0.0084, 0.0077, 0.0095, 0.0439, 0.0307,
        0.0341, 0.0146, 0.0234, 0.0524, 0.0877, 0.1754, 0.1147, 0.0857],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,066][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0321, 0.0282, 0.0227, 0.0126, 0.0276, 0.0294, 0.1154, 0.0519, 0.1695,
        0.0157, 0.0294, 0.0399, 0.0368, 0.0920, 0.1436, 0.1186, 0.0346],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,069][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0626, 0.0499, 0.0737, 0.0568, 0.0538, 0.0609, 0.0567, 0.0324, 0.0628,
        0.0665, 0.0638, 0.0593, 0.0667, 0.0492, 0.0624, 0.0615, 0.0609],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,073][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0836, 0.0609, 0.0460, 0.0311, 0.0508, 0.0494, 0.0426, 0.0674, 0.1089,
        0.0535, 0.0266, 0.0437, 0.0700, 0.0386, 0.0650, 0.0883, 0.0736],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,078][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0247, 0.0829, 0.0164, 0.0330, 0.0242, 0.0274, 0.0240, 0.0529, 0.0333,
        0.0356, 0.0621, 0.0399, 0.1288, 0.0517, 0.1203, 0.0820, 0.1606],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,082][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0003, 0.0405, 0.0457, 0.0390, 0.0722, 0.0635, 0.0712, 0.0487, 0.0899,
        0.0662, 0.0548, 0.0518, 0.0485, 0.0776, 0.0586, 0.0639, 0.0420, 0.0657],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,087][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0012, 0.0020, 0.1546, 0.0792, 0.0435, 0.0069, 0.0212, 0.1600, 0.1080,
        0.1354, 0.1517, 0.0214, 0.0175, 0.0320, 0.0213, 0.0097, 0.0149, 0.0194],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,092][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0874, 0.0078, 0.0122, 0.0641, 0.0104, 0.0053, 0.0061, 0.0605, 0.0440,
        0.0321, 0.0144, 0.0262, 0.0631, 0.0840, 0.0941, 0.1239, 0.1551, 0.1091],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,096][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0213, 0.0143, 0.0128, 0.0461, 0.0270, 0.0283, 0.0765, 0.0523, 0.0866,
        0.0217, 0.0384, 0.0161, 0.0388, 0.0764, 0.2544, 0.0733, 0.0375, 0.0781],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,097][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.1182, 0.0045, 0.0093, 0.0450, 0.0043, 0.0025, 0.0035, 0.0584, 0.0190,
        0.0237, 0.0122, 0.0452, 0.0505, 0.1567, 0.1089, 0.1312, 0.1109, 0.0960],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,098][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0329, 0.0242, 0.0506, 0.0746, 0.0646, 0.0495, 0.0609, 0.0585, 0.0962,
        0.0565, 0.0737, 0.0618, 0.0561, 0.0633, 0.0586, 0.0537, 0.0432, 0.0212],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,099][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0034, 0.0473, 0.0515, 0.0544, 0.0524, 0.0625, 0.0604, 0.0564, 0.0624,
        0.0606, 0.0576, 0.0581, 0.0575, 0.0633, 0.0648, 0.0682, 0.0648, 0.0544],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,102][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.2264, 0.0150, 0.0344, 0.0429, 0.0086, 0.0098, 0.0110, 0.0353, 0.0348,
        0.0352, 0.0142, 0.0190, 0.0453, 0.0827, 0.1116, 0.0955, 0.0780, 0.1002],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,106][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0235, 0.0291, 0.0173, 0.0089, 0.0166, 0.0230, 0.1170, 0.0566, 0.1629,
        0.0117, 0.0199, 0.0408, 0.0250, 0.1050, 0.1457, 0.1306, 0.0244, 0.0420],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,111][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0589, 0.0318, 0.0474, 0.0606, 0.0748, 0.0545, 0.0548, 0.0400, 0.0700,
        0.0612, 0.0627, 0.0665, 0.0572, 0.0604, 0.0473, 0.0623, 0.0557, 0.0337],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,116][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0813, 0.1146, 0.0365, 0.0257, 0.0415, 0.0316, 0.0328, 0.0586, 0.0730,
        0.0390, 0.0267, 0.0403, 0.0519, 0.0336, 0.0465, 0.0649, 0.0587, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,120][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0266, 0.0693, 0.0178, 0.0221, 0.0205, 0.0226, 0.0225, 0.0506, 0.0382,
        0.0334, 0.0615, 0.0341, 0.1256, 0.0712, 0.1232, 0.0576, 0.1228, 0.0806],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,125][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0003, 0.0432, 0.0460, 0.0378, 0.0646, 0.0590, 0.0629, 0.0477, 0.0761,
        0.0640, 0.0490, 0.0463, 0.0460, 0.0733, 0.0576, 0.0590, 0.0407, 0.0638,
        0.0629], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,129][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0008, 0.0017, 0.3990, 0.0846, 0.0309, 0.0076, 0.0353, 0.0872, 0.0452,
        0.0784, 0.0769, 0.0119, 0.0098, 0.0271, 0.0140, 0.0070, 0.0076, 0.0131,
        0.0618], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,130][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.1215, 0.0078, 0.0082, 0.0584, 0.0074, 0.0028, 0.0043, 0.0517, 0.0229,
        0.0177, 0.0110, 0.0134, 0.0366, 0.0662, 0.0652, 0.1034, 0.0939, 0.1075,
        0.2000], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,130][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0179, 0.0167, 0.0079, 0.0333, 0.0232, 0.0438, 0.0897, 0.0484, 0.0754,
        0.0198, 0.0308, 0.0124, 0.0355, 0.0630, 0.2297, 0.0711, 0.0346, 0.1134,
        0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,131][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.1514, 0.0039, 0.0031, 0.0348, 0.0026, 0.0012, 0.0023, 0.0523, 0.0108,
        0.0158, 0.0097, 0.0207, 0.0279, 0.1191, 0.0873, 0.0902, 0.0795, 0.1086,
        0.1788], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,134][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0417, 0.0268, 0.0302, 0.0564, 0.0697, 0.0466, 0.0477, 0.0500, 0.0454,
        0.0889, 0.0729, 0.0748, 0.0691, 0.0411, 0.0483, 0.0833, 0.0454, 0.0322,
        0.0296], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,138][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0041, 0.0460, 0.0489, 0.0522, 0.0498, 0.0581, 0.0562, 0.0536, 0.0584,
        0.0566, 0.0547, 0.0543, 0.0540, 0.0595, 0.0603, 0.0642, 0.0606, 0.0519,
        0.0570], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,142][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.3102, 0.0136, 0.0118, 0.0269, 0.0048, 0.0041, 0.0045, 0.0283, 0.0180,
        0.0209, 0.0100, 0.0096, 0.0309, 0.0639, 0.0848, 0.0639, 0.0629, 0.0916,
        0.1392], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,147][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0336, 0.0278, 0.0212, 0.0131, 0.0215, 0.0289, 0.1137, 0.0394, 0.1497,
        0.0181, 0.0248, 0.0350, 0.0334, 0.0697, 0.1414, 0.0975, 0.0325, 0.0634,
        0.0354], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,152][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0721, 0.0368, 0.0378, 0.0536, 0.0512, 0.0501, 0.0521, 0.0317, 0.0577,
        0.0789, 0.0579, 0.0574, 0.0633, 0.0567, 0.0465, 0.0530, 0.0599, 0.0441,
        0.0392], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,156][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0812, 0.0614, 0.0491, 0.0288, 0.0386, 0.0371, 0.0329, 0.0560, 0.0678,
        0.0443, 0.0238, 0.0440, 0.0676, 0.0357, 0.0504, 0.0837, 0.0720, 0.0662,
        0.0592], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,161][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0106, 0.0788, 0.0221, 0.0215, 0.0185, 0.0204, 0.0300, 0.0579, 0.0491,
        0.0453, 0.0636, 0.0322, 0.0811, 0.0920, 0.1723, 0.0453, 0.0705, 0.0663,
        0.0221], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,162][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0003, 0.0407, 0.0461, 0.0355, 0.0614, 0.0581, 0.0593, 0.0457, 0.0719,
        0.0623, 0.0468, 0.0447, 0.0424, 0.0650, 0.0552, 0.0521, 0.0382, 0.0636,
        0.0649, 0.0458], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,163][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0006, 0.0024, 0.2244, 0.0644, 0.0415, 0.0096, 0.0293, 0.0836, 0.0853,
        0.1194, 0.0879, 0.0191, 0.0200, 0.0371, 0.0166, 0.0107, 0.0194, 0.0241,
        0.0710, 0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,164][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.2065, 0.0060, 0.0057, 0.0195, 0.0031, 0.0012, 0.0017, 0.0154, 0.0058,
        0.0064, 0.0044, 0.0063, 0.0143, 0.0167, 0.0203, 0.0502, 0.0410, 0.0412,
        0.0858, 0.4484], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,167][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0181, 0.0161, 0.0127, 0.0187, 0.0200, 0.0312, 0.1023, 0.0629, 0.0833,
        0.0175, 0.0302, 0.0191, 0.0272, 0.0906, 0.2043, 0.0415, 0.0292, 0.0899,
        0.0506, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,170][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.2455, 0.0034, 0.0034, 0.0125, 0.0017, 0.0007, 0.0012, 0.0187, 0.0039,
        0.0069, 0.0039, 0.0131, 0.0137, 0.0418, 0.0227, 0.0335, 0.0333, 0.0500,
        0.0859, 0.4042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,175][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0270, 0.0550, 0.0835, 0.0375, 0.0252, 0.0598, 0.0356, 0.0640, 0.0730,
        0.0453, 0.0582, 0.0678, 0.0682, 0.0323, 0.0505, 0.0318, 0.0399, 0.0573,
        0.0593, 0.0288], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,179][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0035, 0.0439, 0.0468, 0.0490, 0.0463, 0.0544, 0.0533, 0.0508, 0.0543,
        0.0532, 0.0513, 0.0510, 0.0512, 0.0558, 0.0567, 0.0604, 0.0575, 0.0497,
        0.0542, 0.0566], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,184][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.3459, 0.0092, 0.0136, 0.0186, 0.0049, 0.0022, 0.0033, 0.0116, 0.0137,
        0.0122, 0.0068, 0.0099, 0.0162, 0.0318, 0.0435, 0.0362, 0.0373, 0.0478,
        0.1076, 0.2275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,189][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0320, 0.0206, 0.0168, 0.0073, 0.0169, 0.0362, 0.0949, 0.0332, 0.1184,
        0.0107, 0.0163, 0.0413, 0.0255, 0.0941, 0.1117, 0.1337, 0.0290, 0.0533,
        0.0363, 0.0718], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,193][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0499, 0.0429, 0.0567, 0.0427, 0.0447, 0.0444, 0.0575, 0.0277, 0.0618,
        0.0518, 0.0568, 0.0497, 0.0520, 0.0516, 0.0508, 0.0494, 0.0490, 0.0509,
        0.0617, 0.0482], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,194][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.1310, 0.0519, 0.0360, 0.0273, 0.0331, 0.0343, 0.0291, 0.0477, 0.0728,
        0.0459, 0.0190, 0.0444, 0.0666, 0.0286, 0.0509, 0.0807, 0.0653, 0.0537,
        0.0406, 0.0408], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,195][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0266, 0.1334, 0.0240, 0.0296, 0.0203, 0.0195, 0.0255, 0.0539, 0.0379,
        0.0303, 0.0574, 0.0345, 0.0867, 0.0681, 0.0845, 0.0652, 0.0782, 0.0477,
        0.0180, 0.0586], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,196][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0002, 0.0404, 0.0442, 0.0391, 0.0625, 0.0550, 0.0610, 0.0437, 0.0680,
        0.0559, 0.0433, 0.0446, 0.0392, 0.0641, 0.0523, 0.0551, 0.0355, 0.0586,
        0.0574, 0.0474, 0.0326], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,199][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0005, 0.0015, 0.1470, 0.0735, 0.0341, 0.0101, 0.0314, 0.0918, 0.1018,
        0.1592, 0.0967, 0.0261, 0.0291, 0.0256, 0.0172, 0.0091, 0.0290, 0.0216,
        0.0478, 0.0229, 0.0239], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,202][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0591, 0.0032, 0.0050, 0.0260, 0.0034, 0.0014, 0.0018, 0.0158, 0.0086,
        0.0068, 0.0038, 0.0066, 0.0137, 0.0212, 0.0205, 0.0401, 0.0323, 0.0316,
        0.0788, 0.5599, 0.0602], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,207][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0102, 0.0211, 0.0136, 0.0206, 0.0254, 0.0333, 0.0830, 0.0655, 0.0627,
        0.0143, 0.0397, 0.0186, 0.0225, 0.0987, 0.2024, 0.0412, 0.0192, 0.1029,
        0.0507, 0.0367, 0.0176], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,212][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0653, 0.0030, 0.0033, 0.0146, 0.0015, 0.0008, 0.0015, 0.0201, 0.0045,
        0.0066, 0.0037, 0.0146, 0.0142, 0.0542, 0.0356, 0.0408, 0.0239, 0.0539,
        0.0943, 0.4992, 0.0444], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,216][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0326, 0.0402, 0.0668, 0.0796, 0.0290, 0.0536, 0.0411, 0.0453, 0.0465,
        0.0348, 0.0461, 0.0677, 0.0493, 0.0440, 0.0385, 0.0591, 0.0243, 0.0478,
        0.0608, 0.0692, 0.0234], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,221][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0026, 0.0409, 0.0444, 0.0466, 0.0439, 0.0517, 0.0507, 0.0483, 0.0512,
        0.0505, 0.0489, 0.0486, 0.0484, 0.0527, 0.0539, 0.0570, 0.0540, 0.0466,
        0.0512, 0.0535, 0.0543], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,225][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.2002, 0.0146, 0.0114, 0.0221, 0.0037, 0.0034, 0.0041, 0.0178, 0.0123,
        0.0148, 0.0064, 0.0109, 0.0217, 0.0325, 0.0645, 0.0507, 0.0340, 0.0646,
        0.0923, 0.2725, 0.0456], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,226][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0275, 0.0241, 0.0194, 0.0149, 0.0290, 0.0291, 0.1022, 0.0368, 0.1279,
        0.0149, 0.0255, 0.0352, 0.0320, 0.0679, 0.1325, 0.0973, 0.0285, 0.0368,
        0.0329, 0.0669, 0.0186], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,227][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0484, 0.0394, 0.0575, 0.0456, 0.0424, 0.0487, 0.0475, 0.0249, 0.0494,
        0.0534, 0.0502, 0.0473, 0.0515, 0.0393, 0.0497, 0.0497, 0.0479, 0.0456,
        0.0621, 0.0512, 0.0482], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,228][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0692, 0.0489, 0.0374, 0.0257, 0.0420, 0.0411, 0.0342, 0.0521, 0.0848,
        0.0430, 0.0218, 0.0364, 0.0581, 0.0321, 0.0520, 0.0703, 0.0604, 0.0504,
        0.0424, 0.0390, 0.0590], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,231][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0274, 0.1380, 0.0250, 0.0299, 0.0218, 0.0265, 0.0287, 0.0516, 0.0263,
        0.0266, 0.0605, 0.0347, 0.0837, 0.0431, 0.1001, 0.0501, 0.0842, 0.0435,
        0.0126, 0.0298, 0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,299][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:41,301][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,302][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,302][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,303][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,304][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,304][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,305][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,306][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,306][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,307][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,308][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,311][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:41,314][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([8.3218e-04, 9.9917e-01], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,314][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0589, 0.9411], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,318][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.4023, 0.5977], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,323][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.4743, 0.5257], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,327][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.2901, 0.7099], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,331][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.6141, 0.3859], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,332][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.4909, 0.5091], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,333][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.6481, 0.3519], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,334][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.6488, 0.3512], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,334][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.9399, 0.0601], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,337][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.7221, 0.2779], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,340][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.6778, 0.3222], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:41,342][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([4.6685e-04, 2.9637e-01, 7.0317e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,347][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0267, 0.3874, 0.5859], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,351][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.2480, 0.2286, 0.5234], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,356][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.4403, 0.4288, 0.1309], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,361][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0805, 0.1549, 0.7646], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,364][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.5061, 0.2150, 0.2789], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,365][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.4368, 0.3322, 0.2309], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,366][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.6053, 0.1408, 0.2540], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,366][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.5424, 0.1806, 0.2770], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,367][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.8509, 0.0882, 0.0609], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,370][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.7407, 0.1592, 0.1001], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,374][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.5526, 0.2608, 0.1866], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:41,377][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([2.5395e-04, 2.2868e-01, 5.8707e-01, 1.8399e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,381][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0181, 0.2465, 0.3588, 0.3766], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,386][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.4553, 0.0786, 0.1318, 0.3343], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,391][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.2672, 0.1178, 0.1186, 0.4964], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,395][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.1806, 0.0092, 0.0270, 0.7832], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,397][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.4911, 0.2064, 0.1875, 0.1150], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,398][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.5855, 0.1352, 0.1764, 0.1029], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,398][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.5532, 0.0700, 0.1981, 0.1787], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,399][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.7502, 0.0495, 0.0478, 0.1526], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,400][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.7950, 0.0835, 0.0676, 0.0538], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,403][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.7494, 0.0888, 0.0766, 0.0852], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,406][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.5006, 0.1408, 0.0421, 0.3165], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:41,409][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([1.7982e-04, 1.8584e-01, 5.0348e-01, 1.3916e-01, 1.7135e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,414][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0169, 0.1832, 0.2542, 0.2668, 0.2788], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,419][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.2280, 0.0565, 0.1855, 0.4394, 0.0906], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,423][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.1024, 0.0534, 0.0894, 0.6690, 0.0858], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,428][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.0175, 0.0054, 0.0154, 0.8523, 0.1094], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,430][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.5936, 0.2351, 0.0602, 0.0493, 0.0618], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,430][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.3823, 0.1058, 0.2109, 0.2635, 0.0375], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,431][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.4184, 0.1141, 0.1453, 0.2179, 0.1044], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,432][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.4926, 0.1119, 0.0746, 0.2079, 0.1131], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,433][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.6262, 0.1212, 0.1081, 0.0897, 0.0548], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,436][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.4543, 0.1302, 0.1057, 0.1541, 0.1556], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,441][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.5663, 0.1812, 0.0275, 0.1455, 0.0795], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:41,443][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([1.3500e-04, 1.5996e-01, 4.2405e-01, 1.1377e-01, 1.4231e-01, 1.5978e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,448][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0113, 0.1371, 0.1997, 0.2111, 0.2154, 0.2255], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,453][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.1286, 0.0448, 0.1226, 0.5812, 0.0780, 0.0447], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,458][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0846, 0.0686, 0.0856, 0.5142, 0.2066, 0.0404], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,462][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.0031, 0.0022, 0.0080, 0.8492, 0.0985, 0.0389], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,463][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.4638, 0.1373, 0.1330, 0.0994, 0.1129, 0.0535], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,464][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.3542, 0.1340, 0.1155, 0.2754, 0.1105, 0.0104], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,464][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.3212, 0.0990, 0.1131, 0.3068, 0.0839, 0.0760], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,465][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.4917, 0.0626, 0.0748, 0.2226, 0.0910, 0.0572], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,469][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.5425, 0.1200, 0.0779, 0.0708, 0.1589, 0.0299], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,473][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.5595, 0.1042, 0.0924, 0.1049, 0.0667, 0.0723], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,478][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.3065, 0.1318, 0.0319, 0.2026, 0.0531, 0.2741], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:41,481][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([1.5597e-04, 1.1501e-01, 2.8733e-01, 7.8038e-02, 9.5429e-02, 1.0843e-01,
        3.1560e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,486][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0091, 0.1115, 0.1608, 0.1726, 0.1758, 0.1856, 0.1847],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,490][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.1807, 0.0458, 0.1090, 0.4733, 0.0918, 0.0568, 0.0426],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,495][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.1062, 0.0305, 0.0448, 0.4682, 0.1151, 0.1128, 0.1225],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,495][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.0517, 0.0032, 0.0074, 0.6818, 0.0876, 0.0507, 0.1175],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,496][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.3978, 0.2608, 0.0671, 0.0657, 0.0886, 0.0982, 0.0217],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,497][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.3096, 0.1494, 0.0972, 0.2646, 0.0990, 0.0412, 0.0391],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,498][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.5382, 0.0717, 0.0469, 0.1980, 0.0426, 0.0458, 0.0569],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,501][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.3935, 0.0676, 0.0467, 0.2425, 0.0917, 0.1042, 0.0537],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,506][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.5484, 0.0974, 0.0740, 0.0631, 0.1428, 0.0357, 0.0387],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,510][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.5181, 0.0533, 0.0640, 0.1110, 0.0858, 0.0446, 0.1233],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,515][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.3741, 0.0759, 0.0307, 0.1432, 0.0497, 0.1165, 0.2099],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:41,518][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([7.8112e-05, 9.3373e-02, 2.3162e-01, 6.8792e-02, 8.7644e-02, 1.0076e-01,
        3.0667e-01, 1.1105e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,523][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0082, 0.0958, 0.1357, 0.1444, 0.1498, 0.1541, 0.1519, 0.1601],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,527][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.3306, 0.0190, 0.0455, 0.2703, 0.0477, 0.0211, 0.0238, 0.2419],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,528][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.0376, 0.0139, 0.0180, 0.6218, 0.0437, 0.0844, 0.0995, 0.0811],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,529][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0096, 0.0018, 0.0017, 0.6595, 0.0499, 0.0105, 0.0606, 0.2064],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,530][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.3826, 0.1374, 0.0448, 0.0609, 0.1006, 0.1497, 0.0448, 0.0792],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,531][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.5619, 0.0540, 0.0309, 0.0915, 0.0403, 0.0096, 0.0212, 0.1906],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,535][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.5623, 0.0465, 0.0403, 0.1248, 0.0381, 0.0189, 0.0209, 0.1482],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,540][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.5318, 0.0301, 0.0229, 0.1237, 0.0411, 0.0506, 0.0191, 0.1808],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,544][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.5503, 0.0781, 0.0632, 0.0503, 0.0760, 0.0434, 0.0859, 0.0527],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,549][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.4257, 0.1075, 0.0815, 0.0887, 0.0640, 0.0462, 0.0720, 0.1145],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,554][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.5745, 0.0705, 0.0185, 0.1053, 0.0358, 0.0703, 0.0596, 0.0654],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:41,556][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([6.2353e-05, 7.9729e-02, 2.0878e-01, 6.3473e-02, 7.9278e-02, 9.6393e-02,
        2.8842e-01, 9.8428e-02, 8.5435e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,560][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0073, 0.0828, 0.1163, 0.1228, 0.1286, 0.1323, 0.1321, 0.1393, 0.1383],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,560][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.0927, 0.0188, 0.0408, 0.2693, 0.0430, 0.0225, 0.0296, 0.3500, 0.1331],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,561][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0458, 0.0344, 0.0351, 0.2123, 0.0670, 0.0449, 0.2070, 0.2475, 0.1060],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,562][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0134, 0.0020, 0.0029, 0.5804, 0.0377, 0.0107, 0.0658, 0.2002, 0.0869],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,565][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.3212, 0.1364, 0.0321, 0.0436, 0.0685, 0.1399, 0.0927, 0.1214, 0.0441],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,568][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.1376, 0.0794, 0.0824, 0.1177, 0.0544, 0.0097, 0.0661, 0.3777, 0.0750],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,572][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.4397, 0.0505, 0.0336, 0.0869, 0.0394, 0.0268, 0.0434, 0.1831, 0.0966],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,576][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.2729, 0.0191, 0.0135, 0.0860, 0.0354, 0.0543, 0.0371, 0.3943, 0.0875],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,581][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.5453, 0.0768, 0.0436, 0.0490, 0.0793, 0.0430, 0.0624, 0.0749, 0.0258],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,586][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.4936, 0.0937, 0.0406, 0.0661, 0.0620, 0.0398, 0.0685, 0.0787, 0.0569],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,590][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.3878, 0.0767, 0.0137, 0.0715, 0.0245, 0.0739, 0.0852, 0.0661, 0.2007],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:41,592][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([3.3428e-05, 7.9645e-02, 2.0794e-01, 5.7718e-02, 7.7065e-02, 9.0247e-02,
        3.0588e-01, 9.0681e-02, 8.1982e-02, 8.8107e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,593][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0073, 0.0740, 0.1004, 0.1063, 0.1108, 0.1138, 0.1132, 0.1203, 0.1199,
        0.1340], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,594][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.1486, 0.0191, 0.0449, 0.2475, 0.0420, 0.0206, 0.0322, 0.2313, 0.1335,
        0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,594][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0223, 0.0143, 0.0192, 0.1477, 0.0512, 0.0317, 0.1476, 0.3412, 0.2013,
        0.0235], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,597][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.0024, 0.0008, 0.0021, 0.3784, 0.0374, 0.0115, 0.0782, 0.2077, 0.2158,
        0.0657], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,600][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.3190, 0.1262, 0.0300, 0.0379, 0.0371, 0.0918, 0.0711, 0.1414, 0.0816,
        0.0640], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,604][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.2182, 0.0749, 0.0499, 0.0758, 0.0347, 0.0078, 0.0361, 0.3448, 0.1356,
        0.0221], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,608][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.4437, 0.0498, 0.0434, 0.0960, 0.0337, 0.0238, 0.0303, 0.1471, 0.0601,
        0.0723], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,613][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.2901, 0.0300, 0.0157, 0.0595, 0.0266, 0.0410, 0.0429, 0.2765, 0.1167,
        0.1011], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,618][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.4418, 0.0885, 0.0542, 0.0494, 0.0825, 0.0428, 0.0700, 0.0747, 0.0581,
        0.0378], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,622][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.3212, 0.0596, 0.0803, 0.0967, 0.0638, 0.0651, 0.0701, 0.0886, 0.0768,
        0.0779], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,624][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.2336, 0.0723, 0.0169, 0.0842, 0.0221, 0.0841, 0.0612, 0.0561, 0.1817,
        0.1879], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:41,625][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([4.1287e-05, 7.8561e-02, 2.2138e-01, 5.5935e-02, 7.3128e-02, 8.6689e-02,
        2.8859e-01, 9.5179e-02, 7.8925e-02, 9.3758e-03, 1.2196e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,626][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0061, 0.0657, 0.0904, 0.0947, 0.0981, 0.1014, 0.1009, 0.1052, 0.1055,
        0.1195, 0.1125], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,627][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.1149, 0.0198, 0.0361, 0.1950, 0.0394, 0.0222, 0.0265, 0.2339, 0.1658,
        0.1027, 0.0437], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,629][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0087, 0.0103, 0.0182, 0.2611, 0.0350, 0.0582, 0.1211, 0.1403, 0.2916,
        0.0287, 0.0269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,632][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.0004, 0.0007, 0.0019, 0.3714, 0.0274, 0.0076, 0.0508, 0.3301, 0.1550,
        0.0506, 0.0043], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,636][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.1745, 0.1402, 0.0404, 0.0344, 0.0758, 0.1083, 0.1139, 0.0709, 0.0962,
        0.1083, 0.0371], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,641][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.2580, 0.0695, 0.0330, 0.0920, 0.0431, 0.0110, 0.0272, 0.1996, 0.1435,
        0.0737, 0.0495], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,646][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.3472, 0.0484, 0.0392, 0.0885, 0.0294, 0.0242, 0.0241, 0.1690, 0.0852,
        0.0979, 0.0468], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,650][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.2558, 0.0261, 0.0205, 0.0735, 0.0303, 0.0465, 0.0332, 0.2050, 0.1297,
        0.1396, 0.0398], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,655][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.3630, 0.0658, 0.0574, 0.0525, 0.0592, 0.0625, 0.0986, 0.0874, 0.0505,
        0.0649, 0.0384], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,656][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.3636, 0.0595, 0.0574, 0.0936, 0.0676, 0.0528, 0.0436, 0.0736, 0.0589,
        0.0929, 0.0364], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,657][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.2099, 0.0534, 0.0170, 0.0654, 0.0265, 0.0713, 0.0588, 0.0531, 0.2101,
        0.1586, 0.0759], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:41,658][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([9.0592e-05, 7.0403e-02, 1.8805e-01, 5.3736e-02, 6.1457e-02, 7.8030e-02,
        2.2362e-01, 8.7140e-02, 7.4985e-02, 1.1415e-02, 1.3249e-02, 1.3782e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,659][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0045, 0.0572, 0.0801, 0.0852, 0.0890, 0.0926, 0.0923, 0.0970, 0.0968,
        0.1088, 0.1034, 0.0932], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,661][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.1201, 0.0194, 0.0373, 0.2381, 0.0399, 0.0162, 0.0200, 0.1844, 0.1222,
        0.0919, 0.0404, 0.0701], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,665][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0163, 0.0130, 0.0129, 0.1807, 0.0347, 0.0307, 0.1701, 0.2302, 0.1550,
        0.0691, 0.0686, 0.0189], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,669][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0012, 0.0005, 0.0020, 0.1752, 0.0381, 0.0120, 0.0605, 0.2493, 0.3213,
        0.0980, 0.0118, 0.0300], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,673][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.1002, 0.0733, 0.0502, 0.0635, 0.0662, 0.0993, 0.1094, 0.0864, 0.0872,
        0.1230, 0.0909, 0.0505], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,678][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.2527, 0.0594, 0.0363, 0.0615, 0.0338, 0.0094, 0.0338, 0.2226, 0.1212,
        0.0467, 0.0590, 0.0638], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,683][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.3282, 0.0632, 0.0497, 0.0970, 0.0205, 0.0218, 0.0308, 0.1152, 0.0978,
        0.0930, 0.0374, 0.0453], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,687][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1943, 0.0335, 0.0217, 0.1166, 0.0406, 0.0433, 0.0380, 0.1679, 0.1007,
        0.1460, 0.0455, 0.0520], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,689][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.3903, 0.0729, 0.0667, 0.0639, 0.0723, 0.0383, 0.0576, 0.0773, 0.0425,
        0.0517, 0.0352, 0.0312], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,690][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.3729, 0.0659, 0.0605, 0.0796, 0.0453, 0.0444, 0.0676, 0.0624, 0.0562,
        0.0648, 0.0296, 0.0507], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,690][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.1736, 0.0593, 0.0192, 0.0893, 0.0183, 0.0721, 0.0440, 0.0499, 0.1263,
        0.1436, 0.0404, 0.1640], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:41,691][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([5.5487e-05, 6.6723e-02, 1.7179e-01, 5.4526e-02, 6.6864e-02, 7.8346e-02,
        2.3056e-01, 7.9883e-02, 7.0650e-02, 1.0504e-02, 1.1998e-02, 1.1591e-01,
        4.2199e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,694][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0044, 0.0525, 0.0745, 0.0788, 0.0814, 0.0852, 0.0853, 0.0886, 0.0887,
        0.1004, 0.0945, 0.0855, 0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,697][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.1166, 0.0122, 0.0286, 0.2362, 0.0305, 0.0126, 0.0167, 0.1823, 0.0898,
        0.0697, 0.0317, 0.0635, 0.1096], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,701][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0127, 0.0104, 0.0197, 0.0741, 0.0409, 0.0143, 0.2292, 0.3140, 0.0717,
        0.0204, 0.0651, 0.1048, 0.0229], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,705][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0031, 0.0008, 0.0028, 0.2490, 0.0555, 0.0130, 0.0731, 0.1961, 0.2087,
        0.0625, 0.0062, 0.0474, 0.0817], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,710][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0998, 0.0742, 0.0478, 0.0718, 0.0637, 0.0688, 0.1037, 0.0746, 0.0639,
        0.1290, 0.0604, 0.0699, 0.0724], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,715][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2486, 0.0585, 0.0378, 0.0585, 0.0258, 0.0040, 0.0376, 0.2430, 0.0755,
        0.0302, 0.0523, 0.0819, 0.0463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,719][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.3916, 0.0372, 0.0468, 0.0932, 0.0161, 0.0144, 0.0157, 0.0939, 0.0545,
        0.0618, 0.0232, 0.0424, 0.1092], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,721][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.2193, 0.0268, 0.0227, 0.1114, 0.0347, 0.0270, 0.0232, 0.1484, 0.0842,
        0.1010, 0.0365, 0.0662, 0.0987], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,722][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.3754, 0.0753, 0.0627, 0.0603, 0.0656, 0.0366, 0.0623, 0.0777, 0.0416,
        0.0442, 0.0313, 0.0314, 0.0358], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,723][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3620, 0.0421, 0.0509, 0.0793, 0.0427, 0.0516, 0.0453, 0.0708, 0.0656,
        0.0604, 0.0287, 0.0499, 0.0505], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,724][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1384, 0.0453, 0.0192, 0.0759, 0.0200, 0.0504, 0.0406, 0.0420, 0.0826,
        0.1203, 0.0353, 0.0853, 0.2447], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:41,725][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([8.4282e-05, 5.6089e-02, 1.5011e-01, 4.2642e-02, 4.7223e-02, 5.6094e-02,
        1.5945e-01, 6.0878e-02, 5.2011e-02, 8.3195e-03, 1.0327e-02, 1.0779e-01,
        3.0151e-02, 2.1884e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,728][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0038, 0.0487, 0.0702, 0.0736, 0.0760, 0.0786, 0.0782, 0.0821, 0.0825,
        0.0935, 0.0885, 0.0796, 0.0748, 0.0700], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,733][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.1625, 0.0079, 0.0170, 0.1528, 0.0183, 0.0061, 0.0106, 0.1648, 0.0704,
        0.0594, 0.0248, 0.0473, 0.1076, 0.1504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,737][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0416, 0.0107, 0.0151, 0.0598, 0.0408, 0.0310, 0.1749, 0.1525, 0.1388,
        0.0409, 0.0626, 0.0428, 0.0601, 0.1283], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,742][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0292, 0.0007, 0.0011, 0.1523, 0.0384, 0.0083, 0.0383, 0.1120, 0.1397,
        0.0396, 0.0038, 0.0203, 0.0626, 0.3538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,747][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.2221, 0.1052, 0.0548, 0.0489, 0.0624, 0.0406, 0.0468, 0.0580, 0.0459,
        0.0767, 0.0584, 0.0486, 0.0552, 0.0763], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,751][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.1563, 0.0252, 0.0248, 0.0632, 0.0197, 0.0033, 0.0159, 0.1649, 0.0895,
        0.0340, 0.0325, 0.0724, 0.0787, 0.2195], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,753][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.4794, 0.0140, 0.0269, 0.0496, 0.0108, 0.0077, 0.0113, 0.0796, 0.0435,
        0.0419, 0.0255, 0.0265, 0.0695, 0.1137], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,754][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.3230, 0.0171, 0.0109, 0.0717, 0.0186, 0.0161, 0.0112, 0.1197, 0.0485,
        0.0747, 0.0287, 0.0423, 0.0828, 0.1347], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,755][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.4418, 0.0666, 0.0511, 0.0545, 0.0770, 0.0277, 0.0547, 0.0523, 0.0336,
        0.0379, 0.0271, 0.0277, 0.0315, 0.0166], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,756][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.4023, 0.0420, 0.0446, 0.0795, 0.0514, 0.0404, 0.0409, 0.0527, 0.0453,
        0.0609, 0.0311, 0.0440, 0.0406, 0.0242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,759][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.2370, 0.0428, 0.0115, 0.0565, 0.0163, 0.0325, 0.0299, 0.0361, 0.0892,
        0.0946, 0.0317, 0.0632, 0.1797, 0.0788], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:41,761][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([4.0126e-05, 5.3686e-02, 1.4748e-01, 4.1008e-02, 4.9899e-02, 5.5683e-02,
        1.7261e-01, 5.5108e-02, 5.0641e-02, 6.6897e-03, 8.0551e-03, 9.5984e-02,
        2.9420e-02, 2.0770e-01, 2.6005e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,765][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0038, 0.0461, 0.0652, 0.0685, 0.0705, 0.0735, 0.0731, 0.0765, 0.0765,
        0.0871, 0.0824, 0.0741, 0.0695, 0.0652, 0.0680], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,770][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.0786, 0.0052, 0.0091, 0.1378, 0.0132, 0.0041, 0.0073, 0.1677, 0.0562,
        0.0421, 0.0178, 0.0337, 0.0810, 0.1867, 0.1596], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,774][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0555, 0.0130, 0.0053, 0.0477, 0.0428, 0.0138, 0.0857, 0.1243, 0.0441,
        0.0140, 0.0273, 0.0180, 0.0110, 0.1321, 0.3653], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,777][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([1.4701e-02, 1.0359e-04, 1.7133e-04, 4.3380e-02, 1.2110e-02, 1.0309e-03,
        1.0437e-02, 3.4547e-02, 4.9372e-02, 9.3011e-03, 7.2286e-04, 3.8422e-03,
        1.6327e-02, 1.9450e-01, 6.0945e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,782][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.0824, 0.0906, 0.0493, 0.0687, 0.0770, 0.0907, 0.0645, 0.0628, 0.0562,
        0.0849, 0.0606, 0.0461, 0.0543, 0.0817, 0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,785][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0513, 0.0134, 0.0124, 0.0431, 0.0088, 0.0017, 0.0082, 0.1220, 0.0458,
        0.0201, 0.0216, 0.0514, 0.0439, 0.3337, 0.2225], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,786][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.1668, 0.0115, 0.0143, 0.0629, 0.0115, 0.0050, 0.0093, 0.0979, 0.0451,
        0.0370, 0.0164, 0.0267, 0.0644, 0.1525, 0.2787], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,787][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.2045, 0.0179, 0.0097, 0.0788, 0.0196, 0.0165, 0.0097, 0.1111, 0.0565,
        0.0728, 0.0161, 0.0352, 0.0725, 0.1385, 0.1405], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,788][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.4118, 0.0595, 0.0424, 0.0534, 0.0655, 0.0330, 0.0586, 0.0667, 0.0413,
        0.0340, 0.0296, 0.0267, 0.0283, 0.0279, 0.0213], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,790][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.2446, 0.0640, 0.0414, 0.0764, 0.0495, 0.0314, 0.0435, 0.0783, 0.0440,
        0.0692, 0.0396, 0.0496, 0.0538, 0.0311, 0.0834], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,794][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.1933, 0.0372, 0.0152, 0.0818, 0.0162, 0.0336, 0.0255, 0.0261, 0.0832,
        0.0760, 0.0254, 0.0518, 0.1403, 0.0771, 0.1174], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:41,796][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([6.1055e-05, 4.9545e-02, 1.3516e-01, 3.7799e-02, 4.2213e-02, 5.3413e-02,
        1.6782e-01, 5.8153e-02, 5.3701e-02, 6.6253e-03, 8.2000e-03, 9.9692e-02,
        2.6796e-02, 2.1244e-01, 2.5709e-02, 2.2671e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,801][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0038, 0.0434, 0.0602, 0.0638, 0.0654, 0.0687, 0.0684, 0.0716, 0.0714,
        0.0804, 0.0759, 0.0687, 0.0645, 0.0609, 0.0633, 0.0697],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,805][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.1754, 0.0141, 0.0184, 0.1048, 0.0155, 0.0064, 0.0116, 0.0867, 0.0486,
        0.0348, 0.0183, 0.0314, 0.0663, 0.1100, 0.1148, 0.1428],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,810][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0094, 0.0085, 0.0156, 0.0374, 0.0176, 0.0115, 0.0907, 0.1280, 0.0753,
        0.0154, 0.0174, 0.0421, 0.0284, 0.1573, 0.3040, 0.0414],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,812][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([4.0880e-03, 4.1875e-04, 1.1330e-03, 2.5772e-02, 1.6584e-02, 3.0863e-03,
        1.5828e-02, 2.9946e-02, 3.3474e-02, 1.5875e-02, 1.3945e-03, 7.9827e-03,
        2.2917e-02, 1.4747e-01, 6.0144e-01, 7.2598e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,817][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0903, 0.0584, 0.0364, 0.0410, 0.0428, 0.0782, 0.0758, 0.0540, 0.0479,
        0.0875, 0.0664, 0.0531, 0.0727, 0.0920, 0.0592, 0.0443],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,818][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.2091, 0.0456, 0.0272, 0.0257, 0.0138, 0.0032, 0.0158, 0.0861, 0.0320,
        0.0186, 0.0223, 0.0311, 0.0295, 0.1778, 0.2050, 0.0571],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,819][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.1490, 0.0230, 0.0235, 0.0421, 0.0131, 0.0120, 0.0241, 0.0461, 0.0442,
        0.0409, 0.0174, 0.0201, 0.0444, 0.0916, 0.3356, 0.0728],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,820][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.2339, 0.0152, 0.0089, 0.0403, 0.0210, 0.0248, 0.0185, 0.0835, 0.0445,
        0.0707, 0.0247, 0.0328, 0.0681, 0.0950, 0.1492, 0.0688],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,822][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.3426, 0.0596, 0.0480, 0.0534, 0.0580, 0.0392, 0.0668, 0.0571, 0.0392,
        0.0428, 0.0288, 0.0285, 0.0343, 0.0296, 0.0427, 0.0294],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,826][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.3390, 0.0528, 0.0494, 0.0690, 0.0354, 0.0418, 0.0489, 0.0464, 0.0471,
        0.0522, 0.0244, 0.0405, 0.0420, 0.0256, 0.0564, 0.0292],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,830][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.1543, 0.0371, 0.0150, 0.0660, 0.0155, 0.0384, 0.0307, 0.0264, 0.0703,
        0.0890, 0.0251, 0.0615, 0.1616, 0.0503, 0.0433, 0.1155],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:41,833][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([5.3363e-05, 5.1233e-02, 1.3016e-01, 4.0658e-02, 4.7406e-02, 5.4380e-02,
        1.6348e-01, 5.6766e-02, 5.3279e-02, 7.2931e-03, 8.5325e-03, 9.1276e-02,
        2.9262e-02, 1.8450e-01, 2.6199e-02, 2.2068e-02, 3.3443e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,837][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0034, 0.0402, 0.0556, 0.0596, 0.0613, 0.0644, 0.0638, 0.0679, 0.0670,
        0.0753, 0.0712, 0.0645, 0.0598, 0.0567, 0.0586, 0.0655, 0.0653],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,842][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1087, 0.0081, 0.0147, 0.0957, 0.0138, 0.0057, 0.0080, 0.0778, 0.0399,
        0.0296, 0.0160, 0.0273, 0.0572, 0.1021, 0.1003, 0.1566, 0.1384],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,847][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0089, 0.0062, 0.0070, 0.0307, 0.0181, 0.0070, 0.0743, 0.1186, 0.0359,
        0.0069, 0.0216, 0.0298, 0.0103, 0.1736, 0.3451, 0.0723, 0.0338],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,849][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([8.6750e-03, 4.3747e-04, 9.1070e-04, 4.0554e-02, 1.4995e-02, 3.9447e-03,
        1.8276e-02, 2.4625e-02, 3.8178e-02, 1.3353e-02, 1.5280e-03, 1.1042e-02,
        2.1872e-02, 1.1439e-01, 4.9348e-01, 1.1059e-01, 8.3147e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,850][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0592, 0.0921, 0.0364, 0.0645, 0.0529, 0.0552, 0.0560, 0.0533, 0.0344,
        0.0775, 0.0623, 0.0435, 0.0587, 0.1086, 0.0516, 0.0560, 0.0377],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,851][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1625, 0.0319, 0.0188, 0.0332, 0.0139, 0.0024, 0.0150, 0.0960, 0.0367,
        0.0160, 0.0237, 0.0337, 0.0248, 0.1398, 0.2171, 0.0887, 0.0459],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,852][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2201, 0.0212, 0.0221, 0.0484, 0.0084, 0.0077, 0.0095, 0.0439, 0.0307,
        0.0341, 0.0146, 0.0234, 0.0524, 0.0877, 0.1754, 0.1147, 0.0857],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,855][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1427, 0.0176, 0.0113, 0.0575, 0.0193, 0.0186, 0.0116, 0.0764, 0.0394,
        0.0595, 0.0196, 0.0339, 0.0661, 0.1329, 0.1259, 0.1027, 0.0647],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,858][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.3874, 0.0573, 0.0466, 0.0472, 0.0535, 0.0307, 0.0489, 0.0581, 0.0313,
        0.0364, 0.0255, 0.0256, 0.0286, 0.0237, 0.0412, 0.0290, 0.0289],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,863][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3696, 0.0398, 0.0333, 0.0580, 0.0315, 0.0382, 0.0368, 0.0512, 0.0452,
        0.0470, 0.0233, 0.0363, 0.0361, 0.0252, 0.0669, 0.0268, 0.0348],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,868][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1226, 0.0316, 0.0099, 0.0525, 0.0131, 0.0299, 0.0220, 0.0241, 0.0593,
        0.0972, 0.0259, 0.0537, 0.1544, 0.0422, 0.0400, 0.0692, 0.1525],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:41,870][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([2.4518e-05, 4.5716e-02, 1.3298e-01, 3.6614e-02, 4.4761e-02, 5.3568e-02,
        1.6969e-01, 5.8140e-02, 5.0962e-02, 6.0472e-03, 7.4732e-03, 9.5590e-02,
        2.6404e-02, 1.9249e-01, 2.3236e-02, 1.9664e-02, 2.9934e-02, 6.7077e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,875][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0033, 0.0380, 0.0540, 0.0572, 0.0595, 0.0603, 0.0602, 0.0629, 0.0632,
        0.0721, 0.0676, 0.0608, 0.0571, 0.0540, 0.0561, 0.0620, 0.0621, 0.0496],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,880][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0874, 0.0078, 0.0122, 0.0641, 0.0104, 0.0053, 0.0061, 0.0605, 0.0440,
        0.0321, 0.0144, 0.0262, 0.0631, 0.0840, 0.0941, 0.1239, 0.1551, 0.1091],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,882][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0116, 0.0024, 0.0042, 0.0669, 0.0141, 0.0064, 0.0400, 0.0416, 0.0461,
        0.0125, 0.0167, 0.0164, 0.0319, 0.0939, 0.3573, 0.1013, 0.1025, 0.0342],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,883][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([5.2504e-03, 2.1117e-04, 9.1992e-04, 4.6088e-02, 1.1650e-02, 4.0786e-03,
        1.2454e-02, 2.1098e-02, 5.8387e-02, 1.5662e-02, 1.6851e-03, 1.2721e-02,
        2.6265e-02, 9.7549e-02, 4.0909e-01, 1.1659e-01, 1.2984e-01, 3.0462e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,884][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0528, 0.0444, 0.0917, 0.0656, 0.0348, 0.0492, 0.0380, 0.0490, 0.0407,
        0.0936, 0.0369, 0.0493, 0.0539, 0.0695, 0.0604, 0.0673, 0.0568, 0.0461],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,885][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.1636, 0.0190, 0.0120, 0.0302, 0.0125, 0.0024, 0.0099, 0.0615, 0.0325,
        0.0177, 0.0193, 0.0378, 0.0378, 0.1174, 0.1764, 0.0882, 0.0872, 0.0745],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,888][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.2264, 0.0150, 0.0344, 0.0429, 0.0086, 0.0098, 0.0110, 0.0353, 0.0348,
        0.0352, 0.0142, 0.0190, 0.0453, 0.0827, 0.1116, 0.0955, 0.0780, 0.1002],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,891][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.1329, 0.0141, 0.0168, 0.0682, 0.0152, 0.0136, 0.0100, 0.0562, 0.0430,
        0.0666, 0.0139, 0.0334, 0.0559, 0.0787, 0.1479, 0.0991, 0.0885, 0.0460],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,896][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.3447, 0.0279, 0.0383, 0.0515, 0.0651, 0.0291, 0.0573, 0.0631, 0.0391,
        0.0404, 0.0316, 0.0304, 0.0337, 0.0261, 0.0359, 0.0282, 0.0370, 0.0204],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,901][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.2332, 0.0718, 0.0444, 0.0666, 0.0400, 0.0300, 0.0457, 0.0675, 0.0485,
        0.0410, 0.0280, 0.0365, 0.0377, 0.0238, 0.0475, 0.0260, 0.0411, 0.0704],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,905][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.1496, 0.0255, 0.0084, 0.0533, 0.0119, 0.0310, 0.0185, 0.0208, 0.0644,
        0.0675, 0.0205, 0.0485, 0.1252, 0.0526, 0.0547, 0.0813, 0.1158, 0.0505],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:41,908][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([7.3867e-05, 5.0823e-02, 1.2331e-01, 3.7271e-02, 4.3147e-02, 4.9336e-02,
        1.4306e-01, 5.3313e-02, 4.7482e-02, 6.9777e-03, 8.4659e-03, 8.5207e-02,
        2.6647e-02, 1.7028e-01, 2.4630e-02, 2.1572e-02, 3.1075e-02, 7.8141e-03,
        6.9507e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,913][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0027, 0.0355, 0.0509, 0.0539, 0.0554, 0.0570, 0.0569, 0.0603, 0.0602,
        0.0683, 0.0638, 0.0577, 0.0540, 0.0513, 0.0531, 0.0584, 0.0584, 0.0470,
        0.0551], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,914][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.1215, 0.0078, 0.0082, 0.0584, 0.0074, 0.0028, 0.0043, 0.0517, 0.0229,
        0.0177, 0.0110, 0.0134, 0.0366, 0.0662, 0.0652, 0.1034, 0.0939, 0.1075,
        0.2000], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,915][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0076, 0.0027, 0.0012, 0.0497, 0.0066, 0.0131, 0.0422, 0.0309, 0.0407,
        0.0130, 0.0073, 0.0094, 0.0475, 0.0526, 0.3128, 0.1276, 0.1541, 0.0720,
        0.0089], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,916][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([1.4713e-03, 5.0614e-05, 8.9615e-05, 3.5243e-02, 4.4336e-03, 7.0747e-04,
        4.6465e-03, 1.4862e-02, 2.2016e-02, 6.0396e-03, 6.5324e-04, 3.7069e-03,
        1.1342e-02, 9.2412e-02, 5.6499e-01, 8.0190e-02, 9.4549e-02, 3.2034e-02,
        3.0562e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,917][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0443, 0.0356, 0.0364, 0.0781, 0.0565, 0.0361, 0.0367, 0.0536, 0.0581,
        0.0823, 0.0382, 0.0427, 0.0609, 0.0780, 0.0446, 0.0636, 0.0562, 0.0520,
        0.0458], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,920][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.1182, 0.0272, 0.0100, 0.0202, 0.0076, 0.0016, 0.0055, 0.0480, 0.0238,
        0.0105, 0.0162, 0.0182, 0.0206, 0.1432, 0.1867, 0.0594, 0.0548, 0.1101,
        0.1181], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,924][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.3102, 0.0136, 0.0118, 0.0269, 0.0048, 0.0041, 0.0045, 0.0283, 0.0180,
        0.0209, 0.0100, 0.0096, 0.0309, 0.0639, 0.0848, 0.0639, 0.0629, 0.0916,
        0.1392], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,928][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.1320, 0.0138, 0.0080, 0.0607, 0.0190, 0.0124, 0.0070, 0.0556, 0.0362,
        0.0594, 0.0129, 0.0218, 0.0546, 0.0760, 0.1008, 0.0959, 0.0758, 0.0625,
        0.0954], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,933][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.3726, 0.0472, 0.0282, 0.0463, 0.0535, 0.0237, 0.0409, 0.0484, 0.0306,
        0.0376, 0.0259, 0.0260, 0.0333, 0.0264, 0.0401, 0.0316, 0.0349, 0.0291,
        0.0238], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,938][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.3859, 0.0590, 0.0352, 0.0503, 0.0272, 0.0278, 0.0343, 0.0441, 0.0295,
        0.0478, 0.0191, 0.0277, 0.0295, 0.0219, 0.0371, 0.0220, 0.0296, 0.0440,
        0.0280], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,942][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.1201, 0.0217, 0.0154, 0.0775, 0.0072, 0.0291, 0.0158, 0.0194, 0.0465,
        0.0639, 0.0129, 0.0682, 0.1497, 0.0434, 0.0405, 0.0693, 0.0971, 0.0629,
        0.0393], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:41,945][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([6.4706e-05, 4.4677e-02, 1.2060e-01, 3.4575e-02, 3.7999e-02, 4.7120e-02,
        1.3893e-01, 5.2969e-02, 4.8642e-02, 6.4934e-03, 7.7267e-03, 8.5913e-02,
        2.3615e-02, 1.6465e-01, 2.2182e-02, 2.0182e-02, 2.8593e-02, 6.8593e-03,
        6.5076e-02, 4.3135e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,947][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0026, 0.0338, 0.0480, 0.0506, 0.0517, 0.0543, 0.0542, 0.0571, 0.0571,
        0.0643, 0.0600, 0.0543, 0.0509, 0.0485, 0.0503, 0.0549, 0.0550, 0.0449,
        0.0522, 0.0551], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,948][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.2065, 0.0060, 0.0057, 0.0195, 0.0031, 0.0012, 0.0017, 0.0154, 0.0058,
        0.0064, 0.0044, 0.0063, 0.0143, 0.0167, 0.0203, 0.0502, 0.0410, 0.0412,
        0.0858, 0.4484], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,949][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0193, 0.0036, 0.0049, 0.0140, 0.0090, 0.0046, 0.0952, 0.0829, 0.0470,
        0.0084, 0.0099, 0.0301, 0.0142, 0.1222, 0.2295, 0.0437, 0.0903, 0.0621,
        0.0408, 0.0684], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,949][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([3.9875e-02, 1.1864e-04, 1.9662e-04, 8.0425e-03, 3.1437e-03, 4.5852e-04,
        3.7916e-03, 3.5026e-03, 5.0474e-03, 1.7738e-03, 1.6837e-04, 1.8818e-03,
        4.4461e-03, 2.8879e-02, 7.7336e-02, 2.8912e-02, 5.5939e-02, 2.5550e-02,
        1.8563e-02, 6.9237e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,953][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0571, 0.0416, 0.0529, 0.0296, 0.0470, 0.0653, 0.0443, 0.0403, 0.0602,
        0.0839, 0.0498, 0.0314, 0.0477, 0.0472, 0.0540, 0.0428, 0.0572, 0.0444,
        0.0647, 0.0387], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,956][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.3681, 0.0315, 0.0177, 0.0122, 0.0049, 0.0009, 0.0063, 0.0376, 0.0102,
        0.0068, 0.0101, 0.0139, 0.0133, 0.0651, 0.0655, 0.0326, 0.0305, 0.0669,
        0.0998, 0.1059], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,961][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.3459, 0.0092, 0.0136, 0.0186, 0.0049, 0.0022, 0.0033, 0.0116, 0.0137,
        0.0122, 0.0068, 0.0099, 0.0162, 0.0318, 0.0435, 0.0362, 0.0373, 0.0478,
        0.1076, 0.2275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,965][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.3777, 0.0092, 0.0049, 0.0224, 0.0139, 0.0089, 0.0055, 0.0282, 0.0200,
        0.0346, 0.0100, 0.0146, 0.0288, 0.0292, 0.0587, 0.0418, 0.0526, 0.0276,
        0.0547, 0.1568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,970][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.4175, 0.0491, 0.0362, 0.0356, 0.0453, 0.0221, 0.0404, 0.0398, 0.0247,
        0.0287, 0.0205, 0.0210, 0.0258, 0.0191, 0.0295, 0.0246, 0.0279, 0.0273,
        0.0294, 0.0355], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,975][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.4312, 0.0347, 0.0366, 0.0410, 0.0235, 0.0242, 0.0358, 0.0340, 0.0283,
        0.0355, 0.0177, 0.0262, 0.0263, 0.0184, 0.0369, 0.0230, 0.0267, 0.0286,
        0.0280, 0.0435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,979][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.2114, 0.0216, 0.0089, 0.0660, 0.0096, 0.0229, 0.0158, 0.0138, 0.0430,
        0.0533, 0.0152, 0.0361, 0.0876, 0.0234, 0.0236, 0.0684, 0.0709, 0.0377,
        0.0206, 0.1500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:41,981][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([5.2832e-05, 4.2647e-02, 1.1528e-01, 3.7393e-02, 4.1466e-02, 4.5972e-02,
        1.3052e-01, 4.6888e-02, 4.5174e-02, 7.1218e-03, 7.8358e-03, 7.5112e-02,
        2.7225e-02, 1.4321e-01, 2.2675e-02, 1.8371e-02, 2.7903e-02, 7.0898e-03,
        5.9089e-02, 3.7746e-02, 6.1227e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,982][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0028, 0.0326, 0.0451, 0.0479, 0.0492, 0.0517, 0.0512, 0.0543, 0.0538,
        0.0603, 0.0570, 0.0518, 0.0483, 0.0458, 0.0474, 0.0523, 0.0523, 0.0426,
        0.0489, 0.0518, 0.0530], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,983][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0591, 0.0032, 0.0050, 0.0260, 0.0034, 0.0014, 0.0018, 0.0158, 0.0086,
        0.0068, 0.0038, 0.0066, 0.0137, 0.0212, 0.0205, 0.0401, 0.0323, 0.0316,
        0.0788, 0.5599, 0.0602], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,985][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0108, 0.0059, 0.0060, 0.0230, 0.0153, 0.0054, 0.0670, 0.0969, 0.0302,
        0.0057, 0.0192, 0.0243, 0.0076, 0.1477, 0.2339, 0.0521, 0.0232, 0.0748,
        0.0343, 0.0832, 0.0334], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,987][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([5.2702e-03, 2.7305e-04, 5.0909e-04, 1.4762e-02, 5.5734e-03, 1.8127e-03,
        7.1219e-03, 8.1752e-03, 1.1728e-02, 4.9942e-03, 6.5082e-04, 5.0657e-03,
        8.4326e-03, 3.7471e-02, 1.4634e-01, 3.5299e-02, 2.9202e-02, 2.4999e-02,
        2.6750e-02, 5.8783e-01, 3.7732e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,992][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0481, 0.0693, 0.0314, 0.0493, 0.0417, 0.0464, 0.0455, 0.0427, 0.0310,
        0.0642, 0.0509, 0.0348, 0.0460, 0.0798, 0.0397, 0.0450, 0.0312, 0.0581,
        0.0372, 0.0725, 0.0352], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:41,997][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1684, 0.0283, 0.0149, 0.0199, 0.0069, 0.0012, 0.0074, 0.0461, 0.0144,
        0.0073, 0.0116, 0.0181, 0.0131, 0.0675, 0.0989, 0.0467, 0.0249, 0.0725,
        0.1071, 0.1870, 0.0378], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,001][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2002, 0.0146, 0.0114, 0.0221, 0.0037, 0.0034, 0.0041, 0.0178, 0.0123,
        0.0148, 0.0064, 0.0109, 0.0217, 0.0325, 0.0645, 0.0507, 0.0340, 0.0646,
        0.0923, 0.2725, 0.0456], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,006][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1278, 0.0109, 0.0063, 0.0296, 0.0106, 0.0103, 0.0060, 0.0370, 0.0213,
        0.0324, 0.0102, 0.0176, 0.0347, 0.0603, 0.0599, 0.0532, 0.0336, 0.0417,
        0.0768, 0.2699, 0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,010][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.3831, 0.0477, 0.0371, 0.0389, 0.0428, 0.0248, 0.0391, 0.0462, 0.0250,
        0.0302, 0.0204, 0.0213, 0.0240, 0.0192, 0.0324, 0.0247, 0.0247, 0.0264,
        0.0286, 0.0370, 0.0264], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,013][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3681, 0.0330, 0.0295, 0.0487, 0.0247, 0.0300, 0.0275, 0.0378, 0.0369,
        0.0376, 0.0169, 0.0281, 0.0286, 0.0193, 0.0521, 0.0222, 0.0270, 0.0253,
        0.0268, 0.0526, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,014][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1156, 0.0239, 0.0073, 0.0430, 0.0092, 0.0210, 0.0147, 0.0155, 0.0406,
        0.0672, 0.0167, 0.0411, 0.1103, 0.0264, 0.0261, 0.0523, 0.1060, 0.0420,
        0.0179, 0.0987, 0.1046], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,018][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:42,021][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[2802],
        [ 388],
        [ 424],
        [ 576],
        [ 668],
        [1322],
        [2840],
        [2576],
        [7730],
        [2658],
        [5706],
        [1037],
        [1348],
        [ 473],
        [1466],
        [ 569],
        [ 402],
        [ 225],
        [1058],
        [1260],
        [ 254]], device='cuda:0')
[2024-07-23 21:06:42,024][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 2930],
        [ 1103],
        [ 2048],
        [ 2684],
        [ 1685],
        [ 5692],
        [ 7114],
        [ 9949],
        [17802],
        [ 8473],
        [17145],
        [ 4754],
        [ 4844],
        [ 1262],
        [ 3100],
        [ 2278],
        [ 1173],
        [ 2769],
        [ 3215],
        [ 4294],
        [ 1286]], device='cuda:0')
[2024-07-23 21:06:42,027][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[8444],
        [9387],
        [6786],
        [5907],
        [6219],
        [6485],
        [6235],
        [6441],
        [6175],
        [5629],
        [5634],
        [5764],
        [5815],
        [5646],
        [5642],
        [5435],
        [5351],
        [5541],
        [5436],
        [5380],
        [5326]], device='cuda:0')
[2024-07-23 21:06:42,031][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[4260],
        [5347],
        [7659],
        [8968],
        [8006],
        [8120],
        [7955],
        [8472],
        [7643],
        [7874],
        [7619],
        [7798],
        [7759],
        [7782],
        [7723],
        [7635],
        [7674],
        [7634],
        [7518],
        [7404],
        [7511]], device='cuda:0')
[2024-07-23 21:06:42,034][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[21268],
        [ 3322],
        [ 8306],
        [ 9740],
        [ 9050],
        [ 8776],
        [ 7858],
        [ 5966],
        [ 4110],
        [ 4821],
        [ 4365],
        [ 5028],
        [ 4714],
        [ 5235],
        [ 6392],
        [ 5749],
        [ 5667],
        [ 5295],
        [ 6907],
        [ 5458],
        [ 5180]], device='cuda:0')
[2024-07-23 21:06:42,037][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[28316],
        [14123],
        [17930],
        [23431],
        [26973],
        [27193],
        [25060],
        [26070],
        [25894],
        [26748],
        [26681],
        [26450],
        [26790],
        [25151],
        [24648],
        [25139],
        [24814],
        [24628],
        [23916],
        [23537],
        [23306]], device='cuda:0')
[2024-07-23 21:06:42,040][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[15773],
        [29790],
        [44878],
        [45529],
        [47717],
        [47845],
        [47597],
        [46150],
        [47455],
        [47407],
        [47434],
        [47594],
        [48098],
        [47929],
        [47337],
        [47667],
        [47520],
        [47011],
        [46775],
        [47962],
        [48322]], device='cuda:0')
[2024-07-23 21:06:42,043][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[22122],
        [ 8455],
        [12904],
        [14559],
        [15067],
        [15850],
        [14806],
        [15484],
        [17259],
        [15744],
        [14170],
        [16038],
        [16607],
        [14482],
        [14603],
        [14274],
        [15523],
        [14376],
        [14096],
        [14083],
        [16405]], device='cuda:0')
[2024-07-23 21:06:42,047][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[33085],
        [25883],
        [24118],
        [23950],
        [24646],
        [24529],
        [24725],
        [24580],
        [24735],
        [24493],
        [24255],
        [24127],
        [23751],
        [23568],
        [23510],
        [23517],
        [23194],
        [23351],
        [23261],
        [23161],
        [23004]], device='cuda:0')
[2024-07-23 21:06:42,049][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 9316],
        [40036],
        [35968],
        [18908],
        [20100],
        [15445],
        [14613],
        [18023],
        [22373],
        [19906],
        [20761],
        [20458],
        [15685],
        [16432],
        [21396],
        [22149],
        [17493],
        [21751],
        [24506],
        [12592],
        [12574]], device='cuda:0')
[2024-07-23 21:06:42,051][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[37090],
        [ 9364],
        [11970],
        [11062],
        [12310],
        [12590],
        [14380],
        [13444],
        [11335],
        [10877],
        [10537],
        [10357],
        [11387],
        [10590],
        [10317],
        [10292],
        [10538],
        [10511],
        [11185],
        [11307],
        [11346]], device='cuda:0')
[2024-07-23 21:06:42,053][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[18312],
        [15636],
        [14947],
        [14716],
        [14114],
        [13726],
        [13891],
        [14794],
        [15162],
        [14533],
        [14595],
        [14447],
        [14702],
        [14347],
        [14170],
        [13834],
        [13822],
        [13690],
        [13431],
        [13415],
        [13526]], device='cuda:0')
[2024-07-23 21:06:42,055][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[15788],
        [10889],
        [14152],
        [14529],
        [16326],
        [17821],
        [19002],
        [21644],
        [23373],
        [23145],
        [22928],
        [23711],
        [24051],
        [24616],
        [24349],
        [23481],
        [24289],
        [20085],
        [22273],
        [22084],
        [22711]], device='cuda:0')
[2024-07-23 21:06:42,058][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 5650],
        [10479],
        [ 8840],
        [ 9374],
        [10248],
        [ 8818],
        [ 9045],
        [11001],
        [11957],
        [11985],
        [10201],
        [10187],
        [ 9652],
        [11748],
        [13406],
        [13240],
        [13523],
        [13871],
        [13794],
        [13217],
        [12845]], device='cuda:0')
[2024-07-23 21:06:42,061][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[5598],
        [7155],
        [4787],
        [3154],
        [6521],
        [3619],
        [8017],
        [4800],
        [5869],
        [4319],
        [6104],
        [3969],
        [3416],
        [5428],
        [5784],
        [2882],
        [4190],
        [2396],
        [5598],
        [4084],
        [2801]], device='cuda:0')
[2024-07-23 21:06:42,064][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[2593],
        [7099],
        [6709],
        [6121],
        [5663],
        [5479],
        [4987],
        [4726],
        [4569],
        [4551],
        [4564],
        [4392],
        [4281],
        [4182],
        [4191],
        [4138],
        [4127],
        [4133],
        [4122],
        [4110],
        [4016]], device='cuda:0')
[2024-07-23 21:06:42,067][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 2431],
        [10172],
        [11401],
        [11966],
        [11709],
        [12639],
        [13278],
        [13186],
        [13092],
        [13261],
        [13408],
        [13276],
        [13253],
        [13331],
        [13372],
        [13344],
        [13380],
        [13379],
        [13393],
        [13414],
        [13388]], device='cuda:0')
[2024-07-23 21:06:42,070][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[14550],
        [ 5700],
        [ 5118],
        [ 6172],
        [ 6944],
        [ 7162],
        [ 6954],
        [ 6116],
        [ 6444],
        [ 6854],
        [ 7229],
        [ 7023],
        [ 6986],
        [ 6647],
        [ 5467],
        [ 5353],
        [ 5860],
        [ 6396],
        [ 6214],
        [ 7617],
        [ 7928]], device='cuda:0')
[2024-07-23 21:06:42,074][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[5771],
        [7818],
        [3453],
        [ 427],
        [ 424],
        [ 280],
        [ 166],
        [ 190],
        [ 268],
        [ 247],
        [ 101],
        [ 192],
        [ 516],
        [ 207],
        [1419],
        [ 915],
        [1067],
        [ 829],
        [ 662],
        [ 646],
        [ 638]], device='cuda:0')
[2024-07-23 21:06:42,077][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[7504],
        [4366],
        [6718],
        [2543],
        [2324],
        [2286],
        [2239],
        [2486],
        [2654],
        [2816],
        [2902],
        [2916],
        [2656],
        [2531],
        [3437],
        [3242],
        [3110],
        [2976],
        [3304],
        [1795],
        [1941]], device='cuda:0')
[2024-07-23 21:06:42,080][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 3966],
        [ 5935],
        [12205],
        [12259],
        [ 7240],
        [12112],
        [11933],
        [11117],
        [10428],
        [10195],
        [12104],
        [13441],
        [14612],
        [13132],
        [15421],
        [14758],
        [15666],
        [17264],
        [16762],
        [16265],
        [15995]], device='cuda:0')
[2024-07-23 21:06:42,083][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[24339],
        [29980],
        [30034],
        [25010],
        [16087],
        [14409],
        [15838],
        [21591],
        [14772],
        [15711],
        [17487],
        [17366],
        [17106],
        [18147],
        [16565],
        [17906],
        [14628],
        [13799],
        [13806],
        [15250],
        [ 9650]], device='cuda:0')
[2024-07-23 21:06:42,086][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[2198],
        [ 145],
        [ 659],
        [ 648],
        [ 428],
        [ 486],
        [ 576],
        [ 577],
        [ 458],
        [ 458],
        [ 399],
        [ 439],
        [ 708],
        [ 677],
        [ 885],
        [1161],
        [1297],
        [ 961],
        [ 712],
        [1051],
        [1163]], device='cuda:0')
[2024-07-23 21:06:42,088][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[14561],
        [31193],
        [35348],
        [25743],
        [32471],
        [32732],
        [31807],
        [32700],
        [22886],
        [23618],
        [22650],
        [22389],
        [25191],
        [30415],
        [28531],
        [30102],
        [28919],
        [28976],
        [28083],
        [35525],
        [31500]], device='cuda:0')
[2024-07-23 21:06:42,089][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[15402],
        [14230],
        [13709],
        [14252],
        [14080],
        [13533],
        [13683],
        [13711],
        [13714],
        [14674],
        [15659],
        [15843],
        [15906],
        [15576],
        [15339],
        [15920],
        [16229],
        [16627],
        [16475],
        [16368],
        [16718]], device='cuda:0')
[2024-07-23 21:06:42,092][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[19177],
        [ 1845],
        [ 1632],
        [ 1958],
        [ 2152],
        [ 1614],
        [ 2530],
        [ 2435],
        [ 2473],
        [ 3954],
        [ 3460],
        [ 3136],
        [ 3369],
        [ 3021],
        [ 3860],
        [ 3539],
        [ 3379],
        [ 3679],
        [ 3025],
        [ 3283],
        [ 3489]], device='cuda:0')
[2024-07-23 21:06:42,094][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[13841],
        [16156],
        [22245],
        [16544],
        [18191],
        [20605],
        [16814],
        [20990],
        [26803],
        [26705],
        [26871],
        [25503],
        [23927],
        [25222],
        [24235],
        [22839],
        [21477],
        [21463],
        [21180],
        [19351],
        [18754]], device='cuda:0')
[2024-07-23 21:06:42,098][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[29770],
        [34401],
        [29887],
        [35967],
        [38291],
        [38185],
        [37599],
        [36854],
        [38673],
        [37158],
        [36851],
        [36280],
        [34586],
        [35180],
        [33535],
        [32928],
        [33149],
        [33611],
        [34864],
        [33810],
        [35076]], device='cuda:0')
[2024-07-23 21:06:42,101][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[37157],
        [24568],
        [28507],
        [35852],
        [32519],
        [37217],
        [29849],
        [34749],
        [31281],
        [33072],
        [30448],
        [32824],
        [36095],
        [32487],
        [29511],
        [34689],
        [33182],
        [35101],
        [29485],
        [35046],
        [37344]], device='cuda:0')
[2024-07-23 21:06:42,104][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257],
        [24257]], device='cuda:0')
[2024-07-23 21:06:42,178][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:42,183][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,185][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,186][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,187][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,187][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,188][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,190][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,192][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,195][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,199][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,203][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,207][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:42,211][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.4249, 0.5751], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,216][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.1535, 0.8465], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,218][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.3071, 0.6929], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,218][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.1725, 0.8275], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,219][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.5506, 0.4494], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,220][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7104, 0.2896], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,220][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1239, 0.8761], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,223][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8592, 0.1408], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,227][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9355, 0.0645], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,231][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0223, 0.9777], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,236][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6816, 0.3184], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,241][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.2982, 0.7018], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:42,245][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.3373, 0.3388, 0.3239], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,250][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0210, 0.3346, 0.6443], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,250][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.1667, 0.4215, 0.4118], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,251][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0924, 0.4398, 0.4678], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,252][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.6510, 0.2719, 0.0771], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,252][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.3860, 0.4326, 0.1814], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,255][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0792, 0.3956, 0.5252], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,259][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.5950, 0.1718, 0.2333], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,263][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.9384, 0.0246, 0.0370], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,264][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0135, 0.3121, 0.6744], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,265][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.4659, 0.2620, 0.2721], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,269][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.1257, 0.3956, 0.4787], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:42,273][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.2474, 0.2656, 0.2427, 0.2442], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,278][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.1923, 0.1125, 0.4326, 0.2626], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,283][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.1182, 0.2965, 0.2891, 0.2962], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,284][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0655, 0.3019, 0.3179, 0.3148], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,284][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0486, 0.0981, 0.0197, 0.8336], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,285][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.2951, 0.2653, 0.1672, 0.2724], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,286][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0359, 0.2711, 0.2511, 0.4419], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,288][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.5823, 0.0869, 0.1327, 0.1982], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,291][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.8804, 0.0119, 0.0237, 0.0840], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,296][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0070, 0.1603, 0.6168, 0.2160], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,300][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.3630, 0.2092, 0.2165, 0.2112], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,305][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.1133, 0.2488, 0.3123, 0.3256], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:42,310][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.1655, 0.2368, 0.2202, 0.2248, 0.1527], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,314][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.1132, 0.1560, 0.2389, 0.4044, 0.0875], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,316][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0956, 0.2244, 0.2207, 0.2249, 0.2344], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,316][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0519, 0.2287, 0.2416, 0.2388, 0.2390], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,317][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.0304, 0.0883, 0.1325, 0.5257, 0.2231], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,318][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.1836, 0.1760, 0.2250, 0.2466, 0.1688], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,320][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0574, 0.1779, 0.1674, 0.3082, 0.2891], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,323][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.4240, 0.0822, 0.1373, 0.2658, 0.0907], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,328][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.8338, 0.0116, 0.0186, 0.1242, 0.0118], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,332][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0135, 0.2909, 0.4211, 0.1186, 0.1559], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,337][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.3517, 0.1624, 0.1697, 0.1655, 0.1506], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,342][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.0833, 0.1833, 0.2382, 0.2667, 0.2285], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:42,346][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1415, 0.2050, 0.1810, 0.1859, 0.1267, 0.1600], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,348][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0224, 0.0925, 0.2222, 0.5821, 0.0305, 0.0503], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,349][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0786, 0.1827, 0.1794, 0.1838, 0.1918, 0.1837], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,349][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0363, 0.1838, 0.1952, 0.1953, 0.1987, 0.1907], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,350][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.0096, 0.1838, 0.1028, 0.3108, 0.0663, 0.3266], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,352][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.1574, 0.2034, 0.1832, 0.1380, 0.1643, 0.1536], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,355][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0346, 0.1742, 0.1652, 0.2373, 0.2096, 0.1791], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,360][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.3174, 0.0772, 0.1310, 0.2992, 0.1069, 0.0683], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,364][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.6142, 0.0244, 0.0218, 0.2919, 0.0360, 0.0116], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,369][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0080, 0.1503, 0.3785, 0.2276, 0.1728, 0.0629], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,374][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.2795, 0.1474, 0.1516, 0.1469, 0.1363, 0.1383], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,378][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.0643, 0.1611, 0.2132, 0.2194, 0.1661, 0.1759], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:42,380][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.1037, 0.1678, 0.1595, 0.1618, 0.1104, 0.1364, 0.1604],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,381][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0125, 0.0434, 0.1449, 0.7050, 0.0171, 0.0388, 0.0383],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,381][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0642, 0.1548, 0.1523, 0.1561, 0.1628, 0.1559, 0.1540],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,382][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.0336, 0.1537, 0.1622, 0.1615, 0.1629, 0.1581, 0.1679],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,384][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.0378, 0.0374, 0.0386, 0.3234, 0.0360, 0.3541, 0.1727],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,387][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.0861, 0.1110, 0.1016, 0.1670, 0.1343, 0.2510, 0.1490],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,392][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.0349, 0.1267, 0.1470, 0.1929, 0.1954, 0.1191, 0.1840],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,396][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.3125, 0.0552, 0.1110, 0.2722, 0.0719, 0.0656, 0.1116],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,401][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.7466, 0.0136, 0.0192, 0.1792, 0.0153, 0.0121, 0.0141],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,406][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0053, 0.1660, 0.2289, 0.1969, 0.1079, 0.2419, 0.0532],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,410][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.2526, 0.1270, 0.1307, 0.1268, 0.1173, 0.1196, 0.1260],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,412][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0463, 0.1383, 0.1791, 0.2178, 0.1525, 0.1613, 0.1047],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:42,413][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.1078, 0.1382, 0.1364, 0.1381, 0.0956, 0.1192, 0.1362, 0.1285],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,413][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0215, 0.0274, 0.1114, 0.2071, 0.0085, 0.0108, 0.0134, 0.5998],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,414][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0545, 0.1327, 0.1308, 0.1340, 0.1393, 0.1331, 0.1319, 0.1436],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,417][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.0274, 0.1305, 0.1383, 0.1368, 0.1382, 0.1331, 0.1418, 0.1538],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,420][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0198, 0.0641, 0.0197, 0.1989, 0.0676, 0.2484, 0.2346, 0.1469],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,424][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.1172, 0.0998, 0.0546, 0.0594, 0.0849, 0.1184, 0.1147, 0.3509],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,428][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.0592, 0.1156, 0.1193, 0.1715, 0.1435, 0.0786, 0.1294, 0.1828],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,433][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.3621, 0.0359, 0.0607, 0.1794, 0.0529, 0.0518, 0.0525, 0.2047],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,438][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.6688, 0.0103, 0.0125, 0.1207, 0.0241, 0.0077, 0.0082, 0.1476],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,442][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.0027, 0.0557, 0.2090, 0.1403, 0.0625, 0.3331, 0.1338, 0.0629],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,444][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.2180, 0.1128, 0.1168, 0.1132, 0.1055, 0.1069, 0.1114, 0.1155],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,445][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.0473, 0.1200, 0.1515, 0.1705, 0.1571, 0.1744, 0.0932, 0.0860],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:42,446][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0906, 0.1321, 0.1202, 0.1240, 0.0839, 0.1044, 0.1235, 0.1150, 0.1063],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,447][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0204, 0.0125, 0.0346, 0.0877, 0.0096, 0.0050, 0.0206, 0.7460, 0.0637],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,449][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0464, 0.1157, 0.1139, 0.1159, 0.1209, 0.1154, 0.1146, 0.1250, 0.1323],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,453][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0243, 0.1146, 0.1205, 0.1195, 0.1197, 0.1161, 0.1235, 0.1353, 0.1264],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,457][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0159, 0.0272, 0.0339, 0.0889, 0.0465, 0.2294, 0.1369, 0.3439, 0.0773],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,462][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.1270, 0.0880, 0.0569, 0.1104, 0.0719, 0.1033, 0.1186, 0.2239, 0.1002],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,467][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.0326, 0.0885, 0.0957, 0.1215, 0.1192, 0.0829, 0.1065, 0.1432, 0.2100],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,471][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.2511, 0.0439, 0.0603, 0.1717, 0.0515, 0.0536, 0.0661, 0.2323, 0.0695],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,476][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.6171, 0.0070, 0.0070, 0.0924, 0.0103, 0.0047, 0.0088, 0.1972, 0.0553],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,477][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0023, 0.0282, 0.0869, 0.0444, 0.0709, 0.4319, 0.1712, 0.1478, 0.0164],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,477][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.1915, 0.1020, 0.1050, 0.1017, 0.0950, 0.0964, 0.1011, 0.1039, 0.1034],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,478][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0383, 0.1176, 0.1538, 0.1526, 0.1336, 0.1269, 0.0896, 0.0920, 0.0956],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:42,480][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0811, 0.1171, 0.1060, 0.1089, 0.0753, 0.0920, 0.1099, 0.1042, 0.0979,
        0.1076], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,483][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0203, 0.0138, 0.0296, 0.1138, 0.0069, 0.0077, 0.0131, 0.6377, 0.0678,
        0.0894], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,488][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0434, 0.1017, 0.0996, 0.1020, 0.1061, 0.1020, 0.1012, 0.1095, 0.1164,
        0.1180], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,492][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0192, 0.1014, 0.1069, 0.1059, 0.1066, 0.1037, 0.1104, 0.1207, 0.1146,
        0.1106], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,497][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.0063, 0.0394, 0.0624, 0.1234, 0.0308, 0.1978, 0.1333, 0.2887, 0.0581,
        0.0596], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,502][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0522, 0.0547, 0.0646, 0.0920, 0.0744, 0.0858, 0.0797, 0.2233, 0.1955,
        0.0779], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,506][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0196, 0.0952, 0.1011, 0.1446, 0.1177, 0.0803, 0.0942, 0.1327, 0.1470,
        0.0677], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,508][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.1950, 0.0355, 0.0643, 0.1462, 0.0484, 0.0397, 0.0557, 0.2110, 0.0776,
        0.1265], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,509][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.7496, 0.0057, 0.0053, 0.0625, 0.0069, 0.0041, 0.0044, 0.0946, 0.0434,
        0.0234], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,510][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0059, 0.1188, 0.1227, 0.1075, 0.0536, 0.1131, 0.1223, 0.1951, 0.0687,
        0.0923], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,510][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.1861, 0.0900, 0.0941, 0.0918, 0.0843, 0.0856, 0.0897, 0.0923, 0.0919,
        0.0942], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,511][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0284, 0.1043, 0.1385, 0.1388, 0.1059, 0.1101, 0.0763, 0.0765, 0.1090,
        0.1122], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:42,514][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0678, 0.1034, 0.0973, 0.1015, 0.0688, 0.0859, 0.1011, 0.0973, 0.0906,
        0.1009, 0.0856], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,518][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0056, 0.0115, 0.0283, 0.0686, 0.0063, 0.0086, 0.0120, 0.6072, 0.0565,
        0.1813, 0.0140], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,522][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.0386, 0.0910, 0.0895, 0.0917, 0.0954, 0.0915, 0.0908, 0.0980, 0.1040,
        0.1056, 0.1038], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,527][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.0175, 0.0903, 0.0959, 0.0948, 0.0961, 0.0932, 0.0992, 0.1082, 0.1027,
        0.0997, 0.1023], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,532][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.0096, 0.0425, 0.0204, 0.1026, 0.0422, 0.2593, 0.1614, 0.2129, 0.0465,
        0.0628, 0.0397], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,536][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.0489, 0.0362, 0.0589, 0.0744, 0.0610, 0.0662, 0.0878, 0.2823, 0.1163,
        0.0909, 0.0770], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,540][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0346, 0.0693, 0.0733, 0.1228, 0.1096, 0.0759, 0.0956, 0.1294, 0.1578,
        0.0692, 0.0624], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,541][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.1507, 0.0302, 0.0457, 0.1476, 0.0479, 0.0511, 0.0455, 0.1722, 0.0928,
        0.1595, 0.0568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,542][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.4456, 0.0095, 0.0107, 0.1091, 0.0214, 0.0071, 0.0112, 0.1608, 0.1234,
        0.0758, 0.0254], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,543][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0007, 0.0416, 0.1052, 0.0596, 0.0564, 0.1581, 0.2985, 0.0790, 0.0988,
        0.0765, 0.0256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,546][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.1623, 0.0838, 0.0866, 0.0843, 0.0776, 0.0790, 0.0830, 0.0856, 0.0854,
        0.0866, 0.0859], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,549][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0349, 0.0834, 0.1099, 0.1209, 0.1077, 0.1119, 0.0650, 0.0645, 0.1021,
        0.1181, 0.0816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:42,554][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0713, 0.1016, 0.0861, 0.0893, 0.0607, 0.0756, 0.0915, 0.0873, 0.0808,
        0.0910, 0.0775, 0.0872], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,558][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0079, 0.0173, 0.0363, 0.0953, 0.0051, 0.0053, 0.0147, 0.6195, 0.0750,
        0.0565, 0.0277, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,563][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0331, 0.0821, 0.0812, 0.0833, 0.0870, 0.0833, 0.0827, 0.0897, 0.0948,
        0.0967, 0.0952, 0.0911], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,568][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0140, 0.0817, 0.0872, 0.0865, 0.0883, 0.0847, 0.0904, 0.0995, 0.0941,
        0.0920, 0.0944, 0.0871], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,572][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0085, 0.0272, 0.0297, 0.1451, 0.0251, 0.1492, 0.2501, 0.1620, 0.0186,
        0.0975, 0.0413, 0.0457], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,573][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0295, 0.0592, 0.0364, 0.0470, 0.0581, 0.0679, 0.0971, 0.2785, 0.1150,
        0.1112, 0.0689, 0.0311], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,574][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0242, 0.0739, 0.0946, 0.1324, 0.0975, 0.0801, 0.0880, 0.1221, 0.1226,
        0.0597, 0.0448, 0.0601], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,575][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.1297, 0.0292, 0.0421, 0.1273, 0.0409, 0.0394, 0.0541, 0.1715, 0.0836,
        0.1384, 0.0552, 0.0886], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,577][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.5005, 0.0099, 0.0088, 0.1021, 0.0136, 0.0063, 0.0097, 0.1571, 0.0775,
        0.0476, 0.0312, 0.0358], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,580][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0009, 0.0582, 0.0927, 0.1075, 0.0535, 0.1179, 0.2939, 0.0736, 0.0447,
        0.0595, 0.0352, 0.0624], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,584][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1299, 0.0798, 0.0815, 0.0784, 0.0739, 0.0745, 0.0781, 0.0803, 0.0796,
        0.0820, 0.0813, 0.0808], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,589][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0338, 0.0797, 0.1055, 0.1104, 0.0880, 0.0923, 0.0619, 0.0581, 0.0756,
        0.1003, 0.0787, 0.1157], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:42,593][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0613, 0.0923, 0.0805, 0.0815, 0.0562, 0.0682, 0.0838, 0.0810, 0.0739,
        0.0818, 0.0708, 0.0792, 0.0896], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,598][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0092, 0.0155, 0.0407, 0.1226, 0.0047, 0.0039, 0.0099, 0.4820, 0.0807,
        0.0482, 0.0189, 0.0468, 0.1168], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,603][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0306, 0.0755, 0.0742, 0.0761, 0.0794, 0.0762, 0.0760, 0.0823, 0.0871,
        0.0887, 0.0874, 0.0838, 0.0826], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,604][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0138, 0.0754, 0.0801, 0.0796, 0.0813, 0.0776, 0.0826, 0.0911, 0.0861,
        0.0842, 0.0860, 0.0796, 0.0826], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,605][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0099, 0.0239, 0.0565, 0.1301, 0.0678, 0.1304, 0.1453, 0.2241, 0.0320,
        0.0653, 0.0572, 0.0441, 0.0134], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,606][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0301, 0.0454, 0.0408, 0.0528, 0.0688, 0.0668, 0.0681, 0.2407, 0.1355,
        0.0667, 0.0804, 0.0476, 0.0562], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,607][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0224, 0.0911, 0.0962, 0.1274, 0.0914, 0.0697, 0.0806, 0.1078, 0.1034,
        0.0555, 0.0403, 0.0486, 0.0656], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,609][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.1243, 0.0244, 0.0381, 0.1101, 0.0352, 0.0323, 0.0396, 0.1387, 0.0610,
        0.1044, 0.0434, 0.0867, 0.1617], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,612][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.5305, 0.0072, 0.0083, 0.1016, 0.0098, 0.0045, 0.0067, 0.1297, 0.0479,
        0.0353, 0.0237, 0.0339, 0.0610], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,617][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0005, 0.0440, 0.1312, 0.1032, 0.0763, 0.0715, 0.2658, 0.0688, 0.0362,
        0.0737, 0.0291, 0.0526, 0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,621][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.1307, 0.0714, 0.0741, 0.0724, 0.0671, 0.0679, 0.0714, 0.0732, 0.0729,
        0.0750, 0.0742, 0.0740, 0.0758], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,626][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0282, 0.0724, 0.0984, 0.1052, 0.0775, 0.0820, 0.0539, 0.0558, 0.0771,
        0.0840, 0.0724, 0.1280, 0.0651], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:42,631][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0535, 0.0755, 0.0715, 0.0739, 0.0513, 0.0631, 0.0738, 0.0729, 0.0684,
        0.0756, 0.0651, 0.0742, 0.0844, 0.0969], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,635][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0198, 0.0036, 0.0095, 0.0585, 0.0017, 0.0017, 0.0021, 0.3148, 0.0340,
        0.0367, 0.0121, 0.0294, 0.0943, 0.3817], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,637][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0288, 0.0691, 0.0680, 0.0702, 0.0731, 0.0702, 0.0696, 0.0760, 0.0801,
        0.0814, 0.0804, 0.0771, 0.0762, 0.0799], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,638][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0127, 0.0697, 0.0741, 0.0736, 0.0741, 0.0716, 0.0760, 0.0840, 0.0790,
        0.0771, 0.0791, 0.0734, 0.0764, 0.0793], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,639][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0315, 0.0166, 0.0156, 0.1282, 0.0356, 0.0973, 0.2176, 0.1157, 0.0070,
        0.0586, 0.0277, 0.0735, 0.0189, 0.1561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,640][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.0680, 0.0278, 0.0266, 0.0663, 0.0489, 0.0555, 0.0553, 0.1312, 0.1020,
        0.1002, 0.0533, 0.0353, 0.0624, 0.1673], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,642][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.0258, 0.0795, 0.0874, 0.0921, 0.0911, 0.0626, 0.0816, 0.1121, 0.1245,
        0.0531, 0.0460, 0.0398, 0.0476, 0.0568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,646][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.1609, 0.0143, 0.0208, 0.0895, 0.0208, 0.0196, 0.0168, 0.1195, 0.0435,
        0.0916, 0.0328, 0.0741, 0.1657, 0.1301], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,651][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.5331, 0.0040, 0.0045, 0.0600, 0.0053, 0.0025, 0.0039, 0.1025, 0.0348,
        0.0303, 0.0173, 0.0269, 0.0704, 0.1044], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,655][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0016, 0.0602, 0.2477, 0.0920, 0.0406, 0.0427, 0.1097, 0.0625, 0.0495,
        0.0447, 0.0461, 0.0793, 0.0722, 0.0512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,660][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.1215, 0.0667, 0.0689, 0.0669, 0.0622, 0.0628, 0.0659, 0.0680, 0.0673,
        0.0689, 0.0687, 0.0686, 0.0702, 0.0734], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,664][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0285, 0.0619, 0.0897, 0.0996, 0.0769, 0.0779, 0.0528, 0.0517, 0.0699,
        0.0855, 0.0616, 0.1173, 0.0660, 0.0607], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:42,669][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0439, 0.0716, 0.0666, 0.0693, 0.0469, 0.0579, 0.0675, 0.0672, 0.0639,
        0.0701, 0.0599, 0.0679, 0.0784, 0.0916, 0.0772], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,670][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ different] are: tensor([3.3007e-03, 9.7742e-04, 1.2008e-03, 1.6055e-02, 5.2708e-04, 4.3624e-04,
        1.1080e-03, 1.2366e-01, 7.9827e-03, 6.9524e-03, 2.8038e-03, 5.6970e-03,
        3.0733e-02, 4.4836e-01, 3.5021e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,671][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0281, 0.0643, 0.0634, 0.0653, 0.0678, 0.0649, 0.0644, 0.0702, 0.0740,
        0.0751, 0.0741, 0.0712, 0.0701, 0.0733, 0.0740], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,672][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0122, 0.0644, 0.0680, 0.0684, 0.0696, 0.0662, 0.0705, 0.0780, 0.0735,
        0.0715, 0.0735, 0.0677, 0.0705, 0.0737, 0.0721], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,674][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0077, 0.0276, 0.0075, 0.0556, 0.0245, 0.0650, 0.2513, 0.2648, 0.0216,
        0.0487, 0.0398, 0.0386, 0.0086, 0.1293, 0.0095], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,678][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.0579, 0.0206, 0.0135, 0.0493, 0.0373, 0.0423, 0.0283, 0.1423, 0.0836,
        0.0495, 0.0358, 0.0276, 0.0339, 0.2434, 0.1345], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,683][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0198, 0.0700, 0.0790, 0.0895, 0.0871, 0.0638, 0.0788, 0.0825, 0.1064,
        0.0519, 0.0387, 0.0369, 0.0457, 0.0499, 0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,687][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0821, 0.0097, 0.0118, 0.0802, 0.0166, 0.0129, 0.0141, 0.0934, 0.0433,
        0.0762, 0.0255, 0.0510, 0.1314, 0.1541, 0.1976], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,692][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.3741, 0.0041, 0.0044, 0.0504, 0.0047, 0.0017, 0.0031, 0.1072, 0.0342,
        0.0309, 0.0123, 0.0198, 0.0578, 0.1141, 0.1813], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,696][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0003, 0.0492, 0.0796, 0.1009, 0.0386, 0.1196, 0.1600, 0.0659, 0.0530,
        0.0652, 0.0392, 0.0565, 0.0825, 0.0751, 0.0145], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,701][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.1212, 0.0619, 0.0643, 0.0623, 0.0573, 0.0579, 0.0611, 0.0632, 0.0627,
        0.0638, 0.0634, 0.0639, 0.0653, 0.0685, 0.0632], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,702][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0193, 0.0605, 0.0829, 0.0933, 0.0727, 0.0769, 0.0461, 0.0482, 0.0714,
        0.0832, 0.0645, 0.1049, 0.0587, 0.0603, 0.0572], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:42,703][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0452, 0.0701, 0.0611, 0.0630, 0.0442, 0.0541, 0.0653, 0.0608, 0.0579,
        0.0648, 0.0543, 0.0625, 0.0717, 0.0850, 0.0723, 0.0678],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,704][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0132, 0.0086, 0.0139, 0.0180, 0.0032, 0.0019, 0.0047, 0.1819, 0.0230,
        0.0221, 0.0073, 0.0103, 0.0343, 0.2006, 0.4029, 0.0540],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,707][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0258, 0.0596, 0.0587, 0.0604, 0.0630, 0.0603, 0.0599, 0.0653, 0.0690,
        0.0697, 0.0687, 0.0662, 0.0653, 0.0686, 0.0690, 0.0705],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,710][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0118, 0.0606, 0.0639, 0.0636, 0.0649, 0.0623, 0.0663, 0.0726, 0.0687,
        0.0672, 0.0688, 0.0637, 0.0660, 0.0685, 0.0673, 0.0640],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,715][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0077, 0.0170, 0.0063, 0.1047, 0.0288, 0.1996, 0.1344, 0.1664, 0.0304,
        0.1303, 0.0304, 0.0408, 0.0235, 0.0605, 0.0076, 0.0116],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,719][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0425, 0.0261, 0.0166, 0.0290, 0.0372, 0.0381, 0.0490, 0.0950, 0.0514,
        0.0668, 0.0226, 0.0333, 0.0470, 0.1984, 0.1986, 0.0483],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,724][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0149, 0.0644, 0.0721, 0.1380, 0.0853, 0.0594, 0.0655, 0.0826, 0.0864,
        0.0472, 0.0373, 0.0375, 0.0487, 0.0360, 0.0666, 0.0580],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,729][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.1293, 0.0251, 0.0227, 0.0588, 0.0248, 0.0204, 0.0293, 0.0729, 0.0369,
        0.0625, 0.0295, 0.0496, 0.0855, 0.0870, 0.1475, 0.1182],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,733][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.5082, 0.0064, 0.0056, 0.0367, 0.0050, 0.0024, 0.0036, 0.0526, 0.0185,
        0.0158, 0.0108, 0.0178, 0.0350, 0.0667, 0.1074, 0.1074],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,734][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0007, 0.0455, 0.0713, 0.0404, 0.0405, 0.0955, 0.2653, 0.0874, 0.0253,
        0.0481, 0.0497, 0.0475, 0.0456, 0.0497, 0.0334, 0.0541],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,735][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.1035, 0.0590, 0.0604, 0.0587, 0.0550, 0.0554, 0.0583, 0.0601, 0.0596,
        0.0611, 0.0607, 0.0605, 0.0619, 0.0647, 0.0601, 0.0609],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,736][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0207, 0.0557, 0.0790, 0.0718, 0.0595, 0.0630, 0.0423, 0.0477, 0.0565,
        0.0721, 0.0562, 0.1042, 0.0558, 0.0584, 0.0577, 0.0995],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:42,739][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0502, 0.0645, 0.0564, 0.0584, 0.0411, 0.0499, 0.0605, 0.0562, 0.0528,
        0.0594, 0.0509, 0.0564, 0.0651, 0.0784, 0.0662, 0.0636, 0.0701],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,742][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0040, 0.0073, 0.0083, 0.0216, 0.0013, 0.0009, 0.0017, 0.0976, 0.0130,
        0.0115, 0.0054, 0.0094, 0.0389, 0.2356, 0.3659, 0.1036, 0.0740],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,747][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0225, 0.0558, 0.0551, 0.0567, 0.0591, 0.0566, 0.0562, 0.0610, 0.0648,
        0.0658, 0.0648, 0.0622, 0.0613, 0.0644, 0.0648, 0.0661, 0.0629],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,751][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0110, 0.0572, 0.0603, 0.0597, 0.0608, 0.0582, 0.0619, 0.0680, 0.0644,
        0.0628, 0.0643, 0.0596, 0.0617, 0.0641, 0.0629, 0.0601, 0.0629],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,756][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0098, 0.0122, 0.0146, 0.1261, 0.0496, 0.1203, 0.1096, 0.1439, 0.0287,
        0.0886, 0.0364, 0.0434, 0.0243, 0.1056, 0.0172, 0.0198, 0.0498],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,761][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0302, 0.0213, 0.0196, 0.0224, 0.0405, 0.0352, 0.0397, 0.1008, 0.0557,
        0.0417, 0.0284, 0.0222, 0.0246, 0.1996, 0.2039, 0.0493, 0.0649],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,765][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0159, 0.0764, 0.0791, 0.1152, 0.0725, 0.0502, 0.0644, 0.0803, 0.0687,
        0.0419, 0.0312, 0.0372, 0.0480, 0.0406, 0.0668, 0.0494, 0.0624],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,766][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0930, 0.0149, 0.0176, 0.0541, 0.0163, 0.0152, 0.0174, 0.0586, 0.0303,
        0.0527, 0.0228, 0.0430, 0.0821, 0.0869, 0.1191, 0.1274, 0.1486],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,767][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5062, 0.0037, 0.0031, 0.0394, 0.0038, 0.0014, 0.0020, 0.0410, 0.0146,
        0.0113, 0.0081, 0.0119, 0.0223, 0.0398, 0.0862, 0.1176, 0.0876],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,768][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0002, 0.0484, 0.0715, 0.0742, 0.0495, 0.0674, 0.1901, 0.0575, 0.0262,
        0.0559, 0.0443, 0.0378, 0.0478, 0.0725, 0.0525, 0.0689, 0.0352],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,771][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0982, 0.0547, 0.0568, 0.0555, 0.0517, 0.0521, 0.0548, 0.0562, 0.0559,
        0.0576, 0.0570, 0.0568, 0.0583, 0.0604, 0.0566, 0.0580, 0.0594],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,774][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0190, 0.0542, 0.0696, 0.0738, 0.0554, 0.0596, 0.0387, 0.0403, 0.0523,
        0.0613, 0.0578, 0.0876, 0.0470, 0.0532, 0.0520, 0.1099, 0.0681],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:42,779][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0380, 0.0591, 0.0529, 0.0555, 0.0383, 0.0473, 0.0572, 0.0537, 0.0509,
        0.0578, 0.0481, 0.0558, 0.0644, 0.0750, 0.0639, 0.0608, 0.0701, 0.0513],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,784][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0030, 0.0045, 0.0081, 0.0281, 0.0011, 0.0009, 0.0017, 0.0745, 0.0116,
        0.0115, 0.0032, 0.0149, 0.0314, 0.2561, 0.2252, 0.1159, 0.1429, 0.0654],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,788][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0216, 0.0522, 0.0516, 0.0534, 0.0557, 0.0533, 0.0526, 0.0574, 0.0609,
        0.0617, 0.0607, 0.0587, 0.0578, 0.0602, 0.0609, 0.0624, 0.0594, 0.0595],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,793][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0106, 0.0533, 0.0563, 0.0559, 0.0567, 0.0551, 0.0584, 0.0638, 0.0609,
        0.0590, 0.0606, 0.0567, 0.0582, 0.0604, 0.0595, 0.0568, 0.0592, 0.0585],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,797][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0104, 0.0182, 0.0082, 0.0756, 0.0213, 0.1338, 0.2150, 0.2023, 0.0425,
        0.0366, 0.0270, 0.0409, 0.0107, 0.0804, 0.0044, 0.0087, 0.0161, 0.0479],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,798][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0440, 0.0125, 0.0263, 0.0374, 0.0280, 0.0319, 0.0247, 0.0788, 0.0620,
        0.0477, 0.0442, 0.0230, 0.0447, 0.1399, 0.1631, 0.0442, 0.0775, 0.0701],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,799][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0212, 0.0532, 0.0574, 0.0764, 0.0649, 0.0491, 0.0589, 0.0676, 0.0784,
        0.0389, 0.0307, 0.0300, 0.0363, 0.0342, 0.0696, 0.0406, 0.0401, 0.1526],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,800][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.1671, 0.0145, 0.0162, 0.0485, 0.0134, 0.0150, 0.0178, 0.0655, 0.0262,
        0.0439, 0.0266, 0.0377, 0.0639, 0.0635, 0.1057, 0.0982, 0.1200, 0.0565],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,803][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.3845, 0.0052, 0.0048, 0.0395, 0.0058, 0.0021, 0.0033, 0.0439, 0.0241,
        0.0164, 0.0097, 0.0146, 0.0308, 0.0378, 0.0976, 0.1026, 0.1218, 0.0554],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,807][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0003, 0.0217, 0.1863, 0.1368, 0.0316, 0.0390, 0.1306, 0.0274, 0.0313,
        0.0559, 0.0188, 0.0386, 0.0460, 0.0359, 0.0497, 0.0764, 0.0511, 0.0226],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,811][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.1021, 0.0512, 0.0536, 0.0523, 0.0480, 0.0487, 0.0511, 0.0525, 0.0523,
        0.0533, 0.0528, 0.0535, 0.0546, 0.0570, 0.0527, 0.0539, 0.0559, 0.0546],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,816][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0138, 0.0431, 0.0628, 0.0716, 0.0548, 0.0558, 0.0359, 0.0349, 0.0512,
        0.0624, 0.0440, 0.0807, 0.0511, 0.0457, 0.0507, 0.1055, 0.0723, 0.0635],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:42,821][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0478, 0.0568, 0.0506, 0.0517, 0.0367, 0.0452, 0.0529, 0.0505, 0.0474,
        0.0533, 0.0453, 0.0504, 0.0576, 0.0693, 0.0584, 0.0570, 0.0632, 0.0497,
        0.0561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,825][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0027, 0.0049, 0.0039, 0.0145, 0.0006, 0.0006, 0.0009, 0.0599, 0.0054,
        0.0056, 0.0021, 0.0031, 0.0151, 0.1662, 0.1563, 0.0775, 0.0772, 0.1035,
        0.3000], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,829][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0199, 0.0493, 0.0486, 0.0503, 0.0523, 0.0502, 0.0497, 0.0542, 0.0576,
        0.0583, 0.0575, 0.0553, 0.0546, 0.0570, 0.0576, 0.0588, 0.0560, 0.0565,
        0.0563], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,830][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0104, 0.0508, 0.0534, 0.0533, 0.0532, 0.0516, 0.0545, 0.0603, 0.0567,
        0.0554, 0.0567, 0.0531, 0.0549, 0.0567, 0.0559, 0.0536, 0.0560, 0.0549,
        0.0587], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,831][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0116, 0.0071, 0.0089, 0.1141, 0.0238, 0.0733, 0.2317, 0.1186, 0.0325,
        0.0623, 0.0371, 0.0506, 0.0158, 0.1240, 0.0086, 0.0184, 0.0251, 0.0150,
        0.0214], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,832][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0237, 0.0165, 0.0067, 0.0284, 0.0227, 0.0271, 0.0243, 0.1015, 0.0527,
        0.0497, 0.0286, 0.0314, 0.0345, 0.1163, 0.1447, 0.0719, 0.0996, 0.0958,
        0.0236], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,835][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0228, 0.0436, 0.0662, 0.0809, 0.0588, 0.0432, 0.0517, 0.0684, 0.0785,
        0.0381, 0.0282, 0.0280, 0.0340, 0.0326, 0.0604, 0.0372, 0.0356, 0.0962,
        0.0957], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,839][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.1098, 0.0123, 0.0098, 0.0457, 0.0131, 0.0110, 0.0111, 0.0405, 0.0260,
        0.0481, 0.0178, 0.0309, 0.0683, 0.0606, 0.0682, 0.1121, 0.1327, 0.0583,
        0.1238], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,843][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.5141, 0.0026, 0.0016, 0.0174, 0.0020, 0.0007, 0.0009, 0.0207, 0.0084,
        0.0053, 0.0052, 0.0055, 0.0124, 0.0267, 0.0462, 0.0617, 0.0570, 0.0409,
        0.1707], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,848][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0002, 0.0234, 0.0508, 0.1399, 0.0583, 0.0425, 0.0782, 0.0436, 0.0331,
        0.0750, 0.0273, 0.0515, 0.0673, 0.0686, 0.0303, 0.0791, 0.0610, 0.0345,
        0.0356], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,853][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0968, 0.0484, 0.0509, 0.0497, 0.0452, 0.0462, 0.0481, 0.0497, 0.0495,
        0.0507, 0.0504, 0.0508, 0.0518, 0.0542, 0.0501, 0.0511, 0.0527, 0.0512,
        0.0523], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,857][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0123, 0.0427, 0.0551, 0.0592, 0.0568, 0.0568, 0.0365, 0.0344, 0.0471,
        0.0648, 0.0448, 0.0778, 0.0506, 0.0404, 0.0438, 0.0871, 0.0731, 0.0649,
        0.0519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:42,861][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0530, 0.0580, 0.0484, 0.0492, 0.0362, 0.0427, 0.0518, 0.0472, 0.0445,
        0.0500, 0.0428, 0.0470, 0.0542, 0.0658, 0.0539, 0.0536, 0.0588, 0.0470,
        0.0519, 0.0441], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,862][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ by] are: tensor([3.2726e-02, 2.5778e-03, 5.7781e-03, 3.5861e-03, 7.0858e-04, 3.7207e-04,
        6.4165e-04, 2.3242e-02, 4.5993e-03, 4.0625e-03, 1.3909e-03, 2.1364e-03,
        7.2549e-03, 2.3199e-02, 2.9283e-02, 2.8443e-02, 2.8716e-02, 2.2566e-02,
        2.6125e-01, 5.1747e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,863][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0191, 0.0470, 0.0460, 0.0475, 0.0494, 0.0474, 0.0470, 0.0513, 0.0544,
        0.0549, 0.0543, 0.0524, 0.0514, 0.0540, 0.0544, 0.0557, 0.0528, 0.0534,
        0.0533, 0.0543], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,864][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0104, 0.0488, 0.0510, 0.0500, 0.0508, 0.0490, 0.0520, 0.0568, 0.0538,
        0.0521, 0.0538, 0.0503, 0.0515, 0.0533, 0.0524, 0.0504, 0.0523, 0.0520,
        0.0554, 0.0539], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,868][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0107, 0.0092, 0.0077, 0.2355, 0.0181, 0.0654, 0.0960, 0.1426, 0.0074,
        0.0990, 0.0234, 0.0279, 0.0170, 0.0395, 0.0108, 0.0119, 0.0237, 0.0110,
        0.0116, 0.1314], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,872][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0316, 0.0200, 0.0093, 0.0176, 0.0306, 0.0414, 0.0227, 0.0589, 0.0459,
        0.0422, 0.0211, 0.0380, 0.0274, 0.1333, 0.1498, 0.0362, 0.0672, 0.1164,
        0.0324, 0.0580], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,877][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0142, 0.0548, 0.0617, 0.1105, 0.0638, 0.0409, 0.0455, 0.0586, 0.0558,
        0.0317, 0.0257, 0.0243, 0.0307, 0.0243, 0.0425, 0.0383, 0.0342, 0.0816,
        0.0769, 0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,882][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.1817, 0.0158, 0.0117, 0.0288, 0.0127, 0.0088, 0.0110, 0.0312, 0.0142,
        0.0261, 0.0158, 0.0214, 0.0389, 0.0328, 0.0378, 0.0711, 0.0737, 0.0410,
        0.0946, 0.2310], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,885][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ by] are: tensor([5.1183e-01, 1.3457e-03, 9.1076e-04, 5.5812e-03, 8.1062e-04, 2.2136e-04,
        3.5434e-04, 6.2808e-03, 1.8774e-03, 1.5672e-03, 1.8693e-03, 1.9828e-03,
        3.8184e-03, 4.7429e-03, 6.8538e-03, 1.7809e-02, 2.0961e-02, 1.2719e-02,
        5.2922e-02, 3.4554e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,889][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0002, 0.0180, 0.1267, 0.0456, 0.0335, 0.1102, 0.1528, 0.0442, 0.0340,
        0.0524, 0.0276, 0.0389, 0.0393, 0.0272, 0.0263, 0.0556, 0.0541, 0.0189,
        0.0614, 0.0330], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,893][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0878, 0.0460, 0.0482, 0.0473, 0.0437, 0.0443, 0.0464, 0.0475, 0.0475,
        0.0488, 0.0482, 0.0483, 0.0495, 0.0514, 0.0477, 0.0489, 0.0504, 0.0490,
        0.0499, 0.0491], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,894][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0176, 0.0421, 0.0544, 0.0525, 0.0471, 0.0572, 0.0326, 0.0312, 0.0406,
        0.0572, 0.0371, 0.0806, 0.0467, 0.0430, 0.0420, 0.0757, 0.0661, 0.0655,
        0.0528, 0.0581], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:42,895][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0453, 0.0532, 0.0451, 0.0467, 0.0336, 0.0404, 0.0485, 0.0450, 0.0421,
        0.0474, 0.0409, 0.0448, 0.0516, 0.0622, 0.0521, 0.0509, 0.0558, 0.0446,
        0.0495, 0.0426, 0.0578], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,896][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([3.6300e-03, 2.7308e-03, 1.8635e-03, 3.9924e-03, 2.6107e-04, 2.2922e-04,
        3.1045e-04, 1.5284e-02, 2.3042e-03, 2.1718e-03, 1.2427e-03, 1.8311e-03,
        7.0684e-03, 3.4942e-02, 4.4588e-02, 2.0600e-02, 1.2257e-02, 3.5632e-02,
        1.3025e-01, 6.3024e-01, 4.8579e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,899][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0183, 0.0448, 0.0439, 0.0452, 0.0471, 0.0451, 0.0449, 0.0487, 0.0519,
        0.0524, 0.0517, 0.0500, 0.0489, 0.0515, 0.0517, 0.0529, 0.0501, 0.0506,
        0.0505, 0.0516, 0.0481], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,903][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0094, 0.0459, 0.0481, 0.0476, 0.0484, 0.0464, 0.0492, 0.0539, 0.0511,
        0.0499, 0.0512, 0.0475, 0.0491, 0.0508, 0.0498, 0.0478, 0.0499, 0.0493,
        0.0527, 0.0516, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,908][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0066, 0.0090, 0.0134, 0.1185, 0.0343, 0.0728, 0.0849, 0.1191, 0.0162,
        0.0603, 0.0365, 0.0346, 0.0164, 0.1091, 0.0156, 0.0159, 0.0282, 0.0213,
        0.0382, 0.1123, 0.0366], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,912][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0300, 0.0183, 0.0145, 0.0167, 0.0327, 0.0277, 0.0288, 0.0646, 0.0435,
        0.0322, 0.0191, 0.0170, 0.0183, 0.1430, 0.1354, 0.0355, 0.0462, 0.1079,
        0.0466, 0.0539, 0.0680], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,917][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0122, 0.0582, 0.0583, 0.0882, 0.0523, 0.0349, 0.0443, 0.0564, 0.0464,
        0.0300, 0.0223, 0.0263, 0.0346, 0.0283, 0.0457, 0.0351, 0.0456, 0.0864,
        0.0782, 0.0713, 0.0449], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,922][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0840, 0.0103, 0.0095, 0.0283, 0.0088, 0.0078, 0.0088, 0.0271, 0.0143,
        0.0245, 0.0126, 0.0208, 0.0385, 0.0365, 0.0438, 0.0622, 0.0684, 0.0362,
        0.0890, 0.2434, 0.1251], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,925][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([2.9507e-01, 1.1889e-03, 6.6124e-04, 8.2638e-03, 7.1543e-04, 2.7562e-04,
        3.2844e-04, 6.5626e-03, 2.1760e-03, 1.8208e-03, 1.7281e-03, 2.2931e-03,
        3.7727e-03, 5.3429e-03, 1.2061e-02, 2.1404e-02, 1.4621e-02, 1.1737e-02,
        4.2517e-02, 5.2888e-01, 3.8578e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,928][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.7781e-04, 3.7776e-02, 6.6353e-02, 6.6925e-02, 4.2863e-02, 6.3236e-02,
        1.8127e-01, 4.3899e-02, 2.2545e-02, 4.4027e-02, 3.2031e-02, 2.8893e-02,
        3.5490e-02, 4.8593e-02, 3.8968e-02, 5.1455e-02, 2.7763e-02, 2.3465e-02,
        4.7346e-02, 5.2373e-02, 4.4549e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,929][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0817, 0.0439, 0.0459, 0.0450, 0.0417, 0.0421, 0.0443, 0.0454, 0.0452,
        0.0466, 0.0460, 0.0459, 0.0472, 0.0489, 0.0456, 0.0469, 0.0482, 0.0468,
        0.0477, 0.0469, 0.0481], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:42,930][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0146, 0.0409, 0.0519, 0.0562, 0.0424, 0.0457, 0.0293, 0.0304, 0.0397,
        0.0459, 0.0440, 0.0672, 0.0353, 0.0403, 0.0395, 0.0837, 0.0507, 0.0687,
        0.0557, 0.0663, 0.0519], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,011][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:43,015][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,019][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,023][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,026][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,027][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,028][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,028][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,029][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,031][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,033][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,037][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,040][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,045][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7779, 0.2221], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,050][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.6049, 0.3951], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,054][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.6765, 0.3235], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,058][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.9234, 0.0766], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,059][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.9178, 0.0822], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,060][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.8057, 0.1943], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,061][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.8485, 0.1515], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,061][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.8592, 0.1408], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,064][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9355, 0.0645], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,068][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0223, 0.9777], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,072][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.6310, 0.3690], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,077][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.6197, 0.3803], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,082][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.5630, 0.1773, 0.2597], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,086][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.3825, 0.3502, 0.2674], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,091][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.5271, 0.1868, 0.2860], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,091][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.8823, 0.0660, 0.0517], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,092][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.9450, 0.0386, 0.0164], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,093][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.6636, 0.1641, 0.1724], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,093][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.9529, 0.0350, 0.0121], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,096][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.5950, 0.1718, 0.2333], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,100][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.9384, 0.0246, 0.0370], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,104][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0135, 0.3121, 0.6744], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,109][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.8583, 0.0912, 0.0504], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,114][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.5806, 0.2058, 0.2136], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,118][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.8068, 0.0464, 0.0678, 0.0790], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,123][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.5616, 0.1197, 0.1350, 0.1837], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,123][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.3983, 0.1152, 0.1841, 0.3024], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,124][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.8925, 0.0376, 0.0203, 0.0495], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,125][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.9254, 0.0465, 0.0113, 0.0168], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,126][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.6823, 0.0668, 0.0555, 0.1954], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,128][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.8776, 0.0421, 0.0118, 0.0685], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,132][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.5823, 0.0869, 0.1327, 0.1982], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,136][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.8804, 0.0119, 0.0237, 0.0840], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,141][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0070, 0.1603, 0.6168, 0.2160], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,146][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.6357, 0.1544, 0.0671, 0.1428], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,150][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.6159, 0.0961, 0.1218, 0.1662], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:43,155][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.4108, 0.0691, 0.1363, 0.2432, 0.1407], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,156][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.4665, 0.1206, 0.0853, 0.2247, 0.1028], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,156][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.2677, 0.1196, 0.1419, 0.2882, 0.1826], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,157][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.8540, 0.0357, 0.0256, 0.0605, 0.0242], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,158][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.8487, 0.0592, 0.0184, 0.0182, 0.0555], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,161][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.4576, 0.0777, 0.0812, 0.2849, 0.0987], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,166][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.8291, 0.0353, 0.0118, 0.0630, 0.0608], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,170][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.4240, 0.0822, 0.1373, 0.2658, 0.0907], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,175][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.8338, 0.0116, 0.0186, 0.1242, 0.0118], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,180][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0135, 0.2909, 0.4211, 0.1186, 0.1559], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,184][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.5899, 0.1417, 0.0546, 0.0936, 0.1202], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,187][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.4530, 0.1022, 0.1372, 0.2379, 0.0696], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:43,188][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.3141, 0.0659, 0.1343, 0.3098, 0.1105, 0.0654], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,188][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.2820, 0.1223, 0.0968, 0.3617, 0.0735, 0.0636], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,189][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.2059, 0.0836, 0.1253, 0.2345, 0.2142, 0.1365], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,191][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.7483, 0.0386, 0.0311, 0.1090, 0.0461, 0.0268], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,194][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.8358, 0.0648, 0.0141, 0.0251, 0.0442, 0.0160], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,199][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.3279, 0.0742, 0.0879, 0.3317, 0.0882, 0.0901], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,203][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.7811, 0.0635, 0.0121, 0.0627, 0.0283, 0.0523], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,208][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.3174, 0.0772, 0.1310, 0.2992, 0.1069, 0.0683], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,213][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.6142, 0.0244, 0.0218, 0.2919, 0.0360, 0.0116], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,217][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0080, 0.1503, 0.3785, 0.2276, 0.1728, 0.0629], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,219][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.5417, 0.0974, 0.0574, 0.0971, 0.1157, 0.0908], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,220][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.2781, 0.0931, 0.1112, 0.4047, 0.0632, 0.0496], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:43,221][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.2890, 0.0487, 0.1257, 0.2755, 0.1371, 0.0681, 0.0559],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,221][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.2202, 0.0897, 0.0817, 0.4465, 0.0506, 0.0540, 0.0574],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,223][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.2011, 0.0683, 0.0977, 0.2113, 0.1728, 0.1384, 0.1103],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,226][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.8406, 0.0241, 0.0185, 0.0591, 0.0234, 0.0181, 0.0163],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,231][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.8901, 0.0391, 0.0115, 0.0153, 0.0261, 0.0078, 0.0100],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,235][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.2679, 0.0553, 0.0771, 0.3533, 0.0816, 0.0717, 0.0931],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,240][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.8715, 0.0287, 0.0099, 0.0339, 0.0215, 0.0179, 0.0165],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,245][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.3125, 0.0552, 0.1110, 0.2722, 0.0719, 0.0656, 0.1116],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,249][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.7466, 0.0136, 0.0192, 0.1792, 0.0153, 0.0121, 0.0141],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,251][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0053, 0.1660, 0.2289, 0.1969, 0.1079, 0.2419, 0.0532],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,252][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.3341, 0.0976, 0.0648, 0.1454, 0.1462, 0.1109, 0.1009],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,253][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.3172, 0.0714, 0.0884, 0.3762, 0.0441, 0.0444, 0.0583],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:43,254][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.3251, 0.0325, 0.0580, 0.1413, 0.0653, 0.0281, 0.0281, 0.3216],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,256][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.3008, 0.0535, 0.0559, 0.1643, 0.0301, 0.0223, 0.0260, 0.3471],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,259][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.2109, 0.0687, 0.0872, 0.1715, 0.1236, 0.0810, 0.0708, 0.1865],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,263][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.6944, 0.0326, 0.0276, 0.0780, 0.0368, 0.0207, 0.0215, 0.0883],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,268][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.7142, 0.0667, 0.0134, 0.0235, 0.0794, 0.0160, 0.0165, 0.0704],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,273][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.3849, 0.0312, 0.0304, 0.1270, 0.0399, 0.0271, 0.0337, 0.3259],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,277][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.8275, 0.0343, 0.0090, 0.0313, 0.0223, 0.0157, 0.0111, 0.0487],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,282][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.3621, 0.0359, 0.0607, 0.1794, 0.0529, 0.0518, 0.0525, 0.2047],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,283][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.6688, 0.0103, 0.0125, 0.1207, 0.0241, 0.0077, 0.0082, 0.1476],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,284][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0027, 0.0557, 0.2090, 0.1403, 0.0625, 0.3331, 0.1338, 0.0629],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,285][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.3712, 0.1151, 0.0675, 0.1143, 0.0814, 0.0856, 0.0501, 0.1147],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,286][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.4373, 0.0318, 0.0439, 0.1508, 0.0250, 0.0166, 0.0208, 0.2738],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:43,288][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.2227, 0.0338, 0.0532, 0.1370, 0.0625, 0.0306, 0.0313, 0.3437, 0.0851],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,291][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.1790, 0.0351, 0.0277, 0.1094, 0.0340, 0.0150, 0.0404, 0.4733, 0.0861],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,296][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.2614, 0.0543, 0.0641, 0.1401, 0.1032, 0.0667, 0.0676, 0.1816, 0.0611],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,300][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.7020, 0.0279, 0.0202, 0.0601, 0.0255, 0.0155, 0.0177, 0.1014, 0.0298],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,305][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.8467, 0.0322, 0.0050, 0.0153, 0.0390, 0.0072, 0.0079, 0.0300, 0.0167],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,309][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.2635, 0.0267, 0.0260, 0.1446, 0.0364, 0.0254, 0.0339, 0.3288, 0.1147],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,314][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.8878, 0.0198, 0.0045, 0.0170, 0.0125, 0.0099, 0.0053, 0.0264, 0.0168],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,316][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.2511, 0.0439, 0.0603, 0.1717, 0.0515, 0.0536, 0.0661, 0.2323, 0.0695],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,316][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.6171, 0.0070, 0.0070, 0.0924, 0.0103, 0.0047, 0.0088, 0.1972, 0.0553],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,317][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0023, 0.0282, 0.0869, 0.0444, 0.0709, 0.4319, 0.1712, 0.1478, 0.0164],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,318][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.4071, 0.0591, 0.0338, 0.0845, 0.0926, 0.0669, 0.0481, 0.1201, 0.0877],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,321][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.2288, 0.0277, 0.0306, 0.1315, 0.0280, 0.0165, 0.0397, 0.3836, 0.1136],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:43,324][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.3223, 0.0266, 0.0352, 0.0889, 0.0421, 0.0206, 0.0219, 0.2502, 0.0923,
        0.0999], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,328][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.2234, 0.0330, 0.0239, 0.1175, 0.0255, 0.0175, 0.0258, 0.3536, 0.0757,
        0.1043], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,332][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.1672, 0.0448, 0.0568, 0.1159, 0.1036, 0.0658, 0.0742, 0.1706, 0.0686,
        0.1324], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,337][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.6482, 0.0262, 0.0207, 0.0639, 0.0296, 0.0185, 0.0186, 0.0944, 0.0425,
        0.0375], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,342][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.7730, 0.0395, 0.0159, 0.0260, 0.0412, 0.0130, 0.0119, 0.0396, 0.0225,
        0.0174], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,346][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.1892, 0.0271, 0.0282, 0.1176, 0.0362, 0.0271, 0.0332, 0.2796, 0.1213,
        0.1405], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,348][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.7285, 0.0452, 0.0112, 0.0463, 0.0280, 0.0252, 0.0136, 0.0489, 0.0188,
        0.0342], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,349][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.1950, 0.0355, 0.0643, 0.1462, 0.0484, 0.0397, 0.0557, 0.2110, 0.0776,
        0.1265], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,350][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.7496, 0.0057, 0.0053, 0.0625, 0.0069, 0.0041, 0.0044, 0.0946, 0.0434,
        0.0234], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,350][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0059, 0.1188, 0.1227, 0.1075, 0.0536, 0.1131, 0.1223, 0.1951, 0.0687,
        0.0923], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,353][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.3599, 0.0612, 0.0425, 0.0848, 0.0589, 0.0559, 0.0560, 0.0959, 0.0984,
        0.0866], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,356][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.2674, 0.0322, 0.0282, 0.1291, 0.0245, 0.0102, 0.0280, 0.2847, 0.1178,
        0.0779], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:43,360][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.1460, 0.0246, 0.0375, 0.1034, 0.0496, 0.0266, 0.0296, 0.2853, 0.1232,
        0.1308, 0.0434], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,364][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.1276, 0.0330, 0.0271, 0.0925, 0.0272, 0.0217, 0.0284, 0.3569, 0.0770,
        0.1729, 0.0358], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,369][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.0908, 0.0419, 0.0663, 0.1132, 0.1067, 0.0735, 0.0651, 0.1792, 0.0680,
        0.1235, 0.0718], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,374][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.5241, 0.0291, 0.0270, 0.0627, 0.0396, 0.0261, 0.0309, 0.1171, 0.0563,
        0.0500, 0.0371], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,378][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.7392, 0.0458, 0.0107, 0.0181, 0.0599, 0.0128, 0.0144, 0.0405, 0.0176,
        0.0232, 0.0178], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,380][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.1538, 0.0214, 0.0263, 0.1034, 0.0344, 0.0275, 0.0397, 0.2691, 0.1248,
        0.1358, 0.0639], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,381][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.7190, 0.0275, 0.0085, 0.0358, 0.0259, 0.0216, 0.0130, 0.0559, 0.0234,
        0.0334, 0.0361], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,382][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.1507, 0.0302, 0.0457, 0.1476, 0.0479, 0.0511, 0.0455, 0.1722, 0.0928,
        0.1595, 0.0568], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,383][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.4456, 0.0095, 0.0107, 0.1091, 0.0214, 0.0071, 0.0112, 0.1608, 0.1234,
        0.0758, 0.0254], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,385][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0007, 0.0416, 0.1052, 0.0596, 0.0564, 0.1581, 0.2985, 0.0790, 0.0988,
        0.0765, 0.0256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,388][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.2852, 0.0710, 0.0384, 0.0832, 0.0635, 0.0610, 0.0518, 0.1071, 0.0851,
        0.1014, 0.0524], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,392][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.1588, 0.0258, 0.0337, 0.1185, 0.0224, 0.0174, 0.0291, 0.3054, 0.1226,
        0.1158, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:43,397][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.2075, 0.0414, 0.0349, 0.0854, 0.0377, 0.0219, 0.0254, 0.2370, 0.0974,
        0.1146, 0.0488, 0.0479], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,402][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.1284, 0.0389, 0.0278, 0.1030, 0.0219, 0.0150, 0.0295, 0.3608, 0.0887,
        0.0863, 0.0511, 0.0485], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,406][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.1149, 0.0424, 0.0577, 0.1035, 0.0830, 0.0612, 0.0639, 0.1580, 0.0671,
        0.1084, 0.0706, 0.0695], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,411][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.5113, 0.0286, 0.0239, 0.0645, 0.0349, 0.0196, 0.0241, 0.1098, 0.0503,
        0.0522, 0.0390, 0.0418], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,412][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.6615, 0.0449, 0.0167, 0.0303, 0.0428, 0.0212, 0.0208, 0.0542, 0.0275,
        0.0295, 0.0252, 0.0253], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,413][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.1628, 0.0232, 0.0216, 0.0933, 0.0324, 0.0251, 0.0335, 0.2323, 0.1155,
        0.1436, 0.0579, 0.0587], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,414][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.6612, 0.0363, 0.0137, 0.0518, 0.0238, 0.0236, 0.0150, 0.0545, 0.0182,
        0.0335, 0.0219, 0.0465], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,415][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.1297, 0.0292, 0.0421, 0.1273, 0.0409, 0.0394, 0.0541, 0.1715, 0.0836,
        0.1384, 0.0552, 0.0886], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,417][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.5005, 0.0099, 0.0088, 0.1021, 0.0136, 0.0063, 0.0097, 0.1571, 0.0775,
        0.0476, 0.0312, 0.0358], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,421][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0009, 0.0582, 0.0927, 0.1075, 0.0535, 0.1179, 0.2939, 0.0736, 0.0447,
        0.0595, 0.0352, 0.0624], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,425][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.3010, 0.0593, 0.0405, 0.0738, 0.0770, 0.0464, 0.0380, 0.1027, 0.0826,
        0.0739, 0.0444, 0.0602], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,430][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.1624, 0.0323, 0.0348, 0.1242, 0.0220, 0.0170, 0.0314, 0.2462, 0.1099,
        0.1081, 0.0482, 0.0635], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:43,434][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.2254, 0.0234, 0.0295, 0.0732, 0.0276, 0.0170, 0.0189, 0.2367, 0.0684,
        0.0969, 0.0419, 0.0540, 0.0872], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,439][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.1415, 0.0328, 0.0262, 0.1124, 0.0187, 0.0109, 0.0200, 0.2847, 0.0835,
        0.0702, 0.0361, 0.0475, 0.1156], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,444][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0956, 0.0337, 0.0427, 0.0830, 0.0765, 0.0447, 0.0609, 0.1545, 0.0531,
        0.1063, 0.0724, 0.0624, 0.1141], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,445][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.5517, 0.0250, 0.0190, 0.0609, 0.0299, 0.0164, 0.0161, 0.0856, 0.0378,
        0.0396, 0.0293, 0.0354, 0.0534], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,446][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.7467, 0.0342, 0.0089, 0.0225, 0.0364, 0.0127, 0.0105, 0.0414, 0.0187,
        0.0172, 0.0150, 0.0185, 0.0173], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,447][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.1621, 0.0216, 0.0201, 0.0876, 0.0272, 0.0192, 0.0229, 0.2105, 0.0884,
        0.1058, 0.0490, 0.0540, 0.1315], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,448][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.7170, 0.0360, 0.0104, 0.0395, 0.0154, 0.0180, 0.0111, 0.0375, 0.0124,
        0.0274, 0.0157, 0.0290, 0.0304], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,451][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.1243, 0.0244, 0.0381, 0.1101, 0.0352, 0.0323, 0.0396, 0.1387, 0.0610,
        0.1044, 0.0434, 0.0867, 0.1617], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,455][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.5305, 0.0072, 0.0083, 0.1016, 0.0098, 0.0045, 0.0067, 0.1297, 0.0479,
        0.0353, 0.0237, 0.0339, 0.0610], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,460][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0005, 0.0440, 0.1312, 0.1032, 0.0763, 0.0715, 0.2658, 0.0688, 0.0362,
        0.0737, 0.0291, 0.0526, 0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,465][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.3507, 0.0472, 0.0380, 0.0774, 0.0585, 0.0414, 0.0322, 0.0870, 0.0798,
        0.0566, 0.0319, 0.0491, 0.0502], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,469][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1574, 0.0218, 0.0366, 0.1167, 0.0155, 0.0097, 0.0197, 0.2408, 0.0865,
        0.0837, 0.0336, 0.0817, 0.0962], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:43,474][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.1957, 0.0151, 0.0170, 0.0546, 0.0217, 0.0110, 0.0126, 0.2171, 0.0506,
        0.0837, 0.0325, 0.0478, 0.0884, 0.1521], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,477][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.1790, 0.0126, 0.0098, 0.0695, 0.0090, 0.0056, 0.0064, 0.2054, 0.0434,
        0.0538, 0.0249, 0.0323, 0.0905, 0.2578], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,477][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.1053, 0.0332, 0.0383, 0.0930, 0.0762, 0.0428, 0.0510, 0.1334, 0.0461,
        0.0902, 0.0614, 0.0552, 0.1091, 0.0646], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,478][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.4914, 0.0192, 0.0182, 0.0570, 0.0254, 0.0137, 0.0140, 0.0912, 0.0348,
        0.0430, 0.0312, 0.0347, 0.0650, 0.0612], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,479][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.8062, 0.0250, 0.0080, 0.0176, 0.0225, 0.0080, 0.0090, 0.0237, 0.0146,
        0.0134, 0.0126, 0.0142, 0.0124, 0.0128], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,482][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.1917, 0.0148, 0.0137, 0.0758, 0.0182, 0.0109, 0.0133, 0.1606, 0.0570,
        0.0805, 0.0340, 0.0350, 0.1083, 0.1863], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,485][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.7241, 0.0366, 0.0096, 0.0278, 0.0140, 0.0141, 0.0085, 0.0422, 0.0174,
        0.0227, 0.0199, 0.0277, 0.0247, 0.0105], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,489][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.1609, 0.0143, 0.0208, 0.0895, 0.0208, 0.0196, 0.0168, 0.1195, 0.0435,
        0.0916, 0.0328, 0.0741, 0.1657, 0.1301], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,493][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.5331, 0.0040, 0.0045, 0.0600, 0.0053, 0.0025, 0.0039, 0.1025, 0.0348,
        0.0303, 0.0173, 0.0269, 0.0704, 0.1044], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,498][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0016, 0.0602, 0.2477, 0.0920, 0.0406, 0.0427, 0.1097, 0.0625, 0.0495,
        0.0447, 0.0461, 0.0793, 0.0722, 0.0512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,503][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.1863, 0.0713, 0.0465, 0.0967, 0.0729, 0.0518, 0.0381, 0.1015, 0.0705,
        0.0551, 0.0378, 0.0440, 0.0472, 0.0804], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,507][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.1887, 0.0128, 0.0131, 0.0869, 0.0097, 0.0059, 0.0095, 0.1885, 0.0454,
        0.0598, 0.0216, 0.0487, 0.0829, 0.2264], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:43,509][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.1530, 0.0107, 0.0126, 0.0446, 0.0149, 0.0067, 0.0098, 0.1931, 0.0603,
        0.0618, 0.0249, 0.0293, 0.0571, 0.1466, 0.1746], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,510][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0591, 0.0072, 0.0033, 0.0377, 0.0056, 0.0033, 0.0061, 0.1420, 0.0231,
        0.0248, 0.0130, 0.0154, 0.0591, 0.3588, 0.2416], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,511][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.1230, 0.0337, 0.0323, 0.0819, 0.0675, 0.0374, 0.0464, 0.1166, 0.0326,
        0.0831, 0.0550, 0.0486, 0.0833, 0.0485, 0.1100], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,512][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.3617, 0.0198, 0.0136, 0.0673, 0.0365, 0.0133, 0.0166, 0.1167, 0.0359,
        0.0364, 0.0314, 0.0311, 0.0549, 0.0724, 0.0925], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,514][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.6363, 0.0434, 0.0123, 0.0310, 0.0402, 0.0172, 0.0190, 0.0474, 0.0197,
        0.0241, 0.0201, 0.0186, 0.0189, 0.0255, 0.0264], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,518][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.1574, 0.0075, 0.0080, 0.0583, 0.0131, 0.0070, 0.0097, 0.1256, 0.0433,
        0.0565, 0.0239, 0.0235, 0.0702, 0.1590, 0.2370], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,523][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.6040, 0.0549, 0.0126, 0.0389, 0.0262, 0.0224, 0.0147, 0.0449, 0.0161,
        0.0318, 0.0240, 0.0324, 0.0338, 0.0116, 0.0318], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,527][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0821, 0.0097, 0.0118, 0.0802, 0.0166, 0.0129, 0.0141, 0.0934, 0.0433,
        0.0762, 0.0255, 0.0510, 0.1314, 0.1541, 0.1976], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,532][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.3741, 0.0041, 0.0044, 0.0504, 0.0047, 0.0017, 0.0031, 0.1072, 0.0342,
        0.0309, 0.0123, 0.0198, 0.0578, 0.1141, 0.1813], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,536][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0003, 0.0492, 0.0796, 0.1009, 0.0386, 0.1196, 0.1600, 0.0659, 0.0530,
        0.0652, 0.0392, 0.0565, 0.0825, 0.0751, 0.0145], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,541][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.1982, 0.0556, 0.0457, 0.0883, 0.0527, 0.0411, 0.0285, 0.0714, 0.0591,
        0.0551, 0.0309, 0.0423, 0.0455, 0.0814, 0.1043], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,542][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0819, 0.0068, 0.0102, 0.0512, 0.0051, 0.0030, 0.0062, 0.1433, 0.0329,
        0.0379, 0.0131, 0.0258, 0.0539, 0.2296, 0.2992], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:43,543][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.3020, 0.0244, 0.0208, 0.0319, 0.0237, 0.0097, 0.0109, 0.1336, 0.0332,
        0.0527, 0.0270, 0.0270, 0.0361, 0.0847, 0.1276, 0.0546],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,544][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0985, 0.0226, 0.0127, 0.0323, 0.0137, 0.0068, 0.0124, 0.1449, 0.0362,
        0.0425, 0.0195, 0.0189, 0.0538, 0.1851, 0.2247, 0.0755],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,547][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.0781, 0.0295, 0.0381, 0.0689, 0.0644, 0.0389, 0.0511, 0.0977, 0.0428,
        0.0790, 0.0513, 0.0471, 0.0856, 0.0504, 0.1054, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,550][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.5317, 0.0223, 0.0133, 0.0430, 0.0243, 0.0132, 0.0134, 0.0595, 0.0272,
        0.0272, 0.0232, 0.0244, 0.0343, 0.0346, 0.0580, 0.0505],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,555][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.7595, 0.0362, 0.0074, 0.0181, 0.0358, 0.0110, 0.0067, 0.0242, 0.0148,
        0.0153, 0.0086, 0.0116, 0.0121, 0.0099, 0.0144, 0.0144],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,559][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.1508, 0.0142, 0.0119, 0.0503, 0.0176, 0.0126, 0.0149, 0.0976, 0.0429,
        0.0657, 0.0266, 0.0289, 0.0713, 0.1211, 0.1729, 0.1008],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,564][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.5923, 0.0308, 0.0092, 0.0522, 0.0233, 0.0217, 0.0112, 0.0473, 0.0155,
        0.0334, 0.0238, 0.0352, 0.0312, 0.0071, 0.0152, 0.0506],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,569][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.1293, 0.0251, 0.0227, 0.0588, 0.0248, 0.0204, 0.0293, 0.0729, 0.0369,
        0.0625, 0.0295, 0.0496, 0.0855, 0.0870, 0.1475, 0.1182],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,573][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.5082, 0.0064, 0.0056, 0.0367, 0.0050, 0.0024, 0.0036, 0.0526, 0.0185,
        0.0158, 0.0108, 0.0178, 0.0350, 0.0667, 0.1074, 0.1074],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,574][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0007, 0.0455, 0.0713, 0.0404, 0.0405, 0.0955, 0.2653, 0.0874, 0.0253,
        0.0481, 0.0497, 0.0475, 0.0456, 0.0497, 0.0334, 0.0541],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,575][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.2265, 0.0387, 0.0253, 0.0518, 0.0459, 0.0351, 0.0268, 0.0856, 0.0650,
        0.0505, 0.0307, 0.0453, 0.0514, 0.0767, 0.1061, 0.0386],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,576][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0981, 0.0206, 0.0278, 0.0408, 0.0142, 0.0067, 0.0156, 0.0985, 0.0440,
        0.0374, 0.0192, 0.0260, 0.0313, 0.1570, 0.2675, 0.0952],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:43,578][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.2886, 0.0135, 0.0142, 0.0387, 0.0161, 0.0080, 0.0074, 0.0890, 0.0244,
        0.0445, 0.0218, 0.0220, 0.0359, 0.0722, 0.1040, 0.0944, 0.1052],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,581][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0813, 0.0196, 0.0092, 0.0363, 0.0081, 0.0042, 0.0063, 0.1035, 0.0256,
        0.0282, 0.0159, 0.0170, 0.0555, 0.1993, 0.2084, 0.1057, 0.0759],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,586][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0813, 0.0234, 0.0267, 0.0574, 0.0555, 0.0318, 0.0395, 0.0862, 0.0325,
        0.0663, 0.0450, 0.0385, 0.0693, 0.0427, 0.0972, 0.0660, 0.1407],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,591][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.4905, 0.0187, 0.0132, 0.0409, 0.0196, 0.0108, 0.0098, 0.0536, 0.0216,
        0.0244, 0.0198, 0.0230, 0.0350, 0.0375, 0.0573, 0.0633, 0.0609],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,595][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.7326, 0.0336, 0.0075, 0.0179, 0.0299, 0.0099, 0.0074, 0.0312, 0.0125,
        0.0147, 0.0118, 0.0143, 0.0133, 0.0115, 0.0168, 0.0164, 0.0188],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,600][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1494, 0.0109, 0.0098, 0.0432, 0.0155, 0.0095, 0.0108, 0.0804, 0.0377,
        0.0528, 0.0205, 0.0240, 0.0559, 0.1005, 0.1517, 0.0933, 0.1340],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,605][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.6775, 0.0306, 0.0097, 0.0366, 0.0131, 0.0126, 0.0089, 0.0313, 0.0093,
        0.0226, 0.0147, 0.0251, 0.0210, 0.0064, 0.0113, 0.0297, 0.0397],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,606][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0930, 0.0149, 0.0176, 0.0541, 0.0163, 0.0152, 0.0174, 0.0586, 0.0303,
        0.0527, 0.0228, 0.0430, 0.0821, 0.0869, 0.1191, 0.1274, 0.1486],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,607][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.5062, 0.0037, 0.0031, 0.0394, 0.0038, 0.0014, 0.0020, 0.0410, 0.0146,
        0.0113, 0.0081, 0.0119, 0.0223, 0.0398, 0.0862, 0.1176, 0.0876],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,608][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0002, 0.0484, 0.0715, 0.0742, 0.0495, 0.0674, 0.1901, 0.0575, 0.0262,
        0.0559, 0.0443, 0.0378, 0.0478, 0.0725, 0.0525, 0.0689, 0.0352],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,610][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2639, 0.0337, 0.0294, 0.0629, 0.0516, 0.0320, 0.0209, 0.0726, 0.0521,
        0.0482, 0.0286, 0.0382, 0.0395, 0.0777, 0.0808, 0.0354, 0.0328],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,613][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0982, 0.0109, 0.0131, 0.0404, 0.0075, 0.0035, 0.0061, 0.0844, 0.0307,
        0.0268, 0.0142, 0.0245, 0.0274, 0.1315, 0.2054, 0.1730, 0.1023],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:43,618][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.1640, 0.0114, 0.0130, 0.0333, 0.0142, 0.0084, 0.0093, 0.0908, 0.0366,
        0.0481, 0.0198, 0.0303, 0.0411, 0.0745, 0.1361, 0.0712, 0.1254, 0.0724],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,622][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0783, 0.0147, 0.0102, 0.0440, 0.0075, 0.0043, 0.0063, 0.0839, 0.0231,
        0.0271, 0.0115, 0.0218, 0.0461, 0.1905, 0.1468, 0.1038, 0.1034, 0.0768],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,627][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0665, 0.0228, 0.0278, 0.0577, 0.0558, 0.0355, 0.0353, 0.0679, 0.0273,
        0.0661, 0.0385, 0.0439, 0.0739, 0.0405, 0.0866, 0.0671, 0.1358, 0.0512],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,631][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.4104, 0.0182, 0.0140, 0.0387, 0.0240, 0.0134, 0.0121, 0.0572, 0.0278,
        0.0273, 0.0224, 0.0290, 0.0346, 0.0313, 0.0540, 0.0611, 0.0621, 0.0626],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,636][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.6217, 0.0426, 0.0098, 0.0216, 0.0349, 0.0126, 0.0158, 0.0474, 0.0155,
        0.0190, 0.0196, 0.0160, 0.0157, 0.0179, 0.0210, 0.0218, 0.0230, 0.0243],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,638][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.1147, 0.0130, 0.0112, 0.0378, 0.0134, 0.0108, 0.0128, 0.0811, 0.0365,
        0.0443, 0.0254, 0.0260, 0.0537, 0.0916, 0.1532, 0.0691, 0.1162, 0.0889],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,638][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.5026, 0.0382, 0.0110, 0.0376, 0.0231, 0.0205, 0.0107, 0.0427, 0.0151,
        0.0253, 0.0232, 0.0341, 0.0326, 0.0093, 0.0215, 0.0411, 0.0396, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,639][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.1671, 0.0145, 0.0162, 0.0485, 0.0134, 0.0150, 0.0178, 0.0655, 0.0262,
        0.0439, 0.0266, 0.0377, 0.0639, 0.0635, 0.1057, 0.0982, 0.1200, 0.0565],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,640][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.3845, 0.0052, 0.0048, 0.0395, 0.0058, 0.0021, 0.0033, 0.0439, 0.0241,
        0.0164, 0.0097, 0.0146, 0.0308, 0.0378, 0.0976, 0.1026, 0.1218, 0.0554],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,643][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0003, 0.0217, 0.1863, 0.1368, 0.0316, 0.0390, 0.1306, 0.0274, 0.0313,
        0.0559, 0.0188, 0.0386, 0.0460, 0.0359, 0.0497, 0.0764, 0.0511, 0.0226],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,647][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.1534, 0.0430, 0.0308, 0.0640, 0.0451, 0.0346, 0.0300, 0.0690, 0.0503,
        0.0481, 0.0284, 0.0508, 0.0473, 0.0729, 0.0994, 0.0496, 0.0383, 0.0449],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,651][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0861, 0.0133, 0.0172, 0.0351, 0.0064, 0.0061, 0.0071, 0.0584, 0.0325,
        0.0334, 0.0151, 0.0281, 0.0357, 0.1035, 0.1715, 0.1368, 0.1164, 0.0974],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:43,656][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.2531, 0.0154, 0.0091, 0.0307, 0.0094, 0.0049, 0.0049, 0.0671, 0.0195,
        0.0316, 0.0127, 0.0164, 0.0233, 0.0560, 0.0747, 0.0686, 0.0888, 0.0959,
        0.1179], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,661][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0667, 0.0158, 0.0059, 0.0276, 0.0053, 0.0035, 0.0045, 0.0789, 0.0159,
        0.0194, 0.0100, 0.0097, 0.0338, 0.1669, 0.1314, 0.0895, 0.0807, 0.1095,
        0.1249], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,665][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0818, 0.0213, 0.0258, 0.0622, 0.0443, 0.0281, 0.0293, 0.0650, 0.0267,
        0.0552, 0.0380, 0.0384, 0.0660, 0.0385, 0.0849, 0.0644, 0.1273, 0.0497,
        0.0532], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,670][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.4593, 0.0146, 0.0087, 0.0352, 0.0131, 0.0077, 0.0067, 0.0494, 0.0158,
        0.0216, 0.0164, 0.0211, 0.0292, 0.0280, 0.0432, 0.0600, 0.0550, 0.0551,
        0.0601], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,671][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.7944, 0.0222, 0.0054, 0.0155, 0.0156, 0.0069, 0.0066, 0.0176, 0.0087,
        0.0099, 0.0083, 0.0090, 0.0095, 0.0091, 0.0089, 0.0119, 0.0127, 0.0178,
        0.0102], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,671][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.1337, 0.0093, 0.0067, 0.0401, 0.0126, 0.0069, 0.0076, 0.0663, 0.0294,
        0.0381, 0.0168, 0.0168, 0.0443, 0.0657, 0.1103, 0.0744, 0.1170, 0.0690,
        0.1348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,672][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.5978, 0.0256, 0.0095, 0.0307, 0.0161, 0.0132, 0.0065, 0.0286, 0.0109,
        0.0217, 0.0181, 0.0292, 0.0235, 0.0083, 0.0145, 0.0314, 0.0327, 0.0431,
        0.0387], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,675][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.1098, 0.0123, 0.0098, 0.0457, 0.0131, 0.0110, 0.0111, 0.0405, 0.0260,
        0.0481, 0.0178, 0.0309, 0.0683, 0.0606, 0.0682, 0.1121, 0.1327, 0.0583,
        0.1238], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,679][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.5141, 0.0026, 0.0016, 0.0174, 0.0020, 0.0007, 0.0009, 0.0207, 0.0084,
        0.0053, 0.0052, 0.0055, 0.0124, 0.0267, 0.0462, 0.0617, 0.0570, 0.0409,
        0.1707], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,683][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0002, 0.0234, 0.0508, 0.1399, 0.0583, 0.0425, 0.0782, 0.0436, 0.0331,
        0.0750, 0.0273, 0.0515, 0.0673, 0.0686, 0.0303, 0.0791, 0.0610, 0.0345,
        0.0356], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,688][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.2331, 0.0310, 0.0228, 0.0484, 0.0375, 0.0251, 0.0190, 0.0634, 0.0553,
        0.0394, 0.0299, 0.0423, 0.0455, 0.0675, 0.0876, 0.0400, 0.0404, 0.0338,
        0.0380], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,693][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.1440, 0.0105, 0.0051, 0.0325, 0.0066, 0.0029, 0.0042, 0.0590, 0.0202,
        0.0198, 0.0126, 0.0144, 0.0209, 0.0654, 0.1149, 0.1066, 0.0867, 0.0968,
        0.1768], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:43,697][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.5167, 0.0107, 0.0063, 0.0096, 0.0074, 0.0021, 0.0021, 0.0236, 0.0062,
        0.0103, 0.0076, 0.0061, 0.0083, 0.0132, 0.0168, 0.0195, 0.0294, 0.0443,
        0.0571, 0.2027], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,702][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.1527, 0.0106, 0.0075, 0.0125, 0.0054, 0.0024, 0.0032, 0.0436, 0.0135,
        0.0156, 0.0077, 0.0078, 0.0215, 0.0497, 0.0487, 0.0531, 0.0477, 0.0460,
        0.1218, 0.3290], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,703][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.0951, 0.0236, 0.0260, 0.0499, 0.0493, 0.0271, 0.0290, 0.0580, 0.0262,
        0.0498, 0.0338, 0.0306, 0.0541, 0.0309, 0.0572, 0.0527, 0.1000, 0.0442,
        0.0473, 0.1153], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,704][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.6679, 0.0152, 0.0064, 0.0147, 0.0098, 0.0044, 0.0039, 0.0221, 0.0086,
        0.0089, 0.0097, 0.0105, 0.0112, 0.0094, 0.0149, 0.0256, 0.0200, 0.0332,
        0.0324, 0.0712], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,705][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.8271, 0.0258, 0.0042, 0.0094, 0.0162, 0.0054, 0.0043, 0.0146, 0.0061,
        0.0072, 0.0066, 0.0060, 0.0055, 0.0049, 0.0058, 0.0084, 0.0088, 0.0105,
        0.0069, 0.0161], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,707][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.2408, 0.0099, 0.0053, 0.0218, 0.0089, 0.0044, 0.0041, 0.0318, 0.0126,
        0.0203, 0.0104, 0.0106, 0.0201, 0.0261, 0.0394, 0.0369, 0.0453, 0.0514,
        0.0781, 0.3221], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,711][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.5656, 0.0218, 0.0074, 0.0358, 0.0166, 0.0141, 0.0060, 0.0268, 0.0094,
        0.0218, 0.0159, 0.0222, 0.0172, 0.0045, 0.0075, 0.0301, 0.0249, 0.0311,
        0.0308, 0.0906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,716][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.1817, 0.0158, 0.0117, 0.0288, 0.0127, 0.0088, 0.0110, 0.0312, 0.0142,
        0.0261, 0.0158, 0.0214, 0.0389, 0.0328, 0.0378, 0.0711, 0.0737, 0.0410,
        0.0946, 0.2310], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,718][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([5.1183e-01, 1.3457e-03, 9.1076e-04, 5.5812e-03, 8.1062e-04, 2.2136e-04,
        3.5434e-04, 6.2808e-03, 1.8774e-03, 1.5672e-03, 1.8693e-03, 1.9828e-03,
        3.8184e-03, 4.7429e-03, 6.8538e-03, 1.7809e-02, 2.0961e-02, 1.2719e-02,
        5.2922e-02, 3.4554e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,723][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0002, 0.0180, 0.1267, 0.0456, 0.0335, 0.1102, 0.1528, 0.0442, 0.0340,
        0.0524, 0.0276, 0.0389, 0.0393, 0.0272, 0.0263, 0.0556, 0.0541, 0.0189,
        0.0614, 0.0330], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,728][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.1989, 0.0319, 0.0200, 0.0482, 0.0486, 0.0305, 0.0247, 0.0697, 0.0479,
        0.0408, 0.0296, 0.0355, 0.0378, 0.0591, 0.0820, 0.0346, 0.0351, 0.0338,
        0.0333, 0.0578], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,732][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.1877, 0.0078, 0.0050, 0.0117, 0.0039, 0.0016, 0.0026, 0.0256, 0.0074,
        0.0092, 0.0072, 0.0084, 0.0089, 0.0302, 0.0396, 0.0380, 0.0334, 0.0534,
        0.1019, 0.4164], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:43,734][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.2735, 0.0087, 0.0061, 0.0143, 0.0066, 0.0030, 0.0026, 0.0270, 0.0074,
        0.0151, 0.0082, 0.0078, 0.0117, 0.0215, 0.0277, 0.0316, 0.0342, 0.0462,
        0.0651, 0.3048, 0.0771], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,735][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0786, 0.0114, 0.0039, 0.0140, 0.0033, 0.0019, 0.0023, 0.0358, 0.0093,
        0.0110, 0.0071, 0.0068, 0.0213, 0.0661, 0.0636, 0.0438, 0.0281, 0.0611,
        0.0784, 0.3839, 0.0683], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,736][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0746, 0.0187, 0.0205, 0.0435, 0.0437, 0.0246, 0.0283, 0.0562, 0.0228,
        0.0461, 0.0317, 0.0272, 0.0470, 0.0290, 0.0634, 0.0470, 0.0930, 0.0389,
        0.0362, 0.0970, 0.1107], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,738][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.4660, 0.0135, 0.0079, 0.0231, 0.0111, 0.0058, 0.0050, 0.0266, 0.0107,
        0.0127, 0.0111, 0.0125, 0.0179, 0.0174, 0.0265, 0.0345, 0.0303, 0.0408,
        0.0460, 0.1327, 0.0476], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,741][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.7545, 0.0273, 0.0062, 0.0130, 0.0219, 0.0067, 0.0053, 0.0209, 0.0088,
        0.0103, 0.0081, 0.0097, 0.0089, 0.0077, 0.0103, 0.0110, 0.0123, 0.0149,
        0.0096, 0.0198, 0.0129], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,746][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1086, 0.0063, 0.0046, 0.0194, 0.0072, 0.0042, 0.0043, 0.0295, 0.0143,
        0.0210, 0.0087, 0.0103, 0.0223, 0.0362, 0.0498, 0.0371, 0.0525, 0.0441,
        0.0833, 0.3322, 0.1041], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,750][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.6260, 0.0213, 0.0068, 0.0267, 0.0099, 0.0085, 0.0057, 0.0213, 0.0063,
        0.0165, 0.0110, 0.0187, 0.0151, 0.0045, 0.0076, 0.0220, 0.0299, 0.0272,
        0.0250, 0.0533, 0.0366], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,755][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0840, 0.0103, 0.0095, 0.0283, 0.0088, 0.0078, 0.0088, 0.0271, 0.0143,
        0.0245, 0.0126, 0.0208, 0.0385, 0.0365, 0.0438, 0.0622, 0.0684, 0.0362,
        0.0890, 0.2434, 0.1251], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,758][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([2.9507e-01, 1.1889e-03, 6.6124e-04, 8.2638e-03, 7.1543e-04, 2.7562e-04,
        3.2844e-04, 6.5626e-03, 2.1760e-03, 1.8208e-03, 1.7281e-03, 2.2931e-03,
        3.7727e-03, 5.3429e-03, 1.2061e-02, 2.1404e-02, 1.4621e-02, 1.1737e-02,
        4.2517e-02, 5.2888e-01, 3.8578e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,761][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.7781e-04, 3.7776e-02, 6.6353e-02, 6.6925e-02, 4.2863e-02, 6.3236e-02,
        1.8127e-01, 4.3899e-02, 2.2545e-02, 4.4027e-02, 3.2031e-02, 2.8893e-02,
        3.5490e-02, 4.8593e-02, 3.8968e-02, 5.1455e-02, 2.7763e-02, 2.3465e-02,
        4.7346e-02, 5.2373e-02, 4.4549e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,766][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3030, 0.0240, 0.0198, 0.0483, 0.0390, 0.0252, 0.0152, 0.0551, 0.0367,
        0.0368, 0.0231, 0.0288, 0.0295, 0.0581, 0.0642, 0.0282, 0.0260, 0.0276,
        0.0297, 0.0526, 0.0291], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,767][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0774, 0.0054, 0.0042, 0.0125, 0.0025, 0.0011, 0.0017, 0.0217, 0.0076,
        0.0073, 0.0048, 0.0075, 0.0071, 0.0282, 0.0399, 0.0469, 0.0259, 0.0359,
        0.0985, 0.4885, 0.0753], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:43,770][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:43,774][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[1482],
        [1148],
        [  92],
        [  24],
        [  42],
        [ 230],
        [ 252],
        [ 813],
        [1091],
        [ 428],
        [ 963],
        [ 288],
        [ 568],
        [2514],
        [4634],
        [ 292],
        [ 131],
        [ 966],
        [ 440],
        [ 144],
        [  32]], device='cuda:0')
[2024-07-23 21:06:43,777][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 2592],
        [ 3979],
        [  663],
        [  324],
        [  305],
        [ 1318],
        [ 1168],
        [ 3980],
        [ 5239],
        [ 1797],
        [ 3521],
        [ 1335],
        [ 2485],
        [ 7465],
        [11902],
        [ 1063],
        [  581],
        [ 3175],
        [ 1519],
        [  776],
        [  150]], device='cuda:0')
[2024-07-23 21:06:43,780][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[3093],
        [2142],
        [2390],
        [1975],
        [1890],
        [1742],
        [1780],
        [1730],
        [1710],
        [1691],
        [1654],
        [1598],
        [1544],
        [1512],
        [1550],
        [1538],
        [1491],
        [1473],
        [1496],
        [1463],
        [1422]], device='cuda:0')
[2024-07-23 21:06:43,784][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[23591],
        [42612],
        [31600],
        [33539],
        [38443],
        [39054],
        [39554],
        [40853],
        [40356],
        [40156],
        [39821],
        [39627],
        [39303],
        [37303],
        [38062],
        [39209],
        [37199],
        [35418],
        [31015],
        [30901],
        [32673]], device='cuda:0')
[2024-07-23 21:06:43,787][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[11829],
        [11944],
        [10782],
        [10810],
        [11916],
        [12514],
        [12851],
        [13059],
        [12920],
        [13141],
        [13308],
        [13275],
        [13430],
        [13123],
        [13130],
        [13095],
        [13169],
        [13272],
        [13186],
        [13112],
        [13117]], device='cuda:0')
[2024-07-23 21:06:43,790][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 582],
        [ 712],
        [ 806],
        [ 913],
        [ 886],
        [ 880],
        [ 889],
        [ 933],
        [ 965],
        [ 985],
        [1009],
        [1005],
        [ 981],
        [ 965],
        [ 935],
        [ 916],
        [ 906],
        [ 914],
        [ 906],
        [ 911],
        [ 891]], device='cuda:0')
[2024-07-23 21:06:43,793][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[6922],
        [2434],
        [3800],
        [1155],
        [ 507],
        [2813],
        [4492],
        [3458],
        [4796],
        [4839],
        [5342],
        [5124],
        [4402],
        [7156],
        [6642],
        [7198],
        [6415],
        [6023],
        [6570],
        [3243],
        [4941]], device='cuda:0')
[2024-07-23 21:06:43,796][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[23331],
        [ 2374],
        [ 2694],
        [ 7385],
        [10831],
        [10640],
        [18277],
        [11450],
        [11973],
        [13781],
        [14328],
        [14229],
        [14816],
        [28302],
        [27667],
        [29330],
        [29711],
        [26941],
        [26213],
        [24634],
        [27138]], device='cuda:0')
[2024-07-23 21:06:43,800][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[6647],
        [1258],
        [1214],
        [1244],
        [1510],
        [1601],
        [1678],
        [1636],
        [1777],
        [1810],
        [1925],
        [1884],
        [1907],
        [1980],
        [2005],
        [1954],
        [2018],
        [1961],
        [1911],
        [1885],
        [1959]], device='cuda:0')
[2024-07-23 21:06:43,803][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[18790],
        [32766],
        [35188],
        [32018],
        [30046],
        [31531],
        [32169],
        [36709],
        [38226],
        [38543],
        [38268],
        [37990],
        [36991],
        [35799],
        [33478],
        [34512],
        [34501],
        [35122],
        [35747],
        [34088],
        [34126]], device='cuda:0')
[2024-07-23 21:06:43,804][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 1256],
        [ 1488],
        [ 1660],
        [ 1897],
        [ 2220],
        [ 5006],
        [ 3090],
        [ 5066],
        [ 5548],
        [ 2953],
        [ 6607],
        [ 6466],
        [ 6048],
        [ 5942],
        [12340],
        [ 8126],
        [ 7367],
        [ 9756],
        [ 7925],
        [ 4255],
        [ 6491]], device='cuda:0')
[2024-07-23 21:06:43,806][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[19202],
        [44332],
        [41303],
        [39622],
        [42503],
        [41218],
        [40791],
        [41040],
        [41723],
        [44130],
        [44120],
        [43665],
        [43709],
        [41401],
        [41341],
        [42716],
        [41066],
        [39887],
        [38589],
        [40259],
        [40452]], device='cuda:0')
[2024-07-23 21:06:43,808][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[4563],
        [3481],
        [2978],
        [2739],
        [2693],
        [2465],
        [2445],
        [2408],
        [2404],
        [2554],
        [2530],
        [2409],
        [2445],
        [2503],
        [2506],
        [2469],
        [2506],
        [2515],
        [2544],
        [2532],
        [2527]], device='cuda:0')
[2024-07-23 21:06:43,811][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[15053],
        [ 6331],
        [ 5816],
        [ 4554],
        [ 4194],
        [ 4365],
        [ 4368],
        [ 4459],
        [ 4779],
        [ 5213],
        [ 5541],
        [ 5492],
        [ 5628],
        [ 5722],
        [ 5759],
        [ 6092],
        [ 6040],
        [ 5700],
        [ 5528],
        [ 5239],
        [ 5162]], device='cuda:0')
[2024-07-23 21:06:43,814][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[2292],
        [2357],
        [2176],
        [1848],
        [2143],
        [1476],
        [1614],
        [1525],
        [ 558],
        [ 556],
        [ 650],
        [1242],
        [ 826],
        [ 475],
        [ 426],
        [ 914],
        [1129],
        [ 799],
        [1338],
        [1085],
        [1640]], device='cuda:0')
[2024-07-23 21:06:43,817][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[30941],
        [31832],
        [26412],
        [26345],
        [24924],
        [24472],
        [24909],
        [29925],
        [29502],
        [27570],
        [26650],
        [27555],
        [27282],
        [24861],
        [19458],
        [20550],
        [19335],
        [21618],
        [23088],
        [20461],
        [17020]], device='cuda:0')
[2024-07-23 21:06:43,820][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[16409],
        [ 3615],
        [ 1497],
        [ 2858],
        [ 3250],
        [ 2002],
        [ 1207],
        [  427],
        [  379],
        [  318],
        [  356],
        [  347],
        [  271],
        [  583],
        [ 1029],
        [  787],
        [  958],
        [  837],
        [  944],
        [  319],
        [  343]], device='cuda:0')
[2024-07-23 21:06:43,824][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[  45],
        [1253],
        [1810],
        [3938],
        [3384],
        [4902],
        [6120],
        [8280],
        [7870],
        [6677],
        [7712],
        [7788],
        [8824],
        [8624],
        [8334],
        [8580],
        [8947],
        [8810],
        [8619],
        [7752],
        [8206]], device='cuda:0')
[2024-07-23 21:06:43,827][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[2519],
        [ 492],
        [ 982],
        [ 454],
        [ 649],
        [1345],
        [ 543],
        [ 742],
        [ 430],
        [ 611],
        [ 970],
        [ 983],
        [ 890],
        [ 747],
        [ 755],
        [ 639],
        [ 641],
        [ 767],
        [ 689],
        [ 353],
        [ 611]], device='cuda:0')
[2024-07-23 21:06:43,830][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[41863],
        [41562],
        [41850],
        [41747],
        [43358],
        [43215],
        [42869],
        [40379],
        [42860],
        [42756],
        [42023],
        [39620],
        [41671],
        [42826],
        [38070],
        [41735],
        [40840],
        [36948],
        [41980],
        [41853],
        [41026]], device='cuda:0')
[2024-07-23 21:06:43,833][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[13539],
        [12357],
        [11649],
        [ 5927],
        [10116],
        [10803],
        [10259],
        [10436],
        [13196],
        [13615],
        [14266],
        [13314],
        [11372],
        [11672],
        [10331],
        [ 9376],
        [ 9160],
        [ 8916],
        [ 7976],
        [ 4245],
        [ 4879]], device='cuda:0')
[2024-07-23 21:06:43,836][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[18654],
        [20667],
        [17132],
        [19981],
        [20615],
        [21299],
        [18130],
        [20394],
        [16589],
        [19375],
        [19318],
        [19738],
        [18675],
        [18200],
        [18353],
        [18059],
        [17637],
        [18992],
        [20005],
        [21805],
        [20679]], device='cuda:0')
[2024-07-23 21:06:43,839][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[20435],
        [23828],
        [14730],
        [18224],
        [24876],
        [25145],
        [29300],
        [35286],
        [35839],
        [34975],
        [34046],
        [33860],
        [31906],
        [31601],
        [29014],
        [29138],
        [26853],
        [28144],
        [27674],
        [28876],
        [26793]], device='cuda:0')
[2024-07-23 21:06:43,841][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[29547],
        [27914],
        [27523],
        [19440],
        [15102],
        [ 3638],
        [ 9042],
        [ 5602],
        [ 4917],
        [10661],
        [ 2613],
        [ 2825],
        [ 2866],
        [ 1892],
        [  668],
        [ 1007],
        [ 1157],
        [  887],
        [ 1756],
        [ 1176],
        [  507]], device='cuda:0')
[2024-07-23 21:06:43,843][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 6442],
        [33287],
        [24709],
        [22378],
        [27910],
        [26973],
        [28360],
        [26279],
        [26447],
        [24939],
        [23045],
        [22044],
        [21760],
        [22977],
        [23468],
        [21296],
        [21846],
        [21595],
        [23339],
        [22742],
        [22051]], device='cuda:0')
[2024-07-23 21:06:43,845][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 2483],
        [13614],
        [ 6037],
        [10399],
        [17534],
        [17406],
        [16615],
        [15169],
        [13578],
        [13798],
        [12811],
        [12635],
        [11805],
        [10290],
        [10149],
        [ 9421],
        [ 9365],
        [ 8574],
        [ 8224],
        [ 8292],
        [ 8427]], device='cuda:0')
[2024-07-23 21:06:43,847][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[12669],
        [14691],
        [15193],
        [14985],
        [12045],
        [ 6560],
        [ 6100],
        [ 6283],
        [ 3859],
        [ 4401],
        [ 3586],
        [ 3243],
        [ 2410],
        [ 1182],
        [ 1085],
        [ 1240],
        [ 1221],
        [ 1791],
        [ 3407],
        [ 4166],
        [ 3150]], device='cuda:0')
[2024-07-23 21:06:43,850][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[27210],
        [19871],
        [29094],
        [30465],
        [23786],
        [27051],
        [28093],
        [26066],
        [29276],
        [27340],
        [29862],
        [30233],
        [32918],
        [34547],
        [35890],
        [37188],
        [37796],
        [36374],
        [34683],
        [39524],
        [39704]], device='cuda:0')
[2024-07-23 21:06:43,852][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[46936],
        [39557],
        [43637],
        [42068],
        [41224],
        [43147],
        [41660],
        [42314],
        [45468],
        [45190],
        [45526],
        [43574],
        [44669],
        [45570],
        [46120],
        [45050],
        [44175],
        [44729],
        [43554],
        [44568],
        [44336]], device='cuda:0')
[2024-07-23 21:06:43,855][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796],
        [4796]], device='cuda:0')
[2024-07-23 21:06:43,935][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:43,936][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,936][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,937][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,938][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,938][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,939][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,940][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,941][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,941][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,942][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,943][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,943][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:43,944][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.6272, 0.3728], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,945][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0547, 0.9453], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,945][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0018, 0.9982], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,950][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.9166, 0.0834], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,955][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.5734, 0.4266], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,960][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.9283, 0.0717], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,965][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.7669, 0.2331], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,971][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.1559, 0.8441], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,972][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.8708, 0.1292], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,972][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.3102, 0.6898], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,973][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0074, 0.9926], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,974][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.2928, 0.7072], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:43,976][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.5194, 0.2009, 0.2797], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,981][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.6430, 0.3062, 0.0508], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,985][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0009, 0.3476, 0.6514], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,990][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.8824, 0.0707, 0.0469], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:43,996][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.4499, 0.2075, 0.3426], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,001][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.4996, 0.0429, 0.4575], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,005][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.6274, 0.1925, 0.1801], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,005][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0789, 0.3240, 0.5971], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,006][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.8262, 0.0819, 0.0919], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,007][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.1167, 0.3773, 0.5060], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,007][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0039, 0.3309, 0.6653], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,010][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.3678, 0.2888, 0.3434], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,014][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.4852, 0.1005, 0.1094, 0.3049], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,019][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0323, 0.7715, 0.1320, 0.0643], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,022][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ by] are: tensor([5.7696e-04, 2.7924e-01, 6.2388e-01, 9.6306e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,027][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.7840, 0.0736, 0.0435, 0.0989], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,032][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.3031, 0.0931, 0.1400, 0.4637], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,037][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.3558, 0.0289, 0.2228, 0.3925], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,039][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.5171, 0.1659, 0.1591, 0.1579], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,039][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0115, 0.2849, 0.4814, 0.2223], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,040][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.7354, 0.0824, 0.1067, 0.0756], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,041][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.1104, 0.2548, 0.3437, 0.2912], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,041][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0018, 0.1791, 0.5520, 0.2671], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,045][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.2633, 0.2294, 0.2718, 0.2355], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,049][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.1821, 0.0841, 0.1245, 0.4400, 0.1692], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,054][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0196, 0.5185, 0.0973, 0.1211, 0.2434], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,058][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([3.7845e-04, 1.6370e-01, 2.8286e-01, 2.8559e-02, 5.2450e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,063][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.7795, 0.0563, 0.0282, 0.0852, 0.0508], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,069][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.2034, 0.0632, 0.0980, 0.5084, 0.1271], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,072][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.2763, 0.0162, 0.1621, 0.3509, 0.1945], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,073][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.4228, 0.1568, 0.1460, 0.1503, 0.1240], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,073][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0324, 0.2044, 0.3063, 0.2008, 0.2562], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,074][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.6208, 0.0743, 0.0966, 0.1065, 0.1018], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,075][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0958, 0.1790, 0.2734, 0.2153, 0.2365], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,078][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.0009, 0.1215, 0.4738, 0.1953, 0.2085], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,082][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.1697, 0.2001, 0.1935, 0.1735, 0.2633], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,087][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1373, 0.0811, 0.1272, 0.4386, 0.1283, 0.0875], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,093][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0075, 0.3720, 0.0421, 0.0344, 0.2274, 0.3166], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,096][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ B] are: tensor([3.0980e-04, 8.7892e-02, 2.7094e-01, 6.3770e-02, 3.6377e-01, 2.1333e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,102][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.7211, 0.0602, 0.0319, 0.1120, 0.0398, 0.0349], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,105][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.1319, 0.0563, 0.0789, 0.5197, 0.0884, 0.1248], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,106][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.2069, 0.0178, 0.1724, 0.2692, 0.1999, 0.1338], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,107][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.4118, 0.1356, 0.1252, 0.1284, 0.1061, 0.0929], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,107][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0049, 0.1016, 0.2450, 0.2068, 0.2137, 0.2280], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,108][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.6832, 0.0393, 0.0587, 0.0780, 0.0659, 0.0748], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,111][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0590, 0.1827, 0.2139, 0.1723, 0.1879, 0.1842], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,116][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0010, 0.0939, 0.3426, 0.2211, 0.2577, 0.0837], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,121][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.1192, 0.1573, 0.1761, 0.1625, 0.1947, 0.1902], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,127][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.1663, 0.0662, 0.0929, 0.4048, 0.1247, 0.0776, 0.0676],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,131][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0481, 0.3869, 0.0287, 0.0655, 0.1393, 0.2256, 0.1058],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,136][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0004, 0.1743, 0.2660, 0.0582, 0.1716, 0.2280, 0.1015],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,139][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.5086, 0.0638, 0.0382, 0.1413, 0.0481, 0.0508, 0.1492],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,140][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.0690, 0.0430, 0.0994, 0.4692, 0.0689, 0.1051, 0.1454],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,140][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.1325, 0.0130, 0.1364, 0.3091, 0.1378, 0.1156, 0.1556],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,141][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.3218, 0.1352, 0.1204, 0.1251, 0.1007, 0.0929, 0.1037],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,143][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0038, 0.0929, 0.2897, 0.2074, 0.1648, 0.1929, 0.0486],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,147][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.6390, 0.0357, 0.0502, 0.0712, 0.0708, 0.0747, 0.0582],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,152][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0228, 0.1380, 0.2170, 0.1830, 0.1598, 0.1952, 0.0842],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,157][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0007, 0.0795, 0.2260, 0.1670, 0.0571, 0.3224, 0.1474],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,163][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0927, 0.1382, 0.1475, 0.1268, 0.1375, 0.1464, 0.2110],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,169][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.2121, 0.0405, 0.0406, 0.1715, 0.0768, 0.0486, 0.0408, 0.3690],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,172][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.1074, 0.1102, 0.0371, 0.0533, 0.2006, 0.1018, 0.0696, 0.3200],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,173][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ista] are: tensor([6.2846e-05, 1.3250e-01, 2.9300e-01, 4.7196e-02, 1.5834e-01, 2.0023e-01,
        4.8011e-02, 1.2066e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,173][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.3683, 0.0502, 0.0341, 0.1177, 0.0555, 0.0453, 0.0645, 0.2644],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,174][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0637, 0.0306, 0.0492, 0.3342, 0.0715, 0.0655, 0.0632, 0.3220],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,175][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.1062, 0.0127, 0.1226, 0.2363, 0.1217, 0.1084, 0.1576, 0.1344],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,179][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.3299, 0.1065, 0.1000, 0.1049, 0.0913, 0.0819, 0.0909, 0.0946],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,183][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0092, 0.0956, 0.1863, 0.1700, 0.1931, 0.1781, 0.0874, 0.0802],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,188][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.7034, 0.0243, 0.0318, 0.0544, 0.0501, 0.0558, 0.0392, 0.0410],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,193][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.0198, 0.1088, 0.1915, 0.1276, 0.1390, 0.1753, 0.0644, 0.1737],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,196][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ista] are: tensor([5.9457e-05, 2.6979e-02, 1.7988e-01, 1.5239e-01, 5.8477e-02, 2.9787e-01,
        2.5219e-01, 3.2172e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,202][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.0872, 0.0911, 0.1109, 0.0990, 0.1392, 0.1363, 0.1698, 0.1665],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,205][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.1008, 0.0291, 0.0395, 0.1786, 0.0567, 0.0388, 0.0485, 0.3812, 0.1269],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,206][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0061, 0.1780, 0.0679, 0.0369, 0.0841, 0.1841, 0.0563, 0.3046, 0.0819],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,207][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ de] are: tensor([7.0776e-05, 3.5536e-02, 1.8304e-01, 2.5168e-02, 1.2963e-01, 1.9344e-01,
        6.8929e-02, 2.8318e-01, 8.1008e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,208][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.4126, 0.0355, 0.0177, 0.0558, 0.0304, 0.0207, 0.0632, 0.1850, 0.1792],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,210][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0791, 0.0204, 0.0245, 0.2193, 0.0633, 0.0320, 0.0386, 0.2809, 0.2420],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,214][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0926, 0.0106, 0.0947, 0.1991, 0.1024, 0.0872, 0.1630, 0.1439, 0.1064],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,219][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.2788, 0.1018, 0.1015, 0.1042, 0.0849, 0.0741, 0.0837, 0.0888, 0.0821],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,224][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0017, 0.0727, 0.1693, 0.1211, 0.1760, 0.1816, 0.0450, 0.0853, 0.1472],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,230][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.3484, 0.0532, 0.0566, 0.0875, 0.0787, 0.0818, 0.0624, 0.0814, 0.1500],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,235][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0174, 0.1069, 0.1628, 0.1313, 0.1416, 0.1556, 0.0605, 0.1463, 0.0776],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,239][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ de] are: tensor([1.3673e-04, 1.0851e-02, 1.7415e-01, 4.7159e-02, 4.8956e-02, 3.9630e-01,
        1.5913e-01, 1.2138e-01, 4.1934e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,239][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0944, 0.0688, 0.0869, 0.0804, 0.1117, 0.1392, 0.1555, 0.1645, 0.0988],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:44,240][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0681, 0.0320, 0.0436, 0.1629, 0.0559, 0.0398, 0.0473, 0.3081, 0.1429,
        0.0993], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,241][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0296, 0.1344, 0.0305, 0.0368, 0.1017, 0.1139, 0.0838, 0.2538, 0.1367,
        0.0788], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,243][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0003, 0.0500, 0.0999, 0.0155, 0.1224, 0.1058, 0.0656, 0.2622, 0.0701,
        0.2081], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,247][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.4995, 0.0293, 0.0115, 0.0577, 0.0190, 0.0128, 0.0255, 0.1482, 0.1001,
        0.0963], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,252][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.1042, 0.0200, 0.0257, 0.1811, 0.0391, 0.0315, 0.0259, 0.1883, 0.2054,
        0.1786], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,257][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.1260, 0.0091, 0.0979, 0.1832, 0.1154, 0.0850, 0.1313, 0.1233, 0.0962,
        0.0327], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,262][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.2392, 0.0918, 0.0905, 0.0938, 0.0779, 0.0720, 0.0802, 0.0873, 0.0823,
        0.0849], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,267][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0014, 0.0541, 0.1697, 0.0934, 0.1500, 0.1013, 0.0432, 0.0759, 0.1458,
        0.1651], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,272][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.6052, 0.0198, 0.0238, 0.0342, 0.0380, 0.0372, 0.0311, 0.0354, 0.0794,
        0.0960], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,273][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0229, 0.0917, 0.1316, 0.1129, 0.1281, 0.1271, 0.0529, 0.1266, 0.0705,
        0.1357], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,274][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0005, 0.0416, 0.1089, 0.1021, 0.0513, 0.2401, 0.1558, 0.1668, 0.0633,
        0.0696], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,275][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0345, 0.0776, 0.0892, 0.0876, 0.0903, 0.0885, 0.1757, 0.1310, 0.1136,
        0.1121], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:44,277][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0531, 0.0289, 0.0429, 0.1496, 0.0589, 0.0431, 0.0500, 0.2800, 0.1400,
        0.0993, 0.0542], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,281][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0008, 0.1021, 0.0345, 0.0479, 0.1137, 0.1592, 0.0719, 0.2404, 0.0598,
        0.0788, 0.0908], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,284][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [za] are: tensor([3.4035e-05, 5.9815e-02, 1.1374e-01, 1.7153e-02, 1.1581e-01, 1.1513e-01,
        5.4650e-02, 1.3516e-01, 9.3529e-02, 1.8145e-01, 1.1353e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,289][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.2004, 0.0239, 0.0156, 0.0598, 0.0271, 0.0286, 0.0418, 0.1888, 0.1835,
        0.1671, 0.0632], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,294][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.0245, 0.0141, 0.0237, 0.1274, 0.0311, 0.0360, 0.0341, 0.1757, 0.2656,
        0.2242, 0.0436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,300][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.1081, 0.0065, 0.0972, 0.2262, 0.1122, 0.0866, 0.1176, 0.1075, 0.0950,
        0.0304, 0.0127], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,306][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.2616, 0.0797, 0.0793, 0.0812, 0.0667, 0.0616, 0.0669, 0.0744, 0.0719,
        0.0732, 0.0835], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,306][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0024, 0.0409, 0.0810, 0.0574, 0.1204, 0.1026, 0.0541, 0.0856, 0.1357,
        0.2143, 0.1056], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,307][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.4795, 0.0209, 0.0275, 0.0464, 0.0381, 0.0464, 0.0332, 0.0398, 0.1045,
        0.1394, 0.0243], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,308][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0157, 0.0584, 0.1019, 0.0825, 0.0935, 0.1154, 0.0423, 0.1126, 0.0779,
        0.1369, 0.1629], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,309][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [za] are: tensor([9.7601e-05, 1.3076e-02, 8.7778e-02, 5.4388e-02, 5.2683e-02, 1.6623e-01,
        2.8240e-01, 5.5901e-02, 1.3744e-01, 1.2842e-01, 2.1587e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,312][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0298, 0.0616, 0.0874, 0.0816, 0.0953, 0.0855, 0.1284, 0.1295, 0.1171,
        0.1029, 0.0809], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:44,317][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0951, 0.0308, 0.0377, 0.1311, 0.0440, 0.0299, 0.0407, 0.2779, 0.1006,
        0.0837, 0.0532, 0.0752], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,322][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0053, 0.0960, 0.0449, 0.0379, 0.0752, 0.0834, 0.0435, 0.2160, 0.1054,
        0.1068, 0.0957, 0.0900], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,325][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ is] are: tensor([1.1706e-05, 2.2866e-02, 6.3094e-02, 1.6673e-02, 8.5800e-02, 8.4129e-02,
        3.8997e-02, 1.0832e-01, 6.4109e-02, 1.7048e-01, 1.3356e-01, 2.1197e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,330][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.2389, 0.0285, 0.0198, 0.0720, 0.0241, 0.0207, 0.0467, 0.1432, 0.1477,
        0.1441, 0.0417, 0.0727], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,336][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0420, 0.0180, 0.0181, 0.1217, 0.0247, 0.0280, 0.0237, 0.1835, 0.1991,
        0.1929, 0.0442, 0.1042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,339][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0824, 0.0102, 0.1095, 0.1647, 0.0957, 0.0755, 0.1298, 0.1103, 0.0804,
        0.0434, 0.0187, 0.0794], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,340][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.2672, 0.0752, 0.0746, 0.0755, 0.0599, 0.0546, 0.0619, 0.0666, 0.0646,
        0.0637, 0.0706, 0.0656], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,341][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0005, 0.0392, 0.0934, 0.0465, 0.1164, 0.0566, 0.0335, 0.0705, 0.1131,
        0.1452, 0.0784, 0.2068], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,342][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1850, 0.0361, 0.0426, 0.0663, 0.0519, 0.0737, 0.0563, 0.0586, 0.1429,
        0.1823, 0.0400, 0.0643], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,344][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0116, 0.0648, 0.0950, 0.0802, 0.0821, 0.1124, 0.0366, 0.0953, 0.0634,
        0.1323, 0.1537, 0.0727], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,347][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ is] are: tensor([6.7286e-05, 2.2577e-02, 9.3201e-02, 1.0876e-01, 4.9716e-02, 1.5167e-01,
        2.2207e-01, 7.0077e-02, 1.0200e-01, 9.0081e-02, 2.9602e-02, 6.0183e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,352][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0609, 0.0624, 0.0724, 0.0717, 0.0946, 0.0723, 0.1029, 0.1125, 0.0792,
        0.1055, 0.0847, 0.0809], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:44,356][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1320, 0.0300, 0.0327, 0.1028, 0.0350, 0.0259, 0.0328, 0.2383, 0.0817,
        0.0806, 0.0503, 0.0723, 0.0856], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,361][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0107, 0.0719, 0.0356, 0.0251, 0.1108, 0.0721, 0.0464, 0.1775, 0.1184,
        0.0695, 0.0957, 0.1345, 0.0317], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,364][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ a] are: tensor([5.7813e-06, 1.9371e-02, 5.4637e-02, 1.5597e-02, 7.5482e-02, 4.9027e-02,
        3.9460e-02, 8.1313e-02, 2.6646e-02, 1.2677e-01, 1.0220e-01, 1.3985e-01,
        2.6965e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,369][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.2987, 0.0232, 0.0125, 0.0604, 0.0161, 0.0139, 0.0238, 0.1342, 0.1319,
        0.0933, 0.0376, 0.0619, 0.0925], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,372][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0511, 0.0154, 0.0204, 0.1422, 0.0207, 0.0196, 0.0190, 0.1522, 0.1148,
        0.1446, 0.0246, 0.0848, 0.1906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,373][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0782, 0.0089, 0.1012, 0.1611, 0.1050, 0.0679, 0.1283, 0.1034, 0.0808,
        0.0327, 0.0159, 0.0757, 0.0409], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,374][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.2073, 0.0733, 0.0723, 0.0744, 0.0604, 0.0545, 0.0622, 0.0651, 0.0635,
        0.0645, 0.0705, 0.0663, 0.0657], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,375][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0004, 0.0418, 0.1041, 0.0587, 0.1208, 0.0402, 0.0282, 0.0587, 0.0884,
        0.1094, 0.0611, 0.1797, 0.1085], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,377][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.1848, 0.0311, 0.0437, 0.0619, 0.0503, 0.0627, 0.0466, 0.0582, 0.1212,
        0.1416, 0.0352, 0.0619, 0.1007], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,382][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0121, 0.0606, 0.0890, 0.0776, 0.0885, 0.0987, 0.0362, 0.0868, 0.0588,
        0.1091, 0.1394, 0.0674, 0.0759], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,385][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ a] are: tensor([4.4377e-05, 2.4535e-02, 9.9115e-02, 9.8108e-02, 5.0121e-02, 1.0142e-01,
        2.0289e-01, 5.8659e-02, 4.5298e-02, 1.0139e-01, 1.8716e-02, 6.2148e-02,
        1.3756e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,389][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0481, 0.0519, 0.0624, 0.0660, 0.0806, 0.0635, 0.1202, 0.0894, 0.0684,
        0.0963, 0.0803, 0.0746, 0.0984], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:44,394][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0852, 0.0168, 0.0182, 0.0695, 0.0260, 0.0151, 0.0188, 0.2164, 0.0544,
        0.0625, 0.0357, 0.0575, 0.0777, 0.2462], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,399][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0025, 0.0995, 0.0318, 0.0269, 0.0925, 0.0938, 0.0379, 0.1556, 0.0930,
        0.0645, 0.1153, 0.0844, 0.0438, 0.0585], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,402][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([6.8064e-05, 1.8420e-02, 8.3200e-02, 9.8835e-03, 3.6289e-02, 4.4196e-02,
        3.2773e-02, 4.9660e-02, 6.0472e-02, 1.0036e-01, 7.9163e-02, 1.4053e-01,
        2.8274e-01, 6.2234e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,405][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.2575, 0.0150, 0.0058, 0.0408, 0.0126, 0.0114, 0.0192, 0.1092, 0.0881,
        0.0796, 0.0314, 0.0413, 0.0754, 0.2128], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,406][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0261, 0.0115, 0.0124, 0.1067, 0.0122, 0.0114, 0.0113, 0.0975, 0.0976,
        0.1185, 0.0163, 0.0554, 0.1711, 0.2521], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,407][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.0685, 0.0102, 0.1030, 0.1481, 0.0983, 0.0676, 0.1195, 0.0981, 0.0683,
        0.0379, 0.0130, 0.0659, 0.0414, 0.0600], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,408][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.2217, 0.0676, 0.0677, 0.0681, 0.0548, 0.0483, 0.0545, 0.0598, 0.0578,
        0.0575, 0.0643, 0.0614, 0.0595, 0.0569], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,410][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0002, 0.0346, 0.0977, 0.0599, 0.0765, 0.0508, 0.0193, 0.0435, 0.0829,
        0.1027, 0.0461, 0.1454, 0.1591, 0.0812], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,414][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.1556, 0.0303, 0.0419, 0.0580, 0.0477, 0.0550, 0.0399, 0.0541, 0.1179,
        0.1373, 0.0325, 0.0548, 0.0960, 0.0789], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,419][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0115, 0.0528, 0.0845, 0.0763, 0.0807, 0.0900, 0.0293, 0.0790, 0.0530,
        0.1002, 0.1233, 0.0565, 0.0693, 0.0936], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,424][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0002, 0.0171, 0.1559, 0.0673, 0.0294, 0.0404, 0.1275, 0.0547, 0.0387,
        0.0727, 0.0444, 0.0760, 0.2054, 0.0703], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,429][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0355, 0.0526, 0.0784, 0.0839, 0.0750, 0.0582, 0.0962, 0.0926, 0.0735,
        0.0720, 0.0572, 0.0621, 0.0858, 0.0771], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:44,433][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0421, 0.0105, 0.0140, 0.0587, 0.0181, 0.0101, 0.0152, 0.1586, 0.0417,
        0.0443, 0.0268, 0.0403, 0.0550, 0.2299, 0.2347], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,438][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0025, 0.1081, 0.0336, 0.0174, 0.0593, 0.1006, 0.0341, 0.1738, 0.1361,
        0.0482, 0.1095, 0.0611, 0.0288, 0.0638, 0.0230], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,439][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ different] are: tensor([5.4014e-06, 2.1160e-02, 3.5291e-02, 8.8915e-03, 3.9828e-02, 3.9681e-02,
        2.0028e-02, 3.0579e-02, 4.6124e-02, 7.6604e-02, 4.5472e-02, 7.4703e-02,
        1.6493e-01, 5.4378e-02, 3.4232e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,440][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.1389, 0.0128, 0.0054, 0.0350, 0.0115, 0.0087, 0.0175, 0.1221, 0.0827,
        0.0834, 0.0333, 0.0382, 0.0624, 0.2197, 0.1284], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,441][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0182, 0.0063, 0.0074, 0.0671, 0.0096, 0.0092, 0.0093, 0.0667, 0.0662,
        0.0846, 0.0138, 0.0494, 0.1146, 0.2914, 0.1861], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,443][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.0449, 0.0086, 0.0949, 0.1778, 0.0760, 0.0606, 0.1020, 0.1071, 0.0703,
        0.0319, 0.0149, 0.0671, 0.0344, 0.0645, 0.0452], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,447][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.2258, 0.0633, 0.0616, 0.0612, 0.0503, 0.0447, 0.0517, 0.0542, 0.0529,
        0.0516, 0.0584, 0.0545, 0.0548, 0.0526, 0.0624], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,452][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0019, 0.0303, 0.0569, 0.0543, 0.0560, 0.0255, 0.0395, 0.0352, 0.1106,
        0.0744, 0.0532, 0.1598, 0.1440, 0.0714, 0.0869], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,457][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.1303, 0.0233, 0.0363, 0.0571, 0.0385, 0.0466, 0.0356, 0.0504, 0.1042,
        0.1359, 0.0291, 0.0494, 0.0970, 0.0862, 0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,462][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0044, 0.0471, 0.0728, 0.0612, 0.0611, 0.0760, 0.0297, 0.0804, 0.0544,
        0.1082, 0.1169, 0.0530, 0.0737, 0.1117, 0.0492], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,465][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ different] are: tensor([2.3336e-05, 1.2316e-02, 3.0150e-02, 5.9893e-02, 1.9644e-02, 1.0334e-01,
        1.1543e-01, 5.9616e-02, 7.1868e-02, 7.8872e-02, 2.4301e-02, 5.5876e-02,
        1.8459e-01, 1.4027e-01, 4.3810e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,469][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0190, 0.0481, 0.0510, 0.0633, 0.0686, 0.0533, 0.1022, 0.0914, 0.0830,
        0.0626, 0.0520, 0.0633, 0.0836, 0.0896, 0.0690], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:44,471][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0637, 0.0165, 0.0206, 0.0581, 0.0206, 0.0162, 0.0192, 0.1156, 0.0419,
        0.0435, 0.0284, 0.0415, 0.0487, 0.1615, 0.2019, 0.1019],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,472][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0004, 0.0911, 0.0183, 0.0113, 0.0663, 0.1335, 0.0480, 0.2806, 0.0921,
        0.0448, 0.0658, 0.0505, 0.0242, 0.0470, 0.0158, 0.0102],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,473][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ from] are: tensor([1.5233e-06, 3.9289e-03, 1.2753e-02, 1.8400e-03, 1.5548e-02, 2.4724e-02,
        9.2249e-03, 1.8202e-02, 9.4096e-03, 3.7495e-02, 3.2881e-02, 4.0630e-02,
        1.2496e-01, 2.8948e-02, 5.3408e-01, 1.0537e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,474][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.2805, 0.0189, 0.0081, 0.0288, 0.0123, 0.0086, 0.0166, 0.0660, 0.0548,
        0.0516, 0.0258, 0.0321, 0.0484, 0.1496, 0.0826, 0.1151],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,477][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0652, 0.0134, 0.0111, 0.0442, 0.0099, 0.0085, 0.0082, 0.0534, 0.0453,
        0.0515, 0.0137, 0.0342, 0.0818, 0.1265, 0.1641, 0.2689],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,481][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0748, 0.0094, 0.0768, 0.1232, 0.0792, 0.0550, 0.0949, 0.0936, 0.0628,
        0.0307, 0.0147, 0.0745, 0.0426, 0.0659, 0.0607, 0.0412],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,486][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.2023, 0.0618, 0.0593, 0.0593, 0.0518, 0.0450, 0.0511, 0.0518, 0.0511,
        0.0497, 0.0556, 0.0513, 0.0519, 0.0498, 0.0574, 0.0508],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,489][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ from] are: tensor([9.8746e-05, 1.7509e-02, 2.6179e-02, 2.2495e-02, 5.4320e-02, 3.5756e-02,
        2.3045e-02, 4.2955e-02, 4.8115e-02, 8.3872e-02, 4.7269e-02, 1.9409e-01,
        1.4678e-01, 7.2624e-02, 7.6852e-02, 1.0804e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,494][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.1119, 0.0338, 0.0436, 0.0446, 0.0507, 0.0558, 0.0408, 0.0418, 0.0934,
        0.1155, 0.0275, 0.0424, 0.0686, 0.0614, 0.0665, 0.1019],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,498][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0181, 0.0501, 0.0622, 0.0565, 0.0726, 0.0825, 0.0294, 0.0724, 0.0406,
        0.0827, 0.1125, 0.0456, 0.0535, 0.0781, 0.0431, 0.1002],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,501][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ from] are: tensor([2.6447e-05, 1.1784e-02, 8.0383e-02, 3.9893e-02, 2.8153e-02, 1.0238e-01,
        1.5065e-01, 5.8911e-02, 2.5134e-02, 6.2588e-02, 2.9116e-02, 5.4265e-02,
        1.2532e-01, 8.9922e-02, 7.6670e-02, 6.4800e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,504][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0199, 0.0418, 0.0599, 0.0560, 0.0623, 0.0544, 0.0860, 0.0754, 0.0520,
        0.0624, 0.0574, 0.0541, 0.0775, 0.0890, 0.0905, 0.0616],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:44,505][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0681, 0.0151, 0.0167, 0.0588, 0.0181, 0.0121, 0.0122, 0.0969, 0.0359,
        0.0361, 0.0230, 0.0312, 0.0393, 0.1420, 0.1919, 0.1163, 0.0863],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,506][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0007, 0.0561, 0.0327, 0.0206, 0.0563, 0.1096, 0.0488, 0.2289, 0.0847,
        0.0616, 0.0772, 0.0638, 0.0356, 0.0589, 0.0223, 0.0257, 0.0165],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,507][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([4.9101e-07, 8.9191e-03, 1.3212e-02, 3.6709e-03, 2.1083e-02, 1.6456e-02,
        1.0371e-02, 1.6479e-02, 1.0464e-02, 3.1235e-02, 4.3973e-02, 3.4246e-02,
        9.5780e-02, 5.0892e-02, 3.9694e-01, 1.0439e-01, 1.4189e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,510][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2156, 0.0136, 0.0050, 0.0307, 0.0099, 0.0068, 0.0095, 0.0473, 0.0458,
        0.0412, 0.0158, 0.0247, 0.0368, 0.1079, 0.0736, 0.1569, 0.1591],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,513][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0477, 0.0043, 0.0060, 0.0316, 0.0056, 0.0041, 0.0040, 0.0327, 0.0305,
        0.0310, 0.0070, 0.0173, 0.0542, 0.0720, 0.0722, 0.2960, 0.2839],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,518][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0832, 0.0073, 0.0624, 0.1344, 0.0744, 0.0568, 0.0886, 0.0835, 0.0577,
        0.0272, 0.0130, 0.0641, 0.0368, 0.0700, 0.0458, 0.0401, 0.0547],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,523][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1943, 0.0573, 0.0568, 0.0560, 0.0467, 0.0420, 0.0474, 0.0502, 0.0479,
        0.0479, 0.0542, 0.0499, 0.0498, 0.0480, 0.0560, 0.0489, 0.0467],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,526][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.4691e-05, 1.7930e-02, 3.9602e-02, 2.2426e-02, 6.1441e-02, 2.7093e-02,
        1.4171e-02, 3.5315e-02, 6.9268e-02, 7.6283e-02, 3.7634e-02, 1.3022e-01,
        9.6712e-02, 4.3577e-02, 6.1974e-02, 1.2865e-01, 1.3762e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,531][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1221, 0.0231, 0.0322, 0.0407, 0.0367, 0.0422, 0.0293, 0.0349, 0.0752,
        0.0878, 0.0215, 0.0387, 0.0626, 0.0528, 0.0529, 0.0965, 0.1508],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,535][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0133, 0.0443, 0.0614, 0.0558, 0.0682, 0.0737, 0.0282, 0.0594, 0.0398,
        0.0763, 0.0931, 0.0406, 0.0487, 0.0722, 0.0365, 0.1120, 0.0767],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,537][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([1.7078e-05, 1.7210e-02, 4.7583e-02, 5.3643e-02, 3.0938e-02, 8.8533e-02,
        1.0490e-01, 3.8457e-02, 3.5011e-02, 5.3796e-02, 2.0830e-02, 3.7489e-02,
        1.2338e-01, 1.2162e-01, 6.2888e-02, 6.2830e-02, 1.0087e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,538][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0235, 0.0466, 0.0487, 0.0476, 0.0623, 0.0458, 0.0773, 0.0651, 0.0513,
        0.0610, 0.0542, 0.0495, 0.0667, 0.0873, 0.0823, 0.0583, 0.0726],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:44,539][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0492, 0.0148, 0.0178, 0.0476, 0.0226, 0.0182, 0.0209, 0.0937, 0.0369,
        0.0425, 0.0289, 0.0432, 0.0499, 0.1364, 0.1412, 0.0842, 0.0986, 0.0535],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,540][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ language] are: tensor([4.1037e-05, 4.5431e-02, 1.9514e-02, 1.8527e-02, 1.0225e-01, 1.4096e-01,
        2.3448e-02, 1.7390e-01, 1.4837e-01, 5.1358e-02, 1.0856e-01, 3.4964e-02,
        1.4173e-02, 2.5045e-02, 9.5600e-03, 3.0522e-02, 1.1128e-02, 4.2237e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,542][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ language] are: tensor([1.6315e-05, 1.4113e-02, 3.8826e-02, 1.0742e-02, 2.5530e-02, 1.6010e-02,
        1.2316e-02, 1.5638e-02, 1.4637e-02, 4.4923e-02, 2.4552e-02, 4.3188e-02,
        8.9637e-02, 2.9498e-02, 2.8995e-01, 1.1427e-01, 1.9408e-01, 2.2085e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,545][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.1209, 0.0124, 0.0063, 0.0341, 0.0094, 0.0092, 0.0161, 0.0533, 0.0380,
        0.0389, 0.0220, 0.0301, 0.0395, 0.1079, 0.0770, 0.1128, 0.1408, 0.1312],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,550][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0104, 0.0065, 0.0066, 0.0475, 0.0084, 0.0078, 0.0083, 0.0340, 0.0313,
        0.0409, 0.0082, 0.0247, 0.0613, 0.0841, 0.1008, 0.1964, 0.2614, 0.0614],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,555][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0513, 0.0048, 0.0879, 0.1336, 0.0670, 0.0602, 0.0769, 0.0616, 0.0649,
        0.0266, 0.0102, 0.0654, 0.0451, 0.0693, 0.0508, 0.0465, 0.0727, 0.0051],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,559][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.2137, 0.0545, 0.0532, 0.0521, 0.0447, 0.0386, 0.0441, 0.0460, 0.0436,
        0.0429, 0.0497, 0.0459, 0.0447, 0.0436, 0.0509, 0.0451, 0.0426, 0.0442],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,564][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0006, 0.0200, 0.0483, 0.0350, 0.0522, 0.0236, 0.0174, 0.0269, 0.0531,
        0.0606, 0.0306, 0.1136, 0.0683, 0.0585, 0.0751, 0.1404, 0.1528, 0.0230],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,569][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.1452, 0.0192, 0.0304, 0.0383, 0.0305, 0.0380, 0.0231, 0.0276, 0.0631,
        0.0791, 0.0177, 0.0370, 0.0611, 0.0467, 0.0489, 0.0846, 0.1661, 0.0435],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,570][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0106, 0.0429, 0.0668, 0.0535, 0.0569, 0.0660, 0.0249, 0.0602, 0.0373,
        0.0689, 0.0863, 0.0452, 0.0486, 0.0669, 0.0451, 0.0934, 0.0747, 0.0519],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,571][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ language] are: tensor([3.8125e-05, 1.2160e-02, 6.9500e-02, 1.1650e-01, 3.2191e-02, 5.1450e-02,
        1.4818e-01, 2.2281e-02, 2.8477e-02, 7.0985e-02, 1.3471e-02, 3.1763e-02,
        7.6895e-02, 5.5678e-02, 5.6970e-02, 6.0180e-02, 1.3597e-01, 1.7316e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,572][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0194, 0.0393, 0.0617, 0.0617, 0.0559, 0.0440, 0.0835, 0.0565, 0.0564,
        0.0492, 0.0399, 0.0555, 0.0644, 0.0722, 0.0601, 0.0560, 0.0726, 0.0517],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:44,574][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0842, 0.0127, 0.0098, 0.0404, 0.0142, 0.0096, 0.0100, 0.0910, 0.0262,
        0.0343, 0.0239, 0.0252, 0.0347, 0.1105, 0.1453, 0.0844, 0.0757, 0.0596,
        0.1081], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,577][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0027, 0.0710, 0.0140, 0.0150, 0.0214, 0.1017, 0.0304, 0.1767, 0.1302,
        0.0531, 0.1069, 0.0674, 0.0442, 0.0386, 0.0120, 0.0299, 0.0102, 0.0626,
        0.0121], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,580][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ used] are: tensor([2.3640e-05, 1.1426e-02, 3.6068e-02, 1.0714e-02, 3.5810e-02, 2.0104e-02,
        1.5431e-02, 2.6439e-02, 2.0660e-02, 4.9989e-02, 3.2325e-02, 4.0530e-02,
        8.6355e-02, 3.6311e-02, 2.2415e-01, 9.6462e-02, 1.5704e-01, 1.3109e-02,
        8.7050e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,585][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.2757, 0.0106, 0.0038, 0.0192, 0.0071, 0.0041, 0.0066, 0.0298, 0.0223,
        0.0251, 0.0116, 0.0156, 0.0185, 0.0520, 0.0434, 0.0695, 0.0749, 0.0982,
        0.2119], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,590][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0605, 0.0067, 0.0042, 0.0260, 0.0062, 0.0036, 0.0038, 0.0282, 0.0238,
        0.0230, 0.0069, 0.0157, 0.0391, 0.0579, 0.0524, 0.1685, 0.2136, 0.0783,
        0.1817], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,594][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0546, 0.0057, 0.0721, 0.1189, 0.0698, 0.0409, 0.0912, 0.0747, 0.0487,
        0.0316, 0.0097, 0.0624, 0.0360, 0.0522, 0.0439, 0.0398, 0.0804, 0.0050,
        0.0624], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,599][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.1545, 0.0558, 0.0521, 0.0528, 0.0446, 0.0398, 0.0443, 0.0472, 0.0473,
        0.0452, 0.0508, 0.0477, 0.0467, 0.0447, 0.0518, 0.0448, 0.0434, 0.0444,
        0.0423], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,602][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0007, 0.0157, 0.0296, 0.0171, 0.0386, 0.0207, 0.0175, 0.0370, 0.0639,
        0.0799, 0.0537, 0.1184, 0.0777, 0.0426, 0.0577, 0.0999, 0.1525, 0.0149,
        0.0619], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,603][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.1777, 0.0175, 0.0188, 0.0243, 0.0250, 0.0303, 0.0191, 0.0230, 0.0531,
        0.0756, 0.0164, 0.0250, 0.0492, 0.0423, 0.0430, 0.0839, 0.1350, 0.0436,
        0.0973], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,604][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0108, 0.0359, 0.0529, 0.0493, 0.0597, 0.0663, 0.0205, 0.0498, 0.0351,
        0.0731, 0.0796, 0.0316, 0.0408, 0.0599, 0.0303, 0.0943, 0.0655, 0.0525,
        0.0919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,604][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ used] are: tensor([3.6328e-05, 9.6657e-03, 4.6677e-02, 7.8926e-02, 2.7910e-02, 5.4550e-02,
        7.7473e-02, 4.1357e-02, 3.6933e-02, 4.8346e-02, 1.6251e-02, 3.5379e-02,
        1.0739e-01, 6.4315e-02, 5.6414e-02, 7.3656e-02, 1.6307e-01, 1.8605e-02,
        4.3044e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,607][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0241, 0.0317, 0.0437, 0.0500, 0.0555, 0.0387, 0.0755, 0.0611, 0.0541,
        0.0563, 0.0439, 0.0494, 0.0615, 0.0586, 0.0653, 0.0554, 0.0707, 0.0452,
        0.0592], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:44,611][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.1450, 0.0139, 0.0084, 0.0260, 0.0118, 0.0067, 0.0063, 0.0596, 0.0165,
        0.0264, 0.0195, 0.0192, 0.0232, 0.0623, 0.0836, 0.0596, 0.0490, 0.0517,
        0.0705, 0.2407], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,615][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0006, 0.0674, 0.0162, 0.0097, 0.0381, 0.0789, 0.0308, 0.2471, 0.1138,
        0.0498, 0.1019, 0.0490, 0.0201, 0.0369, 0.0184, 0.0123, 0.0166, 0.0621,
        0.0182, 0.0120], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,618][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ by] are: tensor([2.6474e-06, 2.6162e-03, 1.0898e-02, 1.2616e-03, 1.2102e-02, 1.3328e-02,
        6.4282e-03, 1.4715e-02, 9.0369e-03, 2.3783e-02, 2.4402e-02, 4.1668e-02,
        5.9637e-02, 1.8902e-02, 3.8519e-01, 7.5465e-02, 1.2292e-01, 1.2988e-02,
        1.3409e-01, 3.0565e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,623][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.2550, 0.0063, 0.0015, 0.0061, 0.0033, 0.0015, 0.0021, 0.0106, 0.0066,
        0.0076, 0.0042, 0.0050, 0.0058, 0.0128, 0.0081, 0.0261, 0.0268, 0.0394,
        0.0731, 0.4982], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,626][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ by] are: tensor([4.6373e-02, 2.2331e-03, 1.2625e-03, 5.5935e-03, 1.3382e-03, 7.7654e-04,
        6.3622e-04, 6.5517e-03, 4.2583e-03, 4.6337e-03, 1.6158e-03, 2.9977e-03,
        6.4587e-03, 6.9952e-03, 8.0107e-03, 4.0739e-02, 4.1427e-02, 2.5306e-02,
        6.0864e-02, 7.3193e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,631][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0774, 0.0054, 0.0647, 0.1151, 0.0660, 0.0404, 0.0872, 0.0728, 0.0449,
        0.0240, 0.0110, 0.0541, 0.0275, 0.0427, 0.0382, 0.0348, 0.0554, 0.0051,
        0.0549, 0.0787], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,634][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.1521, 0.0539, 0.0534, 0.0520, 0.0447, 0.0380, 0.0430, 0.0448, 0.0430,
        0.0426, 0.0478, 0.0446, 0.0441, 0.0413, 0.0472, 0.0424, 0.0406, 0.0413,
        0.0407, 0.0427], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,635][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ by] are: tensor([1.7259e-04, 1.0067e-02, 1.6098e-02, 7.9487e-03, 2.9845e-02, 1.8314e-02,
        1.0988e-02, 3.2807e-02, 3.3477e-02, 5.9626e-02, 4.3983e-02, 1.3931e-01,
        7.3650e-02, 3.9981e-02, 6.7127e-02, 1.1394e-01, 1.7586e-01, 1.7181e-02,
        6.8653e-02, 4.0975e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,636][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.1520, 0.0227, 0.0263, 0.0216, 0.0270, 0.0288, 0.0205, 0.0214, 0.0439,
        0.0572, 0.0143, 0.0225, 0.0393, 0.0356, 0.0345, 0.0611, 0.1095, 0.0391,
        0.0891, 0.1338], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,637][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0205, 0.0381, 0.0465, 0.0410, 0.0610, 0.0641, 0.0210, 0.0489, 0.0286,
        0.0581, 0.0787, 0.0292, 0.0337, 0.0506, 0.0266, 0.0773, 0.0567, 0.0498,
        0.0776, 0.0920], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,638][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ by] are: tensor([2.7504e-05, 9.3159e-03, 5.9328e-02, 2.6297e-02, 2.9196e-02, 1.1307e-01,
        6.1153e-02, 4.0023e-02, 2.2940e-02, 4.1132e-02, 2.0740e-02, 2.6806e-02,
        5.9748e-02, 3.9775e-02, 6.5422e-02, 6.9987e-02, 1.1851e-01, 2.0355e-02,
        8.9209e-02, 8.6971e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,642][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0255, 0.0290, 0.0438, 0.0381, 0.0503, 0.0404, 0.0725, 0.0545, 0.0411,
        0.0521, 0.0460, 0.0443, 0.0531, 0.0625, 0.0666, 0.0522, 0.0745, 0.0437,
        0.0634, 0.0463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:44,647][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0694, 0.0106, 0.0084, 0.0287, 0.0092, 0.0062, 0.0054, 0.0476, 0.0165,
        0.0206, 0.0127, 0.0155, 0.0204, 0.0696, 0.0952, 0.0629, 0.0450, 0.0428,
        0.0739, 0.2818, 0.0576], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,652][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0006, 0.0561, 0.0262, 0.0178, 0.0494, 0.0912, 0.0431, 0.2099, 0.0779,
        0.0524, 0.0725, 0.0558, 0.0295, 0.0470, 0.0200, 0.0232, 0.0139, 0.0520,
        0.0284, 0.0196, 0.0135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,655][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([7.0092e-07, 7.8305e-03, 1.1582e-02, 2.8110e-03, 1.7010e-02, 1.1939e-02,
        9.4965e-03, 1.1998e-02, 8.4854e-03, 2.2647e-02, 3.4337e-02, 2.6320e-02,
        6.5657e-02, 3.8963e-02, 3.3344e-01, 6.8907e-02, 9.9777e-02, 2.7600e-02,
        8.9275e-02, 3.7717e-02, 7.4203e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,659][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1124, 0.0035, 0.0009, 0.0057, 0.0019, 0.0011, 0.0013, 0.0068, 0.0057,
        0.0059, 0.0027, 0.0036, 0.0048, 0.0121, 0.0084, 0.0228, 0.0202, 0.0334,
        0.0510, 0.5895, 0.1064], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,662][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([2.4558e-02, 9.5289e-04, 8.0917e-04, 4.2485e-03, 8.9033e-04, 5.1821e-04,
        4.3618e-04, 3.8998e-03, 2.9719e-03, 3.0633e-03, 1.0390e-03, 1.9715e-03,
        5.3739e-03, 5.8661e-03, 6.1157e-03, 3.0484e-02, 2.7558e-02, 1.2199e-02,
        3.8197e-02, 6.7019e-01, 1.5866e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,666][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0649, 0.0056, 0.0526, 0.1125, 0.0612, 0.0432, 0.0757, 0.0665, 0.0496,
        0.0212, 0.0109, 0.0541, 0.0303, 0.0554, 0.0365, 0.0340, 0.0463, 0.0053,
        0.0452, 0.0755, 0.0534], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,667][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1857, 0.0473, 0.0456, 0.0453, 0.0383, 0.0337, 0.0388, 0.0408, 0.0397,
        0.0398, 0.0454, 0.0411, 0.0416, 0.0400, 0.0459, 0.0401, 0.0384, 0.0397,
        0.0385, 0.0417, 0.0328], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,668][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([1.0204e-04, 9.8177e-03, 2.3043e-02, 1.8618e-02, 3.6387e-02, 1.7234e-02,
        1.1431e-02, 2.1542e-02, 5.3823e-02, 5.6882e-02, 2.8471e-02, 1.1047e-01,
        7.7868e-02, 3.3298e-02, 3.9970e-02, 1.0279e-01, 1.2655e-01, 9.9214e-03,
        4.7232e-02, 8.1454e-02, 9.3089e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,669][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0880, 0.0168, 0.0211, 0.0247, 0.0220, 0.0274, 0.0168, 0.0198, 0.0407,
        0.0513, 0.0130, 0.0231, 0.0382, 0.0290, 0.0277, 0.0551, 0.0903, 0.0319,
        0.0747, 0.1380, 0.1503], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,672][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0123, 0.0346, 0.0415, 0.0397, 0.0509, 0.0552, 0.0192, 0.0397, 0.0257,
        0.0519, 0.0651, 0.0262, 0.0314, 0.0468, 0.0235, 0.0798, 0.0518, 0.0489,
        0.0738, 0.0962, 0.0858], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,674][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([1.7526e-05, 1.5268e-02, 3.9784e-02, 4.0304e-02, 2.4593e-02, 7.5967e-02,
        9.8750e-02, 2.7968e-02, 2.5511e-02, 4.1616e-02, 1.5796e-02, 2.5107e-02,
        8.7374e-02, 7.5679e-02, 4.7386e-02, 4.0509e-02, 7.4284e-02, 2.5282e-02,
        4.0494e-02, 6.0058e-02, 1.1825e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,678][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0268, 0.0353, 0.0347, 0.0357, 0.0529, 0.0364, 0.0628, 0.0497, 0.0397,
        0.0515, 0.0454, 0.0385, 0.0501, 0.0675, 0.0706, 0.0488, 0.0578, 0.0470,
        0.0526, 0.0437, 0.0527], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:44,758][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:44,759][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,761][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,761][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,762][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,763][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,763][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,766][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,766][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,767][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,768][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,768][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,769][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:44,773][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7633, 0.2367], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,778][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.2751, 0.7249], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,783][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,783][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.9166, 0.0834], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,784][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.5734, 0.4266], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,785][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.9846, 0.0154], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,785][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.8753, 0.1247], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,787][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.1406, 0.8594], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,790][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.7945, 0.2055], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,795][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.6300, 0.3700], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,799][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0074, 0.9926], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,804][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.6926, 0.3074], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:44,809][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.3251, 0.1959, 0.4790], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,813][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.3009, 0.4591, 0.2400], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,815][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0012, 0.1678, 0.8310], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,816][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.8824, 0.0707, 0.0469], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,817][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.4499, 0.2075, 0.3426], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,817][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.9735, 0.0179, 0.0086], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,818][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.8575, 0.0983, 0.0442], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,821][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.3913, 0.4159, 0.1928], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,826][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.5040, 0.1772, 0.3188], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,831][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.2742, 0.3476, 0.3782], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,835][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0039, 0.3309, 0.6653], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,840][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.6906, 0.1126, 0.1968], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:44,844][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.2401, 0.1252, 0.3597, 0.2749], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,847][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0762, 0.3388, 0.3998, 0.1852], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,848][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([4.0495e-04, 1.2946e-01, 7.1732e-01, 1.5282e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,848][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.7840, 0.0736, 0.0435, 0.0989], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,849][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.3031, 0.0931, 0.1400, 0.4637], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,850][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.9417, 0.0241, 0.0139, 0.0202], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,853][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.6729, 0.1302, 0.0704, 0.1266], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,857][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.1411, 0.5537, 0.1519, 0.1533], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,862][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.4576, 0.1151, 0.2865, 0.1407], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,866][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.1606, 0.1649, 0.1607, 0.5137], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,871][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0018, 0.1791, 0.5520, 0.2671], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,876][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.6092, 0.1316, 0.1753, 0.0839], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:44,880][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.2501, 0.0769, 0.2300, 0.3558, 0.0872], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,882][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0374, 0.2066, 0.3152, 0.1821, 0.2586], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,882][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([1.9830e-04, 1.3655e-01, 5.2799e-01, 8.5466e-02, 2.4980e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,883][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.7795, 0.0563, 0.0282, 0.0852, 0.0508], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,884][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.2034, 0.0632, 0.0980, 0.5084, 0.1271], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,886][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.9484, 0.0144, 0.0114, 0.0148, 0.0110], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,889][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.5903, 0.1162, 0.0653, 0.1264, 0.1018], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,894][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0826, 0.3526, 0.1383, 0.1458, 0.2807], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,898][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.2334, 0.1206, 0.1846, 0.2538, 0.2076], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,903][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.2835, 0.0847, 0.1294, 0.3769, 0.1256], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,908][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.0009, 0.1215, 0.4738, 0.1953, 0.2085], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,912][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.6862, 0.0735, 0.0715, 0.0544, 0.1144], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:44,914][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.1212, 0.1086, 0.2390, 0.4222, 0.0680, 0.0410], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,915][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0354, 0.2287, 0.1748, 0.1089, 0.2178, 0.2345], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,915][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([1.6532e-04, 5.2412e-02, 3.9851e-01, 1.2453e-01, 1.6463e-01, 2.5976e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,916][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.7211, 0.0602, 0.0319, 0.1120, 0.0398, 0.0349], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,918][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.1319, 0.0563, 0.0789, 0.5197, 0.0884, 0.1248], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,921][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.9371, 0.0195, 0.0072, 0.0184, 0.0104, 0.0073], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,926][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.4574, 0.1211, 0.0505, 0.1293, 0.1026, 0.1391], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,930][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0588, 0.2410, 0.1185, 0.1691, 0.2698, 0.1427], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,935][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.3835, 0.0597, 0.1103, 0.2557, 0.1345, 0.0563], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,940][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.1646, 0.1209, 0.1165, 0.3658, 0.1099, 0.1224], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,944][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.0010, 0.0939, 0.3426, 0.2211, 0.2577, 0.0837], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,946][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.6553, 0.0618, 0.0778, 0.0653, 0.0877, 0.0521], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:44,947][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.1342, 0.0511, 0.1784, 0.4219, 0.0994, 0.0566, 0.0584],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,947][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0155, 0.1671, 0.1422, 0.0779, 0.1164, 0.1431, 0.3378],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,948][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([1.7452e-04, 6.8247e-02, 2.8387e-01, 1.2328e-01, 7.8579e-02, 3.0930e-01,
        1.3656e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,949][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.5086, 0.0638, 0.0382, 0.1413, 0.0481, 0.0508, 0.1492],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,952][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.0690, 0.0430, 0.0994, 0.4692, 0.0689, 0.1051, 0.1454],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,956][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.8782, 0.0291, 0.0128, 0.0426, 0.0101, 0.0171, 0.0101],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,961][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.4992, 0.0986, 0.0348, 0.0865, 0.0731, 0.0882, 0.1197],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,966][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0523, 0.2047, 0.1211, 0.1551, 0.2160, 0.1223, 0.1284],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,971][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.2412, 0.0371, 0.0844, 0.3016, 0.1750, 0.0747, 0.0860],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,976][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0830, 0.0811, 0.1086, 0.3719, 0.0799, 0.1414, 0.1341],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,979][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.0007, 0.0795, 0.2260, 0.1670, 0.0571, 0.3224, 0.1474],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,980][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.2341, 0.0997, 0.1459, 0.1192, 0.1135, 0.1663, 0.1214],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:44,981][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.2127, 0.0346, 0.0793, 0.1674, 0.0415, 0.0248, 0.0327, 0.4070],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,981][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0203, 0.0917, 0.1089, 0.0755, 0.1051, 0.1652, 0.1671, 0.2662],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,982][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([3.6120e-05, 3.6625e-02, 2.7384e-01, 1.1883e-01, 5.7196e-02, 3.1737e-01,
        1.2322e-01, 7.2882e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,985][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.3683, 0.0502, 0.0341, 0.1177, 0.0555, 0.0453, 0.0645, 0.2644],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,989][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0637, 0.0306, 0.0492, 0.3342, 0.0715, 0.0655, 0.0632, 0.3220],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,995][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.8720, 0.0215, 0.0102, 0.0260, 0.0128, 0.0114, 0.0086, 0.0375],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:44,999][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.4044, 0.0998, 0.0435, 0.0853, 0.0983, 0.0898, 0.0759, 0.1031],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,004][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0323, 0.1817, 0.1047, 0.1363, 0.1817, 0.1073, 0.1392, 0.1168],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,009][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.1102, 0.0420, 0.0755, 0.1675, 0.1004, 0.0719, 0.0546, 0.3778],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,012][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0737, 0.0405, 0.0568, 0.1367, 0.0549, 0.1167, 0.0547, 0.4661],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,013][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([5.9457e-05, 2.6979e-02, 1.7988e-01, 1.5239e-01, 5.8477e-02, 2.9787e-01,
        2.5219e-01, 3.2172e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,014][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.2656, 0.0741, 0.0761, 0.0683, 0.1351, 0.1354, 0.1609, 0.0844],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,015][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0658, 0.0249, 0.0600, 0.1399, 0.0258, 0.0180, 0.0306, 0.4558, 0.1792],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,017][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0089, 0.0825, 0.0567, 0.0419, 0.1047, 0.1635, 0.1533, 0.3128, 0.0756],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,019][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([5.3661e-05, 1.5742e-02, 2.4135e-01, 4.8693e-02, 5.9250e-02, 2.9048e-01,
        1.3042e-01, 1.7137e-01, 4.2644e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,023][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.4126, 0.0355, 0.0177, 0.0558, 0.0304, 0.0207, 0.0632, 0.1850, 0.1792],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,028][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0791, 0.0204, 0.0245, 0.2193, 0.0633, 0.0320, 0.0386, 0.2809, 0.2420],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,033][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.8608, 0.0135, 0.0063, 0.0214, 0.0084, 0.0130, 0.0091, 0.0481, 0.0195],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,038][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.4262, 0.0714, 0.0313, 0.0649, 0.0849, 0.0795, 0.0713, 0.0981, 0.0724],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,043][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0383, 0.1239, 0.0761, 0.0789, 0.1841, 0.1108, 0.1123, 0.0928, 0.1828],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,045][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.0985, 0.0313, 0.0416, 0.1175, 0.0660, 0.0324, 0.0320, 0.2905, 0.2903],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,046][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0352, 0.0211, 0.0280, 0.1219, 0.0515, 0.0847, 0.0700, 0.3914, 0.1962],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,047][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([1.3673e-04, 1.0851e-02, 1.7415e-01, 4.7159e-02, 4.8956e-02, 3.9630e-01,
        1.5913e-01, 1.2138e-01, 4.1934e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,048][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.4214, 0.0290, 0.0981, 0.0411, 0.0846, 0.1309, 0.0770, 0.0914, 0.0263],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,050][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.1845, 0.0235, 0.0347, 0.0779, 0.0251, 0.0112, 0.0192, 0.3234, 0.1625,
        0.1381], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,054][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0065, 0.0615, 0.0979, 0.0600, 0.0612, 0.1053, 0.1747, 0.2503, 0.0730,
        0.1097], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,057][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([1.4473e-04, 3.7444e-02, 1.3379e-01, 4.4120e-02, 6.8760e-02, 1.9494e-01,
        1.2551e-01, 1.9623e-01, 4.9695e-02, 1.4937e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,061][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.4995, 0.0293, 0.0115, 0.0577, 0.0190, 0.0128, 0.0255, 0.1482, 0.1001,
        0.0963], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,067][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.1042, 0.0200, 0.0257, 0.1811, 0.0391, 0.0315, 0.0259, 0.1883, 0.2054,
        0.1786], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,071][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.8423, 0.0139, 0.0076, 0.0232, 0.0087, 0.0123, 0.0078, 0.0366, 0.0269,
        0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,076][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.3218, 0.0574, 0.0279, 0.0587, 0.0714, 0.0735, 0.0830, 0.1205, 0.0875,
        0.0984], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,078][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0220, 0.1153, 0.0775, 0.0803, 0.1392, 0.0541, 0.0980, 0.0730, 0.1843,
        0.1563], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,079][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.2829, 0.0177, 0.0248, 0.0601, 0.0466, 0.0152, 0.0191, 0.1957, 0.1929,
        0.1449], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,080][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0507, 0.0187, 0.0224, 0.0957, 0.0391, 0.0354, 0.0408, 0.2884, 0.1511,
        0.2578], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,081][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.0005, 0.0416, 0.1089, 0.1021, 0.0513, 0.2401, 0.1558, 0.1668, 0.0633,
        0.0696], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,083][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.2708, 0.0585, 0.0589, 0.0580, 0.0924, 0.0618, 0.1370, 0.1035, 0.0574,
        0.1016], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,087][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0586, 0.0219, 0.0415, 0.0945, 0.0278, 0.0163, 0.0301, 0.2931, 0.1608,
        0.2292, 0.0261], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,092][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0192, 0.0447, 0.0389, 0.0280, 0.0813, 0.1164, 0.0965, 0.2131, 0.0758,
        0.1397, 0.1465], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,095][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([2.3470e-05, 2.5966e-02, 1.5890e-01, 4.5588e-02, 5.1845e-02, 1.8738e-01,
        1.5156e-01, 9.8099e-02, 9.0418e-02, 1.5208e-01, 3.8139e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,100][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.2004, 0.0239, 0.0156, 0.0598, 0.0271, 0.0286, 0.0418, 0.1888, 0.1835,
        0.1671, 0.0632], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,104][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.0245, 0.0141, 0.0237, 0.1274, 0.0311, 0.0360, 0.0341, 0.1757, 0.2656,
        0.2242, 0.0436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,109][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.8280, 0.0159, 0.0078, 0.0227, 0.0156, 0.0097, 0.0051, 0.0280, 0.0246,
        0.0233, 0.0192], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,112][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.2552, 0.0642, 0.0294, 0.0583, 0.0694, 0.0813, 0.0609, 0.0998, 0.0939,
        0.1065, 0.0812], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,112][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0161, 0.0844, 0.0557, 0.0591, 0.1177, 0.0670, 0.1023, 0.0856, 0.1424,
        0.1838, 0.0858], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,113][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.0494, 0.0131, 0.0235, 0.0550, 0.0339, 0.0238, 0.0207, 0.1933, 0.2767,
        0.2658, 0.0447], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,114][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0196, 0.0084, 0.0177, 0.0556, 0.0250, 0.0474, 0.0296, 0.2021, 0.2117,
        0.3288, 0.0539], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,116][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([9.7601e-05, 1.3076e-02, 8.7778e-02, 5.4388e-02, 5.2683e-02, 1.6623e-01,
        2.8240e-01, 5.5901e-02, 1.3744e-01, 1.2842e-01, 2.1587e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,120][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.1325, 0.0352, 0.0503, 0.0437, 0.1304, 0.1083, 0.1096, 0.1059, 0.0672,
        0.1666, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,125][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0970, 0.0214, 0.0498, 0.0894, 0.0146, 0.0124, 0.0302, 0.2858, 0.1120,
        0.1600, 0.0242, 0.1032], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,130][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0086, 0.0575, 0.0718, 0.0444, 0.0683, 0.1039, 0.1226, 0.1888, 0.0590,
        0.1019, 0.0809, 0.0923], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,133][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([3.3223e-05, 3.0433e-02, 1.4724e-01, 6.8374e-02, 5.6713e-02, 1.8159e-01,
        1.4418e-01, 8.6421e-02, 5.9769e-02, 1.1616e-01, 3.7777e-02, 7.1322e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,137][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.2389, 0.0285, 0.0198, 0.0720, 0.0241, 0.0207, 0.0467, 0.1432, 0.1477,
        0.1441, 0.0417, 0.0727], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,142][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0420, 0.0180, 0.0181, 0.1217, 0.0247, 0.0280, 0.0237, 0.1835, 0.1991,
        0.1929, 0.0442, 0.1042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,145][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.7147, 0.0231, 0.0118, 0.0244, 0.0150, 0.0177, 0.0115, 0.0479, 0.0356,
        0.0324, 0.0359, 0.0300], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,145][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.1809, 0.0503, 0.0357, 0.0677, 0.0600, 0.0675, 0.0770, 0.1251, 0.0999,
        0.0983, 0.0677, 0.0699], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,146][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0128, 0.1037, 0.0660, 0.0705, 0.1081, 0.0441, 0.1005, 0.0591, 0.1383,
        0.1363, 0.0602, 0.1002], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,147][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0317, 0.0153, 0.0217, 0.0481, 0.0248, 0.0234, 0.0272, 0.1634, 0.2777,
        0.2753, 0.0397, 0.0518], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,150][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0148, 0.0126, 0.0188, 0.0704, 0.0198, 0.0531, 0.0268, 0.1849, 0.1358,
        0.2992, 0.0613, 0.1025], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,151][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([6.7286e-05, 2.2577e-02, 9.3201e-02, 1.0876e-01, 4.9716e-02, 1.5167e-01,
        2.2207e-01, 7.0077e-02, 1.0200e-01, 9.0081e-02, 2.9602e-02, 6.0183e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,156][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.2280, 0.0470, 0.0781, 0.0603, 0.1167, 0.0787, 0.0958, 0.0949, 0.0443,
        0.0817, 0.0429, 0.0315], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:45,160][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0954, 0.0181, 0.0331, 0.0676, 0.0125, 0.0070, 0.0146, 0.2239, 0.0953,
        0.1216, 0.0175, 0.1196, 0.1736], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,165][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0058, 0.0489, 0.0494, 0.0358, 0.0433, 0.0824, 0.0865, 0.1420, 0.0586,
        0.0921, 0.0680, 0.1147, 0.1725], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,168][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([2.7387e-05, 2.5671e-02, 1.3373e-01, 5.4895e-02, 5.7150e-02, 1.4030e-01,
        1.4932e-01, 6.9723e-02, 2.7031e-02, 1.2521e-01, 3.5196e-02, 6.4309e-02,
        1.1744e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,173][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.2987, 0.0232, 0.0125, 0.0604, 0.0161, 0.0139, 0.0238, 0.1342, 0.1319,
        0.0933, 0.0376, 0.0619, 0.0925], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,177][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0511, 0.0154, 0.0204, 0.1422, 0.0207, 0.0196, 0.0190, 0.1522, 0.1148,
        0.1446, 0.0246, 0.0848, 0.1906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,178][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.9026, 0.0073, 0.0033, 0.0089, 0.0035, 0.0046, 0.0020, 0.0167, 0.0100,
        0.0090, 0.0113, 0.0122, 0.0085], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,179][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.2679, 0.0490, 0.0272, 0.0548, 0.0520, 0.0585, 0.0656, 0.0902, 0.0822,
        0.0812, 0.0648, 0.0635, 0.0433], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,180][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0196, 0.1154, 0.0679, 0.0665, 0.1177, 0.0307, 0.0707, 0.0487, 0.1160,
        0.1094, 0.0606, 0.0937, 0.0831], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,182][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0834, 0.0163, 0.0246, 0.0602, 0.0283, 0.0150, 0.0167, 0.1629, 0.1997,
        0.1642, 0.0313, 0.0597, 0.1377], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,185][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0150, 0.0111, 0.0180, 0.0692, 0.0227, 0.0329, 0.0234, 0.1772, 0.1234,
        0.1843, 0.0489, 0.0902, 0.1836], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,185][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([4.4377e-05, 2.4535e-02, 9.9115e-02, 9.8108e-02, 5.0121e-02, 1.0142e-01,
        2.0289e-01, 5.8659e-02, 4.5298e-02, 1.0139e-01, 1.8716e-02, 6.2148e-02,
        1.3756e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,189][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.1761, 0.0371, 0.0863, 0.0658, 0.1024, 0.0552, 0.1382, 0.0664, 0.0406,
        0.0901, 0.0324, 0.0390, 0.0705], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:45,194][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0351, 0.0113, 0.0168, 0.0459, 0.0065, 0.0033, 0.0053, 0.1420, 0.0325,
        0.0772, 0.0085, 0.0560, 0.1734, 0.3861], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,199][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0102, 0.0454, 0.0340, 0.0265, 0.0512, 0.0635, 0.0664, 0.1468, 0.0612,
        0.1088, 0.0947, 0.0824, 0.1344, 0.0743], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,202][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([3.9586e-05, 2.0217e-02, 1.8321e-01, 2.6280e-02, 2.1842e-02, 6.3091e-02,
        8.1647e-02, 5.5996e-02, 5.1082e-02, 8.8849e-02, 3.6244e-02, 8.9968e-02,
        2.1947e-01, 6.2057e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,206][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.2575, 0.0150, 0.0058, 0.0408, 0.0126, 0.0114, 0.0192, 0.1092, 0.0881,
        0.0796, 0.0314, 0.0413, 0.0754, 0.2128], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,211][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0261, 0.0115, 0.0124, 0.1067, 0.0122, 0.0114, 0.0113, 0.0975, 0.0976,
        0.1185, 0.0163, 0.0554, 0.1711, 0.2521], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,212][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.8111, 0.0090, 0.0063, 0.0187, 0.0070, 0.0074, 0.0035, 0.0248, 0.0243,
        0.0171, 0.0139, 0.0163, 0.0200, 0.0206], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,213][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.2758, 0.0500, 0.0198, 0.0511, 0.0450, 0.0487, 0.0454, 0.0911, 0.0783,
        0.0831, 0.0581, 0.0546, 0.0424, 0.0565], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,213][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0097, 0.0782, 0.0561, 0.0513, 0.0788, 0.0322, 0.0484, 0.0410, 0.1293,
        0.1326, 0.0550, 0.0849, 0.1231, 0.0794], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,216][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0709, 0.0087, 0.0138, 0.0519, 0.0225, 0.0084, 0.0105, 0.1134, 0.2069,
        0.1408, 0.0285, 0.0418, 0.1250, 0.1570], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,219][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0176, 0.0076, 0.0119, 0.0491, 0.0130, 0.0172, 0.0097, 0.0959, 0.0702,
        0.1270, 0.0258, 0.0543, 0.1658, 0.3349], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,224][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0002, 0.0171, 0.1559, 0.0673, 0.0294, 0.0404, 0.1275, 0.0547, 0.0387,
        0.0727, 0.0444, 0.0760, 0.2054, 0.0703], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,229][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.1671, 0.0399, 0.0877, 0.0714, 0.0975, 0.0808, 0.1022, 0.0790, 0.0471,
        0.0740, 0.0314, 0.0262, 0.0621, 0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:45,233][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0246, 0.0046, 0.0085, 0.0217, 0.0035, 0.0016, 0.0030, 0.0550, 0.0229,
        0.0547, 0.0049, 0.0279, 0.0866, 0.4730, 0.2076], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,238][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0067, 0.0560, 0.0430, 0.0269, 0.0384, 0.0687, 0.0432, 0.1027, 0.0621,
        0.1001, 0.0581, 0.0701, 0.1368, 0.0779, 0.1093], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,241][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([8.6629e-06, 1.7348e-02, 8.7479e-02, 4.0129e-02, 2.6610e-02, 1.0144e-01,
        9.1033e-02, 4.5534e-02, 6.0337e-02, 9.2997e-02, 2.0979e-02, 6.8761e-02,
        1.6787e-01, 6.7026e-02, 1.1245e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,243][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.1389, 0.0128, 0.0054, 0.0350, 0.0115, 0.0087, 0.0175, 0.1221, 0.0827,
        0.0834, 0.0333, 0.0382, 0.0624, 0.2197, 0.1284], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,244][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0182, 0.0063, 0.0074, 0.0671, 0.0096, 0.0092, 0.0093, 0.0667, 0.0662,
        0.0846, 0.0138, 0.0494, 0.1146, 0.2914, 0.1861], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,245][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.7275, 0.0129, 0.0063, 0.0242, 0.0096, 0.0076, 0.0044, 0.0315, 0.0193,
        0.0201, 0.0151, 0.0191, 0.0191, 0.0220, 0.0615], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,246][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.2198, 0.0476, 0.0237, 0.0644, 0.0436, 0.0412, 0.0632, 0.0883, 0.0801,
        0.0720, 0.0587, 0.0465, 0.0386, 0.0484, 0.0637], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,249][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0121, 0.0714, 0.0494, 0.0593, 0.0637, 0.0215, 0.0736, 0.0318, 0.1429,
        0.0780, 0.0418, 0.0866, 0.1059, 0.0741, 0.0880], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,252][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0185, 0.0067, 0.0103, 0.0443, 0.0124, 0.0061, 0.0081, 0.0916, 0.1104,
        0.1302, 0.0165, 0.0298, 0.1236, 0.2138, 0.1778], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,256][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0049, 0.0039, 0.0036, 0.0190, 0.0050, 0.0096, 0.0086, 0.0665, 0.0458,
        0.0972, 0.0165, 0.0256, 0.0967, 0.3685, 0.2287], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,259][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([2.3336e-05, 1.2316e-02, 3.0150e-02, 5.9893e-02, 1.9644e-02, 1.0334e-01,
        1.1543e-01, 5.9616e-02, 7.1868e-02, 7.8872e-02, 2.4301e-02, 5.5876e-02,
        1.8459e-01, 1.4027e-01, 4.3810e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,263][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0799, 0.0423, 0.0314, 0.0385, 0.0682, 0.0561, 0.1650, 0.0683, 0.0766,
        0.0974, 0.0352, 0.0277, 0.0743, 0.0732, 0.0660], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:45,268][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.1213, 0.0106, 0.0132, 0.0207, 0.0045, 0.0034, 0.0049, 0.0482, 0.0147,
        0.0309, 0.0058, 0.0256, 0.0411, 0.1624, 0.1218, 0.3709],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,273][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0039, 0.0408, 0.0353, 0.0255, 0.0376, 0.0784, 0.0823, 0.1094, 0.0310,
        0.0956, 0.0458, 0.0646, 0.1116, 0.0745, 0.0957, 0.0681],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,276][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([5.2677e-06, 1.0108e-02, 7.3477e-02, 1.6691e-02, 2.4063e-02, 1.3362e-01,
        7.9624e-02, 4.5244e-02, 2.0377e-02, 6.4920e-02, 2.5932e-02, 4.1933e-02,
        1.1931e-01, 4.1562e-02, 2.1551e-01, 8.7618e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,277][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.2805, 0.0189, 0.0081, 0.0288, 0.0123, 0.0086, 0.0166, 0.0660, 0.0548,
        0.0516, 0.0258, 0.0321, 0.0484, 0.1496, 0.0826, 0.1151],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,278][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0652, 0.0134, 0.0111, 0.0442, 0.0099, 0.0085, 0.0082, 0.0534, 0.0453,
        0.0515, 0.0137, 0.0342, 0.0818, 0.1265, 0.1641, 0.2689],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,278][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.7770, 0.0128, 0.0045, 0.0101, 0.0089, 0.0072, 0.0048, 0.0302, 0.0212,
        0.0118, 0.0131, 0.0164, 0.0098, 0.0125, 0.0288, 0.0310],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,281][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.1536, 0.0482, 0.0225, 0.0463, 0.0525, 0.0588, 0.0512, 0.0879, 0.0795,
        0.0691, 0.0627, 0.0511, 0.0337, 0.0536, 0.0748, 0.0544],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,284][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0067, 0.0753, 0.0283, 0.0363, 0.0754, 0.0327, 0.0779, 0.0392, 0.0802,
        0.0989, 0.0479, 0.0915, 0.1137, 0.0691, 0.0692, 0.0577],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,289][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0465, 0.0195, 0.0213, 0.0248, 0.0237, 0.0099, 0.0130, 0.0659, 0.0902,
        0.0789, 0.0205, 0.0216, 0.0466, 0.0978, 0.1477, 0.2722],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,294][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0143, 0.0059, 0.0059, 0.0204, 0.0101, 0.0182, 0.0095, 0.0629, 0.0260,
        0.0688, 0.0254, 0.0315, 0.0638, 0.2039, 0.2464, 0.1868],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,297][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([2.6447e-05, 1.1784e-02, 8.0383e-02, 3.9893e-02, 2.8153e-02, 1.0238e-01,
        1.5065e-01, 5.8911e-02, 2.5134e-02, 6.2588e-02, 2.9116e-02, 5.4265e-02,
        1.2532e-01, 8.9922e-02, 7.6670e-02, 6.4800e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,301][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.1596, 0.0275, 0.0527, 0.0314, 0.0763, 0.0610, 0.0996, 0.0806, 0.0268,
        0.0496, 0.0386, 0.0299, 0.0582, 0.0634, 0.1041, 0.0407],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:45,306][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0459, 0.0049, 0.0069, 0.0147, 0.0029, 0.0011, 0.0015, 0.0302, 0.0115,
        0.0166, 0.0027, 0.0146, 0.0224, 0.1101, 0.0857, 0.4369, 0.1915],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,309][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0065, 0.0478, 0.0277, 0.0334, 0.0350, 0.0617, 0.0478, 0.1044, 0.0336,
        0.0809, 0.0515, 0.0541, 0.1069, 0.0695, 0.1009, 0.0831, 0.0552],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,309][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([8.7767e-06, 1.7852e-02, 6.2651e-02, 2.9753e-02, 3.0190e-02, 1.0396e-01,
        7.9575e-02, 3.1544e-02, 1.8943e-02, 5.7105e-02, 2.9399e-02, 2.9100e-02,
        8.2418e-02, 7.0751e-02, 1.8323e-01, 6.7267e-02, 1.0625e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,310][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2156, 0.0136, 0.0050, 0.0307, 0.0099, 0.0068, 0.0095, 0.0473, 0.0458,
        0.0412, 0.0158, 0.0247, 0.0368, 0.1079, 0.0736, 0.1569, 0.1591],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,311][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0477, 0.0043, 0.0060, 0.0316, 0.0056, 0.0041, 0.0040, 0.0327, 0.0305,
        0.0310, 0.0070, 0.0173, 0.0542, 0.0720, 0.0722, 0.2960, 0.2839],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,314][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.8844, 0.0062, 0.0020, 0.0071, 0.0026, 0.0033, 0.0012, 0.0109, 0.0073,
        0.0054, 0.0066, 0.0072, 0.0054, 0.0050, 0.0137, 0.0220, 0.0096],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,318][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2257, 0.0404, 0.0183, 0.0396, 0.0345, 0.0446, 0.0452, 0.0756, 0.0606,
        0.0566, 0.0527, 0.0489, 0.0318, 0.0494, 0.0862, 0.0508, 0.0392],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,322][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0145, 0.0907, 0.0424, 0.0398, 0.0872, 0.0271, 0.0584, 0.0369, 0.1202,
        0.0929, 0.0423, 0.0653, 0.0705, 0.0449, 0.0543, 0.0593, 0.0534],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,327][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0555, 0.0065, 0.0084, 0.0205, 0.0126, 0.0048, 0.0049, 0.0449, 0.0585,
        0.0410, 0.0103, 0.0178, 0.0337, 0.0709, 0.0728, 0.3147, 0.2221],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,332][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0090, 0.0046, 0.0050, 0.0181, 0.0063, 0.0103, 0.0063, 0.0487, 0.0203,
        0.0454, 0.0137, 0.0202, 0.0427, 0.1450, 0.1519, 0.2119, 0.2405],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,335][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.7078e-05, 1.7210e-02, 4.7583e-02, 5.3643e-02, 3.0938e-02, 8.8533e-02,
        1.0490e-01, 3.8457e-02, 3.5011e-02, 5.3796e-02, 2.0830e-02, 3.7489e-02,
        1.2338e-01, 1.2162e-01, 6.2888e-02, 6.2830e-02, 1.0087e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,339][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1379, 0.0464, 0.0379, 0.0420, 0.0904, 0.0466, 0.1019, 0.0597, 0.0366,
        0.0544, 0.0345, 0.0243, 0.0482, 0.0817, 0.0786, 0.0426, 0.0362],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:45,341][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0188, 0.0056, 0.0083, 0.0125, 0.0033, 0.0025, 0.0030, 0.0227, 0.0095,
        0.0185, 0.0031, 0.0223, 0.0425, 0.1043, 0.0675, 0.2437, 0.3591, 0.0527],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,342][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0066, 0.0675, 0.0472, 0.0344, 0.0539, 0.0567, 0.0765, 0.1121, 0.0185,
        0.0762, 0.0587, 0.0575, 0.0597, 0.0342, 0.0659, 0.0619, 0.0434, 0.0692],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,343][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([8.8148e-06, 1.2199e-02, 9.5718e-02, 4.6713e-02, 2.3256e-02, 4.9753e-02,
        6.5195e-02, 2.2454e-02, 1.9410e-02, 7.3664e-02, 1.5881e-02, 3.7251e-02,
        8.8589e-02, 3.9429e-02, 1.2572e-01, 7.6729e-02, 1.9166e-01, 1.6376e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,344][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.1209, 0.0124, 0.0063, 0.0341, 0.0094, 0.0092, 0.0161, 0.0533, 0.0380,
        0.0389, 0.0220, 0.0301, 0.0395, 0.1079, 0.0770, 0.1128, 0.1408, 0.1312],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,347][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0104, 0.0065, 0.0066, 0.0475, 0.0084, 0.0078, 0.0083, 0.0340, 0.0313,
        0.0409, 0.0082, 0.0247, 0.0613, 0.0841, 0.1008, 0.1964, 0.2614, 0.0614],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,350][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.7157, 0.0083, 0.0053, 0.0129, 0.0070, 0.0088, 0.0025, 0.0213, 0.0115,
        0.0110, 0.0161, 0.0179, 0.0143, 0.0115, 0.0368, 0.0377, 0.0188, 0.0428],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,355][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.1718, 0.0307, 0.0176, 0.0513, 0.0420, 0.0427, 0.0524, 0.0631, 0.0566,
        0.0583, 0.0485, 0.0506, 0.0349, 0.0553, 0.0622, 0.0464, 0.0541, 0.0617],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,360][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0063, 0.0603, 0.0415, 0.0508, 0.0556, 0.0231, 0.0370, 0.0267, 0.0771,
        0.0669, 0.0253, 0.0740, 0.0641, 0.0690, 0.0663, 0.0905, 0.1018, 0.0638],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,364][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0350, 0.0073, 0.0109, 0.0198, 0.0097, 0.0052, 0.0040, 0.0353, 0.0428,
        0.0393, 0.0116, 0.0243, 0.0476, 0.0728, 0.0768, 0.1834, 0.3170, 0.0570],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,369][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0080, 0.0051, 0.0086, 0.0190, 0.0068, 0.0110, 0.0056, 0.0415, 0.0210,
        0.0413, 0.0153, 0.0260, 0.0475, 0.1097, 0.1680, 0.1343, 0.2655, 0.0658],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,371][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([3.8125e-05, 1.2160e-02, 6.9500e-02, 1.1650e-01, 3.2191e-02, 5.1450e-02,
        1.4818e-01, 2.2281e-02, 2.8477e-02, 7.0985e-02, 1.3471e-02, 3.1763e-02,
        7.6895e-02, 5.5678e-02, 5.6970e-02, 6.0180e-02, 1.3597e-01, 1.7316e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,373][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0690, 0.0514, 0.0687, 0.0651, 0.0789, 0.0435, 0.1228, 0.0441, 0.0420,
        0.0676, 0.0224, 0.0256, 0.0478, 0.0448, 0.0615, 0.0343, 0.0505, 0.0598],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:45,374][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.1584, 0.0072, 0.0040, 0.0107, 0.0019, 0.0010, 0.0014, 0.0199, 0.0055,
        0.0118, 0.0025, 0.0075, 0.0164, 0.0555, 0.0358, 0.1958, 0.1296, 0.0716,
        0.2635], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,375][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0093, 0.0467, 0.0316, 0.0171, 0.0266, 0.0390, 0.0364, 0.0955, 0.0368,
        0.0672, 0.0512, 0.0545, 0.1027, 0.0434, 0.0608, 0.0700, 0.0753, 0.0548,
        0.0810], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,376][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([2.4203e-05, 1.5746e-02, 1.0343e-01, 5.2850e-02, 4.0513e-02, 6.6210e-02,
        9.1435e-02, 4.8422e-02, 2.9282e-02, 8.0649e-02, 1.9145e-02, 3.0957e-02,
        7.3864e-02, 3.6131e-02, 9.0109e-02, 5.7796e-02, 1.1573e-01, 1.1031e-02,
        3.6679e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,379][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.2757, 0.0106, 0.0038, 0.0192, 0.0071, 0.0041, 0.0066, 0.0298, 0.0223,
        0.0251, 0.0116, 0.0156, 0.0185, 0.0520, 0.0434, 0.0695, 0.0749, 0.0982,
        0.2119], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,383][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0605, 0.0067, 0.0042, 0.0260, 0.0062, 0.0036, 0.0038, 0.0282, 0.0238,
        0.0230, 0.0069, 0.0157, 0.0391, 0.0579, 0.0524, 0.1685, 0.2136, 0.0783,
        0.1817], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,387][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.7399, 0.0079, 0.0029, 0.0105, 0.0070, 0.0049, 0.0028, 0.0230, 0.0167,
        0.0073, 0.0152, 0.0129, 0.0099, 0.0083, 0.0214, 0.0303, 0.0140, 0.0377,
        0.0273], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,392][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.2033, 0.0316, 0.0130, 0.0311, 0.0311, 0.0381, 0.0382, 0.0644, 0.0735,
        0.0532, 0.0492, 0.0458, 0.0288, 0.0448, 0.0755, 0.0445, 0.0357, 0.0549,
        0.0433], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,397][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0235, 0.0616, 0.0395, 0.0251, 0.0643, 0.0204, 0.0428, 0.0418, 0.1024,
        0.0854, 0.0543, 0.0632, 0.0586, 0.0370, 0.0528, 0.0540, 0.0628, 0.0379,
        0.0726], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,401][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0567, 0.0083, 0.0056, 0.0132, 0.0092, 0.0046, 0.0043, 0.0307, 0.0365,
        0.0413, 0.0094, 0.0089, 0.0250, 0.0468, 0.0462, 0.1749, 0.1579, 0.0593,
        0.2613], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,405][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0153, 0.0047, 0.0039, 0.0152, 0.0051, 0.0066, 0.0034, 0.0243, 0.0172,
        0.0383, 0.0099, 0.0108, 0.0313, 0.0737, 0.0855, 0.1290, 0.1968, 0.0780,
        0.2510], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,406][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([3.6328e-05, 9.6657e-03, 4.6677e-02, 7.8926e-02, 2.7910e-02, 5.4550e-02,
        7.7473e-02, 4.1357e-02, 3.6933e-02, 4.8346e-02, 1.6251e-02, 3.5379e-02,
        1.0739e-01, 6.4315e-02, 5.6414e-02, 7.3656e-02, 1.6307e-01, 1.8605e-02,
        4.3044e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,407][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.1151, 0.0282, 0.0497, 0.0448, 0.0843, 0.0363, 0.1205, 0.0582, 0.0459,
        0.0703, 0.0247, 0.0258, 0.0474, 0.0385, 0.0663, 0.0339, 0.0373, 0.0455,
        0.0273], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:45,408][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([1.0324e-01, 2.5764e-03, 9.4235e-04, 1.4445e-03, 4.6660e-04, 1.8864e-04,
        2.5267e-04, 3.6245e-03, 7.1819e-04, 1.7964e-03, 5.4207e-04, 1.5686e-03,
        2.0708e-03, 7.1532e-03, 3.9102e-03, 2.4130e-02, 1.6761e-02, 1.6485e-02,
        5.0670e-02, 7.6146e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,411][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0040, 0.0310, 0.0402, 0.0163, 0.0267, 0.0475, 0.0564, 0.0744, 0.0286,
        0.0669, 0.0402, 0.0637, 0.0900, 0.0560, 0.0733, 0.0549, 0.0483, 0.0514,
        0.0883, 0.0419], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,413][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([6.2858e-06, 8.3189e-03, 7.2026e-02, 1.1844e-02, 2.2166e-02, 1.1231e-01,
        7.0832e-02, 3.9713e-02, 1.8736e-02, 5.7963e-02, 2.2315e-02, 3.6108e-02,
        6.1190e-02, 2.5264e-02, 1.5545e-01, 5.6484e-02, 1.3018e-01, 1.0149e-02,
        5.7347e-02, 3.1602e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,418][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.2550, 0.0063, 0.0015, 0.0061, 0.0033, 0.0015, 0.0021, 0.0106, 0.0066,
        0.0076, 0.0042, 0.0050, 0.0058, 0.0128, 0.0081, 0.0261, 0.0268, 0.0394,
        0.0731, 0.4982], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,420][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([4.6373e-02, 2.2331e-03, 1.2625e-03, 5.5935e-03, 1.3382e-03, 7.7654e-04,
        6.3622e-04, 6.5517e-03, 4.2583e-03, 4.6337e-03, 1.6158e-03, 2.9977e-03,
        6.4587e-03, 6.9952e-03, 8.0107e-03, 4.0739e-02, 4.1427e-02, 2.5306e-02,
        6.0864e-02, 7.3193e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,425][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.8665, 0.0073, 0.0019, 0.0046, 0.0037, 0.0028, 0.0014, 0.0120, 0.0066,
        0.0034, 0.0069, 0.0047, 0.0030, 0.0032, 0.0077, 0.0107, 0.0051, 0.0155,
        0.0114, 0.0215], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,429][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.2500, 0.0437, 0.0205, 0.0385, 0.0442, 0.0414, 0.0310, 0.0526, 0.0358,
        0.0470, 0.0456, 0.0378, 0.0221, 0.0269, 0.0428, 0.0373, 0.0271, 0.0565,
        0.0431, 0.0563], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,434][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0157, 0.0628, 0.0218, 0.0168, 0.0580, 0.0227, 0.0436, 0.0413, 0.0706,
        0.0781, 0.0502, 0.0663, 0.0610, 0.0364, 0.0577, 0.0564, 0.0742, 0.0536,
        0.0723, 0.0406], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,438][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0864, 0.0059, 0.0042, 0.0037, 0.0044, 0.0010, 0.0015, 0.0106, 0.0080,
        0.0069, 0.0033, 0.0026, 0.0049, 0.0124, 0.0109, 0.0395, 0.0339, 0.0243,
        0.1462, 0.5895], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,439][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0125, 0.0021, 0.0011, 0.0038, 0.0022, 0.0026, 0.0010, 0.0090, 0.0031,
        0.0080, 0.0038, 0.0041, 0.0070, 0.0179, 0.0213, 0.0308, 0.0451, 0.0315,
        0.0805, 0.7125], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,439][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([2.7504e-05, 9.3159e-03, 5.9328e-02, 2.6297e-02, 2.9196e-02, 1.1307e-01,
        6.1153e-02, 4.0023e-02, 2.2940e-02, 4.1132e-02, 2.0740e-02, 2.6806e-02,
        5.9748e-02, 3.9775e-02, 6.5422e-02, 6.9987e-02, 1.1851e-01, 2.0355e-02,
        8.9209e-02, 8.6971e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,440][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.1297, 0.0245, 0.0497, 0.0222, 0.0666, 0.0421, 0.0999, 0.0669, 0.0316,
        0.0594, 0.0296, 0.0259, 0.0439, 0.0499, 0.0703, 0.0344, 0.0404, 0.0543,
        0.0422, 0.0165], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:45,443][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([2.3396e-02, 8.1224e-04, 5.0288e-04, 1.0055e-03, 2.2512e-04, 7.5484e-05,
        7.3456e-05, 1.6195e-03, 4.5836e-04, 7.4896e-04, 2.0899e-04, 7.7721e-04,
        9.5258e-04, 3.8380e-03, 2.7261e-03, 2.2219e-02, 7.6347e-03, 6.7922e-03,
        3.5120e-02, 8.2940e-01, 6.1418e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,446][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0068, 0.0408, 0.0244, 0.0269, 0.0282, 0.0492, 0.0415, 0.0856, 0.0257,
        0.0647, 0.0410, 0.0443, 0.0796, 0.0544, 0.0780, 0.0573, 0.0370, 0.0611,
        0.0553, 0.0640, 0.0344], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,449][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([1.0344e-05, 1.5030e-02, 5.3260e-02, 2.1770e-02, 2.4371e-02, 7.5433e-02,
        7.6916e-02, 2.6697e-02, 1.5283e-02, 4.5489e-02, 2.6834e-02, 2.3724e-02,
        6.4421e-02, 5.3287e-02, 1.6054e-01, 4.7821e-02, 8.4028e-02, 1.8260e-02,
        3.8453e-02, 3.8552e-02, 8.9819e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,453][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1124, 0.0035, 0.0009, 0.0057, 0.0019, 0.0011, 0.0013, 0.0068, 0.0057,
        0.0059, 0.0027, 0.0036, 0.0048, 0.0121, 0.0084, 0.0228, 0.0202, 0.0334,
        0.0510, 0.5895, 0.1064], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,456][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.4558e-02, 9.5289e-04, 8.0917e-04, 4.2485e-03, 8.9033e-04, 5.1821e-04,
        4.3618e-04, 3.8998e-03, 2.9719e-03, 3.0633e-03, 1.0390e-03, 1.9715e-03,
        5.3739e-03, 5.8661e-03, 6.1157e-03, 3.0484e-02, 2.7558e-02, 1.2199e-02,
        3.8197e-02, 6.7019e-01, 1.5866e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,459][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([8.8847e-01, 3.9224e-03, 1.1378e-03, 3.9489e-03, 1.4715e-03, 1.7315e-03,
        5.8619e-04, 5.5233e-03, 3.0329e-03, 2.7806e-03, 4.2000e-03, 3.3138e-03,
        2.5319e-03, 2.1057e-03, 5.7710e-03, 1.1951e-02, 4.6389e-03, 1.1290e-02,
        9.4115e-03, 2.3308e-02, 8.8725e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,464][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.2456, 0.0355, 0.0140, 0.0302, 0.0280, 0.0348, 0.0350, 0.0562, 0.0448,
        0.0426, 0.0432, 0.0359, 0.0215, 0.0344, 0.0673, 0.0372, 0.0253, 0.0561,
        0.0367, 0.0500, 0.0257], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,468][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0192, 0.0648, 0.0299, 0.0260, 0.0633, 0.0200, 0.0439, 0.0287, 0.0962,
        0.0799, 0.0356, 0.0501, 0.0534, 0.0313, 0.0430, 0.0446, 0.0393, 0.0467,
        0.0664, 0.0680, 0.0496], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,470][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([3.0537e-02, 1.5460e-03, 1.0476e-03, 2.6418e-03, 1.7753e-03, 5.7444e-04,
        5.1590e-04, 5.0124e-03, 4.3091e-03, 3.4723e-03, 1.4918e-03, 1.7959e-03,
        2.8030e-03, 4.9808e-03, 4.3991e-03, 2.9915e-02, 1.7140e-02, 1.2033e-02,
        5.7855e-02, 6.8131e-01, 1.3484e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,471][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0049, 0.0014, 0.0009, 0.0032, 0.0012, 0.0017, 0.0010, 0.0065, 0.0024,
        0.0059, 0.0024, 0.0028, 0.0051, 0.0145, 0.0150, 0.0303, 0.0283, 0.0212,
        0.0608, 0.6436, 0.1471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,472][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.7526e-05, 1.5268e-02, 3.9784e-02, 4.0304e-02, 2.4593e-02, 7.5967e-02,
        9.8750e-02, 2.7968e-02, 2.5511e-02, 4.1616e-02, 1.5796e-02, 2.5107e-02,
        8.7374e-02, 7.5679e-02, 4.7386e-02, 4.0509e-02, 7.4284e-02, 2.5282e-02,
        4.0494e-02, 6.0058e-02, 1.1825e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,473][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1475, 0.0338, 0.0297, 0.0303, 0.0741, 0.0369, 0.0894, 0.0499, 0.0295,
        0.0464, 0.0287, 0.0206, 0.0385, 0.0600, 0.0678, 0.0331, 0.0288, 0.0715,
        0.0355, 0.0208, 0.0272], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:45,476][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:45,479][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[1216],
        [ 796],
        [ 110],
        [  61],
        [  11],
        [  55],
        [ 176],
        [ 544],
        [ 469],
        [ 140],
        [ 441],
        [  62],
        [ 166],
        [ 615],
        [ 917],
        [  30],
        [  48],
        [ 248],
        [  81],
        [  67],
        [  15]], device='cuda:0')
[2024-07-23 21:06:45,482][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[1265],
        [1702],
        [ 317],
        [ 162],
        [  22],
        [ 115],
        [ 350],
        [ 948],
        [ 839],
        [ 284],
        [ 764],
        [  97],
        [ 415],
        [1573],
        [2464],
        [  93],
        [ 128],
        [ 493],
        [ 163],
        [ 116],
        [  40]], device='cuda:0')
[2024-07-23 21:06:45,485][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[25579],
        [19206],
        [23282],
        [22395],
        [21136],
        [22860],
        [27999],
        [35074],
        [35843],
        [36255],
        [36869],
        [37659],
        [38840],
        [38640],
        [35254],
        [35339],
        [34289],
        [35188],
        [33882],
        [31555],
        [30764]], device='cuda:0')
[2024-07-23 21:06:45,488][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 2374],
        [   82],
        [  142],
        [  122],
        [  129],
        [ 3224],
        [ 2285],
        [ 9129],
        [12413],
        [14543],
        [17142],
        [18322],
        [20177],
        [18943],
        [18025],
        [19797],
        [22901],
        [18071],
        [16584],
        [14523],
        [16991]], device='cuda:0')
[2024-07-23 21:06:45,491][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[23670],
        [28320],
        [19534],
        [19299],
        [34624],
        [32086],
        [26788],
        [30274],
        [36321],
        [34149],
        [31483],
        [28954],
        [28399],
        [24744],
        [21374],
        [17158],
        [15661],
        [15967],
        [15820],
        [13590],
        [14297]], device='cuda:0')
[2024-07-23 21:06:45,495][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 4377],
        [ 1564],
        [ 1567],
        [ 1230],
        [ 2130],
        [ 2036],
        [ 6265],
        [ 6263],
        [ 5054],
        [ 4848],
        [ 8397],
        [ 8674],
        [ 9441],
        [19789],
        [21585],
        [20499],
        [23789],
        [23119],
        [26910],
        [29834],
        [32001]], device='cuda:0')
[2024-07-23 21:06:45,498][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[11637],
        [ 6881],
        [13602],
        [10395],
        [10650],
        [10531],
        [11900],
        [10262],
        [14568],
        [13314],
        [14827],
        [13380],
        [11672],
        [10438],
        [10176],
        [ 8711],
        [ 7110],
        [ 7312],
        [ 7285],
        [ 4936],
        [ 4769]], device='cuda:0')
[2024-07-23 21:06:45,501][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 7955],
        [ 5880],
        [ 7013],
        [15163],
        [13370],
        [13672],
        [16497],
        [15589],
        [15762],
        [15089],
        [15900],
        [15146],
        [15382],
        [15583],
        [14825],
        [13719],
        [14576],
        [14487],
        [13755],
        [14552],
        [14954]], device='cuda:0')
[2024-07-23 21:06:45,504][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[26129],
        [16423],
        [12818],
        [10446],
        [ 8176],
        [ 8011],
        [ 7419],
        [ 6597],
        [ 5967],
        [ 5377],
        [ 5225],
        [ 5193],
        [ 4767],
        [ 4702],
        [ 4722],
        [ 4755],
        [ 4792],
        [ 4810],
        [ 4743],
        [ 4843],
        [ 4949]], device='cuda:0')
[2024-07-23 21:06:45,507][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 8614],
        [11424],
        [11513],
        [12990],
        [11841],
        [12917],
        [13280],
        [13907],
        [12292],
        [11500],
        [12149],
        [11561],
        [11764],
        [12299],
        [12535],
        [13570],
        [13835],
        [14505],
        [14214],
        [14578],
        [13485]], device='cuda:0')
[2024-07-23 21:06:45,509][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[21160],
        [26441],
        [18600],
        [14905],
        [14136],
        [13914],
        [13346],
        [14591],
        [11150],
        [13531],
        [12208],
        [10856],
        [10458],
        [10233],
        [ 9806],
        [ 9809],
        [ 9379],
        [ 9940],
        [ 9632],
        [ 9009],
        [ 8589]], device='cuda:0')
[2024-07-23 21:06:45,510][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[34827],
        [ 1593],
        [ 3251],
        [ 3347],
        [ 2929],
        [ 2740],
        [ 2710],
        [ 1892],
        [ 1898],
        [ 1931],
        [ 1577],
        [ 1543],
        [ 1469],
        [ 1605],
        [ 1610],
        [ 1495],
        [ 1512],
        [ 1449],
        [ 1534],
        [ 1633],
        [ 1685]], device='cuda:0')
[2024-07-23 21:06:45,513][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[15946],
        [ 2334],
        [ 2222],
        [ 1508],
        [ 2115],
        [ 1838],
        [  744],
        [  691],
        [  982],
        [ 1166],
        [ 1201],
        [ 1138],
        [  929],
        [  998],
        [  944],
        [  660],
        [  768],
        [  736],
        [  855],
        [  739],
        [  765]], device='cuda:0')
[2024-07-23 21:06:45,515][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[39268],
        [30386],
        [34634],
        [40819],
        [39577],
        [39673],
        [33704],
        [33007],
        [32553],
        [32633],
        [33120],
        [33775],
        [33277],
        [33559],
        [32556],
        [33896],
        [34149],
        [34297],
        [33980],
        [34948],
        [35723]], device='cuda:0')
[2024-07-23 21:06:45,518][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[2022],
        [3008],
        [2127],
        [2287],
        [2656],
        [1526],
        [2133],
        [2618],
        [2476],
        [2112],
        [1739],
        [1937],
        [1622],
        [1903],
        [1849],
        [1758],
        [1458],
        [2586],
        [1836],
        [2253],
        [1431]], device='cuda:0')
[2024-07-23 21:06:45,522][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[20277],
        [13857],
        [  384],
        [  241],
        [  289],
        [  459],
        [  552],
        [ 1564],
        [ 3411],
        [ 4165],
        [ 4446],
        [ 3636],
        [ 4025],
        [ 3397],
        [ 3439],
        [ 1901],
        [ 2375],
        [ 3496],
        [ 2351],
        [ 2321],
        [ 2550]], device='cuda:0')
[2024-07-23 21:06:45,525][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[32539],
        [16091],
        [15573],
        [10118],
        [ 9858],
        [ 9998],
        [10264],
        [ 6156],
        [ 5605],
        [ 5327],
        [ 4653],
        [ 5302],
        [ 5095],
        [ 5370],
        [ 5584],
        [ 5378],
        [ 5174],
        [ 5637],
        [ 5095],
        [ 5297],
        [ 5407]], device='cuda:0')
[2024-07-23 21:06:45,528][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[13466],
        [37445],
        [23605],
        [24262],
        [22976],
        [22454],
        [26516],
        [25241],
        [23708],
        [22769],
        [22038],
        [21864],
        [19983],
        [19826],
        [18966],
        [18551],
        [20043],
        [19328],
        [19388],
        [18499],
        [19505]], device='cuda:0')
[2024-07-23 21:06:45,531][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 7465],
        [10200],
        [ 9204],
        [ 6123],
        [ 6811],
        [ 6924],
        [20848],
        [11496],
        [ 9609],
        [11616],
        [11697],
        [12882],
        [11121],
        [17418],
        [14610],
        [14015],
        [14406],
        [10181],
        [ 8378],
        [ 6352],
        [ 7680]], device='cuda:0')
[2024-07-23 21:06:45,535][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 976],
        [ 225],
        [ 799],
        [2334],
        [3098],
        [2771],
        [3459],
        [4263],
        [5899],
        [4362],
        [4626],
        [5680],
        [5035],
        [7554],
        [8398],
        [7991],
        [7659],
        [7431],
        [7982],
        [8182],
        [7439]], device='cuda:0')
[2024-07-23 21:06:45,538][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[13520],
        [12655],
        [12477],
        [13035],
        [12885],
        [13166],
        [14634],
        [14309],
        [16751],
        [18188],
        [18107],
        [18080],
        [16949],
        [18841],
        [16787],
        [17246],
        [16799],
        [14497],
        [15763],
        [17575],
        [17307]], device='cuda:0')
[2024-07-23 21:06:45,541][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 3098],
        [ 9029],
        [12290],
        [20448],
        [24951],
        [24340],
        [21892],
        [23184],
        [19930],
        [17029],
        [17085],
        [17390],
        [17007],
        [17026],
        [17424],
        [17159],
        [16487],
        [16591],
        [16171],
        [17016],
        [16333]], device='cuda:0')
[2024-07-23 21:06:45,544][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[11363],
        [  145],
        [  155],
        [   97],
        [   32],
        [   32],
        [   40],
        [   46],
        [   85],
        [   87],
        [   88],
        [   72],
        [   72],
        [   71],
        [   90],
        [   79],
        [   85],
        [   74],
        [   80],
        [   72],
        [   84]], device='cuda:0')
[2024-07-23 21:06:45,546][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[19242],
        [ 6905],
        [ 8785],
        [10746],
        [ 5719],
        [ 5947],
        [ 6149],
        [ 2905],
        [ 3586],
        [ 3555],
        [ 2594],
        [ 2844],
        [ 2878],
        [ 3419],
        [ 3580],
        [ 3317],
        [ 3690],
        [ 3814],
        [ 3605],
        [ 5990],
        [ 6775]], device='cuda:0')
[2024-07-23 21:06:45,547][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[25045],
        [21211],
        [16283],
        [18269],
        [19322],
        [20841],
        [28225],
        [34301],
        [34486],
        [36807],
        [36872],
        [36466],
        [35216],
        [34051],
        [33403],
        [30455],
        [28364],
        [29240],
        [30255],
        [24204],
        [25004]], device='cuda:0')
[2024-07-23 21:06:45,549][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[32568],
        [42677],
        [41642],
        [41485],
        [42087],
        [42805],
        [40574],
        [39516],
        [39179],
        [40236],
        [39614],
        [40515],
        [42179],
        [43318],
        [43368],
        [43314],
        [44484],
        [44212],
        [45076],
        [44857],
        [44980]], device='cuda:0')
[2024-07-23 21:06:45,552][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[1501],
        [ 710],
        [1375],
        [1445],
        [2502],
        [2040],
        [1022],
        [ 921],
        [1083],
        [1121],
        [1250],
        [1134],
        [1068],
        [1077],
        [ 947],
        [1073],
        [1182],
        [1121],
        [1194],
        [1228],
        [1272]], device='cuda:0')
[2024-07-23 21:06:45,555][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[25757],
        [45849],
        [47110],
        [46519],
        [46428],
        [46141],
        [44075],
        [44610],
        [43308],
        [43439],
        [43982],
        [43932],
        [44623],
        [43406],
        [43565],
        [45155],
        [44485],
        [44568],
        [45355],
        [45461],
        [45039]], device='cuda:0')
[2024-07-23 21:06:45,559][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[46250],
        [44751],
        [47173],
        [47373],
        [46160],
        [47640],
        [47745],
        [47080],
        [46633],
        [47111],
        [47452],
        [46847],
        [47441],
        [46560],
        [46705],
        [46979],
        [47088],
        [45925],
        [46521],
        [46575],
        [47165]], device='cuda:0')
[2024-07-23 21:06:45,562][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921],
        [2921]], device='cuda:0')
[2024-07-23 21:06:45,645][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:45,650][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,653][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,654][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,654][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,655][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,656][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,656][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,657][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,658][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,658][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,659][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,659][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:45,660][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9974, 0.0026], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,661][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.1517, 0.8483], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,662][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1826, 0.8174], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,662][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.4212, 0.5788], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,665][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8215, 0.1785], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,666][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ language] are: tensor([9.9931e-01, 6.9422e-04], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,667][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1815, 0.8185], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,667][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0563, 0.9437], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,668][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.3143, 0.6857], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,671][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.1623, 0.8377], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,674][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.8182, 0.1818], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,678][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8521, 0.1479], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:45,682][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.9919, 0.0031, 0.0049], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,687][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0985, 0.5800, 0.3214], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,692][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0943, 0.3246, 0.5811], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,696][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.2934, 0.2570, 0.4496], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,698][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.2534, 0.7089, 0.0376], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,699][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.9858, 0.0067, 0.0075], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,699][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.3556, 0.4393, 0.2051], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,700][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0102, 0.0842, 0.9055], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,701][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.1433, 0.2770, 0.5797], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,703][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0966, 0.6135, 0.2899], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,707][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.2181, 0.5090, 0.2729], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,712][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.3340, 0.1353, 0.5307], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:45,716][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.9531, 0.0014, 0.0020, 0.0434], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,721][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0761, 0.4778, 0.2774, 0.1687], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,725][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0439, 0.1857, 0.3493, 0.4211], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,730][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.1117, 0.2552, 0.2933, 0.3397], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,731][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.2403, 0.6296, 0.0553, 0.0747], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,731][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.9275, 0.0208, 0.0261, 0.0256], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,732][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.2687, 0.2927, 0.1542, 0.2844], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,733][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0063, 0.0418, 0.3489, 0.6029], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,736][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.1411, 0.2293, 0.4849, 0.1447], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,739][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0818, 0.5003, 0.2249, 0.1930], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,744][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.2235, 0.2851, 0.1251, 0.3663], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,749][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.1606, 0.0839, 0.6432, 0.1123], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:45,754][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.9526, 0.0019, 0.0021, 0.0348, 0.0087], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,759][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0619, 0.4280, 0.2237, 0.1382, 0.1482], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,763][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0287, 0.1303, 0.2843, 0.3188, 0.2379], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,764][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.1706, 0.1079, 0.2480, 0.3365, 0.1369], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,764][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.3228, 0.5092, 0.0369, 0.0701, 0.0611], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,765][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.9854, 0.0020, 0.0025, 0.0076, 0.0025], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,766][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0963, 0.3689, 0.1673, 0.1886, 0.1789], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,769][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0021, 0.0347, 0.3196, 0.5200, 0.1236], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,773][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.0714, 0.2029, 0.4130, 0.1368, 0.1759], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,778][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0677, 0.3723, 0.1720, 0.1479, 0.2401], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,783][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.4425, 0.1295, 0.0938, 0.2428, 0.0914], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,788][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.2311, 0.0455, 0.5008, 0.1810, 0.0416], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:45,794][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.9406, 0.0013, 0.0021, 0.0421, 0.0120, 0.0018], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,796][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0623, 0.3907, 0.1984, 0.1377, 0.1311, 0.0797], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,797][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0224, 0.0874, 0.2058, 0.2596, 0.1971, 0.2275], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,798][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.1241, 0.0768, 0.1675, 0.2873, 0.1044, 0.2400], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,798][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.4170, 0.3279, 0.0220, 0.0377, 0.0545, 0.1408], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,799][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.9581, 0.0054, 0.0045, 0.0097, 0.0042, 0.0181], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,802][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0440, 0.2418, 0.1777, 0.2232, 0.1788, 0.1345], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,807][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0019, 0.0377, 0.2798, 0.4658, 0.1230, 0.0918], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,812][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.0839, 0.1620, 0.3168, 0.1086, 0.1479, 0.1809], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,817][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0788, 0.3117, 0.1365, 0.1228, 0.1790, 0.1712], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,822][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.3924, 0.1910, 0.0776, 0.1587, 0.0576, 0.1226], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,827][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.4217, 0.0738, 0.2448, 0.0783, 0.0456, 0.1358], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:45,829][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [aut] are: tensor([9.4285e-01, 2.1719e-03, 2.2113e-03, 4.2088e-02, 9.0374e-03, 1.1938e-03,
        4.5069e-04], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,830][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0480, 0.3225, 0.1824, 0.1241, 0.1076, 0.0767, 0.1387],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,831][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0226, 0.0777, 0.1741, 0.2112, 0.1580, 0.1767, 0.1796],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,832][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.1368, 0.0939, 0.1579, 0.2197, 0.0817, 0.1886, 0.1214],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,833][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.0474, 0.4000, 0.0222, 0.0589, 0.0766, 0.3656, 0.0293],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,836][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.7801, 0.0178, 0.0253, 0.0357, 0.0170, 0.0911, 0.0330],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,841][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.0636, 0.2294, 0.1223, 0.1819, 0.1292, 0.1102, 0.1634],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,846][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0027, 0.0293, 0.2158, 0.3786, 0.1082, 0.0936, 0.1718],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,851][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0469, 0.1328, 0.2784, 0.0881, 0.1228, 0.1596, 0.1714],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,855][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0482, 0.2885, 0.1254, 0.1141, 0.1565, 0.1483, 0.1191],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,860][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.2087, 0.1124, 0.0670, 0.1619, 0.0480, 0.1027, 0.2992],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,866][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.2827, 0.0488, 0.1190, 0.0418, 0.0400, 0.1284, 0.3394],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:45,866][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ista] are: tensor([9.6819e-01, 1.2139e-03, 1.3831e-03, 2.1412e-02, 4.6352e-03, 1.0686e-03,
        4.2751e-04, 1.6662e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,867][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0351, 0.2437, 0.1530, 0.1064, 0.1155, 0.0726, 0.1440, 0.1296],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,868][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0129, 0.0551, 0.1415, 0.1697, 0.1376, 0.1400, 0.1565, 0.1867],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,869][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.1071, 0.0936, 0.1056, 0.1664, 0.1087, 0.2516, 0.1220, 0.0450],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,872][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0773, 0.2667, 0.0217, 0.0639, 0.0608, 0.3193, 0.0407, 0.1496],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,877][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.4588, 0.0310, 0.0280, 0.0381, 0.0307, 0.1939, 0.0560, 0.1635],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,882][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.0747, 0.1730, 0.0810, 0.1163, 0.1268, 0.1117, 0.1347, 0.1818],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,887][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0016, 0.0212, 0.1786, 0.3380, 0.1005, 0.0843, 0.1748, 0.1011],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,892][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0260, 0.0929, 0.2217, 0.0841, 0.1065, 0.1661, 0.1693, 0.1335],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,897][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ista] are: tensor([0.0474, 0.2631, 0.1060, 0.0942, 0.1366, 0.1320, 0.0914, 0.1293],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,899][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0847, 0.1110, 0.0523, 0.1180, 0.0556, 0.1604, 0.1664, 0.2516],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,900][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.2409, 0.0275, 0.1491, 0.1520, 0.0424, 0.1080, 0.2269, 0.0531],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:45,901][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ de] are: tensor([9.6704e-01, 1.2689e-03, 1.1230e-03, 2.3209e-02, 5.2334e-03, 5.6383e-04,
        3.2334e-04, 1.0286e-03, 2.1221e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,902][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0390, 0.2404, 0.1541, 0.0841, 0.0971, 0.0595, 0.1083, 0.1232, 0.0943],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,904][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0168, 0.0411, 0.0978, 0.1164, 0.0942, 0.1108, 0.1189, 0.1404, 0.2635],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,907][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0425, 0.0718, 0.1425, 0.1629, 0.0606, 0.2001, 0.1221, 0.0403, 0.1572],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,912][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0887, 0.3113, 0.0156, 0.0557, 0.0528, 0.1666, 0.0176, 0.0908, 0.2009],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,918][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.6872, 0.0163, 0.0088, 0.0136, 0.0125, 0.0511, 0.0239, 0.0558, 0.1307],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,922][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.0904, 0.1222, 0.0654, 0.0950, 0.1076, 0.0978, 0.1208, 0.1763, 0.1245],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,927][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0018, 0.0163, 0.1396, 0.2715, 0.0715, 0.0688, 0.1366, 0.0858, 0.2081],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,933][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0277, 0.1045, 0.1933, 0.0728, 0.0788, 0.1003, 0.1270, 0.1222, 0.1735],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,933][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0529, 0.2305, 0.0930, 0.0847, 0.1211, 0.1157, 0.0818, 0.1116, 0.1086],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,934][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.1425, 0.0808, 0.0261, 0.0592, 0.0373, 0.0595, 0.0780, 0.1649, 0.3515],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,935][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0871, 0.0232, 0.1258, 0.0698, 0.0597, 0.0517, 0.2484, 0.2143, 0.1201],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:45,936][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ An] are: tensor([9.5198e-01, 1.5293e-03, 1.6237e-03, 3.2650e-02, 5.9453e-03, 1.2007e-03,
        3.5287e-04, 1.1001e-03, 2.2761e-04, 3.3887e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,939][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0395, 0.2458, 0.1308, 0.0807, 0.0850, 0.0502, 0.1026, 0.1100, 0.0819,
        0.0735], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,944][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0063, 0.0357, 0.0983, 0.1078, 0.0854, 0.0979, 0.1037, 0.1206, 0.2411,
        0.1032], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,949][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.1493, 0.0613, 0.0932, 0.0943, 0.0752, 0.1552, 0.0999, 0.0342, 0.0855,
        0.1519], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,954][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.3556, 0.1869, 0.0136, 0.0237, 0.0241, 0.1075, 0.0095, 0.0416, 0.1133,
        0.1242], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,958][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.9602, 0.0028, 0.0017, 0.0028, 0.0022, 0.0049, 0.0015, 0.0076, 0.0127,
        0.0035], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,963][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0178, 0.1349, 0.0965, 0.1070, 0.0902, 0.0722, 0.1284, 0.1530, 0.0954,
        0.1046], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,966][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0008, 0.0160, 0.1541, 0.2841, 0.0710, 0.0597, 0.1265, 0.0688, 0.1906,
        0.0284], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,966][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0470, 0.0732, 0.1657, 0.0620, 0.0713, 0.0953, 0.1044, 0.1156, 0.1125,
        0.1529], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,967][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0426, 0.1811, 0.0847, 0.0733, 0.1083, 0.0972, 0.0714, 0.0954, 0.0909,
        0.1551], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,968][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.3666, 0.0577, 0.0318, 0.0670, 0.0445, 0.0452, 0.0527, 0.0746, 0.0856,
        0.1743], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,970][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.1990, 0.0359, 0.0992, 0.0311, 0.0131, 0.0623, 0.1011, 0.2087, 0.1196,
        0.1301], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:45,972][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [za] are: tensor([9.0250e-01, 2.5145e-03, 3.2031e-03, 5.7677e-02, 1.3800e-02, 2.4052e-03,
        9.9680e-04, 3.0759e-03, 6.5859e-04, 7.6657e-03, 5.5027e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,977][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0310, 0.1856, 0.1102, 0.0727, 0.0868, 0.0525, 0.1049, 0.0977, 0.0823,
        0.0729, 0.1034], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,982][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.0040, 0.0293, 0.0815, 0.0932, 0.0801, 0.0935, 0.0934, 0.1074, 0.2329,
        0.1007, 0.0840], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,986][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.1194, 0.0383, 0.0462, 0.1326, 0.0689, 0.1713, 0.0838, 0.0146, 0.0654,
        0.1454, 0.1142], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,991][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.3601, 0.0973, 0.0091, 0.0236, 0.0264, 0.1039, 0.0118, 0.0619, 0.1538,
        0.1344, 0.0178], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,996][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.5968, 0.0152, 0.0182, 0.0261, 0.0182, 0.0357, 0.0145, 0.0674, 0.1379,
        0.0528, 0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,998][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0181, 0.1133, 0.0834, 0.0944, 0.0892, 0.0770, 0.1167, 0.1353, 0.0919,
        0.1010, 0.0798], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:45,999][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0007, 0.0165, 0.1620, 0.2997, 0.0698, 0.0556, 0.1140, 0.0598, 0.1851,
        0.0247, 0.0123], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,000][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0134, 0.0665, 0.1500, 0.0575, 0.0747, 0.0869, 0.1231, 0.1034, 0.1150,
        0.1554, 0.0541], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,001][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0310, 0.1448, 0.0705, 0.0610, 0.0988, 0.0853, 0.0564, 0.0858, 0.0791,
        0.1386, 0.1486], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,003][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.1825, 0.0359, 0.0235, 0.0582, 0.0348, 0.0551, 0.0455, 0.1258, 0.1022,
        0.2293, 0.1071], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,007][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.3830, 0.0066, 0.0644, 0.0577, 0.0158, 0.0636, 0.0737, 0.0391, 0.0697,
        0.1978, 0.0285], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,009][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ is] are: tensor([8.9917e-01, 1.6184e-03, 3.6305e-03, 5.9760e-02, 1.2709e-02, 2.5943e-03,
        1.4354e-03, 2.9378e-03, 8.2327e-04, 8.1330e-03, 5.3788e-03, 1.8125e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,014][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0256, 0.1907, 0.1073, 0.0680, 0.0722, 0.0455, 0.0934, 0.0969, 0.0758,
        0.0652, 0.0960, 0.0635], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,019][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0063, 0.0296, 0.0742, 0.0912, 0.0664, 0.0807, 0.0822, 0.0959, 0.1763,
        0.0849, 0.0660, 0.1463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,024][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0210, 0.0356, 0.0513, 0.1265, 0.0470, 0.0730, 0.0658, 0.0141, 0.0639,
        0.1141, 0.1423, 0.2455], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,029][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.3179, 0.2090, 0.0130, 0.0270, 0.0212, 0.0730, 0.0120, 0.0317, 0.0902,
        0.1241, 0.0359, 0.0449], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,031][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.3164, 0.0141, 0.0077, 0.0146, 0.0077, 0.0479, 0.0187, 0.0570, 0.1828,
        0.0589, 0.0262, 0.2479], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,032][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0389, 0.1052, 0.0607, 0.0819, 0.0715, 0.0639, 0.0994, 0.1315, 0.0817,
        0.1129, 0.0830, 0.0694], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,033][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0015, 0.0146, 0.1164, 0.2128, 0.0557, 0.0464, 0.0941, 0.0615, 0.1452,
        0.0257, 0.0146, 0.2115], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,034][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0151, 0.0673, 0.1467, 0.0469, 0.0632, 0.0725, 0.1158, 0.0936, 0.0920,
        0.1401, 0.0814, 0.0655], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,036][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0265, 0.1586, 0.0692, 0.0582, 0.0866, 0.0790, 0.0542, 0.0747, 0.0695,
        0.1318, 0.1278, 0.0639], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,041][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0288, 0.0494, 0.0336, 0.0608, 0.0204, 0.0583, 0.0574, 0.0735, 0.1062,
        0.2547, 0.0823, 0.1746], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,045][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0503, 0.0115, 0.1252, 0.0441, 0.0224, 0.0512, 0.2276, 0.0339, 0.0834,
        0.1990, 0.0788, 0.0727], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,048][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ a] are: tensor([4.9718e-01, 7.2952e-04, 1.6375e-03, 2.7205e-02, 5.5707e-03, 6.7437e-04,
        4.0083e-04, 9.3013e-04, 2.3209e-04, 3.4064e-03, 1.8714e-03, 5.6268e-04,
        4.5960e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,053][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0276, 0.1827, 0.1029, 0.0656, 0.0670, 0.0400, 0.0851, 0.0899, 0.0685,
        0.0601, 0.0858, 0.0573, 0.0675], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,058][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0046, 0.0236, 0.0722, 0.0847, 0.0573, 0.0689, 0.0757, 0.0877, 0.1585,
        0.0681, 0.0545, 0.1322, 0.1119], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,062][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0295, 0.0309, 0.0582, 0.0673, 0.0494, 0.0941, 0.0502, 0.0191, 0.0494,
        0.0914, 0.0817, 0.1973, 0.1816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,064][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.3081, 0.1695, 0.0139, 0.0212, 0.0205, 0.0780, 0.0064, 0.0310, 0.0793,
        0.0820, 0.0315, 0.0673, 0.0912], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,065][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.3787, 0.0129, 0.0133, 0.0230, 0.0137, 0.0408, 0.0118, 0.0679, 0.1176,
        0.0507, 0.0285, 0.1683, 0.0726], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,066][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0302, 0.0999, 0.0561, 0.0813, 0.0730, 0.0574, 0.1036, 0.1228, 0.0758,
        0.0941, 0.0737, 0.0658, 0.0663], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,067][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0009, 0.0123, 0.1094, 0.1987, 0.0497, 0.0422, 0.0878, 0.0515, 0.1279,
        0.0204, 0.0117, 0.2125, 0.0750], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,069][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0144, 0.0665, 0.1257, 0.0411, 0.0533, 0.0731, 0.0820, 0.0887, 0.0939,
        0.1310, 0.0653, 0.0758, 0.0894], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,073][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0277, 0.1461, 0.0634, 0.0532, 0.0775, 0.0726, 0.0513, 0.0708, 0.0666,
        0.1210, 0.1186, 0.0615, 0.0697], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,078][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0243, 0.0460, 0.0212, 0.0298, 0.0194, 0.0394, 0.0366, 0.0592, 0.1008,
        0.1651, 0.0654, 0.1180, 0.2747], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,083][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.1231, 0.0197, 0.0981, 0.0395, 0.0199, 0.0699, 0.0829, 0.0714, 0.0819,
        0.2038, 0.0722, 0.0421, 0.0755], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,086][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([5.6397e-01, 1.3280e-03, 2.8372e-03, 3.9432e-02, 7.1152e-03, 1.0506e-03,
        4.8153e-04, 1.4394e-03, 4.3492e-04, 5.3176e-03, 2.9617e-03, 8.2205e-04,
        3.6637e-01, 6.4461e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,091][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0244, 0.1625, 0.0922, 0.0609, 0.0607, 0.0401, 0.0790, 0.0810, 0.0627,
        0.0554, 0.0858, 0.0549, 0.0683, 0.0722], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,095][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0065, 0.0241, 0.0625, 0.0784, 0.0538, 0.0645, 0.0676, 0.0785, 0.1356,
        0.0664, 0.0536, 0.1135, 0.1031, 0.0919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,097][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0109, 0.0357, 0.0320, 0.0453, 0.0320, 0.0915, 0.0528, 0.0173, 0.0469,
        0.0760, 0.1011, 0.1643, 0.2301, 0.0640], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,098][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0728, 0.1197, 0.0125, 0.0270, 0.0187, 0.0751, 0.0099, 0.0554, 0.1026,
        0.1384, 0.0338, 0.0996, 0.1889, 0.0457], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,099][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.2500, 0.0137, 0.0110, 0.0135, 0.0056, 0.0404, 0.0120, 0.0396, 0.1119,
        0.0425, 0.0194, 0.1366, 0.0914, 0.2124], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,099][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.0233, 0.0795, 0.0460, 0.0759, 0.0621, 0.0536, 0.0754, 0.0975, 0.0686,
        0.0859, 0.0627, 0.0661, 0.0692, 0.1340], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,103][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0013, 0.0063, 0.0652, 0.1400, 0.0338, 0.0348, 0.0675, 0.0452, 0.1047,
        0.0183, 0.0102, 0.1812, 0.0693, 0.2221], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,106][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0145, 0.0565, 0.1094, 0.0366, 0.0513, 0.0686, 0.0773, 0.0798, 0.0904,
        0.1274, 0.0579, 0.0636, 0.0900, 0.0767], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,111][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0250, 0.1420, 0.0612, 0.0520, 0.0702, 0.0717, 0.0464, 0.0640, 0.0580,
        0.1123, 0.1110, 0.0570, 0.0637, 0.0655], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,116][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0563, 0.0510, 0.0348, 0.0595, 0.0222, 0.0390, 0.0252, 0.0651, 0.0819,
        0.1179, 0.0568, 0.0799, 0.2422, 0.0682], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,121][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0419, 0.0087, 0.0733, 0.0242, 0.0302, 0.0495, 0.1079, 0.0188, 0.0687,
        0.2804, 0.0940, 0.0605, 0.1359, 0.0060], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,125][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.3793, 0.0021, 0.0025, 0.0552, 0.0101, 0.0013, 0.0007, 0.0018, 0.0006,
        0.0053, 0.0028, 0.0009, 0.3871, 0.0063, 0.1439], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,130][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0195, 0.1275, 0.0807, 0.0579, 0.0627, 0.0408, 0.0794, 0.0732, 0.0657,
        0.0569, 0.0747, 0.0544, 0.0629, 0.0686, 0.0750], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,130][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0043, 0.0230, 0.0560, 0.0726, 0.0499, 0.0599, 0.0597, 0.0753, 0.1311,
        0.0634, 0.0528, 0.1036, 0.0946, 0.0819, 0.0718], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,131][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0211, 0.0495, 0.0472, 0.0495, 0.0438, 0.0792, 0.0603, 0.0158, 0.0342,
        0.0796, 0.0699, 0.1588, 0.1753, 0.0654, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,132][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0191, 0.0782, 0.0120, 0.0214, 0.0223, 0.0755, 0.0109, 0.0575, 0.1190,
        0.1568, 0.0351, 0.0883, 0.2273, 0.0526, 0.0239], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,135][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.3178, 0.0104, 0.0089, 0.0098, 0.0051, 0.0265, 0.0119, 0.0264, 0.1013,
        0.0406, 0.0151, 0.0878, 0.0805, 0.2083, 0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,139][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0127, 0.0692, 0.0519, 0.0738, 0.0558, 0.0470, 0.0723, 0.1087, 0.0637,
        0.0723, 0.0509, 0.0666, 0.0624, 0.1068, 0.0859], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,143][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0006, 0.0049, 0.0561, 0.1224, 0.0286, 0.0291, 0.0654, 0.0418, 0.1067,
        0.0164, 0.0089, 0.1698, 0.0639, 0.2329, 0.0524], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,148][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0092, 0.0551, 0.0773, 0.0358, 0.0451, 0.0625, 0.0742, 0.0837, 0.0772,
        0.1129, 0.0529, 0.0604, 0.0793, 0.0761, 0.0984], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,153][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0207, 0.1272, 0.0571, 0.0462, 0.0680, 0.0672, 0.0472, 0.0636, 0.0567,
        0.1059, 0.1093, 0.0525, 0.0601, 0.0618, 0.0565], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,157][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0633, 0.0372, 0.0173, 0.0220, 0.0128, 0.0272, 0.0345, 0.0618, 0.0928,
        0.1408, 0.0500, 0.0745, 0.2314, 0.0622, 0.0720], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,162][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0217, 0.0118, 0.0599, 0.0104, 0.0330, 0.0370, 0.0889, 0.0713, 0.1228,
        0.2156, 0.1330, 0.0553, 0.1079, 0.0184, 0.0132], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,163][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.3785, 0.0010, 0.0026, 0.0382, 0.0108, 0.0008, 0.0006, 0.0011, 0.0004,
        0.0042, 0.0022, 0.0006, 0.3275, 0.0047, 0.1466, 0.0802],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,164][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0178, 0.1357, 0.0836, 0.0501, 0.0515, 0.0347, 0.0673, 0.0767, 0.0571,
        0.0520, 0.0713, 0.0515, 0.0617, 0.0650, 0.0776, 0.0463],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,165][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0035, 0.0203, 0.0501, 0.0592, 0.0459, 0.0543, 0.0563, 0.0682, 0.1234,
        0.0581, 0.0471, 0.1019, 0.0923, 0.0832, 0.0671, 0.0691],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,167][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0352, 0.0286, 0.0348, 0.0472, 0.0328, 0.0599, 0.0390, 0.0188, 0.0441,
        0.0805, 0.0664, 0.1217, 0.1826, 0.0579, 0.0500, 0.1004],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,171][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0837, 0.1072, 0.0114, 0.0185, 0.0184, 0.0485, 0.0062, 0.0462, 0.0567,
        0.1319, 0.0438, 0.0588, 0.2385, 0.0548, 0.0225, 0.0529],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,175][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.5310, 0.0122, 0.0054, 0.0070, 0.0034, 0.0159, 0.0043, 0.0093, 0.0192,
        0.0084, 0.0052, 0.0307, 0.0173, 0.0715, 0.0184, 0.2411],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,180][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0172, 0.0711, 0.0478, 0.0648, 0.0616, 0.0493, 0.0768, 0.0905, 0.0595,
        0.0723, 0.0560, 0.0547, 0.0544, 0.0888, 0.0752, 0.0600],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,185][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0007, 0.0098, 0.0711, 0.1370, 0.0404, 0.0338, 0.0681, 0.0394, 0.0993,
        0.0168, 0.0103, 0.1546, 0.0585, 0.1826, 0.0437, 0.0337],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,189][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0135, 0.0526, 0.0943, 0.0316, 0.0426, 0.0661, 0.0685, 0.0688, 0.0770,
        0.0987, 0.0458, 0.0600, 0.0732, 0.0618, 0.0963, 0.0494],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,194][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0262, 0.1262, 0.0520, 0.0441, 0.0651, 0.0645, 0.0416, 0.0588, 0.0522,
        0.1001, 0.1017, 0.0490, 0.0559, 0.0559, 0.0498, 0.0567],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,195][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0804, 0.0325, 0.0110, 0.0289, 0.0214, 0.0347, 0.0372, 0.0513, 0.0536,
        0.1080, 0.0553, 0.0653, 0.1581, 0.0426, 0.0464, 0.1734],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,196][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0472, 0.0125, 0.0858, 0.0192, 0.0197, 0.0512, 0.1048, 0.0806, 0.1026,
        0.1583, 0.1059, 0.0477, 0.0843, 0.0084, 0.0109, 0.0608],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,197][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([7.4530e-02, 1.0190e-04, 2.5776e-04, 4.5514e-03, 1.1890e-03, 1.0111e-04,
        6.4831e-05, 1.3863e-04, 5.6163e-05, 6.7627e-04, 2.6775e-04, 7.9288e-05,
        6.2085e-02, 6.0171e-04, 2.2855e-02, 1.1615e-02, 8.2083e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,199][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0177, 0.1297, 0.0727, 0.0493, 0.0508, 0.0325, 0.0631, 0.0697, 0.0534,
        0.0477, 0.0670, 0.0460, 0.0553, 0.0620, 0.0725, 0.0422, 0.0684],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,203][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0035, 0.0191, 0.0493, 0.0581, 0.0414, 0.0502, 0.0529, 0.0607, 0.1076,
        0.0497, 0.0405, 0.0925, 0.0831, 0.0786, 0.0618, 0.0650, 0.0860],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,208][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0129, 0.0163, 0.0367, 0.0447, 0.0271, 0.0654, 0.0377, 0.0098, 0.0293,
        0.0559, 0.0529, 0.1115, 0.1371, 0.0419, 0.0391, 0.1091, 0.1724],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,212][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.2019, 0.1325, 0.0097, 0.0199, 0.0197, 0.0699, 0.0058, 0.0259, 0.0573,
        0.0790, 0.0298, 0.0439, 0.1031, 0.0435, 0.0162, 0.0602, 0.0815],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,217][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.4249, 0.0051, 0.0029, 0.0046, 0.0022, 0.0073, 0.0017, 0.0076, 0.0127,
        0.0052, 0.0025, 0.0172, 0.0118, 0.0339, 0.0081, 0.2750, 0.1774],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,221][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0216, 0.0767, 0.0338, 0.0595, 0.0521, 0.0465, 0.0598, 0.0779, 0.0477,
        0.0681, 0.0560, 0.0472, 0.0516, 0.0864, 0.0795, 0.0692, 0.0665],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,226][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0007, 0.0073, 0.0574, 0.1147, 0.0339, 0.0320, 0.0614, 0.0362, 0.0883,
        0.0156, 0.0097, 0.1450, 0.0561, 0.1712, 0.0429, 0.0343, 0.0935],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,227][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0101, 0.0451, 0.0897, 0.0299, 0.0379, 0.0513, 0.0578, 0.0615, 0.0719,
        0.0999, 0.0481, 0.0538, 0.0646, 0.0621, 0.0996, 0.0531, 0.0635],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,228][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0232, 0.1208, 0.0499, 0.0421, 0.0602, 0.0598, 0.0409, 0.0549, 0.0507,
        0.0941, 0.0958, 0.0473, 0.0532, 0.0556, 0.0470, 0.0548, 0.0498],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,229][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0284, 0.0224, 0.0092, 0.0140, 0.0081, 0.0193, 0.0149, 0.0186, 0.0368,
        0.0558, 0.0192, 0.0387, 0.1189, 0.0335, 0.0361, 0.1111, 0.4150],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,232][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0919, 0.0121, 0.1041, 0.0252, 0.0154, 0.0372, 0.0836, 0.0609, 0.0813,
        0.1257, 0.0699, 0.0367, 0.0862, 0.0141, 0.0135, 0.0730, 0.0692],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:46,233][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ language] are: tensor([1.2352e-01, 5.7681e-04, 7.0628e-04, 1.0277e-02, 2.9678e-03, 6.3039e-04,
        3.1077e-04, 7.5979e-04, 2.9558e-04, 1.9928e-03, 1.3801e-03, 3.9035e-04,
        8.7020e-02, 1.9631e-03, 4.2749e-02, 2.8221e-02, 6.9028e-01, 5.9612e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,238][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0168, 0.1147, 0.0716, 0.0481, 0.0498, 0.0319, 0.0635, 0.0617, 0.0498,
        0.0483, 0.0626, 0.0460, 0.0547, 0.0569, 0.0641, 0.0425, 0.0760, 0.0411],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,242][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0029, 0.0157, 0.0490, 0.0597, 0.0409, 0.0466, 0.0484, 0.0580, 0.1046,
        0.0507, 0.0430, 0.0858, 0.0767, 0.0697, 0.0561, 0.0593, 0.0770, 0.0559],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,247][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0112, 0.0355, 0.0224, 0.0251, 0.0216, 0.0302, 0.0286, 0.0124, 0.0235,
        0.0542, 0.0654, 0.1047, 0.1188, 0.0276, 0.0389, 0.1076, 0.1968, 0.0755],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,252][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0381, 0.0405, 0.0076, 0.0218, 0.0180, 0.0388, 0.0110, 0.0263, 0.0937,
        0.1278, 0.0339, 0.0460, 0.1704, 0.0527, 0.0283, 0.0795, 0.1374, 0.0282],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,256][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.1931, 0.0082, 0.0051, 0.0064, 0.0024, 0.0172, 0.0063, 0.0149, 0.0267,
        0.0093, 0.0069, 0.0399, 0.0199, 0.0641, 0.0247, 0.2016, 0.1921, 0.1612],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,258][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0103, 0.0629, 0.0510, 0.0638, 0.0475, 0.0466, 0.0679, 0.0603, 0.0493,
        0.0519, 0.0407, 0.0546, 0.0518, 0.0918, 0.0628, 0.0626, 0.0616, 0.0625],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,259][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0003, 0.0065, 0.0675, 0.1352, 0.0322, 0.0271, 0.0569, 0.0307, 0.0867,
        0.0116, 0.0064, 0.1527, 0.0506, 0.1796, 0.0364, 0.0250, 0.0864, 0.0082],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,260][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0076, 0.0497, 0.0832, 0.0330, 0.0491, 0.0643, 0.0655, 0.0463, 0.0687,
        0.1058, 0.0444, 0.0483, 0.0646, 0.0576, 0.0802, 0.0430, 0.0585, 0.0303],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,261][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0202, 0.1116, 0.0462, 0.0407, 0.0596, 0.0597, 0.0392, 0.0554, 0.0457,
        0.0883, 0.0931, 0.0455, 0.0504, 0.0530, 0.0455, 0.0531, 0.0480, 0.0446],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,264][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0308, 0.0337, 0.0138, 0.0303, 0.0146, 0.0297, 0.0241, 0.0457, 0.0323,
        0.0573, 0.0389, 0.0646, 0.1006, 0.0301, 0.0331, 0.0653, 0.2271, 0.1281],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,267][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0279, 0.0084, 0.0431, 0.0188, 0.0188, 0.0448, 0.1574, 0.0278, 0.1217,
        0.1405, 0.1335, 0.0421, 0.0631, 0.0050, 0.0058, 0.0415, 0.0944, 0.0055],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:46,270][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ used] are: tensor([7.7464e-02, 4.5167e-04, 7.6501e-04, 8.8758e-03, 2.5775e-03, 2.9660e-04,
        2.1175e-04, 4.7761e-04, 1.6832e-04, 1.5581e-03, 9.3041e-04, 2.7982e-04,
        9.7689e-02, 2.1287e-03, 4.2881e-02, 2.9912e-02, 7.1599e-01, 3.3773e-03,
        1.3969e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,275][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0182, 0.1196, 0.0678, 0.0435, 0.0505, 0.0302, 0.0636, 0.0612, 0.0485,
        0.0448, 0.0583, 0.0416, 0.0493, 0.0514, 0.0601, 0.0365, 0.0619, 0.0336,
        0.0597], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,279][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0042, 0.0179, 0.0414, 0.0543, 0.0389, 0.0466, 0.0454, 0.0549, 0.0891,
        0.0485, 0.0396, 0.0736, 0.0685, 0.0605, 0.0516, 0.0566, 0.0724, 0.0530,
        0.0831], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,284][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0163, 0.0140, 0.0209, 0.0415, 0.0155, 0.0436, 0.0223, 0.0082, 0.0191,
        0.0433, 0.0887, 0.0858, 0.1513, 0.0348, 0.0381, 0.1087, 0.1879, 0.0231,
        0.0369], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,288][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0517, 0.0787, 0.0056, 0.0200, 0.0094, 0.0441, 0.0066, 0.0384, 0.0523,
        0.1084, 0.0190, 0.0460, 0.1877, 0.0299, 0.0073, 0.0822, 0.1453, 0.0441,
        0.0235], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,290][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ used] are: tensor([4.5050e-01, 2.2503e-03, 8.7810e-04, 8.7451e-04, 3.9121e-04, 1.7156e-03,
        3.7662e-04, 8.8330e-04, 2.3131e-03, 1.1774e-03, 4.3732e-04, 2.7685e-03,
        2.8251e-03, 9.6358e-03, 1.3043e-03, 3.7327e-02, 2.8924e-02, 6.0082e-02,
        3.9534e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,291][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0170, 0.0630, 0.0354, 0.0519, 0.0558, 0.0390, 0.0611, 0.0722, 0.0422,
        0.0610, 0.0484, 0.0393, 0.0368, 0.0732, 0.0646, 0.0540, 0.0517, 0.0641,
        0.0693], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,292][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0007, 0.0057, 0.0516, 0.1083, 0.0279, 0.0273, 0.0554, 0.0328, 0.0849,
        0.0140, 0.0082, 0.1363, 0.0519, 0.1698, 0.0412, 0.0320, 0.0939, 0.0110,
        0.0470], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,293][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0108, 0.0381, 0.0821, 0.0283, 0.0354, 0.0423, 0.0558, 0.0495, 0.0434,
        0.0943, 0.0419, 0.0455, 0.0740, 0.0512, 0.0821, 0.0406, 0.0633, 0.0221,
        0.0993], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,296][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0247, 0.1238, 0.0459, 0.0387, 0.0576, 0.0551, 0.0344, 0.0457, 0.0430,
        0.0907, 0.0874, 0.0422, 0.0480, 0.0505, 0.0413, 0.0507, 0.0438, 0.0415,
        0.0351], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,301][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0413, 0.0232, 0.0100, 0.0155, 0.0080, 0.0137, 0.0087, 0.0155, 0.0314,
        0.0506, 0.0203, 0.0243, 0.0759, 0.0238, 0.0227, 0.0670, 0.2510, 0.0963,
        0.2009], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,306][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0303, 0.0091, 0.0702, 0.0316, 0.0153, 0.0217, 0.0777, 0.0226, 0.0360,
        0.0918, 0.0707, 0.0565, 0.0483, 0.0047, 0.0064, 0.0575, 0.0582, 0.0043,
        0.2871], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:46,308][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ by] are: tensor([1.8436e-01, 3.5366e-04, 5.0434e-04, 7.1627e-03, 2.0529e-03, 2.4870e-04,
        1.7263e-04, 2.8669e-04, 1.1822e-04, 1.2784e-03, 6.8434e-04, 1.4052e-04,
        6.9371e-02, 9.3023e-04, 1.9789e-02, 1.6480e-02, 6.6605e-01, 2.4197e-03,
        8.0820e-03, 1.9508e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,313][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0162, 0.1161, 0.0653, 0.0378, 0.0422, 0.0262, 0.0514, 0.0579, 0.0435,
        0.0406, 0.0563, 0.0398, 0.0480, 0.0478, 0.0617, 0.0357, 0.0614, 0.0378,
        0.0652, 0.0491], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,318][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0034, 0.0161, 0.0397, 0.0469, 0.0346, 0.0413, 0.0458, 0.0523, 0.0825,
        0.0414, 0.0346, 0.0721, 0.0633, 0.0630, 0.0483, 0.0509, 0.0647, 0.0470,
        0.0753, 0.0767], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,322][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0109, 0.0295, 0.0252, 0.0293, 0.0240, 0.0557, 0.0230, 0.0139, 0.0308,
        0.0544, 0.0626, 0.0756, 0.1469, 0.0296, 0.0488, 0.0554, 0.1615, 0.0395,
        0.0363, 0.0471], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,323][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.1056, 0.0911, 0.0079, 0.0162, 0.0219, 0.0527, 0.0079, 0.0466, 0.0567,
        0.0981, 0.0171, 0.0223, 0.1321, 0.0259, 0.0217, 0.0568, 0.1032, 0.0273,
        0.0208, 0.0681], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,324][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ by] are: tensor([4.3325e-01, 1.1739e-03, 1.8139e-04, 2.5654e-04, 1.5950e-04, 7.9981e-04,
        1.1310e-04, 2.2088e-04, 4.0353e-04, 2.2762e-04, 1.4762e-04, 5.8008e-04,
        3.8698e-04, 1.4689e-03, 2.4212e-04, 6.6745e-03, 5.2966e-03, 1.5996e-02,
        6.4852e-02, 4.6757e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,325][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0239, 0.0588, 0.0344, 0.0479, 0.0509, 0.0364, 0.0546, 0.0655, 0.0387,
        0.0625, 0.0516, 0.0333, 0.0347, 0.0543, 0.0604, 0.0487, 0.0493, 0.0594,
        0.0630, 0.0717], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,328][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0009, 0.0066, 0.0494, 0.1020, 0.0288, 0.0291, 0.0556, 0.0328, 0.0749,
        0.0146, 0.0090, 0.1271, 0.0503, 0.1445, 0.0341, 0.0313, 0.0855, 0.0119,
        0.0434, 0.0683], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,331][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0134, 0.0397, 0.0734, 0.0262, 0.0348, 0.0558, 0.0499, 0.0580, 0.0515,
        0.0881, 0.0390, 0.0511, 0.0603, 0.0410, 0.0812, 0.0374, 0.0566, 0.0253,
        0.0823, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,336][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0268, 0.1192, 0.0468, 0.0386, 0.0559, 0.0547, 0.0349, 0.0474, 0.0422,
        0.0859, 0.0847, 0.0394, 0.0449, 0.0461, 0.0399, 0.0463, 0.0403, 0.0399,
        0.0325, 0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,341][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0344, 0.0143, 0.0041, 0.0094, 0.0043, 0.0077, 0.0060, 0.0079, 0.0084,
        0.0279, 0.0122, 0.0097, 0.0308, 0.0106, 0.0095, 0.0298, 0.0884, 0.0642,
        0.0834, 0.5366], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,345][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0392, 0.0104, 0.0785, 0.0198, 0.0223, 0.0380, 0.0541, 0.0488, 0.0275,
        0.0791, 0.0331, 0.0258, 0.0475, 0.0045, 0.0137, 0.0307, 0.0624, 0.0047,
        0.2510, 0.1091], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:46,348][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([5.9626e-02, 8.7164e-05, 1.7762e-04, 3.0015e-03, 7.5169e-04, 6.5192e-05,
        3.9672e-05, 8.8518e-05, 2.9608e-05, 4.0704e-04, 1.8511e-04, 4.3606e-05,
        3.5856e-02, 3.5349e-04, 1.2274e-02, 6.8324e-03, 4.3701e-01, 9.2082e-04,
        3.6942e-03, 8.5410e-03, 4.3001e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,352][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0153, 0.1091, 0.0573, 0.0391, 0.0414, 0.0257, 0.0506, 0.0555, 0.0418,
        0.0377, 0.0546, 0.0365, 0.0446, 0.0485, 0.0571, 0.0330, 0.0537, 0.0350,
        0.0548, 0.0468, 0.0621], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,354][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0030, 0.0145, 0.0383, 0.0440, 0.0322, 0.0389, 0.0419, 0.0463, 0.0779,
        0.0369, 0.0307, 0.0685, 0.0605, 0.0597, 0.0448, 0.0484, 0.0625, 0.0428,
        0.0708, 0.0723, 0.0647], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,355][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0089, 0.0119, 0.0290, 0.0356, 0.0226, 0.0506, 0.0290, 0.0076, 0.0232,
        0.0418, 0.0428, 0.0803, 0.1034, 0.0309, 0.0302, 0.0770, 0.1256, 0.0104,
        0.0377, 0.0595, 0.1420], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,356][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.3092, 0.0742, 0.0049, 0.0134, 0.0126, 0.0423, 0.0030, 0.0139, 0.0294,
        0.0442, 0.0140, 0.0211, 0.0545, 0.0230, 0.0081, 0.0339, 0.0407, 0.0333,
        0.0243, 0.1079, 0.0920], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,357][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.5668e-01, 2.4066e-04, 5.6176e-05, 1.3160e-04, 5.8085e-05, 1.3263e-04,
        1.9506e-05, 9.4639e-05, 1.5681e-04, 5.3641e-05, 3.2671e-05, 2.0395e-04,
        1.1009e-04, 4.0342e-04, 6.0127e-05, 4.0164e-03, 2.3175e-03, 7.5393e-03,
        4.5765e-02, 5.6137e-01, 1.2056e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,360][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0214, 0.0644, 0.0249, 0.0451, 0.0406, 0.0338, 0.0414, 0.0545, 0.0315,
        0.0517, 0.0443, 0.0309, 0.0334, 0.0590, 0.0647, 0.0490, 0.0437, 0.0614,
        0.0642, 0.0827, 0.0574], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,364][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0007, 0.0071, 0.0498, 0.0995, 0.0296, 0.0283, 0.0516, 0.0302, 0.0683,
        0.0129, 0.0081, 0.1166, 0.0456, 0.1303, 0.0329, 0.0278, 0.0731, 0.0103,
        0.0380, 0.0570, 0.0826], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,369][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0090, 0.0359, 0.0733, 0.0239, 0.0311, 0.0403, 0.0449, 0.0477, 0.0553,
        0.0782, 0.0374, 0.0424, 0.0507, 0.0467, 0.0844, 0.0426, 0.0510, 0.0231,
        0.0955, 0.0375, 0.0491], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,373][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0241, 0.1150, 0.0430, 0.0371, 0.0519, 0.0516, 0.0339, 0.0440, 0.0412,
        0.0812, 0.0805, 0.0391, 0.0444, 0.0466, 0.0374, 0.0456, 0.0408, 0.0391,
        0.0312, 0.0344, 0.0380], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,378][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0176, 0.0063, 0.0018, 0.0038, 0.0019, 0.0041, 0.0023, 0.0027, 0.0052,
        0.0109, 0.0032, 0.0055, 0.0196, 0.0052, 0.0044, 0.0183, 0.0665, 0.0356,
        0.0334, 0.3175, 0.4342], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,383][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0645, 0.0051, 0.0599, 0.0153, 0.0077, 0.0202, 0.0369, 0.0310, 0.0383,
        0.0672, 0.0314, 0.0200, 0.0405, 0.0058, 0.0066, 0.0339, 0.0339, 0.0033,
        0.2894, 0.1383, 0.0505], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:46,479][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:46,482][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,482][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,483][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,484][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,485][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,485][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,486][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,487][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,487][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,488][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,488][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,489][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:46,490][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([9.9980e-01, 1.9668e-04], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,490][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.7262, 0.2738], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,495][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.9982, 0.0018], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,499][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0844, 0.9156], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,504][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.9488, 0.0512], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,505][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([9.9931e-01, 6.9422e-04], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,506][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.8239, 0.1761], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,506][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.8817, 0.1183], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,507][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9349, 0.0651], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,509][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.7261, 0.2739], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,512][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.8182, 0.1818], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,513][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.9334, 0.0666], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:46,516][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.9886, 0.0011, 0.0103], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,521][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0351, 0.3654, 0.5995], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,525][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.9653, 0.0049, 0.0299], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,530][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0117, 0.9173, 0.0710], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,535][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.3411, 0.1633, 0.4956], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,538][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.9858, 0.0067, 0.0075], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,539][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.1924, 0.3483, 0.4593], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,540][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0807, 0.6566, 0.2627], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,540][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.1430, 0.3650, 0.4920], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,541][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.4590, 0.2544, 0.2866], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,544][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.2181, 0.5090, 0.2729], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,548][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.8112, 0.0831, 0.1057], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:46,551][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([9.7374e-01, 4.6090e-04, 4.3950e-03, 2.1404e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,556][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0039, 0.2979, 0.4602, 0.2380], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,561][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.8845, 0.0063, 0.0502, 0.0590], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,566][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0171, 0.7885, 0.0732, 0.1211], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,571][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.1375, 0.1556, 0.5077, 0.1992], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,572][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.9275, 0.0208, 0.0261, 0.0256], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,573][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0606, 0.2754, 0.3225, 0.3415], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,574][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0275, 0.4781, 0.3345, 0.1599], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,574][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0496, 0.2765, 0.4265, 0.2473], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,577][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.1397, 0.2099, 0.2341, 0.4164], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,582][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.2235, 0.2851, 0.1251, 0.3663], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,587][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.1896, 0.2678, 0.2899, 0.2527], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:46,590][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([9.7941e-01, 8.2363e-04, 2.2069e-03, 1.3480e-02, 4.0831e-03],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,594][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0105, 0.1148, 0.4639, 0.2911, 0.1196], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,599][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.9553, 0.0021, 0.0179, 0.0136, 0.0111], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,605][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.0662, 0.4978, 0.0858, 0.0806, 0.2695], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,606][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.5154, 0.0866, 0.1902, 0.1711, 0.0367], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,606][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.9854, 0.0020, 0.0025, 0.0076, 0.0025], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,607][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.2062, 0.1006, 0.3464, 0.2669, 0.0799], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,608][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0568, 0.3668, 0.3260, 0.1914, 0.0590], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,611][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.1577, 0.1109, 0.5417, 0.1270, 0.0628], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,615][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.2565, 0.0891, 0.1216, 0.1882, 0.3446], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,620][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.4425, 0.1295, 0.0938, 0.2428, 0.0914], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,625][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.4896, 0.0876, 0.1310, 0.1497, 0.1420], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:46,628][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([9.8766e-01, 2.7636e-04, 1.0533e-03, 4.4044e-03, 3.1893e-03, 3.4172e-03],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,633][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0391, 0.0491, 0.2219, 0.5825, 0.0543, 0.0531], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,638][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.9451, 0.0022, 0.0067, 0.0195, 0.0220, 0.0044], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,639][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0407, 0.4073, 0.0464, 0.0726, 0.2309, 0.2022], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,640][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.6232, 0.0397, 0.0554, 0.1219, 0.0186, 0.1412], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,641][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.9581, 0.0054, 0.0045, 0.0097, 0.0042, 0.0181], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,641][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0249, 0.2447, 0.2222, 0.3569, 0.0746, 0.0767], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,644][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0703, 0.3609, 0.2460, 0.2335, 0.0355, 0.0538], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,649][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.0916, 0.1814, 0.3036, 0.1748, 0.1363, 0.1124], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,654][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.5287, 0.1110, 0.0609, 0.1560, 0.0814, 0.0620], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,658][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.3924, 0.1910, 0.0776, 0.1587, 0.0576, 0.1226], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,663][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.3466, 0.1388, 0.0790, 0.0918, 0.0801, 0.2636], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:46,666][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([9.8647e-01, 2.3705e-04, 1.2353e-03, 5.2944e-03, 3.8026e-03, 2.5909e-03,
        3.7455e-04], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,672][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0025, 0.0542, 0.1894, 0.4165, 0.0145, 0.1474, 0.1755],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,673][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.9369, 0.0011, 0.0127, 0.0094, 0.0124, 0.0065, 0.0209],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,673][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.0139, 0.3325, 0.0267, 0.0643, 0.1373, 0.1483, 0.2770],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,674][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.3324, 0.0299, 0.1829, 0.1551, 0.0262, 0.2162, 0.0572],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,675][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.7801, 0.0178, 0.0253, 0.0357, 0.0170, 0.0911, 0.0330],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,678][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.0213, 0.1892, 0.1082, 0.3008, 0.0336, 0.0874, 0.2593],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,682][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0138, 0.2788, 0.3144, 0.1547, 0.0622, 0.1513, 0.0249],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,687][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.0193, 0.1217, 0.3096, 0.2435, 0.0892, 0.0991, 0.1175],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,693][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.2592, 0.0999, 0.0543, 0.1753, 0.0645, 0.0413, 0.3056],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,698][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.2087, 0.1124, 0.0670, 0.1619, 0.0480, 0.1027, 0.2992],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,703][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.0838, 0.0674, 0.0389, 0.1617, 0.0927, 0.3062, 0.2492],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:46,705][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([9.9031e-01, 1.1117e-04, 5.9988e-04, 3.2667e-03, 1.7224e-03, 1.8876e-03,
        3.6782e-04, 1.7306e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,706][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0004, 0.0285, 0.1304, 0.2444, 0.0320, 0.2157, 0.2808, 0.0678],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,707][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.7313, 0.0050, 0.0226, 0.0482, 0.0365, 0.0183, 0.0892, 0.0489],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,708][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.0020, 0.0980, 0.0103, 0.0215, 0.0836, 0.0806, 0.0483, 0.6557],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,710][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0221, 0.0242, 0.0675, 0.0410, 0.0125, 0.1629, 0.0610, 0.6087],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,714][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.4588, 0.0310, 0.0280, 0.0381, 0.0307, 0.1939, 0.0560, 0.1635],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,719][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.0033, 0.1061, 0.1068, 0.2633, 0.0651, 0.1073, 0.1957, 0.1523],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,724][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0189, 0.1463, 0.2345, 0.1351, 0.0290, 0.1410, 0.0277, 0.2675],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,728][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.0144, 0.0294, 0.1063, 0.1512, 0.1438, 0.1145, 0.0943, 0.3462],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,733][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([0.0862, 0.1338, 0.0592, 0.1712, 0.0853, 0.0877, 0.1660, 0.2106],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,739][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.0847, 0.1110, 0.0523, 0.1180, 0.0556, 0.1604, 0.1664, 0.2516],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,739][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.0327, 0.0525, 0.0444, 0.0569, 0.0484, 0.3495, 0.1699, 0.2458],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:46,740][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([9.8465e-01, 2.9562e-04, 8.3915e-04, 3.7210e-03, 3.2993e-03, 1.8713e-03,
        4.3220e-04, 2.1535e-03, 2.7368e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,741][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0059, 0.0110, 0.3496, 0.0617, 0.0487, 0.1193, 0.1030, 0.1449, 0.1558],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,742][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.7150, 0.0034, 0.0166, 0.0572, 0.0459, 0.0128, 0.0588, 0.0492, 0.0411],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,745][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0047, 0.1173, 0.0087, 0.0142, 0.0432, 0.0496, 0.0539, 0.5374, 0.1711],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,750][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0193, 0.0068, 0.0078, 0.0112, 0.0046, 0.0739, 0.0219, 0.2294, 0.6251],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,755][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.6872, 0.0163, 0.0088, 0.0136, 0.0125, 0.0511, 0.0239, 0.0558, 0.1307],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,760][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.0192, 0.0964, 0.1335, 0.0894, 0.0375, 0.0886, 0.1210, 0.2709, 0.1435],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,764][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0086, 0.0914, 0.0526, 0.0424, 0.0077, 0.0497, 0.0063, 0.0773, 0.6639],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,769][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.0155, 0.0205, 0.0574, 0.0594, 0.0474, 0.1099, 0.0273, 0.4116, 0.2511],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,772][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.1570, 0.0805, 0.0203, 0.1229, 0.0779, 0.0582, 0.1127, 0.1387, 0.2318],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,772][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.1425, 0.0808, 0.0261, 0.0592, 0.0373, 0.0595, 0.0780, 0.1649, 0.3515],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,773][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0451, 0.0183, 0.0116, 0.0277, 0.0287, 0.1414, 0.1132, 0.4124, 0.2017],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:46,774][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([9.8106e-01, 4.2552e-04, 9.4983e-04, 4.4549e-03, 1.2509e-03, 2.2116e-03,
        5.2019e-04, 1.5311e-03, 2.3729e-03, 5.2234e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,776][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0983, 0.0138, 0.1420, 0.1777, 0.0205, 0.0670, 0.1087, 0.1413, 0.1974,
        0.0333], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,780][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.8076, 0.0059, 0.0290, 0.0323, 0.0159, 0.0069, 0.0312, 0.0340, 0.0278,
        0.0093], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,785][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0396, 0.0654, 0.0192, 0.0204, 0.0717, 0.0664, 0.0471, 0.3580, 0.0996,
        0.2127], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,790][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.3926, 0.0082, 0.0134, 0.0171, 0.0058, 0.0334, 0.0130, 0.0980, 0.3448,
        0.0739], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,794][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.9602, 0.0028, 0.0017, 0.0028, 0.0022, 0.0049, 0.0015, 0.0076, 0.0127,
        0.0035], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,799][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.0276, 0.0679, 0.0769, 0.1799, 0.0599, 0.0610, 0.1061, 0.2689, 0.0714,
        0.0805], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,804][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.2035, 0.0917, 0.0564, 0.0285, 0.0119, 0.0389, 0.0070, 0.0545, 0.2255,
        0.2821], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,805][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.0174, 0.0329, 0.1329, 0.1032, 0.0556, 0.0839, 0.0262, 0.2008, 0.1852,
        0.1619], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,806][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.2062, 0.0693, 0.0399, 0.1101, 0.0805, 0.0389, 0.1016, 0.0717, 0.1148,
        0.1670], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,806][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.3666, 0.0577, 0.0318, 0.0670, 0.0445, 0.0452, 0.0527, 0.0746, 0.0856,
        0.1743], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,807][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.2241, 0.0166, 0.0079, 0.0132, 0.0153, 0.0397, 0.0297, 0.0799, 0.0845,
        0.4891], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:46,809][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([9.8163e-01, 1.7961e-04, 6.6049e-04, 6.3122e-03, 9.9856e-04, 1.4905e-03,
        3.1697e-04, 1.4762e-03, 9.7507e-04, 5.1116e-03, 8.5012e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,812][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.1037, 0.0097, 0.0434, 0.0896, 0.0292, 0.0996, 0.1991, 0.0998, 0.2129,
        0.0695, 0.0436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,816][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.7035, 0.0055, 0.0223, 0.0474, 0.0205, 0.0248, 0.0692, 0.0292, 0.0594,
        0.0143, 0.0040], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,821][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0213, 0.0249, 0.0075, 0.0123, 0.1099, 0.0410, 0.0107, 0.3231, 0.1280,
        0.1686, 0.1527], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,826][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.1232, 0.0071, 0.0179, 0.0332, 0.0053, 0.0426, 0.0112, 0.1035, 0.5260,
        0.1178, 0.0122], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,830][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.5968, 0.0152, 0.0182, 0.0261, 0.0182, 0.0357, 0.0145, 0.0674, 0.1379,
        0.0528, 0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,835][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0287, 0.0383, 0.0549, 0.1605, 0.1042, 0.1084, 0.0890, 0.1400, 0.0401,
        0.1064, 0.1295], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,840][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0149, 0.0280, 0.0369, 0.0303, 0.0034, 0.0186, 0.0030, 0.0408, 0.2166,
        0.5845, 0.0231], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,841][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.0209, 0.0096, 0.0462, 0.0692, 0.0882, 0.0573, 0.0640, 0.2861, 0.1422,
        0.1924, 0.0238], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,842][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0611, 0.0253, 0.0234, 0.0825, 0.1179, 0.0555, 0.0724, 0.1136, 0.1775,
        0.1875, 0.0834], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,842][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.1825, 0.0359, 0.0235, 0.0582, 0.0348, 0.0551, 0.0455, 0.1258, 0.1022,
        0.2293, 0.1071], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,843][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.0574, 0.0045, 0.0100, 0.0101, 0.0095, 0.0364, 0.0151, 0.0498, 0.0379,
        0.7320, 0.0372], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:46,845][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([9.0902e-01, 6.5331e-04, 3.5498e-03, 1.4941e-02, 6.2068e-03, 7.1983e-03,
        1.8061e-03, 4.8703e-03, 7.3675e-03, 3.4351e-02, 5.6987e-03, 4.3329e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,847][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([1.2246e-04, 8.0300e-03, 3.7763e-02, 5.6670e-02, 1.3435e-02, 8.5727e-02,
        1.4619e-01, 9.2249e-02, 2.6402e-01, 5.0610e-02, 6.9085e-02, 1.7610e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,852][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.4098, 0.0116, 0.0449, 0.1006, 0.0282, 0.0294, 0.0738, 0.0646, 0.0588,
        0.0283, 0.0130, 0.1372], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,857][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0015, 0.0306, 0.0063, 0.0130, 0.0223, 0.0486, 0.0250, 0.1462, 0.0703,
        0.2024, 0.1635, 0.2704], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,862][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0041, 0.0036, 0.0078, 0.0082, 0.0026, 0.0426, 0.0080, 0.0673, 0.4512,
        0.2143, 0.0218, 0.1684], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,866][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.3164, 0.0141, 0.0077, 0.0146, 0.0077, 0.0479, 0.0187, 0.0570, 0.1828,
        0.0589, 0.0262, 0.2479], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,871][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0011, 0.0608, 0.0953, 0.1162, 0.0240, 0.0651, 0.1173, 0.1243, 0.0469,
        0.0802, 0.1939, 0.0749], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,874][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0062, 0.0283, 0.0103, 0.0081, 0.0020, 0.0125, 0.0021, 0.0249, 0.2578,
        0.4207, 0.0191, 0.2080], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,875][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0026, 0.0138, 0.0468, 0.0537, 0.0187, 0.0813, 0.0214, 0.1825, 0.1824,
        0.2231, 0.0399, 0.1338], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,875][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0311, 0.0478, 0.0384, 0.1041, 0.0549, 0.0344, 0.0938, 0.0478, 0.1068,
        0.2286, 0.0381, 0.1741], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,876][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0288, 0.0494, 0.0336, 0.0608, 0.0204, 0.0583, 0.0574, 0.0735, 0.1062,
        0.2547, 0.0823, 0.1746], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,879][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0051, 0.0050, 0.0032, 0.0076, 0.0052, 0.0314, 0.0197, 0.0664, 0.0426,
        0.6519, 0.1319, 0.0300], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:46,881][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([2.2063e-01, 1.4041e-04, 1.2180e-03, 3.8363e-03, 1.1979e-03, 1.1337e-03,
        3.3188e-04, 1.1597e-03, 1.1350e-03, 5.5782e-03, 1.0891e-03, 1.2367e-03,
        7.6132e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,885][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0010, 0.0104, 0.0822, 0.1530, 0.0168, 0.0663, 0.1379, 0.0736, 0.1506,
        0.0513, 0.0433, 0.1226, 0.0910], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,890][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.3383, 0.0151, 0.0526, 0.1024, 0.0239, 0.0250, 0.0518, 0.0660, 0.0605,
        0.0294, 0.0127, 0.1555, 0.0667], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,895][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0027, 0.0296, 0.0052, 0.0065, 0.0172, 0.0331, 0.0200, 0.1161, 0.0465,
        0.1520, 0.0903, 0.1812, 0.2994], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,899][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0072, 0.0027, 0.0039, 0.0052, 0.0010, 0.0145, 0.0028, 0.0516, 0.2603,
        0.1050, 0.0097, 0.1543, 0.3817], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,904][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.3787, 0.0129, 0.0133, 0.0230, 0.0137, 0.0408, 0.0118, 0.0679, 0.1176,
        0.0507, 0.0285, 0.1683, 0.0726], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,907][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0016, 0.0740, 0.0877, 0.2029, 0.0240, 0.0381, 0.0840, 0.0860, 0.0337,
        0.0362, 0.1043, 0.0685, 0.1588], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,908][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0036, 0.0178, 0.0087, 0.0071, 0.0014, 0.0094, 0.0009, 0.0131, 0.1874,
        0.3032, 0.0135, 0.1512, 0.2825], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,908][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0024, 0.0116, 0.0308, 0.0512, 0.0219, 0.0587, 0.0152, 0.1206, 0.1276,
        0.1817, 0.0376, 0.1220, 0.2186], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,909][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0189, 0.0407, 0.0200, 0.0649, 0.0331, 0.0236, 0.0775, 0.0516, 0.1190,
        0.1498, 0.0442, 0.2145, 0.1422], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,912][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0243, 0.0460, 0.0212, 0.0298, 0.0194, 0.0394, 0.0366, 0.0592, 0.1008,
        0.1651, 0.0654, 0.1180, 0.2747], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,916][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0173, 0.0058, 0.0030, 0.0069, 0.0080, 0.0330, 0.0197, 0.0560, 0.0518,
        0.5221, 0.0950, 0.0202, 0.1612], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:46,919][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([2.7780e-01, 7.3146e-05, 8.3350e-04, 2.5654e-03, 1.3487e-03, 1.2033e-03,
        1.7760e-04, 7.9945e-04, 9.1349e-04, 7.8073e-03, 1.6238e-03, 7.5940e-04,
        7.0176e-01, 2.3303e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,923][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0007, 0.0107, 0.1141, 0.0496, 0.0079, 0.0304, 0.0534, 0.0573, 0.1596,
        0.0695, 0.0922, 0.1586, 0.1675, 0.0285], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,928][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.4502, 0.0078, 0.0209, 0.0562, 0.0393, 0.0275, 0.0555, 0.0621, 0.0483,
        0.0281, 0.0182, 0.0758, 0.0409, 0.0692], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,933][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0011, 0.0508, 0.0092, 0.0063, 0.0433, 0.0303, 0.0124, 0.1098, 0.0279,
        0.1413, 0.1390, 0.1274, 0.2413, 0.0599], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,938][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0225, 0.0023, 0.0031, 0.0070, 0.0017, 0.0133, 0.0024, 0.0363, 0.1467,
        0.0991, 0.0075, 0.1004, 0.3802, 0.1775], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,940][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.2500, 0.0137, 0.0110, 0.0135, 0.0056, 0.0404, 0.0120, 0.0396, 0.1119,
        0.0425, 0.0194, 0.1366, 0.0914, 0.2124], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,941][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.0011, 0.0388, 0.0652, 0.1448, 0.0224, 0.0546, 0.0599, 0.0719, 0.0508,
        0.0459, 0.1121, 0.0673, 0.2385, 0.0267], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,941][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0031, 0.0376, 0.0069, 0.0090, 0.0015, 0.0102, 0.0007, 0.0119, 0.0838,
        0.3243, 0.0111, 0.0687, 0.3497, 0.0816], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,942][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0017, 0.0148, 0.0434, 0.0615, 0.0091, 0.0300, 0.0089, 0.0729, 0.1042,
        0.1522, 0.0247, 0.1162, 0.2581, 0.1023], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,945][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0603, 0.0725, 0.0550, 0.1097, 0.0211, 0.0366, 0.0403, 0.0253, 0.0537,
        0.1170, 0.0281, 0.1515, 0.1467, 0.0822], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,948][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0563, 0.0510, 0.0348, 0.0595, 0.0222, 0.0390, 0.0252, 0.0651, 0.0819,
        0.1179, 0.0568, 0.0799, 0.2422, 0.0682], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,952][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0184, 0.0048, 0.0031, 0.0061, 0.0056, 0.0240, 0.0103, 0.0290, 0.0380,
        0.5010, 0.0599, 0.0226, 0.1758, 0.1014], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:46,955][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([4.1480e-01, 3.5987e-04, 1.1671e-03, 6.0697e-03, 2.2414e-03, 1.9124e-03,
        3.7369e-04, 1.8055e-03, 1.5490e-03, 7.3210e-03, 1.4509e-03, 1.3094e-03,
        5.2153e-01, 3.1769e-03, 3.4937e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,959][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0020, 0.0107, 0.0506, 0.1264, 0.0247, 0.0698, 0.1266, 0.0324, 0.1545,
        0.0601, 0.0345, 0.1278, 0.0729, 0.0435, 0.0635], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,964][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.5853, 0.0089, 0.0184, 0.0383, 0.0259, 0.0206, 0.0971, 0.0266, 0.0182,
        0.0143, 0.0068, 0.0404, 0.0228, 0.0391, 0.0373], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,969][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0024, 0.0273, 0.0021, 0.0022, 0.0080, 0.0112, 0.0105, 0.1020, 0.0275,
        0.0939, 0.0901, 0.0964, 0.3001, 0.0828, 0.1435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,972][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0156, 0.0016, 0.0020, 0.0078, 0.0008, 0.0082, 0.0024, 0.0225, 0.1309,
        0.0539, 0.0047, 0.0946, 0.3336, 0.1630, 0.1584], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,973][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.3178, 0.0104, 0.0089, 0.0098, 0.0051, 0.0265, 0.0119, 0.0264, 0.1013,
        0.0406, 0.0151, 0.0878, 0.0805, 0.2083, 0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,974][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0060, 0.0326, 0.1137, 0.1297, 0.0301, 0.0536, 0.0891, 0.1637, 0.0377,
        0.0354, 0.0543, 0.0643, 0.1343, 0.0278, 0.0275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,975][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0118, 0.0326, 0.0098, 0.0078, 0.0019, 0.0076, 0.0011, 0.0134, 0.1898,
        0.1969, 0.0107, 0.0601, 0.3025, 0.1369, 0.0172], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,978][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0018, 0.0108, 0.0222, 0.0631, 0.0157, 0.0305, 0.0189, 0.1512, 0.1369,
        0.1658, 0.0218, 0.0708, 0.1643, 0.1025, 0.0237], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,981][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0960, 0.0518, 0.0454, 0.0658, 0.0305, 0.0327, 0.0651, 0.0297, 0.0704,
        0.1202, 0.0319, 0.1049, 0.1262, 0.0958, 0.0337], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,986][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.0633, 0.0372, 0.0173, 0.0220, 0.0128, 0.0272, 0.0345, 0.0618, 0.0928,
        0.1408, 0.0500, 0.0745, 0.2314, 0.0622, 0.0720], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,991][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0092, 0.0049, 0.0035, 0.0043, 0.0026, 0.0234, 0.0132, 0.0342, 0.0347,
        0.4701, 0.0333, 0.0123, 0.1774, 0.1038, 0.0729], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:46,993][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([4.5045e-01, 1.4687e-04, 1.2882e-03, 3.1548e-03, 2.6751e-03, 1.1514e-03,
        7.6492e-04, 8.8964e-04, 1.9783e-03, 6.6249e-03, 7.2520e-04, 9.6224e-04,
        4.9516e-01, 3.2178e-03, 1.7639e-02, 1.3169e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:46,998][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0007, 0.0188, 0.0891, 0.0717, 0.0177, 0.1021, 0.1021, 0.0726, 0.1187,
        0.0350, 0.0248, 0.0998, 0.0984, 0.0268, 0.0726, 0.0491],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,002][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.5635, 0.0061, 0.0142, 0.0498, 0.0137, 0.0170, 0.0466, 0.0251, 0.0222,
        0.0164, 0.0043, 0.0514, 0.0351, 0.0542, 0.0266, 0.0539],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,004][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0016, 0.0263, 0.0034, 0.0044, 0.0122, 0.0286, 0.0191, 0.1180, 0.0243,
        0.0751, 0.0804, 0.0791, 0.2532, 0.0422, 0.0899, 0.1424],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,005][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0863, 0.0060, 0.0030, 0.0085, 0.0016, 0.0123, 0.0059, 0.0257, 0.1089,
        0.0200, 0.0079, 0.0426, 0.0774, 0.0685, 0.0736, 0.4517],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,006][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.5310, 0.0122, 0.0054, 0.0070, 0.0034, 0.0159, 0.0043, 0.0093, 0.0192,
        0.0084, 0.0052, 0.0307, 0.0173, 0.0715, 0.0184, 0.2411],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,007][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0025, 0.0618, 0.0877, 0.0805, 0.0335, 0.0487, 0.1181, 0.1289, 0.0343,
        0.0269, 0.0644, 0.0455, 0.1156, 0.0305, 0.0333, 0.0878],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,009][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([4.9257e-02, 2.3219e-02, 1.4807e-03, 2.5928e-03, 6.0224e-04, 9.2839e-04,
        1.0642e-04, 1.0192e-03, 7.5907e-03, 1.2000e-02, 1.2341e-03, 5.3925e-03,
        1.1907e-02, 4.4630e-03, 1.3646e-03, 8.7684e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,012][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0066, 0.0203, 0.0323, 0.0356, 0.0229, 0.0586, 0.0145, 0.0757, 0.0525,
        0.1063, 0.0270, 0.0633, 0.0936, 0.0733, 0.0309, 0.2865],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,016][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0765, 0.0637, 0.0215, 0.0782, 0.0355, 0.0368, 0.0423, 0.0362, 0.0710,
        0.0999, 0.0279, 0.1111, 0.0845, 0.0527, 0.0191, 0.1432],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,021][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.0804, 0.0325, 0.0110, 0.0289, 0.0214, 0.0347, 0.0372, 0.0513, 0.0536,
        0.1080, 0.0553, 0.0653, 0.1581, 0.0426, 0.0464, 0.1734],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,026][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0318, 0.0082, 0.0019, 0.0036, 0.0054, 0.0232, 0.0158, 0.0338, 0.0243,
        0.4584, 0.0547, 0.0130, 0.1129, 0.0812, 0.0363, 0.0957],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,028][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.4445e-01, 1.9139e-05, 2.1990e-04, 5.6878e-04, 3.5512e-04, 2.0857e-04,
        5.7051e-05, 1.5622e-04, 3.1363e-04, 1.5751e-03, 1.3589e-04, 1.6416e-04,
        2.2547e-01, 4.8687e-04, 4.2773e-03, 2.5834e-03, 6.1896e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,033][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0002, 0.0121, 0.0363, 0.1596, 0.0172, 0.0841, 0.1004, 0.0492, 0.0967,
        0.0475, 0.0707, 0.0571, 0.0502, 0.0422, 0.0555, 0.0834, 0.0376],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,036][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4744, 0.0073, 0.0188, 0.0462, 0.0152, 0.0122, 0.0255, 0.0350, 0.0311,
        0.0182, 0.0069, 0.0494, 0.0280, 0.0542, 0.0343, 0.0662, 0.0771],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,037][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0010, 0.0269, 0.0015, 0.0037, 0.0085, 0.0189, 0.0104, 0.0494, 0.0237,
        0.0763, 0.0382, 0.0675, 0.1855, 0.0629, 0.1123, 0.1093, 0.2039],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,038][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.7156e-03, 1.9342e-03, 1.2732e-03, 2.0401e-03, 6.2557e-04, 4.2170e-03,
        4.6455e-04, 7.4839e-03, 2.3826e-02, 1.4689e-02, 1.6286e-03, 1.5449e-02,
        4.3019e-02, 2.0080e-02, 2.4563e-02, 2.9543e-01, 5.3356e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,039][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.4249, 0.0051, 0.0029, 0.0046, 0.0022, 0.0073, 0.0017, 0.0076, 0.0127,
        0.0052, 0.0025, 0.0172, 0.0118, 0.0339, 0.0081, 0.2750, 0.1774],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,042][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0081, 0.0447, 0.0292, 0.1436, 0.0201, 0.0417, 0.1017, 0.0870, 0.0385,
        0.0236, 0.0826, 0.0397, 0.0940, 0.0294, 0.0328, 0.1361, 0.0473],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,044][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([1.6221e-03, 3.7133e-03, 1.1084e-03, 5.7638e-04, 1.3628e-04, 5.0586e-04,
        3.4683e-05, 2.8466e-04, 3.8886e-03, 9.5722e-03, 4.3663e-04, 3.4346e-03,
        9.2188e-03, 3.0745e-03, 7.4655e-04, 7.5273e-01, 2.0891e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,049][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0036, 0.0151, 0.0255, 0.0298, 0.0085, 0.0373, 0.0077, 0.0429, 0.0658,
        0.0763, 0.0108, 0.0353, 0.1019, 0.0485, 0.0118, 0.2481, 0.2311],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,053][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0579, 0.0363, 0.0126, 0.0503, 0.0168, 0.0168, 0.0407, 0.0238, 0.0593,
        0.0629, 0.0210, 0.0969, 0.0640, 0.0609, 0.0115, 0.1158, 0.2525],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,058][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0284, 0.0224, 0.0092, 0.0140, 0.0081, 0.0193, 0.0149, 0.0186, 0.0368,
        0.0558, 0.0192, 0.0387, 0.1189, 0.0335, 0.0361, 0.1111, 0.4150],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,062][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0127, 0.0047, 0.0014, 0.0048, 0.0035, 0.0134, 0.0097, 0.0159, 0.0192,
        0.2908, 0.0320, 0.0068, 0.0781, 0.0569, 0.0230, 0.0926, 0.3344],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,065][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([2.1427e-01, 8.2462e-05, 3.6629e-04, 1.8300e-03, 1.0416e-03, 1.5247e-03,
        1.5660e-04, 1.2102e-03, 1.1070e-03, 3.6946e-03, 8.1241e-04, 8.3352e-04,
        2.3661e-01, 1.1359e-03, 9.4182e-03, 6.0464e-03, 5.1869e-01, 1.1616e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,069][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0003, 0.0060, 0.0471, 0.0621, 0.0094, 0.0254, 0.0656, 0.0204, 0.0649,
        0.0623, 0.0169, 0.0783, 0.0749, 0.0261, 0.0706, 0.0882, 0.2690, 0.0125],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,069][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.1376, 0.0230, 0.0647, 0.0677, 0.0436, 0.0251, 0.0362, 0.0722, 0.0498,
        0.0257, 0.0227, 0.0625, 0.0465, 0.0926, 0.0580, 0.0517, 0.0742, 0.0464],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,070][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0009, 0.0282, 0.0028, 0.0035, 0.0159, 0.0213, 0.0103, 0.0888, 0.0122,
        0.0483, 0.0339, 0.0736, 0.1210, 0.0454, 0.0684, 0.1353, 0.1669, 0.1234],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,071][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([1.5250e-02, 1.5617e-03, 1.0406e-03, 4.9876e-03, 6.2357e-04, 5.0715e-03,
        2.6537e-04, 7.5280e-03, 3.3096e-02, 7.8619e-03, 2.0181e-03, 1.3009e-02,
        4.7823e-02, 1.1007e-02, 1.5083e-02, 1.8342e-01, 5.2255e-01, 1.2781e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,074][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.1931, 0.0082, 0.0051, 0.0064, 0.0024, 0.0172, 0.0063, 0.0149, 0.0267,
        0.0093, 0.0069, 0.0399, 0.0199, 0.0641, 0.0247, 0.2016, 0.1921, 0.1612],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,078][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0002, 0.0169, 0.0223, 0.1533, 0.0172, 0.0343, 0.0510, 0.0706, 0.0301,
        0.0269, 0.1013, 0.0645, 0.1404, 0.0258, 0.0165, 0.1121, 0.0500, 0.0667],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,081][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([3.6124e-03, 4.0773e-03, 1.0670e-03, 2.4930e-03, 1.7221e-04, 1.6788e-03,
        8.7121e-05, 1.3395e-03, 9.2647e-03, 3.1086e-02, 7.4286e-04, 7.0069e-03,
        3.0324e-02, 5.4920e-03, 1.3307e-03, 5.4665e-01, 2.9576e-01, 5.7817e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,085][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0016, 0.0049, 0.0176, 0.0267, 0.0065, 0.0256, 0.0063, 0.0418, 0.0515,
        0.0661, 0.0157, 0.0827, 0.1116, 0.0698, 0.0108, 0.2334, 0.1785, 0.0488],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,090][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0111, 0.0318, 0.0167, 0.0466, 0.0147, 0.0230, 0.0366, 0.0308, 0.0426,
        0.0466, 0.0216, 0.1074, 0.0870, 0.0547, 0.0114, 0.1185, 0.2378, 0.0609],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,095][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0308, 0.0337, 0.0138, 0.0303, 0.0146, 0.0297, 0.0241, 0.0457, 0.0323,
        0.0573, 0.0389, 0.0646, 0.1006, 0.0301, 0.0331, 0.0653, 0.2271, 0.1281],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,099][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0217, 0.0038, 0.0019, 0.0049, 0.0050, 0.0250, 0.0121, 0.0362, 0.0275,
        0.2800, 0.0407, 0.0189, 0.1053, 0.0557, 0.0178, 0.0759, 0.2389, 0.0288],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,101][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([1.5619e-01, 5.8409e-05, 4.1320e-04, 1.1125e-03, 5.4214e-04, 4.0768e-04,
        9.4904e-05, 3.0148e-04, 4.0858e-04, 2.2533e-03, 3.4852e-04, 2.5085e-04,
        2.8335e-01, 1.0095e-03, 7.0144e-03, 4.8381e-03, 5.2688e-01, 4.2029e-04,
        1.4098e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,102][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0010, 0.0102, 0.0731, 0.0956, 0.0230, 0.0562, 0.1310, 0.0501, 0.1114,
        0.0732, 0.0220, 0.0673, 0.0700, 0.0188, 0.0366, 0.0417, 0.0689, 0.0046,
        0.0454], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,103][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.5240, 0.0047, 0.0168, 0.0293, 0.0102, 0.0091, 0.0139, 0.0228, 0.0147,
        0.0136, 0.0039, 0.0402, 0.0272, 0.0401, 0.0174, 0.0462, 0.0621, 0.0138,
        0.0901], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,104][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0016, 0.0190, 0.0028, 0.0047, 0.0081, 0.0111, 0.0043, 0.0326, 0.0156,
        0.0681, 0.0265, 0.0515, 0.1480, 0.0565, 0.1058, 0.1227, 0.1676, 0.0806,
        0.0732], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,105][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([4.1433e-02, 1.0797e-03, 6.1423e-04, 6.4382e-04, 2.5689e-04, 1.5902e-03,
        1.7943e-04, 2.1565e-03, 1.7085e-02, 4.2879e-03, 6.6005e-04, 2.8384e-03,
        9.4322e-03, 1.0692e-02, 6.3288e-03, 7.7226e-02, 1.5929e-01, 5.8985e-02,
        6.0522e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,107][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([4.5050e-01, 2.2503e-03, 8.7810e-04, 8.7451e-04, 3.9121e-04, 1.7156e-03,
        3.7662e-04, 8.8330e-04, 2.3131e-03, 1.1774e-03, 4.3732e-04, 2.7685e-03,
        2.8251e-03, 9.6358e-03, 1.3043e-03, 3.7327e-02, 2.8924e-02, 6.0082e-02,
        3.9534e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,112][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0025, 0.0409, 0.0569, 0.0922, 0.0466, 0.0377, 0.0519, 0.0733, 0.0528,
        0.0336, 0.0596, 0.0230, 0.0872, 0.0272, 0.0281, 0.0512, 0.0272, 0.0613,
        0.1468], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,114][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([6.2829e-02, 5.8936e-03, 6.3213e-04, 1.1625e-03, 1.5832e-04, 3.8072e-04,
        1.6696e-05, 2.9164e-04, 3.1870e-03, 5.2674e-03, 1.5424e-04, 1.4388e-03,
        3.4289e-03, 1.4215e-03, 1.6852e-04, 3.0499e-01, 6.8291e-02, 9.3278e-02,
        4.4701e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,119][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0034, 0.0100, 0.0202, 0.0360, 0.0051, 0.0108, 0.0036, 0.0146, 0.0181,
        0.0243, 0.0038, 0.0272, 0.0776, 0.0338, 0.0062, 0.1527, 0.2157, 0.0388,
        0.2981], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,124][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.1491, 0.0421, 0.0159, 0.0326, 0.0211, 0.0120, 0.0172, 0.0077, 0.0206,
        0.0626, 0.0101, 0.0556, 0.0476, 0.0402, 0.0088, 0.0925, 0.1464, 0.0492,
        0.1686], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,128][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0413, 0.0232, 0.0100, 0.0155, 0.0080, 0.0137, 0.0087, 0.0155, 0.0314,
        0.0506, 0.0203, 0.0243, 0.0759, 0.0238, 0.0227, 0.0670, 0.2510, 0.0963,
        0.2009], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,133][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0079, 0.0015, 0.0009, 0.0018, 0.0013, 0.0097, 0.0055, 0.0194, 0.0216,
        0.2240, 0.0136, 0.0075, 0.0923, 0.0624, 0.0160, 0.0666, 0.3002, 0.0212,
        0.1267], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:47,134][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([2.9148e-01, 4.5720e-05, 3.2375e-04, 1.3411e-03, 4.8720e-04, 3.0849e-04,
        1.3291e-04, 2.1016e-04, 3.8999e-04, 1.6560e-03, 1.7835e-04, 1.6126e-04,
        1.8504e-01, 4.8078e-04, 3.2576e-03, 2.6889e-03, 4.9158e-01, 2.9669e-04,
        9.2728e-03, 1.0664e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,135][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0002, 0.0113, 0.0753, 0.0220, 0.0110, 0.0504, 0.0653, 0.0307, 0.1048,
        0.0532, 0.0194, 0.0709, 0.0852, 0.0125, 0.0619, 0.0522, 0.0571, 0.0198,
        0.1313, 0.0656], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,136][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.5304, 0.0039, 0.0147, 0.0264, 0.0076, 0.0063, 0.0142, 0.0160, 0.0209,
        0.0085, 0.0022, 0.0343, 0.0186, 0.0271, 0.0140, 0.0305, 0.0425, 0.0135,
        0.0961, 0.0722], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,138][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0042, 0.0239, 0.0035, 0.0056, 0.0069, 0.0192, 0.0081, 0.0517, 0.0144,
        0.0432, 0.0317, 0.0377, 0.1039, 0.0360, 0.0655, 0.0700, 0.1183, 0.0905,
        0.1003, 0.1655], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,140][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([8.6046e-03, 2.3715e-04, 7.0686e-05, 8.9815e-05, 2.1757e-05, 2.3664e-04,
        2.3232e-05, 1.6937e-04, 9.5504e-04, 4.2602e-04, 3.2973e-05, 3.4055e-04,
        1.0826e-03, 6.3350e-04, 3.5190e-04, 6.9520e-03, 1.7674e-02, 8.2548e-03,
        4.2013e-02, 9.1183e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,143][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([4.3325e-01, 1.1739e-03, 1.8139e-04, 2.5654e-04, 1.5950e-04, 7.9981e-04,
        1.1310e-04, 2.2088e-04, 4.0353e-04, 2.2762e-04, 1.4762e-04, 5.8008e-04,
        3.8698e-04, 1.4689e-03, 2.4212e-04, 6.6745e-03, 5.2966e-03, 1.5996e-02,
        6.4852e-02, 4.6757e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,148][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0025, 0.0208, 0.0402, 0.0472, 0.0222, 0.0415, 0.0670, 0.0583, 0.0313,
        0.0280, 0.0607, 0.0135, 0.0445, 0.0077, 0.0158, 0.0417, 0.0172, 0.0316,
        0.0909, 0.3174], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,151][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([8.4862e-03, 7.7111e-04, 3.7660e-05, 3.8295e-05, 5.2825e-06, 1.2463e-05,
        4.3346e-07, 4.7554e-06, 4.8869e-05, 1.3170e-04, 4.8435e-06, 2.7767e-05,
        6.7742e-05, 2.5982e-05, 3.4349e-06, 8.3426e-03, 1.5417e-03, 2.4034e-03,
        1.1640e-02, 9.6641e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,155][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0049, 0.0064, 0.0071, 0.0085, 0.0033, 0.0072, 0.0015, 0.0078, 0.0070,
        0.0114, 0.0039, 0.0074, 0.0154, 0.0113, 0.0022, 0.0365, 0.0362, 0.0160,
        0.1021, 0.7039], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,160][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0713, 0.0349, 0.0132, 0.0325, 0.0130, 0.0099, 0.0178, 0.0113, 0.0193,
        0.0437, 0.0085, 0.0296, 0.0220, 0.0172, 0.0050, 0.0411, 0.0702, 0.0373,
        0.0946, 0.4072], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,165][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0344, 0.0143, 0.0041, 0.0094, 0.0043, 0.0077, 0.0060, 0.0079, 0.0084,
        0.0279, 0.0122, 0.0097, 0.0308, 0.0106, 0.0095, 0.0298, 0.0884, 0.0642,
        0.0834, 0.5366], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,166][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0205, 0.0037, 0.0010, 0.0020, 0.0016, 0.0089, 0.0062, 0.0152, 0.0090,
        0.1721, 0.0180, 0.0048, 0.0540, 0.0310, 0.0072, 0.0519, 0.1378, 0.0261,
        0.1353, 0.2937], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:47,167][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.5492e-01, 8.4615e-06, 7.7175e-05, 2.5347e-04, 1.3752e-04, 8.8674e-05,
        2.5943e-05, 7.5184e-05, 9.5862e-05, 5.4075e-04, 5.4136e-05, 5.4558e-05,
        9.0025e-02, 1.8263e-04, 1.4765e-03, 1.0732e-03, 2.5760e-01, 8.7053e-05,
        3.3489e-03, 3.0028e-03, 4.8688e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,167][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0005, 0.0082, 0.0318, 0.1247, 0.0185, 0.0721, 0.0954, 0.0474, 0.0942,
        0.0329, 0.0432, 0.0445, 0.0373, 0.0273, 0.0429, 0.0598, 0.0259, 0.0101,
        0.0383, 0.0900, 0.0548], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,170][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4462, 0.0040, 0.0133, 0.0302, 0.0076, 0.0081, 0.0154, 0.0189, 0.0222,
        0.0111, 0.0029, 0.0266, 0.0184, 0.0373, 0.0192, 0.0328, 0.0497, 0.0133,
        0.0859, 0.0706, 0.0664], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,173][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0011, 0.0174, 0.0010, 0.0026, 0.0047, 0.0088, 0.0052, 0.0347, 0.0118,
        0.0352, 0.0225, 0.0324, 0.0903, 0.0278, 0.0350, 0.0555, 0.0939, 0.0773,
        0.0336, 0.1737, 0.2353], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,175][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.3818e-03, 2.7881e-05, 1.1994e-05, 3.1816e-05, 4.9954e-06, 3.6259e-05,
        2.2831e-06, 4.2709e-05, 1.2941e-04, 7.7816e-05, 6.9725e-06, 8.9264e-05,
        2.7392e-04, 8.6954e-05, 8.0230e-05, 2.8000e-03, 4.6631e-03, 1.5930e-03,
        1.2692e-02, 5.7386e-01, 4.0111e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,178][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([2.5668e-01, 2.4066e-04, 5.6176e-05, 1.3160e-04, 5.8085e-05, 1.3263e-04,
        1.9506e-05, 9.4639e-05, 1.5681e-04, 5.3641e-05, 3.2671e-05, 2.0395e-04,
        1.1009e-04, 4.0342e-04, 6.0127e-05, 4.0164e-03, 2.3175e-03, 7.5393e-03,
        4.5765e-02, 5.6137e-01, 1.2056e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,183][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0086, 0.0147, 0.0133, 0.0636, 0.0109, 0.0183, 0.0386, 0.0379, 0.0213,
        0.0099, 0.0449, 0.0144, 0.0459, 0.0089, 0.0096, 0.0421, 0.0139, 0.0262,
        0.0623, 0.4626, 0.0318], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,186][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([1.1609e-03, 1.9219e-04, 1.7344e-05, 1.1830e-05, 2.5774e-06, 6.5051e-06,
        2.5800e-07, 2.0210e-06, 3.1991e-05, 5.8641e-05, 2.7292e-06, 2.5413e-05,
        4.1851e-05, 1.7424e-05, 2.2934e-06, 7.8340e-03, 1.2973e-03, 1.5424e-03,
        1.7421e-02, 8.4659e-01, 1.2374e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,190][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0020, 0.0037, 0.0044, 0.0063, 0.0013, 0.0049, 0.0006, 0.0042, 0.0060,
        0.0075, 0.0009, 0.0036, 0.0126, 0.0057, 0.0008, 0.0265, 0.0247, 0.0103,
        0.0508, 0.5437, 0.2795], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,195][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0442, 0.0140, 0.0030, 0.0143, 0.0031, 0.0030, 0.0074, 0.0032, 0.0090,
        0.0122, 0.0029, 0.0185, 0.0121, 0.0123, 0.0015, 0.0222, 0.0463, 0.0210,
        0.0473, 0.3914, 0.3111], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,197][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0176, 0.0063, 0.0018, 0.0038, 0.0019, 0.0041, 0.0023, 0.0027, 0.0052,
        0.0109, 0.0032, 0.0055, 0.0196, 0.0052, 0.0044, 0.0183, 0.0665, 0.0356,
        0.0334, 0.3175, 0.4342], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,198][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([1.2016e-02, 1.5852e-03, 3.0489e-04, 1.3923e-03, 7.4777e-04, 2.5953e-03,
        1.8048e-03, 2.9900e-03, 3.0034e-03, 4.9055e-02, 4.6028e-03, 1.2779e-03,
        1.4551e-02, 1.0796e-02, 2.3527e-03, 1.5347e-02, 5.8051e-02, 1.4628e-02,
        3.0666e-02, 2.2987e-01, 5.4236e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:47,202][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:47,206][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[856],
        [ 89],
        [  3],
        [  5],
        [  1],
        [  6],
        [ 11],
        [ 17],
        [  6],
        [  2],
        [ 16],
        [  2],
        [  6],
        [ 35],
        [ 41],
        [  3],
        [  7],
        [ 26],
        [  7],
        [  9],
        [  2]], device='cuda:0')
[2024-07-23 21:06:47,209][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[1152],
        [1120],
        [ 340],
        [ 183],
        [  11],
        [  25],
        [  89],
        [ 200],
        [ 142],
        [  43],
        [ 193],
        [  24],
        [ 102],
        [ 308],
        [ 529],
        [  22],
        [  41],
        [ 174],
        [  68],
        [  64],
        [  11]], device='cuda:0')
[2024-07-23 21:06:47,212][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 5820],
        [ 5913],
        [ 6062],
        [ 7022],
        [ 6978],
        [ 7313],
        [ 7294],
        [ 6612],
        [ 6616],
        [ 7060],
        [ 9142],
        [ 9318],
        [25480],
        [24378],
        [28428],
        [28833],
        [30305],
        [30428],
        [30727],
        [30166],
        [31398]], device='cuda:0')
[2024-07-23 21:06:47,215][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[28937],
        [18634],
        [18862],
        [19794],
        [20444],
        [20794],
        [20689],
        [21117],
        [21119],
        [20660],
        [20640],
        [20593],
        [20387],
        [20079],
        [20163],
        [19870],
        [19538],
        [19329],
        [19275],
        [19075],
        [18872]], device='cuda:0')
[2024-07-23 21:06:47,218][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[9320],
        [3594],
        [4687],
        [5430],
        [4886],
        [4758],
        [4931],
        [4414],
        [4141],
        [4061],
        [4034],
        [4002],
        [4085],
        [4081],
        [4022],
        [3876],
        [3999],
        [3817],
        [3863],
        [3921],
        [4010]], device='cuda:0')
[2024-07-23 21:06:47,222][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 1004],
        [ 6991],
        [12678],
        [12613],
        [13368],
        [13349],
        [14545],
        [14563],
        [15024],
        [14967],
        [14075],
        [13420],
        [15131],
        [15261],
        [15702],
        [16503],
        [16790],
        [16190],
        [16959],
        [16781],
        [17303]], device='cuda:0')
[2024-07-23 21:06:47,225][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[18429],
        [10855],
        [ 8431],
        [ 9855],
        [ 9992],
        [12354],
        [14377],
        [16712],
        [14272],
        [14900],
        [16128],
        [14913],
        [17101],
        [19898],
        [20347],
        [19957],
        [18651],
        [20064],
        [19754],
        [18946],
        [18326]], device='cuda:0')
[2024-07-23 21:06:47,228][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[4300],
        [4315],
        [4245],
        [3861],
        [4070],
        [3870],
        [3673],
        [4125],
        [3493],
        [3882],
        [3403],
        [1894],
        [1960],
        [1590],
        [1570],
        [1730],
        [1612],
        [1682],
        [1204],
        [2266],
        [2215]], device='cuda:0')
[2024-07-23 21:06:47,231][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[43983],
        [43789],
        [42565],
        [40747],
        [42145],
        [42558],
        [43213],
        [43884],
        [42275],
        [39793],
        [38620],
        [36467],
        [35435],
        [33116],
        [32362],
        [32332],
        [30532],
        [30859],
        [30585],
        [30175],
        [28979]], device='cuda:0')
[2024-07-23 21:06:47,234][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[1255],
        [3690],
        [4442],
        [4121],
        [4021],
        [4009],
        [3945],
        [3732],
        [3633],
        [3678],
        [3676],
        [3739],
        [3839],
        [3727],
        [3679],
        [3692],
        [3724],
        [3752],
        [3796],
        [3837],
        [3871]], device='cuda:0')
[2024-07-23 21:06:47,236][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[4071],
        [ 125],
        [ 154],
        [ 205],
        [ 242],
        [ 457],
        [ 720],
        [1303],
        [ 942],
        [1208],
        [1153],
        [1049],
        [1260],
        [1447],
        [1537],
        [1276],
        [1242],
        [1017],
        [ 832],
        [ 860],
        [ 796]], device='cuda:0')
[2024-07-23 21:06:47,237][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[16062],
        [ 7793],
        [ 9247],
        [ 9997],
        [ 9729],
        [ 9411],
        [ 9379],
        [ 7846],
        [ 7068],
        [ 7051],
        [ 7026],
        [ 6782],
        [ 6587],
        [ 6559],
        [ 6315],
        [ 6008],
        [ 5864],
        [ 5512],
        [ 5465],
        [ 5420],
        [ 5282]], device='cuda:0')
[2024-07-23 21:06:47,240][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[14577],
        [  116],
        [   66],
        [  140],
        [  123],
        [  107],
        [  229],
        [  311],
        [  535],
        [  365],
        [  690],
        [  885],
        [ 1932],
        [ 2184],
        [ 2733],
        [ 2435],
        [ 3963],
        [ 2197],
        [ 3531],
        [ 8831],
        [ 6989]], device='cuda:0')
[2024-07-23 21:06:47,242][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[48111],
        [10351],
        [  435],
        [  442],
        [  371],
        [  759],
        [ 1082],
        [  426],
        [   27],
        [   40],
        [  499],
        [  269],
        [  170],
        [  519],
        [  118],
        [   99],
        [  165],
        [  195],
        [  187],
        [  200],
        [  265]], device='cuda:0')
[2024-07-23 21:06:47,245][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[3271],
        [1321],
        [ 909],
        [1571],
        [1330],
        [1890],
        [2443],
        [2309],
        [1933],
        [1363],
        [2923],
        [1657],
        [2270],
        [2443],
        [2128],
        [1348],
        [2034],
        [3367],
        [2818],
        [2273],
        [1998]], device='cuda:0')
[2024-07-23 21:06:47,249][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[30606],
        [30563],
        [27337],
        [22349],
        [24793],
        [27584],
        [27230],
        [28078],
        [26563],
        [25746],
        [25846],
        [10164],
        [ 2915],
        [ 2881],
        [ 2788],
        [ 2801],
        [ 5212],
        [ 4948],
        [ 4768],
        [ 4900],
        [ 4235]], device='cuda:0')
[2024-07-23 21:06:47,252][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 552],
        [3797],
        [5237],
        [4988],
        [5804],
        [5301],
        [4536],
        [4492],
        [4082],
        [3955],
        [4524],
        [3567],
        [3898],
        [4181],
        [4403],
        [4467],
        [5018],
        [5286],
        [4862],
        [4796],
        [5133]], device='cuda:0')
[2024-07-23 21:06:47,255][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[10165],
        [10114],
        [10634],
        [ 7289],
        [ 8863],
        [ 7649],
        [ 8142],
        [ 5002],
        [ 4781],
        [ 6420],
        [ 5526],
        [ 6660],
        [ 7069],
        [ 6691],
        [ 5707],
        [ 6345],
        [ 5917],
        [ 5956],
        [ 6900],
        [ 6730],
        [ 6181]], device='cuda:0')
[2024-07-23 21:06:47,258][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[17124],
        [28132],
        [28595],
        [30896],
        [35126],
        [36192],
        [33019],
        [20312],
        [21142],
        [26660],
        [26757],
        [31022],
        [32305],
        [33898],
        [34233],
        [33226],
        [29735],
        [32376],
        [32894],
        [35436],
        [30425]], device='cuda:0')
[2024-07-23 21:06:47,261][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 802],
        [1289],
        [3442],
        [1703],
        [1666],
        [1241],
        [ 803],
        [ 460],
        [ 139],
        [ 113],
        [ 139],
        [ 103],
        [  89],
        [ 117],
        [ 140],
        [ 204],
        [  68],
        [ 101],
        [ 912],
        [  65],
        [  69]], device='cuda:0')
[2024-07-23 21:06:47,265][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[21448],
        [21425],
        [21337],
        [21450],
        [21599],
        [21593],
        [20696],
        [21065],
        [23729],
        [21992],
        [23959],
        [27603],
        [27345],
        [30715],
        [31461],
        [31067],
        [32994],
        [33952],
        [36099],
        [34701],
        [35848]], device='cuda:0')
[2024-07-23 21:06:47,268][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 194],
        [1110],
        [1034],
        [1388],
        [1108],
        [1625],
        [1790],
        [2208],
        [2425],
        [2736],
        [2985],
        [3341],
        [3490],
        [3677],
        [3271],
        [3397],
        [3802],
        [4337],
        [3194],
        [2587],
        [2448]], device='cuda:0')
[2024-07-23 21:06:47,271][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 5207],
        [ 2704],
        [ 4967],
        [ 3822],
        [ 3314],
        [ 3188],
        [ 2927],
        [ 3244],
        [ 5914],
        [ 3889],
        [ 5111],
        [ 6363],
        [ 7006],
        [ 6245],
        [ 6354],
        [15528],
        [14032],
        [12273],
        [ 8722],
        [ 6885],
        [ 6310]], device='cuda:0')
[2024-07-23 21:06:47,272][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[22320],
        [10783],
        [ 1568],
        [ 1454],
        [ 2763],
        [ 1790],
        [ 1050],
        [ 1211],
        [ 2736],
        [ 2416],
        [ 2148],
        [ 2841],
        [ 3121],
        [ 3156],
        [ 3059],
        [ 2255],
        [ 5013],
        [ 4601],
        [ 3820],
        [ 5983],
        [ 6371]], device='cuda:0')
[2024-07-23 21:06:47,274][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[1633],
        [ 172],
        [ 528],
        [ 878],
        [ 607],
        [ 391],
        [ 621],
        [1683],
        [3348],
        [1991],
        [3349],
        [2697],
        [3159],
        [2081],
        [2080],
        [1546],
        [1529],
        [1421],
        [ 799],
        [ 833],
        [ 835]], device='cuda:0')
[2024-07-23 21:06:47,276][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 8526],
        [ 9298],
        [10211],
        [18362],
        [15932],
        [18090],
        [18705],
        [17392],
        [18022],
        [18434],
        [21052],
        [23835],
        [19302],
        [18072],
        [15733],
        [15454],
        [11508],
        [11993],
        [ 9081],
        [ 9026],
        [10086]], device='cuda:0')
[2024-07-23 21:06:47,279][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[  602],
        [ 4699],
        [17809],
        [37902],
        [ 8647],
        [14649],
        [ 8200],
        [ 9529],
        [13659],
        [ 7057],
        [ 9532],
        [12235],
        [10854],
        [10459],
        [10131],
        [ 9517],
        [12013],
        [10533],
        [13796],
        [ 9136],
        [12302]], device='cuda:0')
[2024-07-23 21:06:47,282][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[49072],
        [49742],
        [48413],
        [48238],
        [48906],
        [49070],
        [49489],
        [49818],
        [49255],
        [49725],
        [49325],
        [48710],
        [48294],
        [48452],
        [48638],
        [47816],
        [47060],
        [47404],
        [47316],
        [49343],
        [49139]], device='cuda:0')
[2024-07-23 21:06:47,285][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[41887],
        [17656],
        [41394],
        [35331],
        [42497],
        [36982],
        [42756],
        [33957],
        [35512],
        [31283],
        [24595],
        [28927],
        [37380],
        [34495],
        [32938],
        [33895],
        [36689],
        [31841],
        [31733],
        [31964],
        [31805]], device='cuda:0')
[2024-07-23 21:06:47,288][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071],
        [2071]], device='cuda:0')
[2024-07-23 21:06:47,395][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:47,400][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,403][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,404][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,405][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,405][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,406][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,408][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,410][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,414][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,417][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,421][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,425][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:47,430][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.7236, 0.2764], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,434][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.2079, 0.7921], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,436][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.9279, 0.0721], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,436][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7194, 0.2806], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,437][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.5510, 0.4490], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,438][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.2017, 0.7983], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,439][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0118, 0.9882], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,441][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.2427, 0.7573], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,445][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.1948, 0.8052], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,449][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0023, 0.9977], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,454][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6235, 0.3765], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,459][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0427, 0.9573], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:47,463][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.2083, 0.2076, 0.5841], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,468][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0090, 0.2522, 0.7388], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,468][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.4323, 0.2857, 0.2820], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,469][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0405, 0.6562, 0.3034], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,470][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.2557, 0.2660, 0.4782], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,471][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0425, 0.5203, 0.4372], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,473][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0020, 0.6558, 0.3422], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,477][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0507, 0.5483, 0.4010], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,481][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0355, 0.7235, 0.2410], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,484][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ used] are: tensor([4.1010e-04, 6.2766e-01, 3.7193e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,489][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0882, 0.4141, 0.4976], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,494][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0031, 0.3996, 0.5973], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:47,498][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.1150, 0.1350, 0.3827, 0.3673], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,500][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0043, 0.1230, 0.6447, 0.2281], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,501][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.1032, 0.2081, 0.3378, 0.3508], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,501][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0321, 0.3878, 0.3860, 0.1941], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,502][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.1447, 0.2170, 0.3319, 0.3064], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,503][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0194, 0.3669, 0.4025, 0.2112], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,506][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0005, 0.3071, 0.2339, 0.4586], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,511][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0356, 0.4218, 0.3065, 0.2361], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,515][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0395, 0.6187, 0.2222, 0.1196], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,518][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ by] are: tensor([8.9213e-05, 3.9037e-01, 3.3498e-01, 2.7456e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,523][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0382, 0.2566, 0.4772, 0.2280], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,527][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0016, 0.2167, 0.3245, 0.4572], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:47,532][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.2325, 0.1250, 0.3695, 0.1546, 0.1184], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,533][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0152, 0.0778, 0.3038, 0.1653, 0.4379], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,533][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.2910, 0.0847, 0.2618, 0.0992, 0.2633], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,534][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0231, 0.2199, 0.2855, 0.2483, 0.2233], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,535][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.2542, 0.1725, 0.1607, 0.1946, 0.2180], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,538][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.0360, 0.2167, 0.2152, 0.1282, 0.4039], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,541][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0013, 0.1199, 0.1622, 0.3413, 0.3752], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,546][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0504, 0.3057, 0.2266, 0.2117, 0.2056], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,550][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.1308, 0.3080, 0.1218, 0.0918, 0.3476], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,553][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([2.1514e-04, 2.0209e-01, 2.0720e-01, 3.0603e-01, 2.8446e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,558][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.1152, 0.1322, 0.3847, 0.1600, 0.2080], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,562][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.0045, 0.0810, 0.1393, 0.2556, 0.5195], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:47,564][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.1507, 0.1289, 0.2545, 0.1681, 0.0755, 0.2223], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,565][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0290, 0.0717, 0.2122, 0.1551, 0.3346, 0.1974], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,566][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.2209, 0.0982, 0.1185, 0.0798, 0.2122, 0.2705], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,566][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.1039, 0.2430, 0.1934, 0.1469, 0.1446, 0.1681], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,568][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.1814, 0.1330, 0.1916, 0.1458, 0.1613, 0.1870], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,571][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.0329, 0.2053, 0.1675, 0.0924, 0.2219, 0.2800], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,576][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0008, 0.1714, 0.1377, 0.2886, 0.2763, 0.1253], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,580][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0524, 0.2516, 0.1762, 0.1551, 0.1518, 0.2129], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,585][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.1082, 0.2605, 0.0866, 0.0652, 0.2689, 0.2105], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,590][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0011, 0.2030, 0.1407, 0.2164, 0.2100, 0.2289], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,594][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0912, 0.2061, 0.2555, 0.1042, 0.0633, 0.2797], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,596][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.0121, 0.1140, 0.1759, 0.3872, 0.2480, 0.0629], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:47,597][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.0319, 0.1180, 0.2264, 0.0893, 0.0375, 0.1049, 0.3920],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,598][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0019, 0.0193, 0.0818, 0.0638, 0.1525, 0.2265, 0.4542],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,598][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0689, 0.0966, 0.2897, 0.1025, 0.1179, 0.1581, 0.1662],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,601][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.0153, 0.1724, 0.2823, 0.0969, 0.1475, 0.2683, 0.0173],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,603][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.2106, 0.1090, 0.1283, 0.1025, 0.1126, 0.1127, 0.2243],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,608][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.0227, 0.1606, 0.1162, 0.0572, 0.1642, 0.2463, 0.2329],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,611][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [aut] are: tensor([3.6980e-04, 9.2170e-02, 7.8564e-02, 1.1757e-01, 1.0860e-01, 2.0727e-01,
        3.9545e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,615][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0273, 0.1946, 0.1449, 0.1223, 0.1158, 0.1953, 0.1999],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,620][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0376, 0.1689, 0.0653, 0.0259, 0.2068, 0.1422, 0.3532],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,623][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [aut] are: tensor([1.3210e-04, 1.5745e-01, 1.0161e-01, 4.1972e-02, 5.1867e-02, 8.3110e-02,
        5.6386e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,628][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0074, 0.0936, 0.2531, 0.0725, 0.0774, 0.4507, 0.0452],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,629][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0011, 0.1042, 0.2143, 0.3133, 0.2092, 0.0654, 0.0925],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:47,629][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.0098, 0.0701, 0.1507, 0.0942, 0.0371, 0.1385, 0.3281, 0.1716],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,630][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0010, 0.0248, 0.0785, 0.0475, 0.1491, 0.2138, 0.3644, 0.1210],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,631][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0530, 0.0705, 0.1353, 0.0740, 0.1313, 0.2466, 0.1594, 0.1299],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,633][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.0093, 0.1244, 0.2451, 0.0658, 0.1096, 0.3558, 0.0776, 0.0125],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,636][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.1046, 0.0836, 0.1187, 0.1064, 0.0975, 0.1120, 0.1978, 0.1792],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,641][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.0128, 0.1438, 0.1182, 0.0570, 0.1541, 0.1931, 0.1912, 0.1298],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,643][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ista] are: tensor([3.9260e-05, 8.1019e-02, 1.0844e-01, 1.1622e-01, 1.8851e-01, 2.6015e-01,
        1.9361e-01, 5.2007e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,648][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0126, 0.1659, 0.1349, 0.1038, 0.1153, 0.1804, 0.1974, 0.0897],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,653][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0385, 0.1996, 0.0311, 0.0366, 0.1682, 0.0757, 0.1912, 0.2590],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,655][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ista] are: tensor([6.6689e-06, 1.2474e-01, 9.5982e-02, 5.0889e-02, 7.9772e-02, 1.6130e-01,
        4.3387e-01, 5.3440e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,660][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0006, 0.0601, 0.2994, 0.0386, 0.0563, 0.4194, 0.1083, 0.0172],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,661][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ista] are: tensor([2.1337e-04, 9.5513e-02, 1.7425e-01, 2.5815e-01, 1.8966e-01, 9.0971e-02,
        1.2507e-01, 6.6165e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:47,662][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0174, 0.0303, 0.0502, 0.0284, 0.0294, 0.1372, 0.1832, 0.1337, 0.3903],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,662][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0020, 0.0124, 0.0687, 0.0456, 0.1488, 0.1328, 0.2972, 0.1204, 0.1720],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,663][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0423, 0.0209, 0.0220, 0.0207, 0.0489, 0.0996, 0.0499, 0.1322, 0.5636],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,666][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0122, 0.1687, 0.2315, 0.0817, 0.1456, 0.2447, 0.0428, 0.0067, 0.0660],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,669][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0530, 0.0994, 0.1134, 0.0981, 0.0730, 0.1312, 0.1308, 0.1214, 0.1797],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,673][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0089, 0.1061, 0.0804, 0.0445, 0.1131, 0.1582, 0.1553, 0.1337, 0.1998],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,676][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ de] are: tensor([9.1652e-05, 3.5976e-02, 6.6945e-02, 9.5398e-02, 2.3544e-01, 2.0401e-01,
        1.2531e-01, 1.2328e-01, 1.1355e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,680][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ de] are: tensor([0.0157, 0.1557, 0.1218, 0.0912, 0.0992, 0.1630, 0.1712, 0.0803, 0.1020],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,685][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0098, 0.0785, 0.0250, 0.0340, 0.0955, 0.0735, 0.2309, 0.2740, 0.1788],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,688][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ de] are: tensor([2.2119e-05, 1.8851e-02, 5.4542e-02, 4.3571e-02, 9.0405e-02, 1.1794e-01,
        4.8850e-01, 1.0201e-01, 8.4160e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,692][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0063, 0.0671, 0.1943, 0.0326, 0.0657, 0.4048, 0.0406, 0.0173, 0.1715],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,693][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0009, 0.0800, 0.1425, 0.1625, 0.1289, 0.0778, 0.0921, 0.1249, 0.1905],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:47,694][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0461, 0.0652, 0.0993, 0.0547, 0.0405, 0.1358, 0.1374, 0.0961, 0.2077,
        0.1171], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,695][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0224, 0.0311, 0.0773, 0.0580, 0.1633, 0.1161, 0.2126, 0.1125, 0.1496,
        0.0572], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,696][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.1099, 0.0615, 0.0655, 0.0724, 0.0972, 0.1265, 0.0801, 0.0798, 0.2230,
        0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,698][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0631, 0.2191, 0.2152, 0.0712, 0.1112, 0.1832, 0.0266, 0.0083, 0.0712,
        0.0307], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,701][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.0981, 0.0728, 0.0815, 0.0652, 0.0813, 0.0936, 0.1509, 0.1456, 0.1063,
        0.1047], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,705][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0228, 0.1055, 0.0816, 0.0553, 0.1327, 0.1553, 0.1127, 0.1040, 0.1233,
        0.1068], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,710][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0038, 0.0515, 0.0730, 0.1014, 0.1416, 0.1227, 0.1493, 0.1116, 0.0787,
        0.1665], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,715][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0220, 0.1447, 0.1081, 0.0920, 0.0904, 0.1296, 0.1339, 0.0666, 0.0895,
        0.1232], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,719][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.0623, 0.1549, 0.0492, 0.0428, 0.0770, 0.0817, 0.1312, 0.1607, 0.1221,
        0.1180], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,724][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0015, 0.0548, 0.0624, 0.0556, 0.0939, 0.0719, 0.3111, 0.1421, 0.1244,
        0.0823], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,725][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0452, 0.1003, 0.1700, 0.0547, 0.0621, 0.2510, 0.0549, 0.0223, 0.1256,
        0.1137], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,726][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0095, 0.0896, 0.1743, 0.2061, 0.1374, 0.0463, 0.0775, 0.0535, 0.1430,
        0.0628], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:47,727][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0678, 0.0196, 0.0470, 0.0451, 0.0326, 0.1562, 0.2499, 0.0867, 0.1772,
        0.0742, 0.0438], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,728][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0856, 0.0221, 0.0609, 0.0560, 0.0767, 0.1067, 0.2683, 0.1255, 0.1154,
        0.0386, 0.0442], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,730][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.1093, 0.0111, 0.0231, 0.0283, 0.0967, 0.1121, 0.1391, 0.0460, 0.3382,
        0.0585, 0.0375], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,734][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.0459, 0.2040, 0.2412, 0.0700, 0.1079, 0.1538, 0.0434, 0.0059, 0.0710,
        0.0400, 0.0168], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,738][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.1851, 0.0460, 0.0661, 0.0916, 0.0674, 0.0873, 0.1411, 0.0802, 0.0652,
        0.0744, 0.0958], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,742][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.0203, 0.1066, 0.0871, 0.0478, 0.1263, 0.1321, 0.1008, 0.0909, 0.1070,
        0.0953, 0.0860], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,747][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0011, 0.0356, 0.0317, 0.0441, 0.1445, 0.1312, 0.1418, 0.0892, 0.0931,
        0.1698, 0.1179], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,752][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0154, 0.1264, 0.1051, 0.0817, 0.0896, 0.1199, 0.1147, 0.0603, 0.0838,
        0.1201, 0.0829], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,756][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.0916, 0.1191, 0.0319, 0.0422, 0.0905, 0.0886, 0.1536, 0.1164, 0.0961,
        0.0897, 0.0803], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,758][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0008, 0.0425, 0.0429, 0.0342, 0.0819, 0.0970, 0.2910, 0.0814, 0.1221,
        0.0842, 0.1219], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,759][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0043, 0.0602, 0.1792, 0.0347, 0.0600, 0.2420, 0.0960, 0.0161, 0.1100,
        0.1570, 0.0404], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,759][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0056, 0.0740, 0.1367, 0.1985, 0.1541, 0.0580, 0.0628, 0.0572, 0.1194,
        0.0730, 0.0606], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:47,760][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0049, 0.0516, 0.0846, 0.0390, 0.0141, 0.1262, 0.0903, 0.0525, 0.1509,
        0.0922, 0.0477, 0.2463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,763][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0005, 0.0108, 0.0470, 0.0466, 0.0846, 0.1174, 0.2138, 0.1301, 0.1223,
        0.0408, 0.0754, 0.1106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,765][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0126, 0.0315, 0.0335, 0.0461, 0.0580, 0.1118, 0.0998, 0.0620, 0.1649,
        0.0829, 0.0552, 0.2415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,766][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0108, 0.1653, 0.1876, 0.0525, 0.0606, 0.0946, 0.0152, 0.0037, 0.0412,
        0.0176, 0.0073, 0.3436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,769][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0122, 0.0391, 0.0473, 0.0692, 0.0412, 0.0693, 0.1471, 0.1031, 0.0745,
        0.0804, 0.1194, 0.1974], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,774][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0058, 0.0874, 0.0752, 0.0396, 0.1020, 0.1438, 0.1212, 0.0793, 0.0979,
        0.0827, 0.0774, 0.0876], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,777][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ is] are: tensor([5.4581e-05, 4.9297e-02, 3.0317e-02, 5.8861e-02, 9.8732e-02, 9.0377e-02,
        7.2342e-02, 3.6157e-02, 5.6029e-02, 2.1479e-01, 1.8540e-01, 1.0764e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,781][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0092, 0.1076, 0.0899, 0.0663, 0.0701, 0.1184, 0.1248, 0.0566, 0.0747,
        0.1137, 0.0856, 0.0831], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,786][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0053, 0.1350, 0.0352, 0.0448, 0.0740, 0.0572, 0.1206, 0.1723, 0.0813,
        0.1006, 0.1025, 0.0712], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,789][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ is] are: tensor([1.6725e-05, 3.9799e-02, 4.3717e-02, 3.1608e-02, 5.4521e-02, 7.0474e-02,
        2.0550e-01, 4.8557e-02, 7.1614e-02, 7.6100e-02, 1.5702e-01, 2.0107e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,793][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0018, 0.0835, 0.1206, 0.0241, 0.0195, 0.2985, 0.0224, 0.0066, 0.0736,
        0.1240, 0.0348, 0.1906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,794][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0003, 0.0624, 0.1128, 0.1930, 0.1156, 0.0657, 0.0738, 0.0364, 0.1247,
        0.0586, 0.0538, 0.1029], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:47,795][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0069, 0.0224, 0.0362, 0.0216, 0.0193, 0.1042, 0.1277, 0.0617, 0.1466,
        0.1170, 0.0567, 0.1630, 0.1166], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,796][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0007, 0.0178, 0.0460, 0.0288, 0.1107, 0.0895, 0.2692, 0.1022, 0.1085,
        0.0470, 0.0608, 0.0635, 0.0553], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,797][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0156, 0.0230, 0.0258, 0.0356, 0.0581, 0.0744, 0.0619, 0.0721, 0.2201,
        0.0726, 0.0508, 0.2157, 0.0742], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,800][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0066, 0.1588, 0.1769, 0.0495, 0.0625, 0.1233, 0.0130, 0.0055, 0.0431,
        0.0261, 0.0148, 0.2564, 0.0634], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,805][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0207, 0.0489, 0.0602, 0.0542, 0.0573, 0.0756, 0.1014, 0.1048, 0.0652,
        0.0730, 0.0977, 0.1854, 0.0557], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,809][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0059, 0.0924, 0.0660, 0.0355, 0.0971, 0.1205, 0.1073, 0.0761, 0.0929,
        0.0794, 0.0712, 0.0794, 0.0763], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,812][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ a] are: tensor([6.0151e-05, 5.2839e-02, 3.1688e-02, 7.1501e-02, 9.5943e-02, 8.5086e-02,
        7.5702e-02, 4.0360e-02, 4.5365e-02, 1.8520e-01, 1.5203e-01, 6.9757e-02,
        9.4469e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,817][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0110, 0.0993, 0.0791, 0.0627, 0.0608, 0.1020, 0.1081, 0.0505, 0.0703,
        0.1005, 0.0734, 0.0758, 0.1065], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,821][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0075, 0.1226, 0.0316, 0.0336, 0.0664, 0.0627, 0.1210, 0.1813, 0.1019,
        0.0870, 0.0776, 0.0487, 0.0580], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,824][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ a] are: tensor([2.2186e-05, 3.8433e-02, 2.8595e-02, 4.1906e-02, 5.2818e-02, 6.2552e-02,
        2.7428e-01, 4.8653e-02, 6.9517e-02, 6.6292e-02, 1.5353e-01, 8.1367e-02,
        8.2036e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,826][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0013, 0.0806, 0.1114, 0.0305, 0.0337, 0.2148, 0.0259, 0.0144, 0.0860,
        0.1101, 0.0428, 0.1563, 0.0922], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,827][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0006, 0.0677, 0.1396, 0.1948, 0.0806, 0.0411, 0.0456, 0.0355, 0.0863,
        0.0481, 0.0553, 0.0935, 0.1112], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:47,828][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0048, 0.0399, 0.0875, 0.0627, 0.0310, 0.1134, 0.1133, 0.0296, 0.0907,
        0.1255, 0.0476, 0.1192, 0.1115, 0.0232], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,828][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0008, 0.0082, 0.0524, 0.0344, 0.1323, 0.1499, 0.1975, 0.0735, 0.1104,
        0.0451, 0.0523, 0.0647, 0.0623, 0.0163], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,831][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0080, 0.0296, 0.0626, 0.0650, 0.0801, 0.0802, 0.0733, 0.0261, 0.0913,
        0.1065, 0.0675, 0.1748, 0.0936, 0.0415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,835][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0076, 0.0919, 0.1253, 0.0435, 0.0442, 0.1237, 0.0148, 0.0032, 0.0376,
        0.0300, 0.0136, 0.3167, 0.0964, 0.0513], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,839][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0182, 0.0658, 0.0777, 0.0487, 0.0500, 0.0599, 0.0969, 0.1018, 0.0524,
        0.0749, 0.0731, 0.1562, 0.0617, 0.0629], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,844][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.0060, 0.0725, 0.0683, 0.0372, 0.0924, 0.1305, 0.1116, 0.0735, 0.0798,
        0.0771, 0.0647, 0.0758, 0.0698, 0.0408], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,847][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([7.2915e-05, 5.3654e-02, 5.4560e-02, 5.9397e-02, 1.0347e-01, 5.6780e-02,
        2.8189e-02, 2.6224e-02, 2.7795e-02, 1.1567e-01, 2.0097e-01, 8.9065e-02,
        1.5678e-01, 2.7370e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,852][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0085, 0.0889, 0.0748, 0.0597, 0.0552, 0.0937, 0.0991, 0.0467, 0.0608,
        0.0933, 0.0678, 0.0686, 0.1003, 0.0827], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,856][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0036, 0.1519, 0.0425, 0.0434, 0.0715, 0.0465, 0.1382, 0.0988, 0.0626,
        0.1008, 0.0730, 0.0546, 0.0706, 0.0419], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,858][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([3.1542e-05, 2.2785e-02, 2.5437e-02, 3.4717e-02, 3.3046e-02, 3.3316e-02,
        9.3273e-02, 5.4798e-02, 5.7977e-02, 5.7095e-02, 2.6732e-01, 1.5706e-01,
        1.2427e-01, 3.8868e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,859][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0029, 0.0828, 0.1009, 0.0240, 0.0195, 0.2239, 0.0306, 0.0090, 0.0639,
        0.1089, 0.0409, 0.1131, 0.1279, 0.0518], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,860][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0005, 0.0587, 0.1152, 0.1652, 0.1008, 0.0649, 0.0370, 0.0258, 0.0625,
        0.0511, 0.0577, 0.0719, 0.1336, 0.0552], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:47,861][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0097, 0.0228, 0.0741, 0.0286, 0.0288, 0.1122, 0.2154, 0.0339, 0.0664,
        0.0880, 0.0354, 0.1375, 0.0836, 0.0357, 0.0279], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,863][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0208, 0.0123, 0.0499, 0.0366, 0.0986, 0.0732, 0.3097, 0.0649, 0.0947,
        0.0300, 0.0339, 0.0553, 0.0508, 0.0400, 0.0294], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,867][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0262, 0.0190, 0.0523, 0.0309, 0.0653, 0.0788, 0.1208, 0.0284, 0.1745,
        0.0713, 0.0393, 0.1809, 0.0742, 0.0281, 0.0100], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,872][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0159, 0.1347, 0.1742, 0.0406, 0.0294, 0.0953, 0.0147, 0.0025, 0.0312,
        0.0202, 0.0094, 0.2174, 0.1023, 0.0956, 0.0167], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,876][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0218, 0.0498, 0.0589, 0.0441, 0.0485, 0.0617, 0.0934, 0.0772, 0.0572,
        0.0666, 0.0833, 0.1629, 0.0504, 0.0546, 0.0697], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,881][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.0135, 0.0635, 0.0605, 0.0347, 0.0839, 0.1184, 0.0945, 0.0568, 0.0795,
        0.0712, 0.0660, 0.0667, 0.0680, 0.0431, 0.0796], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,885][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0005, 0.0387, 0.0313, 0.0538, 0.0740, 0.0684, 0.1509, 0.0303, 0.0394,
        0.1362, 0.1439, 0.0857, 0.1063, 0.0225, 0.0180], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,890][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0096, 0.0894, 0.0715, 0.0543, 0.0548, 0.0871, 0.0900, 0.0434, 0.0604,
        0.0855, 0.0613, 0.0621, 0.0878, 0.0732, 0.0695], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,891][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0106, 0.1349, 0.0456, 0.0276, 0.0756, 0.0527, 0.1544, 0.1161, 0.0694,
        0.0870, 0.0478, 0.0434, 0.0622, 0.0433, 0.0296], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,892][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ different] are: tensor([4.4660e-05, 3.1870e-02, 3.1171e-02, 3.8437e-02, 4.1052e-02, 5.6244e-02,
        2.1620e-01, 3.3368e-02, 6.3313e-02, 4.8644e-02, 1.2422e-01, 1.0345e-01,
        7.9143e-02, 5.6759e-02, 7.6083e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,893][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0054, 0.0522, 0.1583, 0.0267, 0.0179, 0.1590, 0.0445, 0.0071, 0.0849,
        0.0668, 0.0240, 0.1323, 0.1228, 0.0627, 0.0356], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,895][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0035, 0.0700, 0.0773, 0.1755, 0.0794, 0.0458, 0.0424, 0.0234, 0.0812,
        0.0477, 0.0385, 0.0822, 0.1313, 0.0693, 0.0324], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:47,898][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0046, 0.0377, 0.0628, 0.0227, 0.0139, 0.1020, 0.1031, 0.0306, 0.0834,
        0.0726, 0.0444, 0.0824, 0.0819, 0.0386, 0.0475, 0.1718],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,902][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0016, 0.0144, 0.0491, 0.0324, 0.1235, 0.0818, 0.2265, 0.0716, 0.1097,
        0.0343, 0.0382, 0.0584, 0.0493, 0.0160, 0.0449, 0.0484],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,907][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0111, 0.0308, 0.0429, 0.0456, 0.0733, 0.1079, 0.1145, 0.0393, 0.1022,
        0.0764, 0.0541, 0.1042, 0.0671, 0.0279, 0.0131, 0.0896],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,911][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0296, 0.2137, 0.1146, 0.0350, 0.0378, 0.0954, 0.0064, 0.0033, 0.0194,
        0.0167, 0.0108, 0.0870, 0.0389, 0.0513, 0.0222, 0.2179],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,916][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0283, 0.0447, 0.0623, 0.0430, 0.0472, 0.0563, 0.0690, 0.0746, 0.0562,
        0.0625, 0.0877, 0.1451, 0.0583, 0.0554, 0.0526, 0.0568],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,921][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0067, 0.0492, 0.0481, 0.0285, 0.0796, 0.1023, 0.0949, 0.0628, 0.0784,
        0.0679, 0.0635, 0.0704, 0.0629, 0.0397, 0.0645, 0.0806],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,922][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0002, 0.0384, 0.0505, 0.0548, 0.0839, 0.1136, 0.1055, 0.0517, 0.0357,
        0.0800, 0.0882, 0.0761, 0.0870, 0.0235, 0.0211, 0.0897],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,923][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.0089, 0.0891, 0.0651, 0.0493, 0.0512, 0.0823, 0.0813, 0.0395, 0.0533,
        0.0759, 0.0587, 0.0580, 0.0807, 0.0704, 0.0654, 0.0709],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,924][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0074, 0.1456, 0.0511, 0.0449, 0.0764, 0.0440, 0.0953, 0.1100, 0.0427,
        0.0831, 0.0753, 0.0399, 0.0566, 0.0289, 0.0257, 0.0731],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,925][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ from] are: tensor([4.3961e-05, 2.5687e-02, 4.0989e-02, 5.2487e-02, 5.3784e-02, 5.6514e-02,
        1.6178e-01, 3.0002e-02, 5.7496e-02, 4.0524e-02, 9.8333e-02, 1.0808e-01,
        7.8059e-02, 3.7305e-02, 7.4135e-02, 8.4775e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,928][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0279, 0.1045, 0.0743, 0.0269, 0.0129, 0.1285, 0.0125, 0.0053, 0.0323,
        0.0581, 0.0234, 0.0395, 0.0535, 0.0340, 0.0263, 0.3399],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,932][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0032, 0.0669, 0.1038, 0.1332, 0.0747, 0.0374, 0.0384, 0.0237, 0.0917,
        0.0290, 0.0331, 0.0706, 0.1102, 0.0585, 0.0368, 0.0888],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:47,936][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0073, 0.0374, 0.0584, 0.0211, 0.0111, 0.0787, 0.0831, 0.0195, 0.0651,
        0.0518, 0.0236, 0.0757, 0.0665, 0.0275, 0.0222, 0.1706, 0.1803],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,941][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0003, 0.0137, 0.0376, 0.0287, 0.0875, 0.1530, 0.2029, 0.0595, 0.0855,
        0.0441, 0.0589, 0.0431, 0.0391, 0.0139, 0.0387, 0.0414, 0.0521],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,946][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0060, 0.0372, 0.0435, 0.0532, 0.0546, 0.0888, 0.0665, 0.0430, 0.1195,
        0.0552, 0.0332, 0.1193, 0.0623, 0.0197, 0.0071, 0.0833, 0.1077],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,950][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0119, 0.2038, 0.1587, 0.0312, 0.0306, 0.1115, 0.0054, 0.0020, 0.0175,
        0.0097, 0.0045, 0.0765, 0.0295, 0.0277, 0.0080, 0.1700, 0.1014],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,954][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0108, 0.0397, 0.0634, 0.0401, 0.0401, 0.0570, 0.0768, 0.0539, 0.0487,
        0.0584, 0.0733, 0.1574, 0.0453, 0.0539, 0.0513, 0.0646, 0.0656],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,955][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0049, 0.0513, 0.0445, 0.0265, 0.0648, 0.0913, 0.0858, 0.0557, 0.0788,
        0.0548, 0.0486, 0.0601, 0.0580, 0.0315, 0.0517, 0.0748, 0.1171],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,956][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([3.8600e-05, 5.6441e-02, 2.8947e-02, 5.7916e-02, 7.0863e-02, 1.2673e-01,
        7.3770e-02, 3.8134e-02, 2.6045e-02, 1.2099e-01, 1.1994e-01, 4.8815e-02,
        7.0003e-02, 3.0116e-02, 2.3162e-02, 6.7468e-02, 4.0626e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,957][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0080, 0.0853, 0.0594, 0.0466, 0.0478, 0.0809, 0.0832, 0.0363, 0.0479,
        0.0691, 0.0494, 0.0541, 0.0746, 0.0659, 0.0553, 0.0634, 0.0729],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,960][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0038, 0.1301, 0.0343, 0.0288, 0.0623, 0.0452, 0.0969, 0.1363, 0.0608,
        0.0712, 0.0730, 0.0351, 0.0427, 0.0301, 0.0241, 0.0704, 0.0547],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,962][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.2564e-05, 3.8946e-02, 1.7827e-02, 3.6350e-02, 4.4781e-02, 4.3696e-02,
        1.4187e-01, 2.2527e-02, 4.6116e-02, 4.8171e-02, 1.3050e-01, 7.3727e-02,
        7.3195e-02, 5.1118e-02, 9.8465e-02, 7.8679e-02, 5.4025e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,967][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0053, 0.0791, 0.0895, 0.0196, 0.0111, 0.1245, 0.0073, 0.0025, 0.0233,
        0.0437, 0.0131, 0.0339, 0.0381, 0.0178, 0.0093, 0.2656, 0.2164],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,971][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0005, 0.0580, 0.1128, 0.1205, 0.0565, 0.0322, 0.0259, 0.0163, 0.0763,
        0.0300, 0.0364, 0.0721, 0.1037, 0.0550, 0.0550, 0.1081, 0.0407],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:47,976][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0040, 0.0243, 0.0501, 0.0377, 0.0173, 0.0704, 0.0619, 0.0314, 0.0803,
        0.0666, 0.0297, 0.0734, 0.0592, 0.0180, 0.0174, 0.1412, 0.1871, 0.0299],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,981][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0004, 0.0190, 0.0404, 0.0376, 0.0708, 0.1082, 0.1258, 0.0789, 0.0664,
        0.0638, 0.0611, 0.0703, 0.0568, 0.0275, 0.0492, 0.0447, 0.0583, 0.0209],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,985][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0178, 0.0290, 0.0338, 0.0470, 0.0593, 0.0857, 0.0763, 0.0317, 0.0830,
        0.0719, 0.0574, 0.0927, 0.0641, 0.0291, 0.0100, 0.0798, 0.1017, 0.0295],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,987][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0217, 0.0602, 0.0758, 0.0562, 0.0379, 0.0650, 0.0068, 0.0024, 0.0203,
        0.0192, 0.0110, 0.1183, 0.0496, 0.0350, 0.0170, 0.1995, 0.1538, 0.0501],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,988][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0236, 0.0638, 0.0403, 0.0311, 0.0350, 0.0457, 0.0711, 0.0523, 0.0393,
        0.0519, 0.0710, 0.1001, 0.0439, 0.0407, 0.0463, 0.0570, 0.0726, 0.1143],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,989][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0049, 0.0650, 0.0481, 0.0304, 0.0797, 0.0945, 0.0713, 0.0488, 0.0531,
        0.0552, 0.0510, 0.0517, 0.0568, 0.0281, 0.0503, 0.0633, 0.0895, 0.0583],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,989][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ language] are: tensor([2.8222e-05, 6.1092e-02, 4.6242e-02, 7.1634e-02, 7.0908e-02, 7.1642e-02,
        5.6123e-02, 2.5003e-02, 3.2062e-02, 1.0905e-01, 1.1709e-01, 6.9032e-02,
        8.1178e-02, 2.9116e-02, 3.0486e-02, 6.1051e-02, 4.4142e-02, 2.4120e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,992][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0089, 0.0646, 0.0536, 0.0470, 0.0452, 0.0711, 0.0761, 0.0408, 0.0463,
        0.0664, 0.0528, 0.0565, 0.0709, 0.0602, 0.0516, 0.0662, 0.0768, 0.0451],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,996][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0024, 0.1057, 0.0243, 0.0320, 0.0524, 0.0439, 0.1005, 0.0884, 0.0441,
        0.0700, 0.0634, 0.0439, 0.0469, 0.0407, 0.0261, 0.0582, 0.0698, 0.0873],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:47,999][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ language] are: tensor([7.8409e-06, 5.9238e-02, 2.5413e-02, 3.3346e-02, 3.9863e-02, 4.5286e-02,
        7.7910e-02, 1.8845e-02, 3.3095e-02, 8.6429e-02, 1.3321e-01, 6.2473e-02,
        1.0284e-01, 4.9884e-02, 9.7850e-02, 5.3961e-02, 5.5565e-02, 2.4789e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,003][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0055, 0.0829, 0.0542, 0.0314, 0.0152, 0.1159, 0.0083, 0.0056, 0.0250,
        0.0556, 0.0299, 0.0447, 0.0561, 0.0232, 0.0139, 0.2012, 0.1654, 0.0662],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,008][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0004, 0.0435, 0.1048, 0.1031, 0.0683, 0.0379, 0.0282, 0.0256, 0.0658,
        0.0488, 0.0352, 0.0727, 0.1001, 0.0526, 0.0749, 0.0776, 0.0365, 0.0239],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,013][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0037, 0.0195, 0.0397, 0.0203, 0.0062, 0.0305, 0.0258, 0.0108, 0.0502,
        0.0446, 0.0148, 0.0585, 0.0578, 0.0298, 0.0180, 0.1488, 0.1894, 0.0256,
        0.2059], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,017][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0005, 0.0120, 0.0513, 0.0264, 0.1111, 0.0912, 0.1560, 0.0699, 0.0761,
        0.0408, 0.0542, 0.0411, 0.0400, 0.0140, 0.0425, 0.0484, 0.0629, 0.0153,
        0.0463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,019][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0384, 0.0199, 0.0294, 0.0415, 0.0546, 0.0508, 0.0427, 0.0193, 0.1214,
        0.0716, 0.0308, 0.1056, 0.0594, 0.0213, 0.0065, 0.0721, 0.1135, 0.0267,
        0.0744], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,020][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ used] are: tensor([2.3334e-02, 6.4348e-02, 2.9747e-02, 9.7404e-03, 6.9978e-03, 1.9496e-02,
        5.0205e-04, 2.2524e-04, 2.9152e-03, 1.8368e-03, 7.9223e-04, 1.5553e-02,
        5.7096e-03, 6.0412e-03, 1.1480e-03, 4.1127e-02, 1.9833e-02, 4.7020e-02,
        7.0363e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,021][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0240, 0.0285, 0.0490, 0.0377, 0.0262, 0.0466, 0.0586, 0.0570, 0.0323,
        0.0510, 0.0535, 0.1247, 0.0605, 0.0463, 0.0624, 0.0606, 0.0960, 0.0426,
        0.0425], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,022][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.0031, 0.0354, 0.0290, 0.0163, 0.0678, 0.0735, 0.0678, 0.0485, 0.0663,
        0.0506, 0.0458, 0.0546, 0.0526, 0.0318, 0.0501, 0.0637, 0.1080, 0.0395,
        0.0953], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,025][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0001, 0.0481, 0.0468, 0.0592, 0.0872, 0.0814, 0.0413, 0.0278, 0.0376,
        0.0913, 0.0923, 0.0623, 0.0969, 0.0270, 0.0189, 0.0634, 0.0634, 0.0253,
        0.0298], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,029][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0056, 0.0724, 0.0574, 0.0450, 0.0384, 0.0647, 0.0635, 0.0277, 0.0413,
        0.0667, 0.0437, 0.0486, 0.0707, 0.0567, 0.0491, 0.0600, 0.0737, 0.0462,
        0.0686], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,034][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0029, 0.1244, 0.0292, 0.0314, 0.0359, 0.0334, 0.0638, 0.0998, 0.0432,
        0.0654, 0.0579, 0.0360, 0.0499, 0.0268, 0.0198, 0.0572, 0.0933, 0.0941,
        0.0356], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,037][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ used] are: tensor([4.0158e-06, 2.1234e-02, 3.5143e-02, 4.6059e-02, 3.1800e-02, 4.0048e-02,
        1.5481e-01, 2.3783e-02, 4.5863e-02, 3.3421e-02, 5.9152e-02, 9.9695e-02,
        1.0004e-01, 4.1071e-02, 7.9277e-02, 7.4142e-02, 7.6724e-02, 9.0114e-03,
        2.8717e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,040][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ used] are: tensor([2.1663e-02, 2.9173e-02, 1.8930e-02, 4.1341e-03, 1.5850e-03, 2.2665e-02,
        6.4968e-04, 3.1322e-04, 5.6171e-03, 9.8269e-03, 2.7002e-03, 7.0198e-03,
        7.4466e-03, 4.9205e-03, 1.3781e-03, 7.0868e-02, 4.1372e-02, 2.9069e-02,
        7.2067e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,045][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0008, 0.0405, 0.1086, 0.1451, 0.0790, 0.0254, 0.0334, 0.0145, 0.0897,
        0.0457, 0.0472, 0.0452, 0.0716, 0.0241, 0.0313, 0.0826, 0.0283, 0.0151,
        0.0720], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,049][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0034, 0.0142, 0.0240, 0.0132, 0.0038, 0.0481, 0.0468, 0.0229, 0.0536,
        0.0445, 0.0218, 0.0565, 0.0537, 0.0290, 0.0180, 0.1067, 0.0899, 0.0220,
        0.2015, 0.1264], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,051][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0003, 0.0108, 0.0528, 0.0268, 0.1123, 0.0891, 0.2298, 0.0584, 0.0670,
        0.0298, 0.0447, 0.0364, 0.0336, 0.0099, 0.0448, 0.0254, 0.0370, 0.0145,
        0.0476, 0.0290], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,052][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0122, 0.0166, 0.0231, 0.0275, 0.0441, 0.0546, 0.0555, 0.0349, 0.0870,
        0.0535, 0.0301, 0.1287, 0.0479, 0.0153, 0.0044, 0.0639, 0.0874, 0.0210,
        0.0561, 0.1361], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,053][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ by] are: tensor([3.3836e-02, 6.4488e-02, 2.6108e-02, 9.1657e-03, 7.1839e-03, 1.9696e-02,
        5.0365e-04, 2.6177e-04, 2.3952e-03, 2.1808e-03, 1.6327e-03, 9.1792e-03,
        4.1646e-03, 3.5716e-03, 1.4682e-03, 3.1955e-02, 1.5528e-02, 2.8852e-02,
        3.3506e-01, 4.0277e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,054][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0109, 0.0371, 0.0389, 0.0286, 0.0341, 0.0483, 0.0628, 0.0726, 0.0394,
        0.0520, 0.0927, 0.1148, 0.0475, 0.0368, 0.0531, 0.0466, 0.0769, 0.0494,
        0.0334, 0.0242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,057][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.0023, 0.0423, 0.0381, 0.0212, 0.0718, 0.0693, 0.0662, 0.0524, 0.0606,
        0.0483, 0.0466, 0.0573, 0.0501, 0.0255, 0.0470, 0.0545, 0.0892, 0.0350,
        0.0808, 0.0416], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,060][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ by] are: tensor([3.5396e-05, 3.3708e-02, 3.3600e-02, 5.7501e-02, 7.4364e-02, 9.9848e-02,
        6.8860e-02, 3.6522e-02, 2.4267e-02, 8.5945e-02, 8.9315e-02, 5.3838e-02,
        7.7414e-02, 2.4424e-02, 2.2574e-02, 6.4365e-02, 5.1885e-02, 2.3389e-02,
        3.1558e-02, 4.6588e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,064][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0064, 0.0706, 0.0486, 0.0383, 0.0395, 0.0641, 0.0695, 0.0329, 0.0370,
        0.0633, 0.0491, 0.0464, 0.0647, 0.0539, 0.0458, 0.0530, 0.0617, 0.0422,
        0.0597, 0.0534], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,069][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0019, 0.0754, 0.0227, 0.0209, 0.0444, 0.0382, 0.0833, 0.1182, 0.0560,
        0.0548, 0.0693, 0.0335, 0.0402, 0.0310, 0.0200, 0.0607, 0.0514, 0.0848,
        0.0486, 0.0446], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,072][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ by] are: tensor([4.0544e-06, 1.5248e-02, 3.7166e-02, 3.9354e-02, 3.4391e-02, 5.1691e-02,
        1.4141e-01, 2.0261e-02, 2.5868e-02, 3.5592e-02, 1.0450e-01, 9.0405e-02,
        1.0155e-01, 3.0951e-02, 7.8289e-02, 6.0764e-02, 5.8394e-02, 1.0606e-02,
        2.7690e-02, 3.5862e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,075][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ by] are: tensor([1.2441e-02, 3.1544e-02, 1.6618e-02, 5.5643e-03, 1.6078e-03, 2.4002e-02,
        6.7899e-04, 3.3751e-04, 3.7504e-03, 1.2647e-02, 3.1048e-03, 5.0793e-03,
        7.9495e-03, 3.1784e-03, 1.1164e-03, 4.4977e-02, 3.8564e-02, 1.9692e-02,
        2.8968e-01, 4.7746e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,079][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0005, 0.0402, 0.0679, 0.0836, 0.0698, 0.0230, 0.0444, 0.0318, 0.0893,
        0.0321, 0.0496, 0.0524, 0.0746, 0.0309, 0.0389, 0.0674, 0.0304, 0.0204,
        0.0968, 0.0562], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,083][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0067, 0.0265, 0.0374, 0.0164, 0.0052, 0.0433, 0.0324, 0.0094, 0.0363,
        0.0246, 0.0123, 0.0385, 0.0347, 0.0127, 0.0080, 0.0743, 0.0683, 0.0227,
        0.1298, 0.0739, 0.2867], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,084][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0004, 0.0122, 0.0534, 0.0314, 0.1069, 0.1260, 0.1756, 0.0452, 0.0632,
        0.0296, 0.0340, 0.0307, 0.0335, 0.0097, 0.0289, 0.0247, 0.0445, 0.0141,
        0.0352, 0.0259, 0.0747], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,085][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0108, 0.0245, 0.0282, 0.0394, 0.0328, 0.0514, 0.0345, 0.0204, 0.0691,
        0.0309, 0.0166, 0.0684, 0.0357, 0.0100, 0.0022, 0.0361, 0.0494, 0.0174,
        0.0320, 0.1070, 0.2832], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,086][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.0981e-02, 4.2345e-02, 1.7243e-02, 4.5889e-03, 3.1582e-03, 8.7125e-03,
        1.2890e-04, 3.4336e-05, 6.2634e-04, 3.0848e-04, 1.1124e-04, 2.1938e-03,
        1.0362e-03, 8.2971e-04, 1.5175e-04, 9.6555e-03, 4.9326e-03, 1.3276e-02,
        2.5243e-01, 2.8047e-01, 3.4679e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,089][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0127, 0.0287, 0.0518, 0.0345, 0.0351, 0.0470, 0.0641, 0.0460, 0.0403,
        0.0454, 0.0600, 0.1462, 0.0376, 0.0412, 0.0387, 0.0498, 0.0556, 0.0383,
        0.0459, 0.0311, 0.0499], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,093][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0037, 0.0423, 0.0326, 0.0202, 0.0592, 0.0702, 0.0604, 0.0380, 0.0530,
        0.0388, 0.0325, 0.0444, 0.0436, 0.0221, 0.0330, 0.0525, 0.0883, 0.0368,
        0.0687, 0.0356, 0.1242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,096][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([2.8052e-05, 6.2812e-02, 2.9915e-02, 6.6237e-02, 7.6698e-02, 1.0163e-01,
        5.1372e-02, 2.5733e-02, 1.8486e-02, 1.0118e-01, 1.0015e-01, 3.6820e-02,
        5.7963e-02, 2.2945e-02, 1.2446e-02, 4.8175e-02, 2.8844e-02, 2.9749e-02,
        1.6969e-02, 2.7017e-02, 8.4835e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,101][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0091, 0.0719, 0.0471, 0.0368, 0.0367, 0.0637, 0.0633, 0.0275, 0.0363,
        0.0524, 0.0356, 0.0395, 0.0589, 0.0489, 0.0368, 0.0475, 0.0540, 0.0416,
        0.0572, 0.0530, 0.0824], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,106][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0040, 0.1011, 0.0228, 0.0183, 0.0442, 0.0396, 0.0690, 0.1147, 0.0516,
        0.0608, 0.0534, 0.0272, 0.0379, 0.0228, 0.0164, 0.0534, 0.0421, 0.0696,
        0.0310, 0.0372, 0.0827], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,109][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.1467e-05, 2.8344e-02, 1.6108e-02, 5.4146e-02, 4.6883e-02, 5.9218e-02,
        9.6103e-02, 1.3473e-02, 3.0726e-02, 4.4732e-02, 1.1417e-01, 5.7036e-02,
        7.1134e-02, 3.0878e-02, 6.4720e-02, 6.9959e-02, 4.6062e-02, 1.7968e-02,
        8.0500e-03, 2.8586e-02, 1.0170e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,111][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([6.0519e-03, 1.1319e-02, 8.2084e-03, 1.9238e-03, 8.1724e-04, 8.9841e-03,
        1.7783e-04, 6.6208e-05, 7.7782e-04, 2.5094e-03, 4.7939e-04, 1.0125e-03,
        1.9790e-03, 5.4980e-04, 2.4350e-04, 2.0969e-02, 1.2116e-02, 7.3758e-03,
        1.3260e-01, 2.0430e-01, 5.7754e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,115][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0008, 0.0389, 0.1039, 0.0897, 0.0519, 0.0217, 0.0184, 0.0143, 0.0740,
        0.0207, 0.0245, 0.0592, 0.0777, 0.0275, 0.0254, 0.0682, 0.0274, 0.0175,
        0.1004, 0.0695, 0.0685], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,225][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:48,226][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,228][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,230][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,234][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,237][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,241][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,245][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,248][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,253][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,256][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,256][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,257][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:48,258][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7236, 0.2764], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,259][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.2079, 0.7921], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,261][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.9279, 0.0721], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,264][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.9225, 0.0775], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,268][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.4591, 0.5409], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,272][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.5683, 0.4317], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,277][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0118, 0.9882], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,282][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.6328, 0.3672], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,286][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.5456, 0.4544], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,288][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0023, 0.9977], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,289][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.6235, 0.3765], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,290][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0427, 0.9573], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:48,290][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.2083, 0.2076, 0.5841], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,291][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0090, 0.2522, 0.7388], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,294][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.4323, 0.2857, 0.2820], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,299][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.1694, 0.4927, 0.3380], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,304][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0401, 0.6475, 0.3124], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,308][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.1487, 0.2752, 0.5761], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,313][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0020, 0.6558, 0.3422], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,317][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0436, 0.4064, 0.5500], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,320][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.2001, 0.6495, 0.1505], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,321][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([4.1010e-04, 6.2766e-01, 3.7193e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,321][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0882, 0.4141, 0.4976], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,322][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0031, 0.3996, 0.5973], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:48,323][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.1150, 0.1350, 0.3827, 0.3673], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,326][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0043, 0.1230, 0.6447, 0.2281], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,329][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.1032, 0.2081, 0.3378, 0.3508], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,334][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.2096, 0.2430, 0.3741, 0.1733], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,338][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0878, 0.3687, 0.2766, 0.2670], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,343][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0597, 0.1277, 0.5710, 0.2415], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,348][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0005, 0.3071, 0.2339, 0.4586], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,352][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0278, 0.3434, 0.4585, 0.1704], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,353][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.1883, 0.6374, 0.1270, 0.0473], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,354][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([8.9213e-05, 3.9037e-01, 3.3498e-01, 2.7456e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,354][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0382, 0.2566, 0.4772, 0.2280], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,355][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0016, 0.2167, 0.3245, 0.4572], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:48,358][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.2325, 0.1250, 0.3695, 0.1546, 0.1184], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,362][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0152, 0.0778, 0.3038, 0.1653, 0.4379], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,366][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.2910, 0.0847, 0.2618, 0.0992, 0.2633], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,371][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.0989, 0.1918, 0.3185, 0.2389, 0.1519], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,376][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.0680, 0.1513, 0.0804, 0.0864, 0.6139], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,380][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.0995, 0.0403, 0.1151, 0.0674, 0.6777], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,385][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.0013, 0.1199, 0.1622, 0.3413, 0.3752], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,386][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0664, 0.2426, 0.3770, 0.2476, 0.0665], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,386][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.3898, 0.2685, 0.0607, 0.0426, 0.2384], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,387][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([2.1514e-04, 2.0209e-01, 2.0720e-01, 3.0603e-01, 2.8446e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,388][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.1152, 0.1322, 0.3847, 0.1600, 0.2080], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,391][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.0045, 0.0810, 0.1393, 0.2556, 0.5195], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:48,394][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.1507, 0.1289, 0.2545, 0.1681, 0.0755, 0.2223], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,399][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0290, 0.0717, 0.2122, 0.1551, 0.3346, 0.1974], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,404][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.2209, 0.0982, 0.1185, 0.0798, 0.2122, 0.2705], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,409][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.2159, 0.2362, 0.1983, 0.1433, 0.0803, 0.1260], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,413][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.1118, 0.0954, 0.0445, 0.0596, 0.3261, 0.3627], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,418][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.1648, 0.0461, 0.0867, 0.0418, 0.2670, 0.3937], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,418][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0008, 0.1714, 0.1377, 0.2886, 0.2763, 0.1253], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,419][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.1753, 0.2614, 0.2079, 0.1244, 0.0453, 0.1858], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,420][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.5265, 0.1756, 0.0315, 0.0189, 0.1431, 0.1044], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,422][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0011, 0.2030, 0.1407, 0.2164, 0.2100, 0.2289], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,425][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.0912, 0.2061, 0.2555, 0.1042, 0.0633, 0.2797], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,430][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.0121, 0.1140, 0.1759, 0.3872, 0.2480, 0.0629], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:48,434][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.0319, 0.1180, 0.2264, 0.0893, 0.0375, 0.1049, 0.3920],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,439][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0019, 0.0193, 0.0818, 0.0638, 0.1525, 0.2265, 0.4542],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,444][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.0689, 0.0966, 0.2897, 0.1025, 0.1179, 0.1581, 0.1662],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,448][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.0444, 0.1718, 0.3188, 0.1031, 0.0893, 0.2545, 0.0182],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,450][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.0252, 0.0537, 0.0283, 0.0263, 0.2572, 0.2638, 0.3455],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,451][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.1245, 0.0247, 0.0565, 0.0195, 0.1746, 0.4071, 0.1931],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,452][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([3.6980e-04, 9.2170e-02, 7.8564e-02, 1.1757e-01, 1.0860e-01, 2.0727e-01,
        3.9545e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,453][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0344, 0.1229, 0.1953, 0.0977, 0.0249, 0.2527, 0.2720],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,455][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.2517, 0.1449, 0.0392, 0.0097, 0.1382, 0.1231, 0.2932],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,457][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([1.3210e-04, 1.5745e-01, 1.0161e-01, 4.1972e-02, 5.1867e-02, 8.3110e-02,
        5.6386e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,461][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.0074, 0.0936, 0.2531, 0.0725, 0.0774, 0.4507, 0.0452],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,466][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.0011, 0.1042, 0.2143, 0.3133, 0.2092, 0.0654, 0.0925],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:48,471][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.0098, 0.0701, 0.1507, 0.0942, 0.0371, 0.1385, 0.3281, 0.1716],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,475][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0010, 0.0248, 0.0785, 0.0475, 0.1491, 0.2138, 0.3644, 0.1210],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,480][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.0530, 0.0705, 0.1353, 0.0740, 0.1313, 0.2466, 0.1594, 0.1299],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,483][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.0139, 0.1345, 0.2909, 0.0804, 0.0569, 0.3354, 0.0790, 0.0091],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,483][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0029, 0.0321, 0.0134, 0.0257, 0.0794, 0.2083, 0.2160, 0.4222],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,484][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.0631, 0.0380, 0.0759, 0.0245, 0.2288, 0.3481, 0.1708, 0.0507],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,485][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([3.9260e-05, 8.1019e-02, 1.0844e-01, 1.1622e-01, 1.8851e-01, 2.6015e-01,
        1.9361e-01, 5.2007e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,487][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0048, 0.0674, 0.2112, 0.0663, 0.0372, 0.2526, 0.3357, 0.0248],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,490][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.1301, 0.1947, 0.0270, 0.0203, 0.1595, 0.0837, 0.2030, 0.1818],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,493][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([6.6689e-06, 1.2474e-01, 9.5982e-02, 5.0889e-02, 7.9772e-02, 1.6130e-01,
        4.3387e-01, 5.3440e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,498][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.0006, 0.0601, 0.2994, 0.0386, 0.0563, 0.4194, 0.1083, 0.0172],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,501][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([2.1337e-04, 9.5513e-02, 1.7425e-01, 2.5815e-01, 1.8966e-01, 9.0971e-02,
        1.2507e-01, 6.6165e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:48,505][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0174, 0.0303, 0.0502, 0.0284, 0.0294, 0.1372, 0.1832, 0.1337, 0.3903],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,510][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0020, 0.0124, 0.0687, 0.0456, 0.1488, 0.1328, 0.2972, 0.1204, 0.1720],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,515][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.0423, 0.0209, 0.0220, 0.0207, 0.0489, 0.0996, 0.0499, 0.1322, 0.5636],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,516][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0192, 0.1966, 0.2658, 0.0830, 0.0794, 0.2431, 0.0408, 0.0048, 0.0672],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,516][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0012, 0.0144, 0.0092, 0.0132, 0.0709, 0.1137, 0.1495, 0.3694, 0.2586],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,517][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.0333, 0.0200, 0.0303, 0.0139, 0.1300, 0.2401, 0.1208, 0.0594, 0.3520],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,518][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([9.1652e-05, 3.5976e-02, 6.6945e-02, 9.5398e-02, 2.3544e-01, 2.0401e-01,
        1.2531e-01, 1.2328e-01, 1.1355e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,521][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0102, 0.0874, 0.2062, 0.0655, 0.0261, 0.2185, 0.2704, 0.0259, 0.0898],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,525][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.0479, 0.0907, 0.0200, 0.0179, 0.0913, 0.0832, 0.2122, 0.2686, 0.1681],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,528][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([2.2119e-05, 1.8851e-02, 5.4542e-02, 4.3571e-02, 9.0405e-02, 1.1794e-01,
        4.8850e-01, 1.0201e-01, 8.4160e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,532][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.0063, 0.0671, 0.1943, 0.0326, 0.0657, 0.4048, 0.0406, 0.0173, 0.1715],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,537][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0009, 0.0800, 0.1425, 0.1625, 0.1289, 0.0778, 0.0921, 0.1249, 0.1905],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:48,542][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0461, 0.0652, 0.0993, 0.0547, 0.0405, 0.1358, 0.1374, 0.0961, 0.2077,
        0.1171], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,546][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0224, 0.0311, 0.0773, 0.0580, 0.1633, 0.1161, 0.2126, 0.1125, 0.1496,
        0.0572], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,548][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.1099, 0.0615, 0.0655, 0.0724, 0.0972, 0.1265, 0.0801, 0.0798, 0.2230,
        0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,549][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.1002, 0.2263, 0.2238, 0.0635, 0.0718, 0.1748, 0.0232, 0.0064, 0.0684,
        0.0415], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,550][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.0220, 0.0319, 0.0159, 0.0159, 0.0875, 0.1074, 0.1138, 0.3332, 0.1798,
        0.0926], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,550][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.0904, 0.0321, 0.0438, 0.0321, 0.2021, 0.2499, 0.0735, 0.0508, 0.1244,
        0.1008], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,553][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.0038, 0.0515, 0.0730, 0.1014, 0.1416, 0.1227, 0.1493, 0.1116, 0.0787,
        0.1665], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,556][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0390, 0.1618, 0.2037, 0.1048, 0.0371, 0.1415, 0.1487, 0.0188, 0.0762,
        0.0686], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,560][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.2989, 0.1625, 0.0435, 0.0210, 0.0551, 0.0733, 0.0972, 0.0979, 0.0852,
        0.0654], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,564][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0015, 0.0548, 0.0624, 0.0556, 0.0939, 0.0719, 0.3111, 0.1421, 0.1244,
        0.0823], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,569][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.0452, 0.1003, 0.1700, 0.0547, 0.0621, 0.2510, 0.0549, 0.0223, 0.1256,
        0.1137], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,574][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.0095, 0.0896, 0.1743, 0.2061, 0.1374, 0.0463, 0.0775, 0.0535, 0.1430,
        0.0628], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:48,578][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0678, 0.0196, 0.0470, 0.0451, 0.0326, 0.1562, 0.2499, 0.0867, 0.1772,
        0.0742, 0.0438], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,580][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0856, 0.0221, 0.0609, 0.0560, 0.0767, 0.1067, 0.2683, 0.1255, 0.1154,
        0.0386, 0.0442], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,581][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.1093, 0.0111, 0.0231, 0.0283, 0.0967, 0.1121, 0.1391, 0.0460, 0.3382,
        0.0585, 0.0375], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,582][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0406, 0.2090, 0.2562, 0.0715, 0.0625, 0.1557, 0.0373, 0.0053, 0.0717,
        0.0692, 0.0210], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,583][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.0291, 0.0149, 0.0161, 0.0170, 0.0598, 0.1216, 0.1242, 0.2956, 0.2186,
        0.0526, 0.0505], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,585][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.1273, 0.0362, 0.0580, 0.0255, 0.2198, 0.2003, 0.0637, 0.0395, 0.0959,
        0.0913, 0.0426], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,588][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0011, 0.0356, 0.0317, 0.0441, 0.1445, 0.1312, 0.1418, 0.0892, 0.0931,
        0.1698, 0.1179], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,592][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0147, 0.0999, 0.2889, 0.0975, 0.0503, 0.1452, 0.1111, 0.0174, 0.0848,
        0.0723, 0.0177], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,597][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.3860, 0.1059, 0.0218, 0.0187, 0.0591, 0.0768, 0.0898, 0.0777, 0.0775,
        0.0472, 0.0395], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,601][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0008, 0.0425, 0.0429, 0.0342, 0.0819, 0.0970, 0.2910, 0.0814, 0.1221,
        0.0842, 0.1219], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,606][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.0043, 0.0602, 0.1792, 0.0347, 0.0600, 0.2420, 0.0960, 0.0161, 0.1100,
        0.1570, 0.0404], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,611][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.0056, 0.0740, 0.1367, 0.1985, 0.1541, 0.0580, 0.0628, 0.0572, 0.1194,
        0.0730, 0.0606], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:48,612][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0049, 0.0516, 0.0846, 0.0390, 0.0141, 0.1262, 0.0903, 0.0525, 0.1509,
        0.0922, 0.0477, 0.2463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,613][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0005, 0.0108, 0.0470, 0.0466, 0.0846, 0.1174, 0.2138, 0.1301, 0.1223,
        0.0408, 0.0754, 0.1106], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,614][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0126, 0.0315, 0.0335, 0.0461, 0.0580, 0.1118, 0.0998, 0.0620, 0.1649,
        0.0829, 0.0552, 0.2415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,615][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0174, 0.1787, 0.2001, 0.0467, 0.0307, 0.0895, 0.0130, 0.0030, 0.0430,
        0.0290, 0.0088, 0.3401], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,618][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0014, 0.0139, 0.0125, 0.0147, 0.0371, 0.1418, 0.1715, 0.2214, 0.1635,
        0.0494, 0.0446, 0.1281], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,621][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0209, 0.0196, 0.0427, 0.0181, 0.1478, 0.2916, 0.0992, 0.0276, 0.0977,
        0.0765, 0.0347, 0.1237], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,624][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([5.4581e-05, 4.9297e-02, 3.0317e-02, 5.8861e-02, 9.8732e-02, 9.0377e-02,
        7.2342e-02, 3.6157e-02, 5.6029e-02, 2.1479e-01, 1.8540e-01, 1.0764e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,629][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0040, 0.0452, 0.2027, 0.0507, 0.0185, 0.1910, 0.2262, 0.0151, 0.0714,
        0.0592, 0.0198, 0.0963], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,633][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0453, 0.2173, 0.0446, 0.0287, 0.0797, 0.0733, 0.1095, 0.1367, 0.0714,
        0.0683, 0.0637, 0.0615], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,636][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([1.6725e-05, 3.9799e-02, 4.3717e-02, 3.1608e-02, 5.4521e-02, 7.0474e-02,
        2.0550e-01, 4.8557e-02, 7.1614e-02, 7.6100e-02, 1.5702e-01, 2.0107e-01],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,641][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0018, 0.0835, 0.1206, 0.0241, 0.0195, 0.2985, 0.0224, 0.0066, 0.0736,
        0.1240, 0.0348, 0.1906], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,644][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0003, 0.0624, 0.1128, 0.1930, 0.1156, 0.0657, 0.0738, 0.0364, 0.1247,
        0.0586, 0.0538, 0.1029], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:48,645][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0069, 0.0224, 0.0362, 0.0216, 0.0193, 0.1042, 0.1277, 0.0617, 0.1466,
        0.1170, 0.0567, 0.1630, 0.1166], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,646][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0007, 0.0178, 0.0460, 0.0288, 0.1107, 0.0895, 0.2692, 0.1022, 0.1085,
        0.0470, 0.0608, 0.0635, 0.0553], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,647][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0156, 0.0230, 0.0258, 0.0356, 0.0581, 0.0744, 0.0619, 0.0721, 0.2201,
        0.0726, 0.0508, 0.2157, 0.0742], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,649][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0189, 0.1691, 0.2207, 0.0627, 0.0363, 0.1101, 0.0105, 0.0033, 0.0420,
        0.0271, 0.0099, 0.2189, 0.0704], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,652][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0023, 0.0192, 0.0124, 0.0087, 0.0435, 0.0967, 0.1491, 0.1947, 0.1577,
        0.0709, 0.0621, 0.1257, 0.0571], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,657][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0190, 0.0315, 0.0377, 0.0157, 0.1405, 0.2050, 0.0921, 0.0323, 0.1055,
        0.0788, 0.0362, 0.1094, 0.0963], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,659][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([6.0151e-05, 5.2839e-02, 3.1688e-02, 7.1501e-02, 9.5943e-02, 8.5086e-02,
        7.5702e-02, 4.0360e-02, 4.5365e-02, 1.8520e-01, 1.5203e-01, 6.9757e-02,
        9.4469e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,664][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0061, 0.0524, 0.1450, 0.0605, 0.0153, 0.1376, 0.1758, 0.0143, 0.0677,
        0.0484, 0.0150, 0.0824, 0.1796], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,669][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0449, 0.1897, 0.0330, 0.0233, 0.0700, 0.0772, 0.1255, 0.1382, 0.0873,
        0.0687, 0.0551, 0.0437, 0.0434], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,671][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([2.2186e-05, 3.8433e-02, 2.8595e-02, 4.1906e-02, 5.2818e-02, 6.2552e-02,
        2.7428e-01, 4.8653e-02, 6.9517e-02, 6.6292e-02, 1.5353e-01, 8.1367e-02,
        8.2036e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,676][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0013, 0.0806, 0.1114, 0.0305, 0.0337, 0.2148, 0.0259, 0.0144, 0.0860,
        0.1101, 0.0428, 0.1563, 0.0922], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,677][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0006, 0.0677, 0.1396, 0.1948, 0.0806, 0.0411, 0.0456, 0.0355, 0.0863,
        0.0481, 0.0553, 0.0935, 0.1112], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:48,678][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0048, 0.0399, 0.0875, 0.0627, 0.0310, 0.1134, 0.1133, 0.0296, 0.0907,
        0.1255, 0.0476, 0.1192, 0.1115, 0.0232], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,679][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0008, 0.0082, 0.0524, 0.0344, 0.1323, 0.1499, 0.1975, 0.0735, 0.1104,
        0.0451, 0.0523, 0.0647, 0.0623, 0.0163], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,681][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.0080, 0.0296, 0.0626, 0.0650, 0.0801, 0.0802, 0.0733, 0.0261, 0.0913,
        0.1065, 0.0675, 0.1748, 0.0936, 0.0415], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,685][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0172, 0.0940, 0.1475, 0.0566, 0.0276, 0.1144, 0.0133, 0.0021, 0.0403,
        0.0317, 0.0094, 0.2924, 0.1163, 0.0371], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,689][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0012, 0.0346, 0.0184, 0.0112, 0.0601, 0.1567, 0.0852, 0.2323, 0.1040,
        0.0792, 0.0570, 0.0983, 0.0473, 0.0145], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,693][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.0193, 0.0195, 0.0468, 0.0186, 0.1249, 0.2569, 0.1085, 0.0289, 0.0738,
        0.0755, 0.0288, 0.0987, 0.0771, 0.0226], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,696][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([7.2915e-05, 5.3654e-02, 5.4560e-02, 5.9397e-02, 1.0347e-01, 5.6780e-02,
        2.8189e-02, 2.6224e-02, 2.7795e-02, 1.1567e-01, 2.0097e-01, 8.9065e-02,
        1.5678e-01, 2.7370e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,701][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0045, 0.0486, 0.1726, 0.0639, 0.0132, 0.1314, 0.1531, 0.0125, 0.0477,
        0.0471, 0.0139, 0.0609, 0.1722, 0.0585], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,705][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0208, 0.2389, 0.0514, 0.0299, 0.0771, 0.0637, 0.1386, 0.0703, 0.0512,
        0.0730, 0.0430, 0.0494, 0.0502, 0.0426], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,708][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([3.1542e-05, 2.2785e-02, 2.5437e-02, 3.4717e-02, 3.3046e-02, 3.3316e-02,
        9.3273e-02, 5.4798e-02, 5.7977e-02, 5.7095e-02, 2.6732e-01, 1.5706e-01,
        1.2427e-01, 3.8868e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,709][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0029, 0.0828, 0.1009, 0.0240, 0.0195, 0.2239, 0.0306, 0.0090, 0.0639,
        0.1089, 0.0409, 0.1131, 0.1279, 0.0518], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,710][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0005, 0.0587, 0.1152, 0.1652, 0.1008, 0.0649, 0.0370, 0.0258, 0.0625,
        0.0511, 0.0577, 0.0719, 0.1336, 0.0552], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:48,711][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0097, 0.0228, 0.0741, 0.0286, 0.0288, 0.1122, 0.2154, 0.0339, 0.0664,
        0.0880, 0.0354, 0.1375, 0.0836, 0.0357, 0.0279], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,712][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0208, 0.0123, 0.0499, 0.0366, 0.0986, 0.0732, 0.3097, 0.0649, 0.0947,
        0.0300, 0.0339, 0.0553, 0.0508, 0.0400, 0.0294], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,715][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.0262, 0.0190, 0.0523, 0.0309, 0.0653, 0.0788, 0.1208, 0.0284, 0.1745,
        0.0713, 0.0393, 0.1809, 0.0742, 0.0281, 0.0100], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,719][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0384, 0.1299, 0.1864, 0.0393, 0.0184, 0.0903, 0.0133, 0.0018, 0.0368,
        0.0273, 0.0089, 0.2133, 0.1171, 0.0679, 0.0110], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,724][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0058, 0.0227, 0.0100, 0.0058, 0.0467, 0.0789, 0.1336, 0.2702, 0.1232,
        0.0542, 0.0787, 0.0776, 0.0362, 0.0171, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,728][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.0958, 0.0266, 0.0436, 0.0180, 0.1174, 0.2037, 0.0755, 0.0190, 0.0699,
        0.0705, 0.0345, 0.0667, 0.0762, 0.0268, 0.0559], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,733][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0005, 0.0387, 0.0313, 0.0538, 0.0740, 0.0684, 0.1509, 0.0303, 0.0394,
        0.1362, 0.1439, 0.0857, 0.1063, 0.0225, 0.0180], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,738][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0102, 0.0685, 0.1998, 0.0494, 0.0176, 0.1140, 0.1269, 0.0127, 0.0620,
        0.0500, 0.0134, 0.0579, 0.1501, 0.0472, 0.0201], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,742][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0741, 0.1857, 0.0536, 0.0177, 0.0685, 0.0715, 0.1796, 0.0696, 0.0475,
        0.0619, 0.0255, 0.0388, 0.0459, 0.0446, 0.0155], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,744][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([4.4660e-05, 3.1870e-02, 3.1171e-02, 3.8437e-02, 4.1052e-02, 5.6244e-02,
        2.1620e-01, 3.3368e-02, 6.3313e-02, 4.8644e-02, 1.2422e-01, 1.0345e-01,
        7.9143e-02, 5.6759e-02, 7.6083e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,745][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.0054, 0.0522, 0.1583, 0.0267, 0.0179, 0.1590, 0.0445, 0.0071, 0.0849,
        0.0668, 0.0240, 0.1323, 0.1228, 0.0627, 0.0356], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,745][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0035, 0.0700, 0.0773, 0.1755, 0.0794, 0.0458, 0.0424, 0.0234, 0.0812,
        0.0477, 0.0385, 0.0822, 0.1313, 0.0693, 0.0324], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:48,746][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.0046, 0.0377, 0.0628, 0.0227, 0.0139, 0.1020, 0.1031, 0.0306, 0.0834,
        0.0726, 0.0444, 0.0824, 0.0819, 0.0386, 0.0475, 0.1718],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,749][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0016, 0.0144, 0.0491, 0.0324, 0.1235, 0.0818, 0.2265, 0.0716, 0.1097,
        0.0343, 0.0382, 0.0584, 0.0493, 0.0160, 0.0449, 0.0484],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,753][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.0111, 0.0308, 0.0429, 0.0456, 0.0733, 0.1079, 0.1145, 0.0393, 0.1022,
        0.0764, 0.0541, 0.1042, 0.0671, 0.0279, 0.0131, 0.0896],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,757][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.1141, 0.2511, 0.1219, 0.0319, 0.0242, 0.0800, 0.0055, 0.0021, 0.0202,
        0.0172, 0.0083, 0.0602, 0.0353, 0.0343, 0.0129, 0.1807],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,762][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0022, 0.0170, 0.0144, 0.0143, 0.0291, 0.1376, 0.1575, 0.1869, 0.1128,
        0.0628, 0.0518, 0.0756, 0.0585, 0.0119, 0.0307, 0.0369],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,767][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0292, 0.0156, 0.0259, 0.0129, 0.1249, 0.1790, 0.0906, 0.0276, 0.0819,
        0.0704, 0.0351, 0.0909, 0.0755, 0.0249, 0.0348, 0.0809],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,771][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0002, 0.0384, 0.0505, 0.0548, 0.0839, 0.1136, 0.1055, 0.0517, 0.0357,
        0.0800, 0.0882, 0.0761, 0.0870, 0.0235, 0.0211, 0.0897],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,776][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0086, 0.0747, 0.1849, 0.0455, 0.0179, 0.1346, 0.1145, 0.0102, 0.0413,
        0.0347, 0.0125, 0.0521, 0.1365, 0.0480, 0.0191, 0.0648],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,777][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0262, 0.2050, 0.0551, 0.0320, 0.0770, 0.0594, 0.1050, 0.0940, 0.0365,
        0.0707, 0.0606, 0.0413, 0.0442, 0.0294, 0.0173, 0.0462],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,777][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([4.3961e-05, 2.5687e-02, 4.0989e-02, 5.2487e-02, 5.3784e-02, 5.6514e-02,
        1.6178e-01, 3.0002e-02, 5.7496e-02, 4.0524e-02, 9.8333e-02, 1.0808e-01,
        7.8059e-02, 3.7305e-02, 7.4135e-02, 8.4775e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,778][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.0279, 0.1045, 0.0743, 0.0269, 0.0129, 0.1285, 0.0125, 0.0053, 0.0323,
        0.0581, 0.0234, 0.0395, 0.0535, 0.0340, 0.0263, 0.3399],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,781][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0032, 0.0669, 0.1038, 0.1332, 0.0747, 0.0374, 0.0384, 0.0237, 0.0917,
        0.0290, 0.0331, 0.0706, 0.1102, 0.0585, 0.0368, 0.0888],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:48,785][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0073, 0.0374, 0.0584, 0.0211, 0.0111, 0.0787, 0.0831, 0.0195, 0.0651,
        0.0518, 0.0236, 0.0757, 0.0665, 0.0275, 0.0222, 0.1706, 0.1803],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,789][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0003, 0.0137, 0.0376, 0.0287, 0.0875, 0.1530, 0.2029, 0.0595, 0.0855,
        0.0441, 0.0589, 0.0431, 0.0391, 0.0139, 0.0387, 0.0414, 0.0521],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,794][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0060, 0.0372, 0.0435, 0.0532, 0.0546, 0.0888, 0.0665, 0.0430, 0.1195,
        0.0552, 0.0332, 0.1193, 0.0623, 0.0197, 0.0071, 0.0833, 0.1077],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,799][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0509, 0.2300, 0.1933, 0.0331, 0.0182, 0.0981, 0.0046, 0.0011, 0.0182,
        0.0108, 0.0032, 0.0576, 0.0309, 0.0178, 0.0043, 0.1604, 0.0675],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,803][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0008, 0.0123, 0.0065, 0.0043, 0.0201, 0.0958, 0.1107, 0.2206, 0.1070,
        0.0519, 0.0657, 0.1042, 0.0516, 0.0231, 0.0517, 0.0354, 0.0383],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,808][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0158, 0.0139, 0.0225, 0.0113, 0.0745, 0.1336, 0.0710, 0.0199, 0.0948,
        0.0440, 0.0187, 0.0716, 0.0658, 0.0151, 0.0219, 0.0744, 0.2312],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,809][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([3.8600e-05, 5.6441e-02, 2.8947e-02, 5.7916e-02, 7.0863e-02, 1.2673e-01,
        7.3770e-02, 3.8134e-02, 2.6045e-02, 1.2099e-01, 1.1994e-01, 4.8815e-02,
        7.0003e-02, 3.0116e-02, 2.3162e-02, 6.7468e-02, 4.0626e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,810][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0055, 0.0619, 0.1439, 0.0404, 0.0130, 0.1399, 0.1462, 0.0073, 0.0294,
        0.0230, 0.0057, 0.0435, 0.1053, 0.0380, 0.0089, 0.0396, 0.1482],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,811][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0247, 0.2268, 0.0414, 0.0185, 0.0621, 0.0670, 0.1025, 0.0900, 0.0454,
        0.0566, 0.0498, 0.0332, 0.0324, 0.0301, 0.0166, 0.0463, 0.0564],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,812][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.2564e-05, 3.8946e-02, 1.7827e-02, 3.6350e-02, 4.4781e-02, 4.3696e-02,
        1.4187e-01, 2.2527e-02, 4.6116e-02, 4.8171e-02, 1.3050e-01, 7.3727e-02,
        7.3195e-02, 5.1118e-02, 9.8465e-02, 7.8679e-02, 5.4025e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,815][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0053, 0.0791, 0.0895, 0.0196, 0.0111, 0.1245, 0.0073, 0.0025, 0.0233,
        0.0437, 0.0131, 0.0339, 0.0381, 0.0178, 0.0093, 0.2656, 0.2164],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,820][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0005, 0.0580, 0.1128, 0.1205, 0.0565, 0.0322, 0.0259, 0.0163, 0.0763,
        0.0300, 0.0364, 0.0721, 0.1037, 0.0550, 0.0550, 0.1081, 0.0407],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:48,824][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0040, 0.0243, 0.0501, 0.0377, 0.0173, 0.0704, 0.0619, 0.0314, 0.0803,
        0.0666, 0.0297, 0.0734, 0.0592, 0.0180, 0.0174, 0.1412, 0.1871, 0.0299],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,829][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0004, 0.0190, 0.0404, 0.0376, 0.0708, 0.1082, 0.1258, 0.0789, 0.0664,
        0.0638, 0.0611, 0.0703, 0.0568, 0.0275, 0.0492, 0.0447, 0.0583, 0.0209],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,834][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0178, 0.0290, 0.0338, 0.0470, 0.0593, 0.0857, 0.0763, 0.0317, 0.0830,
        0.0719, 0.0574, 0.0927, 0.0641, 0.0291, 0.0100, 0.0798, 0.1017, 0.0295],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,838][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0264, 0.0524, 0.0887, 0.0770, 0.0241, 0.0632, 0.0067, 0.0018, 0.0201,
        0.0214, 0.0085, 0.1256, 0.0568, 0.0267, 0.0118, 0.2160, 0.1308, 0.0420],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,840][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0012, 0.0286, 0.0195, 0.0228, 0.0422, 0.1455, 0.1140, 0.1339, 0.0960,
        0.0460, 0.0528, 0.0854, 0.0420, 0.0179, 0.0298, 0.0323, 0.0361, 0.0541],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,841][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0095, 0.0261, 0.0325, 0.0181, 0.1226, 0.1645, 0.0620, 0.0226, 0.0474,
        0.0522, 0.0284, 0.0583, 0.0680, 0.0149, 0.0264, 0.0595, 0.1380, 0.0489],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,842][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([2.8222e-05, 6.1092e-02, 4.6242e-02, 7.1634e-02, 7.0908e-02, 7.1642e-02,
        5.6123e-02, 2.5003e-02, 3.2062e-02, 1.0905e-01, 1.1709e-01, 6.9032e-02,
        8.1178e-02, 2.9116e-02, 3.0486e-02, 6.1051e-02, 4.4142e-02, 2.4120e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,843][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0072, 0.0271, 0.0832, 0.0503, 0.0153, 0.0917, 0.1151, 0.0196, 0.0359,
        0.0315, 0.0147, 0.0734, 0.1051, 0.0347, 0.0109, 0.0596, 0.1997, 0.0250],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,846][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0126, 0.1729, 0.0233, 0.0216, 0.0485, 0.0523, 0.1028, 0.0688, 0.0350,
        0.0557, 0.0407, 0.0342, 0.0330, 0.0382, 0.0165, 0.0363, 0.0708, 0.1367],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,847][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([7.8409e-06, 5.9238e-02, 2.5413e-02, 3.3346e-02, 3.9863e-02, 4.5286e-02,
        7.7910e-02, 1.8845e-02, 3.3095e-02, 8.6429e-02, 1.3321e-01, 6.2473e-02,
        1.0284e-01, 4.9884e-02, 9.7850e-02, 5.3961e-02, 5.5565e-02, 2.4789e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,852][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0055, 0.0829, 0.0542, 0.0314, 0.0152, 0.1159, 0.0083, 0.0056, 0.0250,
        0.0556, 0.0299, 0.0447, 0.0561, 0.0232, 0.0139, 0.2012, 0.1654, 0.0662],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,856][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0004, 0.0435, 0.1048, 0.1031, 0.0683, 0.0379, 0.0282, 0.0256, 0.0658,
        0.0488, 0.0352, 0.0727, 0.1001, 0.0526, 0.0749, 0.0776, 0.0365, 0.0239],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:48,861][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.0037, 0.0195, 0.0397, 0.0203, 0.0062, 0.0305, 0.0258, 0.0108, 0.0502,
        0.0446, 0.0148, 0.0585, 0.0578, 0.0298, 0.0180, 0.1488, 0.1894, 0.0256,
        0.2059], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,866][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0005, 0.0120, 0.0513, 0.0264, 0.1111, 0.0912, 0.1560, 0.0699, 0.0761,
        0.0408, 0.0542, 0.0411, 0.0400, 0.0140, 0.0425, 0.0484, 0.0629, 0.0153,
        0.0463], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,870][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0384, 0.0199, 0.0294, 0.0415, 0.0546, 0.0508, 0.0427, 0.0193, 0.1214,
        0.0716, 0.0308, 0.1056, 0.0594, 0.0213, 0.0065, 0.0721, 0.1135, 0.0267,
        0.0744], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,872][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([1.0391e-01, 6.4604e-02, 3.1253e-02, 7.8795e-03, 3.7847e-03, 1.6005e-02,
        4.0407e-04, 1.2436e-04, 3.1233e-03, 1.9811e-03, 5.4764e-04, 1.1901e-02,
        5.3607e-03, 3.6229e-03, 5.9162e-04, 3.5076e-02, 1.1508e-02, 4.3930e-02,
        6.5439e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,873][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0024, 0.0161, 0.0131, 0.0081, 0.0314, 0.0870, 0.0740, 0.1477, 0.1219,
        0.0706, 0.0636, 0.0887, 0.0544, 0.0127, 0.0446, 0.0403, 0.0618, 0.0469,
        0.0149], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,874][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0094, 0.0076, 0.0108, 0.0050, 0.1016, 0.1050, 0.0532, 0.0167, 0.0709,
        0.0443, 0.0190, 0.0668, 0.0658, 0.0206, 0.0236, 0.0584, 0.2080, 0.0216,
        0.0915], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,875][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0001, 0.0481, 0.0468, 0.0592, 0.0872, 0.0814, 0.0413, 0.0278, 0.0376,
        0.0913, 0.0923, 0.0623, 0.0969, 0.0270, 0.0189, 0.0634, 0.0634, 0.0253,
        0.0298], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,878][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0048, 0.0359, 0.1174, 0.0434, 0.0067, 0.0605, 0.0447, 0.0032, 0.0206,
        0.0281, 0.0046, 0.0317, 0.1076, 0.0267, 0.0077, 0.0415, 0.2131, 0.0349,
        0.1669], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,883][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0278, 0.1935, 0.0359, 0.0188, 0.0326, 0.0422, 0.0634, 0.0684, 0.0327,
        0.0428, 0.0319, 0.0293, 0.0329, 0.0246, 0.0113, 0.0357, 0.0977, 0.1362,
        0.0426], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,886][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([4.0158e-06, 2.1234e-02, 3.5143e-02, 4.6059e-02, 3.1800e-02, 4.0048e-02,
        1.5481e-01, 2.3783e-02, 4.5863e-02, 3.3421e-02, 5.9152e-02, 9.9695e-02,
        1.0004e-01, 4.1071e-02, 7.9277e-02, 7.4142e-02, 7.6724e-02, 9.0114e-03,
        2.8717e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,888][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([2.1663e-02, 2.9173e-02, 1.8930e-02, 4.1341e-03, 1.5850e-03, 2.2665e-02,
        6.4968e-04, 3.1322e-04, 5.6171e-03, 9.8269e-03, 2.7002e-03, 7.0198e-03,
        7.4466e-03, 4.9205e-03, 1.3781e-03, 7.0868e-02, 4.1372e-02, 2.9069e-02,
        7.2067e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,893][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0008, 0.0405, 0.1086, 0.1451, 0.0790, 0.0254, 0.0334, 0.0145, 0.0897,
        0.0457, 0.0472, 0.0452, 0.0716, 0.0241, 0.0313, 0.0826, 0.0283, 0.0151,
        0.0720], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:48,898][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0034, 0.0142, 0.0240, 0.0132, 0.0038, 0.0481, 0.0468, 0.0229, 0.0536,
        0.0445, 0.0218, 0.0565, 0.0537, 0.0290, 0.0180, 0.1067, 0.0899, 0.0220,
        0.2015, 0.1264], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,902][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0003, 0.0108, 0.0528, 0.0268, 0.1123, 0.0891, 0.2298, 0.0584, 0.0670,
        0.0298, 0.0447, 0.0364, 0.0336, 0.0099, 0.0448, 0.0254, 0.0370, 0.0145,
        0.0476, 0.0290], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,904][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.0122, 0.0166, 0.0231, 0.0275, 0.0441, 0.0546, 0.0555, 0.0349, 0.0870,
        0.0535, 0.0301, 0.1287, 0.0479, 0.0153, 0.0044, 0.0639, 0.0874, 0.0210,
        0.0561, 0.1361], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,905][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([1.4127e-01, 5.4735e-02, 2.3496e-02, 7.4850e-03, 4.0967e-03, 1.4443e-02,
        3.9327e-04, 1.4889e-04, 2.2912e-03, 2.0354e-03, 1.0737e-03, 6.8881e-03,
        3.6746e-03, 2.2324e-03, 7.2922e-04, 2.6803e-02, 8.9989e-03, 2.5137e-02,
        3.0526e-01, 3.6881e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,906][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0026, 0.0135, 0.0085, 0.0083, 0.0194, 0.0791, 0.1184, 0.2374, 0.1136,
        0.0577, 0.0632, 0.0553, 0.0474, 0.0106, 0.0437, 0.0312, 0.0366, 0.0361,
        0.0093, 0.0085], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,907][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0052, 0.0083, 0.0188, 0.0085, 0.1225, 0.0969, 0.0483, 0.0235, 0.0706,
        0.0432, 0.0227, 0.0938, 0.0646, 0.0127, 0.0222, 0.0478, 0.1583, 0.0156,
        0.0782, 0.0383], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,909][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([3.5396e-05, 3.3708e-02, 3.3600e-02, 5.7501e-02, 7.4364e-02, 9.9848e-02,
        6.8860e-02, 3.6522e-02, 2.4267e-02, 8.5945e-02, 8.9315e-02, 5.3838e-02,
        7.7414e-02, 2.4424e-02, 2.2574e-02, 6.4365e-02, 5.1885e-02, 2.3389e-02,
        3.1558e-02, 4.6588e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,912][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0044, 0.0374, 0.0779, 0.0250, 0.0081, 0.0774, 0.0865, 0.0065, 0.0132,
        0.0254, 0.0093, 0.0332, 0.0830, 0.0257, 0.0076, 0.0340, 0.1309, 0.0272,
        0.1321, 0.1552], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,916][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0142, 0.1288, 0.0212, 0.0118, 0.0437, 0.0488, 0.0864, 0.0897, 0.0475,
        0.0434, 0.0510, 0.0287, 0.0294, 0.0339, 0.0125, 0.0413, 0.0492, 0.1331,
        0.0533, 0.0322], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,919][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([4.0544e-06, 1.5248e-02, 3.7166e-02, 3.9354e-02, 3.4391e-02, 5.1691e-02,
        1.4141e-01, 2.0261e-02, 2.5868e-02, 3.5592e-02, 1.0450e-01, 9.0405e-02,
        1.0155e-01, 3.0951e-02, 7.8289e-02, 6.0764e-02, 5.8394e-02, 1.0606e-02,
        2.7690e-02, 3.5862e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,922][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([1.2441e-02, 3.1544e-02, 1.6618e-02, 5.5643e-03, 1.6078e-03, 2.4002e-02,
        6.7899e-04, 3.3751e-04, 3.7504e-03, 1.2647e-02, 3.1048e-03, 5.0793e-03,
        7.9495e-03, 3.1784e-03, 1.1164e-03, 4.4977e-02, 3.8564e-02, 1.9692e-02,
        2.8968e-01, 4.7746e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,926][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0005, 0.0402, 0.0679, 0.0836, 0.0698, 0.0230, 0.0444, 0.0318, 0.0893,
        0.0321, 0.0496, 0.0524, 0.0746, 0.0309, 0.0389, 0.0674, 0.0304, 0.0204,
        0.0968, 0.0562], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:48,931][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0067, 0.0265, 0.0374, 0.0164, 0.0052, 0.0433, 0.0324, 0.0094, 0.0363,
        0.0246, 0.0123, 0.0385, 0.0347, 0.0127, 0.0080, 0.0743, 0.0683, 0.0227,
        0.1298, 0.0739, 0.2867], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,936][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0004, 0.0122, 0.0534, 0.0314, 0.1069, 0.1260, 0.1756, 0.0452, 0.0632,
        0.0296, 0.0340, 0.0307, 0.0335, 0.0097, 0.0289, 0.0247, 0.0445, 0.0141,
        0.0352, 0.0259, 0.0747], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,937][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0108, 0.0245, 0.0282, 0.0394, 0.0328, 0.0514, 0.0345, 0.0204, 0.0691,
        0.0309, 0.0166, 0.0684, 0.0357, 0.0100, 0.0022, 0.0361, 0.0494, 0.0174,
        0.0320, 0.1070, 0.2832], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,938][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([6.4849e-02, 4.5232e-02, 1.8768e-02, 4.1592e-03, 1.7490e-03, 7.9514e-03,
        1.1388e-04, 1.8480e-05, 6.9795e-04, 4.3238e-04, 9.3366e-05, 1.8531e-03,
        1.0822e-03, 4.9095e-04, 8.0858e-05, 9.5253e-03, 2.9039e-03, 1.2776e-02,
        2.5624e-01, 2.7176e-01, 2.9922e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,939][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0013, 0.0091, 0.0049, 0.0037, 0.0203, 0.0847, 0.1252, 0.2258, 0.1111,
        0.0427, 0.0531, 0.0980, 0.0472, 0.0155, 0.0315, 0.0245, 0.0286, 0.0417,
        0.0075, 0.0055, 0.0181], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,941][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0103, 0.0054, 0.0096, 0.0060, 0.0640, 0.0807, 0.0292, 0.0081, 0.0421,
        0.0207, 0.0073, 0.0445, 0.0378, 0.0073, 0.0080, 0.0370, 0.1335, 0.0136,
        0.0452, 0.0222, 0.3674], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,943][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([2.8052e-05, 6.2812e-02, 2.9915e-02, 6.6237e-02, 7.6698e-02, 1.0163e-01,
        5.1372e-02, 2.5733e-02, 1.8486e-02, 1.0118e-01, 1.0015e-01, 3.6820e-02,
        5.7963e-02, 2.2945e-02, 1.2446e-02, 4.8175e-02, 2.8844e-02, 2.9749e-02,
        1.6969e-02, 2.7017e-02, 8.4835e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,947][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0081, 0.0334, 0.0665, 0.0151, 0.0040, 0.0564, 0.0458, 0.0019, 0.0085,
        0.0069, 0.0013, 0.0107, 0.0370, 0.0101, 0.0015, 0.0114, 0.0423, 0.0168,
        0.0783, 0.1022, 0.4417], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,952][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0418, 0.1835, 0.0266, 0.0101, 0.0353, 0.0554, 0.0711, 0.0697, 0.0362,
        0.0436, 0.0293, 0.0218, 0.0253, 0.0208, 0.0092, 0.0316, 0.0404, 0.0988,
        0.0322, 0.0238, 0.0934], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,955][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.1467e-05, 2.8344e-02, 1.6108e-02, 5.4146e-02, 4.6883e-02, 5.9218e-02,
        9.6103e-02, 1.3473e-02, 3.0726e-02, 4.4732e-02, 1.1417e-01, 5.7036e-02,
        7.1134e-02, 3.0878e-02, 6.4720e-02, 6.9959e-02, 4.6062e-02, 1.7968e-02,
        8.0500e-03, 2.8586e-02, 1.0170e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,958][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([6.0519e-03, 1.1319e-02, 8.2084e-03, 1.9238e-03, 8.1724e-04, 8.9841e-03,
        1.7783e-04, 6.6208e-05, 7.7782e-04, 2.5094e-03, 4.7939e-04, 1.0125e-03,
        1.9790e-03, 5.4980e-04, 2.4350e-04, 2.0969e-02, 1.2116e-02, 7.3758e-03,
        1.3260e-01, 2.0430e-01, 5.7754e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,963][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0008, 0.0389, 0.1039, 0.0897, 0.0519, 0.0217, 0.0184, 0.0143, 0.0740,
        0.0207, 0.0245, 0.0592, 0.0777, 0.0275, 0.0254, 0.0682, 0.0274, 0.0175,
        0.1004, 0.0695, 0.0685], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:48,966][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:48,969][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 741],
        [ 553],
        [2045],
        [ 604],
        [ 323],
        [ 152],
        [ 219],
        [ 288],
        [  65],
        [  84],
        [ 140],
        [  87],
        [  92],
        [ 165],
        [ 236],
        [  49],
        [ 117],
        [ 189],
        [ 258],
        [ 246],
        [ 383]], device='cuda:0')
[2024-07-23 21:06:48,971][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 669],
        [ 507],
        [1268],
        [ 387],
        [ 151],
        [  94],
        [ 163],
        [ 165],
        [  41],
        [  57],
        [ 102],
        [  60],
        [  60],
        [ 145],
        [ 197],
        [  47],
        [  93],
        [ 162],
        [ 211],
        [ 216],
        [ 316]], device='cuda:0')
[2024-07-23 21:06:48,973][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 6012],
        [36818],
        [46841],
        [40327],
        [41255],
        [34830],
        [23411],
        [17287],
        [21163],
        [23143],
        [15482],
        [21248],
        [17900],
        [21701],
        [18233],
        [22297],
        [23952],
        [24192],
        [32997],
        [31064],
        [29565]], device='cuda:0')
[2024-07-23 21:06:48,974][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 9106],
        [33446],
        [33495],
        [27390],
        [32531],
        [36431],
        [37508],
        [39673],
        [39881],
        [39606],
        [38989],
        [37947],
        [38124],
        [39076],
        [36900],
        [38911],
        [40112],
        [39089],
        [40270],
        [39729],
        [39223]], device='cuda:0')
[2024-07-23 21:06:48,976][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[21943],
        [31898],
        [23372],
        [28375],
        [31770],
        [40539],
        [31481],
        [38221],
        [40239],
        [40402],
        [40127],
        [38596],
        [38461],
        [36513],
        [36998],
        [37744],
        [36989],
        [36656],
        [34923],
        [34326],
        [35774]], device='cuda:0')
[2024-07-23 21:06:48,979][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 9832],
        [33927],
        [48912],
        [48741],
        [48905],
        [49096],
        [49273],
        [49278],
        [49156],
        [49071],
        [49030],
        [47317],
        [47635],
        [46157],
        [46737],
        [47701],
        [47888],
        [45579],
        [38929],
        [40407],
        [40016]], device='cuda:0')
[2024-07-23 21:06:48,982][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[17808],
        [32088],
        [27740],
        [22871],
        [21058],
        [21213],
        [21784],
        [21704],
        [22289],
        [21898],
        [20074],
        [19003],
        [19586],
        [19609],
        [20234],
        [20354],
        [20696],
        [22445],
        [21748],
        [21169],
        [20796]], device='cuda:0')
[2024-07-23 21:06:48,985][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[387],
        [149],
        [287],
        [273],
        [160],
        [150],
        [181],
        [164],
        [138],
        [111],
        [117],
        [153],
        [155],
        [173],
        [194],
        [192],
        [216],
        [210],
        [285],
        [277],
        [295]], device='cuda:0')
[2024-07-23 21:06:48,989][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[24695],
        [40102],
        [41909],
        [39874],
        [44002],
        [43727],
        [34111],
        [39374],
        [38793],
        [36664],
        [33482],
        [32269],
        [33817],
        [34563],
        [32232],
        [34302],
        [34348],
        [35109],
        [36345],
        [35762],
        [36474]], device='cuda:0')
[2024-07-23 21:06:48,992][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[14995],
        [36019],
        [34092],
        [33070],
        [30841],
        [32546],
        [35607],
        [35968],
        [34568],
        [34118],
        [33564],
        [34102],
        [33529],
        [32120],
        [31586],
        [31127],
        [30900],
        [31038],
        [30876],
        [31154],
        [30629]], device='cuda:0')
[2024-07-23 21:06:48,995][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[31687],
        [21745],
        [31861],
        [29721],
        [49813],
        [44588],
        [38199],
        [39598],
        [26806],
        [20511],
        [19883],
        [16174],
        [14169],
        [12682],
        [14531],
        [13433],
        [12566],
        [ 8094],
        [ 8609],
        [ 9842],
        [10716]], device='cuda:0')
[2024-07-23 21:06:48,998][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[35413],
        [16598],
        [12760],
        [ 9820],
        [ 6373],
        [ 3997],
        [ 2518],
        [ 2242],
        [ 2955],
        [ 3862],
        [ 2909],
        [ 2185],
        [ 2464],
        [ 3070],
        [ 1675],
        [ 1541],
        [ 1365],
        [ 1569],
        [ 1429],
        [ 1396],
        [ 1321]], device='cuda:0')
[2024-07-23 21:06:49,001][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[18782],
        [17395],
        [24949],
        [25573],
        [27848],
        [25364],
        [25954],
        [26119],
        [25665],
        [25452],
        [25920],
        [26895],
        [27298],
        [27763],
        [28761],
        [32251],
        [31608],
        [30784],
        [33624],
        [27494],
        [28577]], device='cuda:0')
[2024-07-23 21:06:49,005][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[31425],
        [35879],
        [46688],
        [47786],
        [47079],
        [47729],
        [47122],
        [46043],
        [45488],
        [46470],
        [46312],
        [45212],
        [45121],
        [44679],
        [43917],
        [44204],
        [44114],
        [43607],
        [45237],
        [43649],
        [44082]], device='cuda:0')
[2024-07-23 21:06:49,008][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 9768],
        [ 3399],
        [ 3495],
        [ 6807],
        [10134],
        [ 6612],
        [ 4052],
        [ 3850],
        [ 7693],
        [ 7735],
        [ 2797],
        [ 4988],
        [12214],
        [ 4714],
        [ 6202],
        [ 5390],
        [ 9969],
        [ 2096],
        [ 2337],
        [ 4677],
        [11212]], device='cuda:0')
[2024-07-23 21:06:49,009][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[13930],
        [ 1922],
        [ 1458],
        [  948],
        [  671],
        [ 1223],
        [ 1076],
        [  891],
        [  675],
        [  958],
        [  992],
        [  655],
        [  813],
        [  845],
        [  941],
        [ 1219],
        [ 1289],
        [ 1198],
        [ 1343],
        [ 1233],
        [ 1593]], device='cuda:0')
[2024-07-23 21:06:49,011][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[31343],
        [34275],
        [36980],
        [33529],
        [36972],
        [28699],
        [ 7026],
        [ 8728],
        [12249],
        [15304],
        [ 9711],
        [10874],
        [10577],
        [11409],
        [10124],
        [11806],
        [ 8311],
        [10250],
        [11487],
        [10033],
        [ 9563]], device='cuda:0')
[2024-07-23 21:06:49,013][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 1043],
        [ 2696],
        [17243],
        [10641],
        [10146],
        [ 4982],
        [ 8564],
        [ 5905],
        [10450],
        [ 7247],
        [ 7566],
        [12096],
        [13779],
        [11700],
        [12498],
        [ 9676],
        [10542],
        [ 9995],
        [12558],
        [10982],
        [10334]], device='cuda:0')
[2024-07-23 21:06:49,016][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[27007],
        [20679],
        [ 4264],
        [ 4356],
        [ 5805],
        [ 5264],
        [ 6845],
        [ 7916],
        [ 7965],
        [ 7193],
        [ 7934],
        [10241],
        [ 9101],
        [10495],
        [ 9637],
        [ 7516],
        [ 7504],
        [ 8167],
        [ 5537],
        [ 5834],
        [ 7772]], device='cuda:0')
[2024-07-23 21:06:49,019][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 1471],
        [ 4656],
        [10299],
        [14902],
        [ 8565],
        [ 4762],
        [ 3134],
        [ 1464],
        [ 1208],
        [ 1669],
        [ 1468],
        [ 1602],
        [ 2037],
        [ 2319],
        [ 2011],
        [ 2276],
        [ 2448],
        [ 2731],
        [ 3124],
        [ 2405],
        [ 2381]], device='cuda:0')
[2024-07-23 21:06:49,022][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[14169],
        [ 2450],
        [ 1711],
        [ 1120],
        [ 1444],
        [ 1003],
        [ 1071],
        [ 1019],
        [  794],
        [  848],
        [  885],
        [  932],
        [  920],
        [  959],
        [  986],
        [  889],
        [  813],
        [  966],
        [  905],
        [  877],
        [  745]], device='cuda:0')
[2024-07-23 21:06:49,025][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[41751],
        [24110],
        [22334],
        [22321],
        [24857],
        [21111],
        [19317],
        [17516],
        [17891],
        [19862],
        [22548],
        [28004],
        [26921],
        [28674],
        [26683],
        [24781],
        [25091],
        [26170],
        [25800],
        [24818],
        [25574]], device='cuda:0')
[2024-07-23 21:06:49,029][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 6549],
        [22525],
        [12146],
        [13306],
        [16139],
        [21910],
        [16223],
        [14258],
        [16817],
        [21364],
        [20824],
        [21502],
        [27671],
        [27238],
        [27873],
        [28931],
        [29838],
        [34276],
        [30393],
        [30989],
        [36816]], device='cuda:0')
[2024-07-23 21:06:49,032][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 5166],
        [30561],
        [32632],
        [33293],
        [32405],
        [27342],
        [31782],
        [30400],
        [40040],
        [38858],
        [39742],
        [38972],
        [39851],
        [39264],
        [39256],
        [40482],
        [40861],
        [43593],
        [44247],
        [45227],
        [44280]], device='cuda:0')
[2024-07-23 21:06:49,035][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 1253],
        [39107],
        [20278],
        [13169],
        [11459],
        [ 9002],
        [ 7366],
        [ 5075],
        [ 6003],
        [ 6726],
        [ 6459],
        [ 6443],
        [ 5009],
        [ 7657],
        [ 6114],
        [ 6495],
        [ 8440],
        [ 9429],
        [ 6695],
        [ 6240],
        [ 7935]], device='cuda:0')
[2024-07-23 21:06:49,038][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 703],
        [3598],
        [3299],
        [3332],
        [2918],
        [3956],
        [4286],
        [4336],
        [3867],
        [3637],
        [3474],
        [3359],
        [3188],
        [3329],
        [3179],
        [4299],
        [4357],
        [3971],
        [2956],
        [2689],
        [3650]], device='cuda:0')
[2024-07-23 21:06:49,041][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[18795],
        [ 9073],
        [11012],
        [ 8153],
        [11291],
        [ 8971],
        [ 9735],
        [10274],
        [11097],
        [11031],
        [10659],
        [10426],
        [10358],
        [10478],
        [10522],
        [10789],
        [10857],
        [11069],
        [11275],
        [11699],
        [11629]], device='cuda:0')
[2024-07-23 21:06:49,044][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[41420],
        [40066],
        [42279],
        [45687],
        [43947],
        [46875],
        [47683],
        [48632],
        [47188],
        [47016],
        [47435],
        [46138],
        [45863],
        [45012],
        [45999],
        [45448],
        [45128],
        [43762],
        [44955],
        [46007],
        [44343]], device='cuda:0')
[2024-07-23 21:06:49,046][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[33560],
        [40932],
        [40703],
        [36835],
        [33693],
        [40912],
        [44058],
        [45141],
        [39137],
        [38651],
        [45393],
        [41765],
        [33003],
        [41259],
        [37436],
        [39856],
        [33884],
        [45288],
        [44694],
        [39949],
        [30421]], device='cuda:0')
[2024-07-23 21:06:49,047][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032],
        [2032]], device='cuda:0')
[2024-07-23 21:06:49,166][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:49,170][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,175][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,178][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,178][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,179][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,180][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,180][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,182][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,184][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,188][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,191][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,196][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:49,200][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0150, 0.9850], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,205][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0048, 0.9952], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,209][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1490, 0.8510], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,210][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.3359, 0.6641], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,211][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.7618, 0.2382], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,212][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.7418, 0.2582], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,212][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0509, 0.9491], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,214][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.9964, 0.0036], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,217][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.2927, 0.7073], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,222][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.6411, 0.3589], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,226][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.9241, 0.0759], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,231][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0544, 0.9456], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:49,234][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ used] are: tensor([4.4509e-04, 7.6868e-01, 2.3087e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,237][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ used] are: tensor([2.3535e-04, 5.1733e-01, 4.8243e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,242][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0114, 0.4106, 0.5780], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,242][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0238, 0.2245, 0.7517], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,243][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.4461, 0.1806, 0.3733], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,244][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.2875, 0.5602, 0.1523], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,244][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0113, 0.6416, 0.3471], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,247][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.9959, 0.0018, 0.0024], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,249][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0236, 0.2389, 0.7374], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,254][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0388, 0.7594, 0.2018], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,259][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.1377, 0.0113, 0.8510], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,263][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0052, 0.2820, 0.7128], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:49,268][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0007, 0.5618, 0.2570, 0.1804], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,271][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ by] are: tensor([1.3593e-04, 2.9248e-01, 3.0817e-01, 3.9921e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,274][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0346, 0.2856, 0.3599, 0.3199], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,275][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0410, 0.1665, 0.5560, 0.2364], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,276][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.1923, 0.2337, 0.4574, 0.1167], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,276][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.5507, 0.2412, 0.0988, 0.1093], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,277][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0080, 0.5200, 0.2741, 0.1979], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,280][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.9438, 0.0035, 0.0062, 0.0464], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,284][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0844, 0.2355, 0.2392, 0.4409], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,288][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0383, 0.5234, 0.3222, 0.1161], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,293][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0088, 0.0010, 0.0368, 0.9534], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,298][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0279, 0.0705, 0.1789, 0.7227], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:49,302][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.0015, 0.3264, 0.2017, 0.1142, 0.3561], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,306][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0003, 0.1907, 0.3158, 0.2354, 0.2578], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,307][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0636, 0.2281, 0.2815, 0.2396, 0.1871], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,308][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0895, 0.1008, 0.3848, 0.1989, 0.2260], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,309][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([0.2327, 0.1900, 0.3447, 0.1067, 0.1259], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,309][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.2120, 0.4007, 0.2093, 0.1164, 0.0616], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,312][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0243, 0.3307, 0.2591, 0.1287, 0.2571], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,314][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([9.9198e-01, 2.1489e-04, 5.9256e-04, 3.6630e-03, 3.5521e-03],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,319][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.1777, 0.0734, 0.1600, 0.1776, 0.4113], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,323][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0462, 0.2995, 0.4634, 0.1096, 0.0813], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,328][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.0108, 0.0013, 0.0349, 0.9185, 0.0346], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,333][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.0072, 0.0557, 0.1184, 0.2937, 0.5251], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:49,337][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.0021, 0.3334, 0.1746, 0.1217, 0.2162, 0.1520], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,339][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0011, 0.1481, 0.1811, 0.2188, 0.2501, 0.2009], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,339][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0343, 0.2222, 0.2510, 0.2810, 0.1239, 0.0877], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,340][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0507, 0.1425, 0.2894, 0.1744, 0.2015, 0.1415], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,341][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ B] are: tensor([0.2937, 0.1932, 0.3073, 0.0850, 0.0399, 0.0809], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,343][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ B] are: tensor([0.2184, 0.4027, 0.1683, 0.1349, 0.0348, 0.0408], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,346][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0242, 0.2840, 0.1892, 0.1122, 0.1701, 0.2202], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,351][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.9633, 0.0018, 0.0037, 0.0128, 0.0100, 0.0085], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,355][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.3488, 0.0816, 0.0877, 0.1370, 0.1310, 0.2140], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,360][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0221, 0.4416, 0.2412, 0.0700, 0.0333, 0.1918], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,365][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0040, 0.0009, 0.0262, 0.6917, 0.0214, 0.2558], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,369][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.0035, 0.0593, 0.0870, 0.3060, 0.2791, 0.2650], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:49,371][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [aut] are: tensor([3.9986e-04, 1.6659e-01, 1.1056e-01, 4.8153e-02, 1.6812e-01, 8.4387e-02,
        4.2178e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,371][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [aut] are: tensor([1.2852e-04, 5.9641e-02, 6.4366e-02, 7.0505e-02, 4.3764e-02, 1.3930e-01,
        6.2229e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,372][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [aut] are: tensor([0.0167, 0.1857, 0.2843, 0.2050, 0.0860, 0.0740, 0.1484],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,373][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [aut] are: tensor([0.0243, 0.0862, 0.2456, 0.1064, 0.1443, 0.1723, 0.2208],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,375][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [aut] are: tensor([0.3065, 0.1168, 0.2197, 0.0789, 0.0322, 0.0661, 0.1798],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,378][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [aut] are: tensor([0.1761, 0.4425, 0.2126, 0.0766, 0.0321, 0.0371, 0.0230],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,383][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [aut] are: tensor([0.0035, 0.1394, 0.0690, 0.0304, 0.0674, 0.0765, 0.6139],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,386][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [aut] are: tensor([9.6506e-01, 9.5174e-04, 1.5258e-03, 5.5921e-03, 4.3423e-03, 1.9124e-03,
        2.0611e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,390][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.0419, 0.0519, 0.0866, 0.0821, 0.1029, 0.2157, 0.4189],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,395][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0029, 0.2468, 0.2958, 0.0245, 0.0264, 0.3266, 0.0769],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,397][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [aut] are: tensor([3.7121e-03, 1.2368e-04, 1.4193e-02, 7.0117e-01, 7.7755e-03, 2.0656e-01,
        6.6463e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,402][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.0012, 0.0227, 0.0416, 0.1219, 0.1026, 0.1343, 0.5757],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:49,403][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ista] are: tensor([9.9378e-05, 1.5541e-01, 8.3226e-02, 3.6783e-02, 2.3849e-01, 1.2958e-01,
        3.3925e-01, 1.7157e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,404][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ista] are: tensor([1.5139e-05, 4.7883e-02, 8.6508e-02, 8.3185e-02, 1.1058e-01, 1.7929e-01,
        4.5139e-01, 4.1150e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,405][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ista] are: tensor([0.0019, 0.2176, 0.2282, 0.1812, 0.1154, 0.0870, 0.1418, 0.0269],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,405][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ista] are: tensor([0.0140, 0.1095, 0.3230, 0.1037, 0.1384, 0.1604, 0.1288, 0.0222],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,408][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ista] are: tensor([0.0204, 0.1810, 0.4065, 0.0801, 0.0280, 0.0736, 0.1988, 0.0116],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,411][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ista] are: tensor([0.0834, 0.2877, 0.2456, 0.1136, 0.0713, 0.0968, 0.0919, 0.0098],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,415][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ista] are: tensor([0.0019, 0.1347, 0.0704, 0.0312, 0.0800, 0.0931, 0.5070, 0.0816],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,419][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.9618, 0.0013, 0.0013, 0.0067, 0.0045, 0.0030, 0.0102, 0.0113],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,424][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0460, 0.1251, 0.1219, 0.1131, 0.1004, 0.2182, 0.1777, 0.0976],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,427][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ista] are: tensor([1.2044e-04, 3.8383e-02, 2.6937e-01, 1.2927e-02, 1.7757e-02, 2.2638e-01,
        4.3112e-01, 3.9400e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,430][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ista] are: tensor([1.2889e-03, 1.4428e-04, 1.5707e-02, 7.5093e-01, 1.3039e-02, 1.6270e-01,
        5.4663e-02, 1.5242e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,433][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ista] are: tensor([1.4988e-04, 2.1417e-02, 3.0971e-02, 6.2464e-02, 8.0430e-02, 1.6423e-01,
        4.2721e-01, 2.1313e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:49,435][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.0003, 0.0919, 0.0653, 0.0276, 0.2429, 0.1221, 0.2497, 0.0300, 0.1702],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,436][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ de] are: tensor([1.2078e-04, 3.2457e-02, 8.0198e-02, 7.7524e-02, 1.4297e-01, 1.6897e-01,
        3.4086e-01, 9.1510e-02, 6.5393e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,437][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ de] are: tensor([0.0088, 0.1526, 0.1765, 0.1060, 0.1370, 0.1057, 0.1936, 0.0444, 0.0755],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,438][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ de] are: tensor([0.0129, 0.0639, 0.1693, 0.1025, 0.1542, 0.2069, 0.1682, 0.0469, 0.0752],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,440][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ de] are: tensor([0.0467, 0.0688, 0.1708, 0.0846, 0.0564, 0.0894, 0.1938, 0.0352, 0.2543],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,443][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0257, 0.3370, 0.3188, 0.1003, 0.0619, 0.0667, 0.0552, 0.0076, 0.0267],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,447][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ de] are: tensor([0.0038, 0.0681, 0.0418, 0.0288, 0.0796, 0.0986, 0.3380, 0.1421, 0.1993],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,450][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ de] are: tensor([9.7148e-01, 4.8896e-04, 7.7963e-04, 3.6757e-03, 3.8164e-03, 1.9306e-03,
        9.6945e-03, 4.7480e-03, 3.3903e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,454][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0789, 0.0308, 0.0753, 0.0411, 0.0686, 0.1362, 0.1954, 0.1423, 0.2314],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,459][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0019, 0.1371, 0.2800, 0.0210, 0.0326, 0.3371, 0.1458, 0.0034, 0.0412],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,464][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0017, 0.0006, 0.0207, 0.5921, 0.0250, 0.2438, 0.0882, 0.0061, 0.0218],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,467][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.0008, 0.0107, 0.0218, 0.0338, 0.0962, 0.1114, 0.2180, 0.2881, 0.2192],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:49,468][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.0033, 0.1802, 0.1121, 0.0647, 0.1419, 0.1117, 0.1229, 0.0276, 0.1263,
        0.1093], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,469][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0018, 0.0574, 0.1089, 0.1077, 0.1247, 0.1376, 0.2285, 0.0899, 0.0787,
        0.0648], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,470][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ An] are: tensor([0.0333, 0.1756, 0.1705, 0.1189, 0.1247, 0.0919, 0.0916, 0.0407, 0.0686,
        0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,472][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0269, 0.0809, 0.1675, 0.0742, 0.1411, 0.1404, 0.1344, 0.0442, 0.0818,
        0.1085], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,475][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ An] are: tensor([0.0457, 0.1101, 0.1973, 0.0808, 0.0534, 0.0730, 0.1641, 0.0222, 0.1724,
        0.0810], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,480][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ An] are: tensor([0.0383, 0.4010, 0.2476, 0.0900, 0.0438, 0.0584, 0.0475, 0.0101, 0.0244,
        0.0388], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,484][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0129, 0.0917, 0.0627, 0.0347, 0.0800, 0.1040, 0.2751, 0.1096, 0.1225,
        0.1067], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,489][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.9119, 0.0020, 0.0051, 0.0144, 0.0182, 0.0073, 0.0204, 0.0119, 0.0043,
        0.0044], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,494][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.1288, 0.0432, 0.0588, 0.0467, 0.0781, 0.1357, 0.1485, 0.1307, 0.1517,
        0.0778], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,498][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0102, 0.1677, 0.2790, 0.0521, 0.0394, 0.2640, 0.1074, 0.0051, 0.0416,
        0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,500][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0252, 0.0030, 0.0344, 0.3326, 0.0382, 0.2798, 0.1086, 0.0138, 0.0552,
        0.1090], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,500][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.0116, 0.0134, 0.0289, 0.0322, 0.1104, 0.1028, 0.1959, 0.2392, 0.1979,
        0.0675], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:49,501][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.0020, 0.1332, 0.0850, 0.0517, 0.1707, 0.1229, 0.1459, 0.0210, 0.0987,
        0.1214, 0.0473], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,502][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0007, 0.0523, 0.0868, 0.0592, 0.1372, 0.1548, 0.2584, 0.0568, 0.0527,
        0.0616, 0.0794], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,505][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [za] are: tensor([0.0116, 0.1778, 0.1547, 0.0985, 0.1232, 0.0789, 0.0933, 0.0318, 0.0641,
        0.0877, 0.0784], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,508][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [za] are: tensor([0.0325, 0.0807, 0.1355, 0.0728, 0.1036, 0.1495, 0.0986, 0.0359, 0.0548,
        0.1307, 0.1055], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,512][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [za] are: tensor([0.0186, 0.1241, 0.2373, 0.0931, 0.0558, 0.0890, 0.1513, 0.0192, 0.1101,
        0.0774, 0.0242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,516][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [za] are: tensor([0.0617, 0.3245, 0.2526, 0.0913, 0.0473, 0.0763, 0.0414, 0.0060, 0.0290,
        0.0344, 0.0355], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,521][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0066, 0.0645, 0.0442, 0.0269, 0.0719, 0.0969, 0.2767, 0.0964, 0.1318,
        0.1009, 0.0833], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,526][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.9271, 0.0014, 0.0019, 0.0084, 0.0088, 0.0052, 0.0119, 0.0144, 0.0049,
        0.0057, 0.0101], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,530][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.1858, 0.0569, 0.0333, 0.0328, 0.0494, 0.1173, 0.0978, 0.0877, 0.1706,
        0.0903, 0.0780], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,532][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0014, 0.0820, 0.3188, 0.0348, 0.0341, 0.2663, 0.1957, 0.0031, 0.0330,
        0.0248, 0.0061], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,533][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0121, 0.0020, 0.0388, 0.4637, 0.0289, 0.2210, 0.0919, 0.0117, 0.0378,
        0.0745, 0.0178], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,534][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0061, 0.0062, 0.0151, 0.0299, 0.1393, 0.0991, 0.1927, 0.1432, 0.1385,
        0.0673, 0.1625], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:49,534][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0005, 0.2384, 0.1197, 0.0624, 0.1589, 0.0879, 0.1427, 0.0113, 0.0540,
        0.0684, 0.0248, 0.0309], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,536][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ is] are: tensor([7.0421e-05, 6.3347e-02, 8.2959e-02, 1.0588e-01, 8.9693e-02, 1.1840e-01,
        2.6758e-01, 3.9416e-02, 4.5458e-02, 5.3157e-02, 6.5690e-02, 6.8358e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,539][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0095, 0.1757, 0.1843, 0.1174, 0.0818, 0.0817, 0.0970, 0.0210, 0.0455,
        0.0689, 0.0732, 0.0439], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,544][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0178, 0.0642, 0.1468, 0.0653, 0.0744, 0.1203, 0.0934, 0.0235, 0.0546,
        0.1024, 0.0846, 0.1526], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,548][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0222, 0.1368, 0.1869, 0.0652, 0.0229, 0.0575, 0.1325, 0.0101, 0.0945,
        0.0621, 0.0158, 0.1936], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,553][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0590, 0.4073, 0.2088, 0.0782, 0.0232, 0.0350, 0.0293, 0.0062, 0.0125,
        0.0332, 0.0485, 0.0587], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,558][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0012, 0.0763, 0.0400, 0.0312, 0.0632, 0.0885, 0.3169, 0.0729, 0.0738,
        0.0820, 0.0740, 0.0799], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,562][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.8664, 0.0010, 0.0030, 0.0138, 0.0206, 0.0060, 0.0165, 0.0124, 0.0039,
        0.0030, 0.0035, 0.0501], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,564][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0467, 0.0577, 0.0419, 0.0430, 0.0629, 0.1559, 0.1243, 0.0984, 0.1590,
        0.0781, 0.0573, 0.0746], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,565][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0006, 0.1576, 0.2629, 0.0335, 0.0177, 0.2699, 0.1375, 0.0024, 0.0452,
        0.0251, 0.0040, 0.0435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,566][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0017, 0.0006, 0.0205, 0.3071, 0.0115, 0.0948, 0.0404, 0.0030, 0.0071,
        0.0320, 0.0059, 0.4754], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,567][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0017, 0.0078, 0.0228, 0.0589, 0.0606, 0.0657, 0.1673, 0.1908, 0.1296,
        0.0294, 0.0679, 0.1977], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:49,569][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.0007, 0.1552, 0.0744, 0.0411, 0.1351, 0.0867, 0.1577, 0.0201, 0.1065,
        0.0913, 0.0391, 0.0430, 0.0490], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,571][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ a] are: tensor([7.5269e-05, 6.0992e-02, 7.0355e-02, 1.1216e-01, 7.8152e-02, 8.2737e-02,
        2.9167e-01, 3.6925e-02, 3.6239e-02, 6.4213e-02, 8.2383e-02, 4.7611e-02,
        3.6482e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,575][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ a] are: tensor([0.0053, 0.1670, 0.1803, 0.0962, 0.0782, 0.0760, 0.1044, 0.0187, 0.0379,
        0.0752, 0.0717, 0.0317, 0.0573], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,580][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ a] are: tensor([0.0072, 0.0705, 0.1174, 0.0449, 0.0920, 0.1008, 0.1147, 0.0333, 0.0600,
        0.1065, 0.0986, 0.0953, 0.0589], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,584][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ a] are: tensor([0.0244, 0.0795, 0.1796, 0.0589, 0.0169, 0.0350, 0.0605, 0.0071, 0.1227,
        0.0624, 0.0163, 0.2386, 0.0979], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,589][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ a] are: tensor([0.0110, 0.3449, 0.1948, 0.0954, 0.0418, 0.0614, 0.0336, 0.0094, 0.0161,
        0.0297, 0.0408, 0.0743, 0.0469], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,594][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ a] are: tensor([0.0011, 0.0595, 0.0285, 0.0245, 0.0557, 0.0700, 0.2737, 0.0905, 0.1227,
        0.0962, 0.0784, 0.0530, 0.0462], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,596][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.8166, 0.0033, 0.0043, 0.0170, 0.0212, 0.0092, 0.0264, 0.0171, 0.0071,
        0.0052, 0.0047, 0.0435, 0.0242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,597][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0166, 0.0560, 0.0671, 0.0649, 0.0668, 0.1234, 0.1448, 0.1071, 0.1106,
        0.0594, 0.0544, 0.0644, 0.0646], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,598][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0008, 0.1293, 0.1722, 0.0267, 0.0258, 0.3472, 0.1607, 0.0044, 0.0293,
        0.0275, 0.0067, 0.0347, 0.0347], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,599][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0047, 0.0012, 0.0142, 0.1909, 0.0186, 0.1360, 0.0668, 0.0063, 0.0199,
        0.0591, 0.0132, 0.3130, 0.1561], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,601][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0008, 0.0070, 0.0191, 0.0524, 0.0656, 0.0811, 0.1434, 0.1320, 0.1084,
        0.0686, 0.1323, 0.1619, 0.0275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:49,604][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.0005, 0.1295, 0.1023, 0.0446, 0.1563, 0.0851, 0.1188, 0.0148, 0.0783,
        0.0881, 0.0481, 0.0488, 0.0543, 0.0306], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,606][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([6.5515e-05, 3.8594e-02, 9.5641e-02, 7.8367e-02, 9.4247e-02, 9.0567e-02,
        2.3322e-01, 3.5288e-02, 6.0194e-02, 5.9370e-02, 9.7365e-02, 7.1513e-02,
        3.8341e-02, 7.2313e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,611][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([0.0128, 0.1214, 0.2345, 0.1236, 0.0529, 0.0781, 0.0655, 0.0168, 0.0307,
        0.0565, 0.0708, 0.0398, 0.0626, 0.0340], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,616][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([0.0098, 0.0385, 0.1787, 0.0445, 0.0732, 0.0984, 0.0712, 0.0239, 0.0384,
        0.0673, 0.1156, 0.1409, 0.0712, 0.0284], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,620][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([0.0410, 0.0296, 0.1262, 0.0569, 0.0184, 0.0432, 0.0619, 0.0081, 0.1094,
        0.0463, 0.0160, 0.3126, 0.0952, 0.0352], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,625][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([0.0907, 0.1869, 0.1458, 0.0625, 0.0403, 0.0497, 0.0307, 0.0084, 0.0350,
        0.0501, 0.0455, 0.1025, 0.0964, 0.0555], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,628][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([0.0005, 0.0771, 0.0422, 0.0206, 0.0478, 0.0639, 0.2286, 0.0601, 0.0980,
        0.1016, 0.0800, 0.0658, 0.0635, 0.0504], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,629][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([9.1224e-01, 7.9604e-04, 2.0702e-03, 8.9856e-03, 9.8387e-03, 3.3666e-03,
        1.1245e-02, 6.8831e-03, 1.6382e-03, 1.9691e-03, 1.8163e-03, 1.9197e-02,
        9.6445e-03, 1.0307e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,630][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0418, 0.0545, 0.0638, 0.0539, 0.0534, 0.1339, 0.1074, 0.0882, 0.1357,
        0.0510, 0.0432, 0.0693, 0.0768, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,631][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0014, 0.0950, 0.2103, 0.0210, 0.0188, 0.3000, 0.1699, 0.0029, 0.0337,
        0.0322, 0.0076, 0.0449, 0.0499, 0.0126], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,633][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([2.3467e-03, 2.7729e-04, 1.2026e-02, 2.2480e-01, 9.8663e-03, 1.3510e-01,
        5.6609e-02, 2.4080e-03, 1.1199e-02, 3.1630e-02, 3.8248e-03, 3.7194e-01,
        1.3281e-01, 5.1596e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,636][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0008, 0.0095, 0.0317, 0.0573, 0.0570, 0.0937, 0.0886, 0.0884, 0.1466,
        0.0720, 0.1459, 0.1680, 0.0368, 0.0036], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:49,641][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.0005, 0.0702, 0.0876, 0.0242, 0.0978, 0.0994, 0.3015, 0.0119, 0.0593,
        0.0693, 0.0312, 0.0539, 0.0463, 0.0269, 0.0201], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,643][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ different] are: tensor([2.0030e-04, 3.2724e-02, 7.0241e-02, 4.0724e-02, 6.5972e-02, 1.1683e-01,
        3.7381e-01, 2.3175e-02, 4.5252e-02, 4.8453e-02, 6.7162e-02, 5.1060e-02,
        2.9605e-02, 7.0333e-03, 2.7768e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,648][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ different] are: tensor([0.0269, 0.1180, 0.1831, 0.0834, 0.0696, 0.0649, 0.1137, 0.0177, 0.0341,
        0.0579, 0.0561, 0.0356, 0.0496, 0.0435, 0.0460], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,652][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ different] are: tensor([0.0076, 0.0386, 0.1245, 0.0483, 0.0568, 0.1011, 0.1566, 0.0190, 0.0356,
        0.0865, 0.0909, 0.1071, 0.0718, 0.0273, 0.0282], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,657][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ different] are: tensor([0.0216, 0.0554, 0.1875, 0.0313, 0.0145, 0.0515, 0.1338, 0.0065, 0.0647,
        0.0495, 0.0172, 0.1808, 0.0944, 0.0749, 0.0163], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,661][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ different] are: tensor([0.0614, 0.1428, 0.1293, 0.0338, 0.0370, 0.0434, 0.0445, 0.0118, 0.0555,
        0.0609, 0.0610, 0.1183, 0.1192, 0.0665, 0.0146], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,661][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ different] are: tensor([0.0009, 0.0394, 0.0322, 0.0162, 0.0470, 0.0737, 0.3025, 0.0427, 0.0981,
        0.0940, 0.0715, 0.0619, 0.0507, 0.0421, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,662][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.8562, 0.0020, 0.0030, 0.0104, 0.0128, 0.0050, 0.0185, 0.0102, 0.0027,
        0.0038, 0.0038, 0.0307, 0.0162, 0.0126, 0.0120], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,663][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.0552, 0.0457, 0.0667, 0.0531, 0.0581, 0.1155, 0.1149, 0.0762, 0.0993,
        0.0531, 0.0552, 0.0673, 0.0649, 0.0262, 0.0486], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,666][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ different] are: tensor([0.0062, 0.0504, 0.2421, 0.0218, 0.0136, 0.2349, 0.2220, 0.0038, 0.0350,
        0.0244, 0.0098, 0.0595, 0.0535, 0.0192, 0.0037], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,670][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0070, 0.0010, 0.0202, 0.1637, 0.0167, 0.1256, 0.0686, 0.0037, 0.0174,
        0.0442, 0.0068, 0.3575, 0.1399, 0.0115, 0.0162], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,675][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0024, 0.0067, 0.0258, 0.0170, 0.0724, 0.0517, 0.1553, 0.1012, 0.1933,
        0.0311, 0.1049, 0.1949, 0.0247, 0.0048, 0.0136], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:49,679][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.0002, 0.1467, 0.1138, 0.0326, 0.1128, 0.0821, 0.2044, 0.0125, 0.0513,
        0.0508, 0.0275, 0.0289, 0.0411, 0.0271, 0.0197, 0.0486],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,682][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ from] are: tensor([1.0797e-04, 4.6605e-02, 8.0941e-02, 9.4046e-02, 7.6413e-02, 1.1199e-01,
        2.6174e-01, 2.7659e-02, 3.3362e-02, 4.3036e-02, 6.8361e-02, 5.0708e-02,
        3.0593e-02, 6.6005e-03, 2.5282e-02, 4.2556e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,687][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ from] are: tensor([0.0082, 0.1357, 0.1652, 0.0973, 0.0810, 0.0824, 0.0819, 0.0162, 0.0317,
        0.0464, 0.0460, 0.0298, 0.0450, 0.0361, 0.0439, 0.0531],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,692][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ from] are: tensor([0.0064, 0.0491, 0.0987, 0.0428, 0.0798, 0.1314, 0.1343, 0.0249, 0.0454,
        0.0740, 0.0784, 0.0905, 0.0559, 0.0242, 0.0302, 0.0340],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,695][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ from] are: tensor([0.0106, 0.1000, 0.2006, 0.0428, 0.0154, 0.0367, 0.0851, 0.0061, 0.0426,
        0.0353, 0.0114, 0.1120, 0.0627, 0.0747, 0.0131, 0.1510],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,696][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ from] are: tensor([0.0952, 0.2287, 0.1490, 0.0572, 0.0278, 0.0433, 0.0272, 0.0065, 0.0177,
        0.0417, 0.0472, 0.0629, 0.0684, 0.0363, 0.0108, 0.0802],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,697][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ from] are: tensor([0.0007, 0.0565, 0.0381, 0.0193, 0.0454, 0.0868, 0.2872, 0.0519, 0.0763,
        0.0786, 0.0658, 0.0538, 0.0437, 0.0371, 0.0202, 0.0385],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,698][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ from] are: tensor([0.8173, 0.0013, 0.0041, 0.0121, 0.0136, 0.0052, 0.0173, 0.0109, 0.0035,
        0.0031, 0.0038, 0.0302, 0.0146, 0.0072, 0.0081, 0.0476],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,701][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0307, 0.0501, 0.0604, 0.0427, 0.0489, 0.1472, 0.1000, 0.0650, 0.0902,
        0.0476, 0.0480, 0.0647, 0.0643, 0.0244, 0.0446, 0.0712],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,704][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ from] are: tensor([0.0052, 0.1542, 0.2142, 0.0352, 0.0191, 0.2555, 0.1108, 0.0028, 0.0281,
        0.0234, 0.0077, 0.0238, 0.0278, 0.0126, 0.0035, 0.0761],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,709][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0055, 0.0013, 0.0226, 0.2045, 0.0179, 0.1277, 0.0453, 0.0043, 0.0179,
        0.0458, 0.0089, 0.2645, 0.1474, 0.0159, 0.0187, 0.0516],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,714][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0024, 0.0109, 0.0195, 0.0370, 0.0709, 0.1022, 0.2447, 0.1092, 0.0991,
        0.0349, 0.0902, 0.1010, 0.0264, 0.0053, 0.0136, 0.0328],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:49,716][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([1.8575e-04, 2.1179e-01, 8.3950e-02, 2.7749e-02, 1.1291e-01, 6.5414e-02,
        1.5777e-01, 9.5122e-03, 6.1316e-02, 4.5399e-02, 2.1110e-02, 2.2764e-02,
        3.3681e-02, 2.8471e-02, 1.5609e-02, 4.5232e-02, 5.7142e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,719][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([4.1304e-05, 5.8540e-02, 6.6928e-02, 9.1124e-02, 6.5071e-02, 9.5963e-02,
        3.1100e-01, 2.7429e-02, 2.5165e-02, 4.1571e-02, 7.3026e-02, 3.1615e-02,
        2.3010e-02, 8.6756e-03, 2.8410e-02, 2.5306e-02, 2.7129e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,724][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0042, 0.1708, 0.1893, 0.1022, 0.0657, 0.0637, 0.0755, 0.0098, 0.0195,
        0.0375, 0.0294, 0.0234, 0.0391, 0.0364, 0.0362, 0.0405, 0.0568],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,727][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0048, 0.0792, 0.1104, 0.0428, 0.0956, 0.1269, 0.1451, 0.0213, 0.0335,
        0.0610, 0.0684, 0.0575, 0.0446, 0.0265, 0.0258, 0.0204, 0.0363],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,728][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0265, 0.0959, 0.1847, 0.0402, 0.0108, 0.0312, 0.0510, 0.0038, 0.0460,
        0.0314, 0.0088, 0.1006, 0.0529, 0.0580, 0.0057, 0.1281, 0.1245],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,729][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0387, 0.2467, 0.1934, 0.0761, 0.0233, 0.0479, 0.0260, 0.0045, 0.0104,
        0.0248, 0.0321, 0.0498, 0.0454, 0.0399, 0.0086, 0.0699, 0.0626],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,730][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0006, 0.0775, 0.0232, 0.0162, 0.0424, 0.0693, 0.2856, 0.0530, 0.1009,
        0.0697, 0.0568, 0.0362, 0.0325, 0.0383, 0.0142, 0.0336, 0.0498],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,733][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.8631, 0.0010, 0.0027, 0.0092, 0.0101, 0.0030, 0.0106, 0.0060, 0.0022,
        0.0015, 0.0017, 0.0146, 0.0090, 0.0050, 0.0053, 0.0283, 0.0265],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,737][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0110, 0.0508, 0.0510, 0.0455, 0.0447, 0.1542, 0.1210, 0.0648, 0.0928,
        0.0404, 0.0401, 0.0522, 0.0502, 0.0219, 0.0426, 0.0537, 0.0632],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,741][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0029, 0.1594, 0.2058, 0.0193, 0.0181, 0.3476, 0.0884, 0.0009, 0.0121,
        0.0140, 0.0028, 0.0113, 0.0180, 0.0040, 0.0008, 0.0334, 0.0614],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,746][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0032, 0.0006, 0.0167, 0.1849, 0.0147, 0.1069, 0.0395, 0.0036, 0.0144,
        0.0309, 0.0069, 0.2578, 0.0778, 0.0124, 0.0110, 0.0392, 0.1796],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,751][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0022, 0.0185, 0.0442, 0.0774, 0.0540, 0.1115, 0.1754, 0.1161, 0.0858,
        0.0405, 0.0747, 0.0987, 0.0261, 0.0046, 0.0141, 0.0354, 0.0209],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:49,755][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0002, 0.1161, 0.0431, 0.0316, 0.1076, 0.0681, 0.1119, 0.0143, 0.0622,
        0.0947, 0.0389, 0.0377, 0.0436, 0.0274, 0.0218, 0.0640, 0.0878, 0.0291],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,758][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ language] are: tensor([2.8291e-05, 4.7635e-02, 5.6826e-02, 8.4320e-02, 1.0113e-01, 8.5102e-02,
        1.9266e-01, 3.1270e-02, 3.4623e-02, 6.0615e-02, 7.9898e-02, 5.9022e-02,
        3.4821e-02, 1.1421e-02, 3.8808e-02, 2.9880e-02, 3.6185e-02, 1.5762e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,759][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0017, 0.1215, 0.1394, 0.0940, 0.0546, 0.0713, 0.0614, 0.0180, 0.0264,
        0.0432, 0.0404, 0.0318, 0.0426, 0.0391, 0.0602, 0.0506, 0.0541, 0.0495],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,760][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0026, 0.0517, 0.1110, 0.0528, 0.0663, 0.0818, 0.0834, 0.0240, 0.0362,
        0.0731, 0.0900, 0.0852, 0.0575, 0.0379, 0.0352, 0.0361, 0.0457, 0.0295],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,761][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0144, 0.0375, 0.0963, 0.0486, 0.0152, 0.0313, 0.0393, 0.0073, 0.0672,
        0.0412, 0.0142, 0.1855, 0.0608, 0.0514, 0.0079, 0.1437, 0.1228, 0.0153],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,762][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0824, 0.1717, 0.1358, 0.0499, 0.0312, 0.0459, 0.0404, 0.0074, 0.0221,
        0.0345, 0.0349, 0.0480, 0.0531, 0.0345, 0.0098, 0.0649, 0.0829, 0.0505],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,765][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0005, 0.0636, 0.0311, 0.0255, 0.0501, 0.0710, 0.1677, 0.0413, 0.0871,
        0.0912, 0.0648, 0.0503, 0.0489, 0.0509, 0.0240, 0.0387, 0.0636, 0.0297],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,769][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.6729, 0.0029, 0.0042, 0.0166, 0.0156, 0.0076, 0.0227, 0.0161, 0.0061,
        0.0056, 0.0051, 0.0501, 0.0245, 0.0173, 0.0130, 0.0570, 0.0535, 0.0092],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,773][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0082, 0.0633, 0.0545, 0.0679, 0.0481, 0.0873, 0.0752, 0.0593, 0.0845,
        0.0384, 0.0475, 0.0590, 0.0632, 0.0254, 0.0489, 0.0574, 0.0604, 0.0515],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,778][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0045, 0.0980, 0.1414, 0.0225, 0.0201, 0.2419, 0.0844, 0.0053, 0.0285,
        0.0334, 0.0127, 0.0321, 0.0402, 0.0104, 0.0034, 0.0691, 0.1088, 0.0432],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,783][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0010, 0.0003, 0.0135, 0.2080, 0.0088, 0.0983, 0.0373, 0.0019, 0.0068,
        0.0165, 0.0030, 0.3224, 0.0537, 0.0058, 0.0055, 0.0292, 0.1873, 0.0006],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,787][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0002, 0.0146, 0.0173, 0.0445, 0.0564, 0.1213, 0.1032, 0.0903, 0.0899,
        0.0783, 0.1559, 0.1113, 0.0317, 0.0034, 0.0198, 0.0347, 0.0165, 0.0108],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:49,791][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.0004, 0.1233, 0.1023, 0.0298, 0.1149, 0.0555, 0.1465, 0.0100, 0.0536,
        0.0460, 0.0171, 0.0217, 0.0339, 0.0246, 0.0161, 0.0593, 0.0775, 0.0212,
        0.0462], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,792][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ used] are: tensor([6.1546e-05, 3.4437e-02, 8.7412e-02, 1.0093e-01, 8.2624e-02, 5.9963e-02,
        2.8867e-01, 2.1063e-02, 3.4713e-02, 3.5698e-02, 4.6637e-02, 4.5293e-02,
        2.8430e-02, 7.2538e-03, 2.2323e-02, 2.8439e-02, 3.9422e-02, 8.7499e-03,
        2.7887e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,793][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ used] are: tensor([0.0040, 0.0940, 0.2455, 0.0720, 0.0487, 0.0483, 0.0571, 0.0089, 0.0211,
        0.0363, 0.0247, 0.0258, 0.0416, 0.0296, 0.0375, 0.0413, 0.0734, 0.0363,
        0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,794][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ used] are: tensor([0.0094, 0.0234, 0.1159, 0.0372, 0.0500, 0.0917, 0.0721, 0.0167, 0.0386,
        0.0570, 0.0628, 0.1222, 0.0591, 0.0318, 0.0314, 0.0317, 0.0656, 0.0336,
        0.0500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,797][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ used] are: tensor([0.0393, 0.0376, 0.1125, 0.0274, 0.0114, 0.0233, 0.0420, 0.0045, 0.0534,
        0.0148, 0.0070, 0.1341, 0.0424, 0.0380, 0.0048, 0.1112, 0.1299, 0.0152,
        0.1512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,801][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ used] are: tensor([0.2114, 0.1821, 0.1123, 0.0429, 0.0129, 0.0299, 0.0104, 0.0025, 0.0068,
        0.0165, 0.0182, 0.0276, 0.0376, 0.0144, 0.0038, 0.0401, 0.0557, 0.0449,
        0.1300], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,805][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ used] are: tensor([0.0009, 0.0420, 0.0439, 0.0188, 0.0453, 0.0632, 0.2468, 0.0442, 0.0794,
        0.0662, 0.0347, 0.0460, 0.0478, 0.0435, 0.0174, 0.0417, 0.0689, 0.0222,
        0.0272], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,808][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ used] are: tensor([9.0165e-01, 3.3322e-04, 1.2420e-03, 4.2707e-03, 7.1635e-03, 1.8073e-03,
        6.8525e-03, 4.1883e-03, 1.5163e-03, 9.0165e-04, 9.2190e-04, 1.5286e-02,
        5.0288e-03, 3.1721e-03, 3.3871e-03, 1.6222e-02, 1.2763e-02, 1.4228e-03,
        1.1871e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,813][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0119, 0.0455, 0.0946, 0.0991, 0.0502, 0.0844, 0.0467, 0.0671, 0.0817,
        0.0235, 0.0282, 0.0562, 0.0575, 0.0207, 0.0300, 0.0631, 0.0631, 0.0364,
        0.0401], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,818][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ used] are: tensor([0.0139, 0.1572, 0.1208, 0.0187, 0.0059, 0.1816, 0.0207, 0.0008, 0.0189,
        0.0141, 0.0021, 0.0081, 0.0143, 0.0023, 0.0004, 0.0328, 0.0441, 0.0311,
        0.3121], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,821][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ used] are: tensor([1.8340e-03, 1.0279e-04, 8.7890e-03, 2.6048e-01, 5.5741e-03, 6.8950e-02,
        2.3214e-02, 9.3245e-04, 4.5478e-03, 1.6223e-02, 1.3683e-03, 2.1938e-01,
        1.0338e-01, 3.2138e-03, 5.6319e-03, 2.7760e-02, 2.4334e-01, 2.1061e-04,
        5.0733e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,824][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0041, 0.0075, 0.0144, 0.0276, 0.0469, 0.0592, 0.1301, 0.1680, 0.1776,
        0.0355, 0.0910, 0.1478, 0.0183, 0.0032, 0.0091, 0.0246, 0.0086, 0.0070,
        0.0195], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:49,825][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.0002, 0.0838, 0.0521, 0.0250, 0.1244, 0.0521, 0.1562, 0.0132, 0.0600,
        0.0508, 0.0321, 0.0269, 0.0391, 0.0302, 0.0169, 0.0617, 0.0840, 0.0246,
        0.0435, 0.0231], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,826][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ by] are: tensor([2.0315e-05, 4.5253e-02, 6.5045e-02, 8.4887e-02, 6.1803e-02, 7.6813e-02,
        2.6176e-01, 2.9563e-02, 2.6054e-02, 4.7222e-02, 7.5622e-02, 3.1109e-02,
        2.8012e-02, 7.3717e-03, 3.6317e-02, 3.0104e-02, 3.0570e-02, 1.2851e-02,
        2.5609e-02, 2.4014e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,827][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ by] are: tensor([0.0021, 0.0774, 0.1178, 0.0677, 0.0570, 0.0513, 0.0724, 0.0151, 0.0232,
        0.0501, 0.0522, 0.0205, 0.0470, 0.0381, 0.0638, 0.0508, 0.0773, 0.0376,
        0.0436, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,830][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ by] are: tensor([0.0049, 0.0316, 0.0755, 0.0308, 0.0577, 0.0925, 0.0994, 0.0238, 0.0353,
        0.0684, 0.0794, 0.0840, 0.0568, 0.0332, 0.0368, 0.0298, 0.0516, 0.0342,
        0.0498, 0.0246], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,833][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ by] are: tensor([0.0133, 0.0310, 0.0865, 0.0295, 0.0090, 0.0182, 0.0306, 0.0058, 0.0567,
        0.0242, 0.0112, 0.1186, 0.0447, 0.0327, 0.0053, 0.0855, 0.1111, 0.0115,
        0.1756, 0.0989], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,838][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ by] are: tensor([0.1553, 0.1723, 0.0875, 0.0494, 0.0147, 0.0254, 0.0093, 0.0041, 0.0086,
        0.0161, 0.0210, 0.0199, 0.0264, 0.0213, 0.0067, 0.0343, 0.0419, 0.0562,
        0.1134, 0.1162], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,842][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ by] are: tensor([0.0004, 0.0432, 0.0269, 0.0227, 0.0365, 0.0637, 0.2295, 0.0528, 0.0784,
        0.0824, 0.0580, 0.0370, 0.0497, 0.0358, 0.0191, 0.0375, 0.0644, 0.0244,
        0.0200, 0.0176], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,845][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ by] are: tensor([7.6142e-01, 6.4999e-04, 2.6410e-03, 1.2638e-02, 1.5634e-02, 3.8351e-03,
        2.0686e-02, 1.0098e-02, 2.9473e-03, 2.2538e-03, 2.7370e-03, 2.4419e-02,
        1.1092e-02, 6.8463e-03, 7.2844e-03, 2.8464e-02, 3.3093e-02, 3.0066e-03,
        2.1762e-02, 2.8495e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,850][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0089, 0.0392, 0.0362, 0.0463, 0.0457, 0.1215, 0.0977, 0.0772, 0.0837,
        0.0374, 0.0396, 0.0421, 0.0573, 0.0245, 0.0419, 0.0503, 0.0471, 0.0373,
        0.0313, 0.0350], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,854][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0032, 0.1098, 0.0719, 0.0121, 0.0074, 0.1466, 0.0322, 0.0014, 0.0161,
        0.0136, 0.0031, 0.0069, 0.0126, 0.0031, 0.0009, 0.0277, 0.0414, 0.0290,
        0.2690, 0.1919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,856][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ by] are: tensor([1.2563e-03, 1.8675e-04, 6.8202e-03, 1.7678e-01, 7.1218e-03, 7.8042e-02,
        2.3156e-02, 1.3969e-03, 5.4121e-03, 2.2024e-02, 2.1781e-03, 2.1021e-01,
        1.0752e-01, 6.2091e-03, 8.2255e-03, 3.0311e-02, 2.6683e-01, 4.0785e-04,
        5.3068e-03, 4.0611e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,857][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0016, 0.0087, 0.0132, 0.0584, 0.0588, 0.0956, 0.1795, 0.1269, 0.0829,
        0.0470, 0.0997, 0.0810, 0.0231, 0.0037, 0.0098, 0.0288, 0.0119, 0.0093,
        0.0181, 0.0418], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:49,858][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0002, 0.1643, 0.1004, 0.0323, 0.1055, 0.0625, 0.1106, 0.0062, 0.0388,
        0.0338, 0.0152, 0.0172, 0.0263, 0.0196, 0.0081, 0.0296, 0.0396, 0.0195,
        0.0183, 0.0094, 0.1423], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,859][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([4.3003e-05, 5.7970e-02, 9.0393e-02, 1.4438e-01, 6.6632e-02, 7.7125e-02,
        2.3709e-01, 1.8437e-02, 2.0614e-02, 3.6056e-02, 5.3213e-02, 2.9410e-02,
        2.1943e-02, 6.7114e-03, 2.2504e-02, 1.9607e-02, 2.0341e-02, 1.1552e-02,
        1.4945e-02, 1.7517e-02, 3.3514e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,862][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0052, 0.1418, 0.1880, 0.1027, 0.0666, 0.0514, 0.0571, 0.0090, 0.0180,
        0.0310, 0.0236, 0.0177, 0.0301, 0.0256, 0.0295, 0.0285, 0.0461, 0.0407,
        0.0235, 0.0203, 0.0436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,866][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0080, 0.0623, 0.1080, 0.0436, 0.0795, 0.1128, 0.1009, 0.0184, 0.0330,
        0.0583, 0.0537, 0.0642, 0.0429, 0.0227, 0.0227, 0.0174, 0.0323, 0.0353,
        0.0230, 0.0152, 0.0458], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,871][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0359, 0.0584, 0.1384, 0.0300, 0.0063, 0.0189, 0.0218, 0.0017, 0.0244,
        0.0128, 0.0037, 0.0590, 0.0291, 0.0255, 0.0018, 0.0527, 0.0583, 0.0124,
        0.1172, 0.0628, 0.2289], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,875][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0482, 0.1939, 0.1394, 0.0563, 0.0119, 0.0279, 0.0099, 0.0017, 0.0040,
        0.0123, 0.0149, 0.0207, 0.0219, 0.0167, 0.0035, 0.0300, 0.0256, 0.0229,
        0.1235, 0.1297, 0.0852], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,880][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0007, 0.0756, 0.0317, 0.0257, 0.0408, 0.0663, 0.2088, 0.0394, 0.0779,
        0.0641, 0.0456, 0.0320, 0.0369, 0.0346, 0.0099, 0.0284, 0.0455, 0.0243,
        0.0119, 0.0118, 0.0881], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,883][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.5922e-01, 5.0962e-04, 1.4683e-03, 8.3129e-03, 9.0877e-03, 2.2367e-03,
        6.4714e-03, 6.1626e-03, 1.5741e-03, 9.3638e-04, 1.2027e-03, 1.4967e-02,
        7.1933e-03, 3.1669e-03, 3.0163e-03, 1.6464e-02, 1.6896e-02, 1.6176e-03,
        1.1402e-02, 1.6481e-02, 1.1617e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,888][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0150, 0.0498, 0.0671, 0.0594, 0.0543, 0.1177, 0.0747, 0.0519, 0.0959,
        0.0365, 0.0283, 0.0442, 0.0430, 0.0200, 0.0356, 0.0421, 0.0473, 0.0349,
        0.0265, 0.0289, 0.0268], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,889][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([4.3119e-03, 1.2450e-01, 8.3991e-02, 7.3901e-03, 3.5568e-03, 1.0623e-01,
        9.4546e-03, 7.2094e-05, 1.9127e-03, 2.5163e-03, 3.3056e-04, 1.2061e-03,
        2.8168e-03, 4.1133e-04, 6.6041e-05, 5.9005e-03, 8.6403e-03, 1.0282e-02,
        1.3071e-01, 6.3709e-02, 4.3198e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,890][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0038, 0.0006, 0.0150, 0.1502, 0.0132, 0.0785, 0.0259, 0.0028, 0.0098,
        0.0207, 0.0051, 0.1940, 0.0580, 0.0098, 0.0080, 0.0268, 0.1240, 0.0008,
        0.0079, 0.0347, 0.2105], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:49,890][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0041, 0.0158, 0.0426, 0.0949, 0.0592, 0.1080, 0.1130, 0.1192, 0.0745,
        0.0317, 0.0582, 0.0991, 0.0233, 0.0022, 0.0077, 0.0246, 0.0142, 0.0094,
        0.0145, 0.0230, 0.0607], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,010][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:50,015][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,019][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,022][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,026][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,027][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,028][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,028][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,029][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,030][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,032][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,034][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,038][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,042][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0150, 0.9850], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,047][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0048, 0.9952], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,052][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.1490, 0.8510], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,056][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.3359, 0.6641], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,059][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.7618, 0.2382], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,060][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.4526, 0.5474], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,060][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0728, 0.9272], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,061][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.3748, 0.6252], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,062][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.2927, 0.7073], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,064][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.6411, 0.3589], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,068][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.6502, 0.3498], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,073][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0544, 0.9456], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,075][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([4.4509e-04, 7.6868e-01, 2.3087e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,078][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([2.3535e-04, 5.1733e-01, 4.8243e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,083][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0114, 0.4106, 0.5780], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,088][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0238, 0.2245, 0.7517], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,091][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.4461, 0.1806, 0.3733], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,092][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.1227, 0.5747, 0.3026], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,092][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0147, 0.6526, 0.3327], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,093][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0347, 0.4126, 0.5527], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,094][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0236, 0.2389, 0.7374], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,097][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0388, 0.7594, 0.2018], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,100][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0629, 0.1873, 0.7498], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,105][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0052, 0.2820, 0.7128], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:50,109][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0007, 0.5618, 0.2570, 0.1804], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,112][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([1.3593e-04, 2.9248e-01, 3.0817e-01, 3.9921e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,117][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.0346, 0.2856, 0.3599, 0.3199], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,121][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0410, 0.1665, 0.5560, 0.2364], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,123][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.1923, 0.2337, 0.4574, 0.1167], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,124][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.2112, 0.3201, 0.3591, 0.1096], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,125][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0096, 0.5218, 0.2675, 0.2011], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,125][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0362, 0.1380, 0.2977, 0.5282], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,128][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0844, 0.2355, 0.2392, 0.4409], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,130][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0383, 0.5234, 0.3222, 0.1161], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,135][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.1018, 0.0736, 0.3050, 0.5196], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,140][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0279, 0.0705, 0.1789, 0.7227], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:50,144][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.0015, 0.3264, 0.2017, 0.1142, 0.3561], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,149][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0003, 0.1907, 0.3158, 0.2354, 0.2578], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,154][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.0636, 0.2281, 0.2815, 0.2396, 0.1871], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,155][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.0895, 0.1008, 0.3848, 0.1989, 0.2260], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,156][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([0.2327, 0.1900, 0.3447, 0.1067, 0.1259], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,157][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.2015, 0.2559, 0.3332, 0.0996, 0.1098], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,158][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.0307, 0.3172, 0.2461, 0.1188, 0.2872], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,160][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.1047, 0.0721, 0.1827, 0.3053, 0.3352], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,163][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.1777, 0.0734, 0.1600, 0.1776, 0.4113], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,168][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0462, 0.2995, 0.4634, 0.1096, 0.0813], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,172][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.1809, 0.0740, 0.1945, 0.2143, 0.3364], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,177][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.0072, 0.0557, 0.1184, 0.2937, 0.5251], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:50,182][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.0021, 0.3334, 0.1746, 0.1217, 0.2162, 0.1520], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,186][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0011, 0.1481, 0.1811, 0.2188, 0.2501, 0.2009], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,188][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.0343, 0.2222, 0.2510, 0.2810, 0.1239, 0.0877], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,189][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0507, 0.1425, 0.2894, 0.1744, 0.2015, 0.1415], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,189][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([0.2937, 0.1932, 0.3073, 0.0850, 0.0399, 0.0809], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,190][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([0.2653, 0.2681, 0.2514, 0.0637, 0.0353, 0.1162], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,192][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0330, 0.2498, 0.1643, 0.0984, 0.1657, 0.2889], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,195][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0469, 0.0883, 0.2175, 0.1897, 0.1490, 0.3086], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,200][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([0.3488, 0.0816, 0.0877, 0.1370, 0.1310, 0.2140], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,204][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0221, 0.4416, 0.2412, 0.0700, 0.0333, 0.1918], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,209][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([0.0711, 0.0350, 0.1005, 0.2139, 0.1615, 0.4180], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,214][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([0.0035, 0.0593, 0.0870, 0.3060, 0.2791, 0.2650], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:50,217][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([3.9986e-04, 1.6659e-01, 1.1056e-01, 4.8153e-02, 1.6812e-01, 8.4387e-02,
        4.2178e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,220][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([1.2852e-04, 5.9641e-02, 6.4366e-02, 7.0505e-02, 4.3764e-02, 1.3930e-01,
        6.2229e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,221][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([0.0167, 0.1857, 0.2843, 0.2050, 0.0860, 0.0740, 0.1484],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,221][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([0.0243, 0.0862, 0.2456, 0.1064, 0.1443, 0.1723, 0.2208],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,222][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([0.3065, 0.1168, 0.2197, 0.0789, 0.0322, 0.0661, 0.1798],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,223][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([0.0702, 0.1936, 0.4212, 0.0477, 0.0447, 0.1926, 0.0300],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,225][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([0.0030, 0.0895, 0.0409, 0.0176, 0.0474, 0.0756, 0.7260],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,228][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0082, 0.0138, 0.0355, 0.0242, 0.0225, 0.0280, 0.8677],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,233][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.0419, 0.0519, 0.0866, 0.0821, 0.1029, 0.2157, 0.4189],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,238][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0029, 0.2468, 0.2958, 0.0245, 0.0264, 0.3266, 0.0769],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,242][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([0.0436, 0.0111, 0.0437, 0.0968, 0.0703, 0.4579, 0.2766],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,247][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.0012, 0.0227, 0.0416, 0.1219, 0.1026, 0.1343, 0.5757],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:50,250][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([9.9378e-05, 1.5541e-01, 8.3226e-02, 3.6783e-02, 2.3849e-01, 1.2958e-01,
        3.3925e-01, 1.7157e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,253][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([1.5139e-05, 4.7883e-02, 8.6508e-02, 8.3185e-02, 1.1058e-01, 1.7929e-01,
        4.5139e-01, 4.1150e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,253][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([0.0019, 0.2176, 0.2282, 0.1812, 0.1154, 0.0870, 0.1418, 0.0269],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,254][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([0.0140, 0.1095, 0.3230, 0.1037, 0.1384, 0.1604, 0.1288, 0.0222],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,255][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([0.0204, 0.1810, 0.4065, 0.0801, 0.0280, 0.0736, 0.1988, 0.0116],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,257][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([0.0024, 0.0708, 0.3702, 0.0346, 0.0440, 0.2399, 0.2353, 0.0027],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,260][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([0.0012, 0.0845, 0.0400, 0.0194, 0.0574, 0.0997, 0.6094, 0.0884],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,265][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0079, 0.0350, 0.0664, 0.0475, 0.0354, 0.0803, 0.6639, 0.0636],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,269][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.0460, 0.1251, 0.1219, 0.1131, 0.1004, 0.2182, 0.1777, 0.0976],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,272][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([1.2044e-04, 3.8383e-02, 2.6937e-01, 1.2927e-02, 1.7757e-02, 2.2638e-01,
        4.3112e-01, 3.9400e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,277][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([0.0081, 0.0108, 0.0535, 0.0666, 0.1036, 0.5463, 0.1917, 0.0195],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,280][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([1.4988e-04, 2.1417e-02, 3.0971e-02, 6.2464e-02, 8.0430e-02, 1.6423e-01,
        4.2721e-01, 2.1313e-01], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:50,285][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.0003, 0.0919, 0.0653, 0.0276, 0.2429, 0.1221, 0.2497, 0.0300, 0.1702],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,285][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([1.2078e-04, 3.2457e-02, 8.0198e-02, 7.7524e-02, 1.4297e-01, 1.6897e-01,
        3.4086e-01, 9.1510e-02, 6.5393e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,286][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([0.0088, 0.1526, 0.1765, 0.1060, 0.1370, 0.1057, 0.1936, 0.0444, 0.0755],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,287][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([0.0129, 0.0639, 0.1693, 0.1025, 0.1542, 0.2069, 0.1682, 0.0469, 0.0752],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,288][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([0.0467, 0.0688, 0.1708, 0.0846, 0.0564, 0.0894, 0.1938, 0.0352, 0.2543],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,291][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.0223, 0.1650, 0.4247, 0.0550, 0.0557, 0.1756, 0.0657, 0.0015, 0.0346],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,294][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([0.0032, 0.0400, 0.0223, 0.0168, 0.0579, 0.1027, 0.3538, 0.1737, 0.2295],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,299][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([0.0153, 0.0294, 0.0607, 0.0465, 0.0501, 0.0796, 0.6209, 0.0454, 0.0521],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,304][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.0789, 0.0308, 0.0753, 0.0411, 0.0686, 0.1362, 0.1954, 0.1423, 0.2314],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,309][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0019, 0.1371, 0.2800, 0.0210, 0.0326, 0.3371, 0.1458, 0.0034, 0.0412],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,313][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([0.0352, 0.0100, 0.0491, 0.0690, 0.1000, 0.4845, 0.1872, 0.0413, 0.0236],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,317][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0008, 0.0107, 0.0218, 0.0338, 0.0962, 0.1114, 0.2180, 0.2881, 0.2192],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:50,318][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.0033, 0.1802, 0.1121, 0.0647, 0.1419, 0.1117, 0.1229, 0.0276, 0.1263,
        0.1093], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,319][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0018, 0.0574, 0.1089, 0.1077, 0.1247, 0.1376, 0.2285, 0.0899, 0.0787,
        0.0648], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,320][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([0.0333, 0.1756, 0.1705, 0.1189, 0.1247, 0.0919, 0.0916, 0.0407, 0.0686,
        0.0841], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,323][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0269, 0.0809, 0.1675, 0.0742, 0.1411, 0.1404, 0.1344, 0.0442, 0.0818,
        0.1085], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,326][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([0.0457, 0.1101, 0.1973, 0.0808, 0.0534, 0.0730, 0.1641, 0.0222, 0.1724,
        0.0810], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,331][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([0.0374, 0.2207, 0.3089, 0.0452, 0.0546, 0.1776, 0.0696, 0.0029, 0.0465,
        0.0366], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,335][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.0133, 0.0647, 0.0424, 0.0243, 0.0652, 0.1128, 0.3019, 0.1298, 0.1368,
        0.1088], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,340][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0161, 0.0570, 0.0951, 0.0687, 0.1016, 0.1079, 0.3956, 0.0521, 0.0384,
        0.0675], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,345][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([0.1288, 0.0432, 0.0588, 0.0467, 0.0781, 0.1357, 0.1485, 0.1307, 0.1517,
        0.0778], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,349][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0102, 0.1677, 0.2790, 0.0521, 0.0394, 0.2640, 0.1074, 0.0051, 0.0416,
        0.0335], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,350][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([0.0867, 0.0342, 0.0968, 0.1191, 0.1326, 0.2653, 0.1310, 0.0451, 0.0326,
        0.0564], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,351][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([0.0116, 0.0134, 0.0289, 0.0322, 0.1104, 0.1028, 0.1959, 0.2392, 0.1979,
        0.0675], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:50,352][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.0020, 0.1332, 0.0850, 0.0517, 0.1707, 0.1229, 0.1459, 0.0210, 0.0987,
        0.1214, 0.0473], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,353][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0007, 0.0523, 0.0868, 0.0592, 0.1372, 0.1548, 0.2584, 0.0568, 0.0527,
        0.0616, 0.0794], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,355][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([0.0116, 0.1778, 0.1547, 0.0985, 0.1232, 0.0789, 0.0933, 0.0318, 0.0641,
        0.0877, 0.0784], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,359][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([0.0325, 0.0807, 0.1355, 0.0728, 0.1036, 0.1495, 0.0986, 0.0359, 0.0548,
        0.1307, 0.1055], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,364][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([0.0186, 0.1241, 0.2373, 0.0931, 0.0558, 0.0890, 0.1513, 0.0192, 0.1101,
        0.0774, 0.0242], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,368][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([0.0037, 0.1248, 0.3748, 0.0383, 0.0481, 0.2153, 0.1103, 0.0015, 0.0426,
        0.0321, 0.0084], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,373][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0058, 0.0401, 0.0261, 0.0166, 0.0550, 0.1045, 0.2984, 0.1136, 0.1490,
        0.1036, 0.0873], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,377][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0066, 0.0517, 0.0547, 0.0469, 0.0618, 0.1117, 0.4644, 0.0486, 0.0333,
        0.0730, 0.0473], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,382][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([0.1858, 0.0569, 0.0333, 0.0328, 0.0494, 0.1173, 0.0978, 0.0877, 0.1706,
        0.0903, 0.0780], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,383][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0014, 0.0820, 0.3188, 0.0348, 0.0341, 0.2663, 0.1957, 0.0031, 0.0330,
        0.0248, 0.0061], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,384][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([0.0263, 0.0399, 0.0905, 0.0789, 0.1219, 0.3039, 0.1625, 0.0369, 0.0301,
        0.0556, 0.0535], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,384][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([0.0061, 0.0062, 0.0151, 0.0299, 0.1393, 0.0991, 0.1927, 0.1432, 0.1385,
        0.0673, 0.1625], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:50,387][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0005, 0.2384, 0.1197, 0.0624, 0.1589, 0.0879, 0.1427, 0.0113, 0.0540,
        0.0684, 0.0248, 0.0309], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,389][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([7.0421e-05, 6.3347e-02, 8.2959e-02, 1.0588e-01, 8.9693e-02, 1.1840e-01,
        2.6758e-01, 3.9416e-02, 4.5458e-02, 5.3157e-02, 6.5690e-02, 6.8358e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,393][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0095, 0.1757, 0.1843, 0.1174, 0.0818, 0.0817, 0.0970, 0.0210, 0.0455,
        0.0689, 0.0732, 0.0439], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,398][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0178, 0.0642, 0.1468, 0.0653, 0.0744, 0.1203, 0.0934, 0.0235, 0.0546,
        0.1024, 0.0846, 0.1526], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,402][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0222, 0.1368, 0.1869, 0.0652, 0.0229, 0.0575, 0.1325, 0.0101, 0.0945,
        0.0621, 0.0158, 0.1936], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,407][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0141, 0.2025, 0.3582, 0.0463, 0.0423, 0.1767, 0.0523, 0.0010, 0.0366,
        0.0237, 0.0071, 0.0393], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,412][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0011, 0.0569, 0.0254, 0.0204, 0.0486, 0.0952, 0.3571, 0.0844, 0.0750,
        0.0812, 0.0743, 0.0804], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,414][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0143, 0.0304, 0.0701, 0.0703, 0.0677, 0.0828, 0.4421, 0.0408, 0.0231,
        0.0322, 0.0186, 0.1076], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,415][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0467, 0.0577, 0.0419, 0.0430, 0.0629, 0.1559, 0.1243, 0.0984, 0.1590,
        0.0781, 0.0573, 0.0746], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,416][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0006, 0.1576, 0.2629, 0.0335, 0.0177, 0.2699, 0.1375, 0.0024, 0.0452,
        0.0251, 0.0040, 0.0435], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,417][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0148, 0.0202, 0.0546, 0.0897, 0.0992, 0.3745, 0.1514, 0.0280, 0.0181,
        0.0354, 0.0479, 0.0662], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,419][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0017, 0.0078, 0.0228, 0.0589, 0.0606, 0.0657, 0.1673, 0.1908, 0.1296,
        0.0294, 0.0679, 0.1977], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:50,422][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.0007, 0.1552, 0.0744, 0.0411, 0.1351, 0.0867, 0.1577, 0.0201, 0.1065,
        0.0913, 0.0391, 0.0430, 0.0490], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,424][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([7.5269e-05, 6.0992e-02, 7.0355e-02, 1.1216e-01, 7.8152e-02, 8.2737e-02,
        2.9167e-01, 3.6925e-02, 3.6239e-02, 6.4213e-02, 8.2383e-02, 4.7611e-02,
        3.6482e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,429][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([0.0053, 0.1670, 0.1803, 0.0962, 0.0782, 0.0760, 0.1044, 0.0187, 0.0379,
        0.0752, 0.0717, 0.0317, 0.0573], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,433][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([0.0072, 0.0705, 0.1174, 0.0449, 0.0920, 0.1008, 0.1147, 0.0333, 0.0600,
        0.1065, 0.0986, 0.0953, 0.0589], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,438][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([0.0244, 0.0795, 0.1796, 0.0589, 0.0169, 0.0350, 0.0605, 0.0071, 0.1227,
        0.0624, 0.0163, 0.2386, 0.0979], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,443][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([0.0054, 0.2052, 0.2834, 0.0490, 0.0595, 0.1914, 0.0539, 0.0019, 0.0406,
        0.0289, 0.0089, 0.0345, 0.0375], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,446][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([0.0012, 0.0443, 0.0189, 0.0163, 0.0451, 0.0689, 0.3019, 0.0986, 0.1324,
        0.0964, 0.0770, 0.0554, 0.0437], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,447][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0069, 0.0381, 0.0596, 0.0453, 0.0528, 0.0743, 0.4930, 0.0377, 0.0312,
        0.0363, 0.0197, 0.0688, 0.0363], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,448][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.0166, 0.0560, 0.0671, 0.0649, 0.0668, 0.1234, 0.1448, 0.1071, 0.1106,
        0.0594, 0.0544, 0.0644, 0.0646], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,449][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0008, 0.1293, 0.1722, 0.0267, 0.0258, 0.3472, 0.1607, 0.0044, 0.0293,
        0.0275, 0.0067, 0.0347, 0.0347], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,451][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([0.0120, 0.0233, 0.0606, 0.0776, 0.1054, 0.2844, 0.1817, 0.0350, 0.0196,
        0.0494, 0.0552, 0.0652, 0.0307], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,454][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([0.0008, 0.0070, 0.0191, 0.0524, 0.0656, 0.0811, 0.1434, 0.1320, 0.1084,
        0.0686, 0.1323, 0.1619, 0.0275], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:50,455][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.0005, 0.1295, 0.1023, 0.0446, 0.1563, 0.0851, 0.1188, 0.0148, 0.0783,
        0.0881, 0.0481, 0.0488, 0.0543, 0.0306], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,457][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([6.5515e-05, 3.8594e-02, 9.5641e-02, 7.8367e-02, 9.4247e-02, 9.0567e-02,
        2.3322e-01, 3.5288e-02, 6.0194e-02, 5.9370e-02, 9.7365e-02, 7.1513e-02,
        3.8341e-02, 7.2313e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,462][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([0.0128, 0.1214, 0.2345, 0.1236, 0.0529, 0.0781, 0.0655, 0.0168, 0.0307,
        0.0565, 0.0708, 0.0398, 0.0626, 0.0340], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,466][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([0.0098, 0.0385, 0.1787, 0.0445, 0.0732, 0.0984, 0.0712, 0.0239, 0.0384,
        0.0673, 0.1156, 0.1409, 0.0712, 0.0284], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,471][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([0.0410, 0.0296, 0.1262, 0.0569, 0.0184, 0.0432, 0.0619, 0.0081, 0.1094,
        0.0463, 0.0160, 0.3126, 0.0952, 0.0352], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,475][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([0.0149, 0.1225, 0.3201, 0.0560, 0.0558, 0.1714, 0.0492, 0.0015, 0.0420,
        0.0268, 0.0094, 0.0499, 0.0558, 0.0246], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,480][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([0.0004, 0.0554, 0.0277, 0.0129, 0.0370, 0.0661, 0.2546, 0.0689, 0.1133,
        0.1087, 0.0872, 0.0669, 0.0582, 0.0429], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,481][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0092, 0.0267, 0.0658, 0.0604, 0.0679, 0.0805, 0.4036, 0.0348, 0.0225,
        0.0341, 0.0219, 0.0757, 0.0355, 0.0615], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,482][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.0418, 0.0545, 0.0638, 0.0539, 0.0534, 0.1339, 0.1074, 0.0882, 0.1357,
        0.0510, 0.0432, 0.0693, 0.0768, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,483][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0014, 0.0950, 0.2103, 0.0210, 0.0188, 0.3000, 0.1699, 0.0029, 0.0337,
        0.0322, 0.0076, 0.0449, 0.0499, 0.0126], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,485][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([0.0254, 0.0121, 0.0492, 0.0776, 0.1014, 0.3184, 0.1452, 0.0290, 0.0200,
        0.0338, 0.0496, 0.1046, 0.0290, 0.0045], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,488][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0008, 0.0095, 0.0317, 0.0573, 0.0570, 0.0937, 0.0886, 0.0884, 0.1466,
        0.0720, 0.1459, 0.1680, 0.0368, 0.0036], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:50,492][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.0005, 0.0702, 0.0876, 0.0242, 0.0978, 0.0994, 0.3015, 0.0119, 0.0593,
        0.0693, 0.0312, 0.0539, 0.0463, 0.0269, 0.0201], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,495][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([2.0030e-04, 3.2724e-02, 7.0241e-02, 4.0724e-02, 6.5972e-02, 1.1683e-01,
        3.7381e-01, 2.3175e-02, 4.5252e-02, 4.8453e-02, 6.7162e-02, 5.1060e-02,
        2.9605e-02, 7.0333e-03, 2.7768e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,499][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([0.0269, 0.1180, 0.1831, 0.0834, 0.0696, 0.0649, 0.1137, 0.0177, 0.0341,
        0.0579, 0.0561, 0.0356, 0.0496, 0.0435, 0.0460], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,504][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([0.0076, 0.0386, 0.1245, 0.0483, 0.0568, 0.1011, 0.1566, 0.0190, 0.0356,
        0.0865, 0.0909, 0.1071, 0.0718, 0.0273, 0.0282], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,509][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([0.0216, 0.0554, 0.1875, 0.0313, 0.0145, 0.0515, 0.1338, 0.0065, 0.0647,
        0.0495, 0.0172, 0.1808, 0.0944, 0.0749, 0.0163], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,512][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([0.0188, 0.1036, 0.3915, 0.0373, 0.0305, 0.1375, 0.0506, 0.0014, 0.0391,
        0.0261, 0.0094, 0.0571, 0.0649, 0.0248, 0.0074], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,513][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([0.0008, 0.0257, 0.0197, 0.0093, 0.0351, 0.0766, 0.3383, 0.0455, 0.1082,
        0.0988, 0.0737, 0.0617, 0.0470, 0.0338, 0.0257], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,514][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0096, 0.0267, 0.0518, 0.0307, 0.0511, 0.0596, 0.4643, 0.0263, 0.0162,
        0.0349, 0.0213, 0.0655, 0.0347, 0.0451, 0.0620], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,515][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([0.0552, 0.0457, 0.0667, 0.0531, 0.0581, 0.1155, 0.1149, 0.0762, 0.0993,
        0.0531, 0.0552, 0.0673, 0.0649, 0.0262, 0.0486], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,518][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([0.0062, 0.0504, 0.2421, 0.0218, 0.0136, 0.2349, 0.2220, 0.0038, 0.0350,
        0.0244, 0.0098, 0.0595, 0.0535, 0.0192, 0.0037], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,521][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([0.0222, 0.0162, 0.0633, 0.0512, 0.0669, 0.2806, 0.2586, 0.0213, 0.0173,
        0.0287, 0.0380, 0.0752, 0.0280, 0.0082, 0.0244], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,526][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([0.0024, 0.0067, 0.0258, 0.0170, 0.0724, 0.0517, 0.1553, 0.1012, 0.1933,
        0.0311, 0.1049, 0.1949, 0.0247, 0.0048, 0.0136], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:50,531][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.0002, 0.1467, 0.1138, 0.0326, 0.1128, 0.0821, 0.2044, 0.0125, 0.0513,
        0.0508, 0.0275, 0.0289, 0.0411, 0.0271, 0.0197, 0.0486],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,534][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([1.0797e-04, 4.6605e-02, 8.0941e-02, 9.4046e-02, 7.6413e-02, 1.1199e-01,
        2.6174e-01, 2.7659e-02, 3.3362e-02, 4.3036e-02, 6.8361e-02, 5.0708e-02,
        3.0593e-02, 6.6005e-03, 2.5282e-02, 4.2556e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,538][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([0.0082, 0.1357, 0.1652, 0.0973, 0.0810, 0.0824, 0.0819, 0.0162, 0.0317,
        0.0464, 0.0460, 0.0298, 0.0450, 0.0361, 0.0439, 0.0531],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,543][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([0.0064, 0.0491, 0.0987, 0.0428, 0.0798, 0.1314, 0.1343, 0.0249, 0.0454,
        0.0740, 0.0784, 0.0905, 0.0559, 0.0242, 0.0302, 0.0340],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,544][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([0.0106, 0.1000, 0.2006, 0.0428, 0.0154, 0.0367, 0.0851, 0.0061, 0.0426,
        0.0353, 0.0114, 0.1120, 0.0627, 0.0747, 0.0131, 0.1510],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,545][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([0.0364, 0.1712, 0.2582, 0.0535, 0.0379, 0.1514, 0.0487, 0.0014, 0.0359,
        0.0278, 0.0098, 0.0318, 0.0459, 0.0306, 0.0078, 0.0516],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,546][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([0.0007, 0.0405, 0.0259, 0.0120, 0.0357, 0.0906, 0.3255, 0.0586, 0.0811,
        0.0804, 0.0662, 0.0538, 0.0402, 0.0295, 0.0179, 0.0415],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,547][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([0.0078, 0.0302, 0.0816, 0.0411, 0.0600, 0.0623, 0.3743, 0.0315, 0.0225,
        0.0335, 0.0232, 0.0678, 0.0336, 0.0270, 0.0533, 0.0502],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,550][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([0.0307, 0.0501, 0.0604, 0.0427, 0.0489, 0.1472, 0.1000, 0.0650, 0.0902,
        0.0476, 0.0480, 0.0647, 0.0643, 0.0244, 0.0446, 0.0712],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,554][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([0.0052, 0.1542, 0.2142, 0.0352, 0.0191, 0.2555, 0.1108, 0.0028, 0.0281,
        0.0234, 0.0077, 0.0238, 0.0278, 0.0126, 0.0035, 0.0761],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,558][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([0.0205, 0.0274, 0.0779, 0.0767, 0.0846, 0.2659, 0.1991, 0.0211, 0.0182,
        0.0324, 0.0370, 0.0580, 0.0293, 0.0071, 0.0238, 0.0211],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,563][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([0.0024, 0.0109, 0.0195, 0.0370, 0.0709, 0.1022, 0.2447, 0.1092, 0.0991,
        0.0349, 0.0902, 0.1010, 0.0264, 0.0053, 0.0136, 0.0328],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:50,566][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.8575e-04, 2.1179e-01, 8.3950e-02, 2.7749e-02, 1.1291e-01, 6.5414e-02,
        1.5777e-01, 9.5122e-03, 6.1316e-02, 4.5399e-02, 2.1110e-02, 2.2764e-02,
        3.3681e-02, 2.8471e-02, 1.5609e-02, 4.5232e-02, 5.7142e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,568][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([4.1304e-05, 5.8540e-02, 6.6928e-02, 9.1124e-02, 6.5071e-02, 9.5963e-02,
        3.1100e-01, 2.7429e-02, 2.5165e-02, 4.1571e-02, 7.3026e-02, 3.1615e-02,
        2.3010e-02, 8.6756e-03, 2.8410e-02, 2.5306e-02, 2.7129e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,573][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0042, 0.1708, 0.1893, 0.1022, 0.0657, 0.0637, 0.0755, 0.0098, 0.0195,
        0.0375, 0.0294, 0.0234, 0.0391, 0.0364, 0.0362, 0.0405, 0.0568],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,577][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0048, 0.0792, 0.1104, 0.0428, 0.0956, 0.1269, 0.1451, 0.0213, 0.0335,
        0.0610, 0.0684, 0.0575, 0.0446, 0.0265, 0.0258, 0.0204, 0.0363],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,577][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0265, 0.0959, 0.1847, 0.0402, 0.0108, 0.0312, 0.0510, 0.0038, 0.0460,
        0.0314, 0.0088, 0.1006, 0.0529, 0.0580, 0.0057, 0.1281, 0.1245],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,578][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0389, 0.1839, 0.3367, 0.0447, 0.0345, 0.1445, 0.0273, 0.0005, 0.0225,
        0.0156, 0.0038, 0.0174, 0.0248, 0.0087, 0.0017, 0.0276, 0.0671],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,579][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0007, 0.0576, 0.0155, 0.0101, 0.0338, 0.0675, 0.3219, 0.0583, 0.1101,
        0.0683, 0.0543, 0.0378, 0.0310, 0.0350, 0.0126, 0.0365, 0.0489],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,582][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0078, 0.0278, 0.0869, 0.0452, 0.0536, 0.0507, 0.3728, 0.0245, 0.0202,
        0.0230, 0.0138, 0.0434, 0.0261, 0.0233, 0.0461, 0.0342, 0.1006],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,586][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0110, 0.0508, 0.0510, 0.0455, 0.0447, 0.1542, 0.1210, 0.0648, 0.0928,
        0.0404, 0.0401, 0.0522, 0.0502, 0.0219, 0.0426, 0.0537, 0.0632],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,590][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0029, 0.1594, 0.2058, 0.0193, 0.0181, 0.3476, 0.0884, 0.0009, 0.0121,
        0.0140, 0.0028, 0.0113, 0.0180, 0.0040, 0.0008, 0.0334, 0.0614],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,595][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0138, 0.0182, 0.0517, 0.0681, 0.0935, 0.3568, 0.2045, 0.0189, 0.0116,
        0.0280, 0.0298, 0.0439, 0.0154, 0.0041, 0.0178, 0.0103, 0.0135],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,600][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0022, 0.0185, 0.0442, 0.0774, 0.0540, 0.1115, 0.1754, 0.1161, 0.0858,
        0.0405, 0.0747, 0.0987, 0.0261, 0.0046, 0.0141, 0.0354, 0.0209],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:50,604][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0002, 0.1161, 0.0431, 0.0316, 0.1076, 0.0681, 0.1119, 0.0143, 0.0622,
        0.0947, 0.0389, 0.0377, 0.0436, 0.0274, 0.0218, 0.0640, 0.0878, 0.0291],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,607][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([2.8291e-05, 4.7635e-02, 5.6826e-02, 8.4320e-02, 1.0113e-01, 8.5102e-02,
        1.9266e-01, 3.1270e-02, 3.4623e-02, 6.0615e-02, 7.9898e-02, 5.9022e-02,
        3.4821e-02, 1.1421e-02, 3.8808e-02, 2.9880e-02, 3.6185e-02, 1.5762e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,609][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0017, 0.1215, 0.1394, 0.0940, 0.0546, 0.0713, 0.0614, 0.0180, 0.0264,
        0.0432, 0.0404, 0.0318, 0.0426, 0.0391, 0.0602, 0.0506, 0.0541, 0.0495],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,610][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0026, 0.0517, 0.1110, 0.0528, 0.0663, 0.0818, 0.0834, 0.0240, 0.0362,
        0.0731, 0.0900, 0.0852, 0.0575, 0.0379, 0.0352, 0.0361, 0.0457, 0.0295],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,611][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0144, 0.0375, 0.0963, 0.0486, 0.0152, 0.0313, 0.0393, 0.0073, 0.0672,
        0.0412, 0.0142, 0.1855, 0.0608, 0.0514, 0.0079, 0.1437, 0.1228, 0.0153],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,612][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0080, 0.1320, 0.1957, 0.0715, 0.0447, 0.1358, 0.0501, 0.0026, 0.0326,
        0.0287, 0.0107, 0.0261, 0.0395, 0.0175, 0.0079, 0.0462, 0.1091, 0.0413],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,615][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0004, 0.0458, 0.0203, 0.0176, 0.0414, 0.0778, 0.1819, 0.0474, 0.1022,
        0.0996, 0.0719, 0.0507, 0.0454, 0.0458, 0.0236, 0.0421, 0.0576, 0.0284],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,619][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0051, 0.0286, 0.0480, 0.0280, 0.0436, 0.0616, 0.2967, 0.0359, 0.0288,
        0.0338, 0.0215, 0.0732, 0.0344, 0.0470, 0.0524, 0.0391, 0.0967, 0.0256],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,624][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0082, 0.0633, 0.0545, 0.0679, 0.0481, 0.0873, 0.0752, 0.0593, 0.0845,
        0.0384, 0.0475, 0.0590, 0.0632, 0.0254, 0.0489, 0.0574, 0.0604, 0.0515],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,628][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0045, 0.0980, 0.1414, 0.0225, 0.0201, 0.2419, 0.0844, 0.0053, 0.0285,
        0.0334, 0.0127, 0.0321, 0.0402, 0.0104, 0.0034, 0.0691, 0.1088, 0.0432],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,633][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0056, 0.0224, 0.0590, 0.0734, 0.0836, 0.2502, 0.1273, 0.0261, 0.0212,
        0.0413, 0.0437, 0.0703, 0.0304, 0.0087, 0.0302, 0.0176, 0.0287, 0.0603],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,637][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0002, 0.0146, 0.0173, 0.0445, 0.0564, 0.1213, 0.1032, 0.0903, 0.0899,
        0.0783, 0.1559, 0.1113, 0.0317, 0.0034, 0.0198, 0.0347, 0.0165, 0.0108],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:50,641][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.0004, 0.1233, 0.1023, 0.0298, 0.1149, 0.0555, 0.1465, 0.0100, 0.0536,
        0.0460, 0.0171, 0.0217, 0.0339, 0.0246, 0.0161, 0.0593, 0.0775, 0.0212,
        0.0462], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,642][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([6.1546e-05, 3.4437e-02, 8.7412e-02, 1.0093e-01, 8.2624e-02, 5.9963e-02,
        2.8867e-01, 2.1063e-02, 3.4713e-02, 3.5698e-02, 4.6637e-02, 4.5293e-02,
        2.8430e-02, 7.2538e-03, 2.2323e-02, 2.8439e-02, 3.9422e-02, 8.7499e-03,
        2.7887e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,643][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([0.0040, 0.0940, 0.2455, 0.0720, 0.0487, 0.0483, 0.0571, 0.0089, 0.0211,
        0.0363, 0.0247, 0.0258, 0.0416, 0.0296, 0.0375, 0.0413, 0.0734, 0.0363,
        0.0538], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,644][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([0.0094, 0.0234, 0.1159, 0.0372, 0.0500, 0.0917, 0.0721, 0.0167, 0.0386,
        0.0570, 0.0628, 0.1222, 0.0591, 0.0318, 0.0314, 0.0317, 0.0656, 0.0336,
        0.0500], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,646][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([0.0393, 0.0376, 0.1125, 0.0274, 0.0114, 0.0233, 0.0420, 0.0045, 0.0534,
        0.0148, 0.0070, 0.1341, 0.0424, 0.0380, 0.0048, 0.1112, 0.1299, 0.0152,
        0.1512], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,650][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([0.0621, 0.1536, 0.1968, 0.0440, 0.0261, 0.1074, 0.0119, 0.0005, 0.0194,
        0.0143, 0.0034, 0.0131, 0.0205, 0.0089, 0.0022, 0.0256, 0.0689, 0.0491,
        0.1722], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,654][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([0.0009, 0.0299, 0.0308, 0.0120, 0.0364, 0.0663, 0.2702, 0.0499, 0.0902,
        0.0697, 0.0347, 0.0449, 0.0460, 0.0367, 0.0164, 0.0491, 0.0676, 0.0202,
        0.0281], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,659][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0097, 0.0226, 0.0691, 0.0287, 0.0590, 0.0517, 0.3240, 0.0231, 0.0184,
        0.0204, 0.0170, 0.0680, 0.0235, 0.0330, 0.0547, 0.0349, 0.0856, 0.0159,
        0.0407], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,664][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.0119, 0.0455, 0.0946, 0.0991, 0.0502, 0.0844, 0.0467, 0.0671, 0.0817,
        0.0235, 0.0282, 0.0562, 0.0575, 0.0207, 0.0300, 0.0631, 0.0631, 0.0364,
        0.0401], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,668][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([0.0139, 0.1572, 0.1208, 0.0187, 0.0059, 0.1816, 0.0207, 0.0008, 0.0189,
        0.0141, 0.0021, 0.0081, 0.0143, 0.0023, 0.0004, 0.0328, 0.0441, 0.0311,
        0.3121], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,670][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([0.0107, 0.0186, 0.0782, 0.0768, 0.0929, 0.2220, 0.1703, 0.0200, 0.0158,
        0.0217, 0.0230, 0.0521, 0.0233, 0.0046, 0.0152, 0.0124, 0.0196, 0.0555,
        0.0673], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,671][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0041, 0.0075, 0.0144, 0.0276, 0.0469, 0.0592, 0.1301, 0.1680, 0.1776,
        0.0355, 0.0910, 0.1478, 0.0183, 0.0032, 0.0091, 0.0246, 0.0086, 0.0070,
        0.0195], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:50,672][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.0002, 0.0838, 0.0521, 0.0250, 0.1244, 0.0521, 0.1562, 0.0132, 0.0600,
        0.0508, 0.0321, 0.0269, 0.0391, 0.0302, 0.0169, 0.0617, 0.0840, 0.0246,
        0.0435, 0.0231], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,674][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([2.0315e-05, 4.5253e-02, 6.5045e-02, 8.4887e-02, 6.1803e-02, 7.6813e-02,
        2.6176e-01, 2.9563e-02, 2.6054e-02, 4.7222e-02, 7.5622e-02, 3.1109e-02,
        2.8012e-02, 7.3717e-03, 3.6317e-02, 3.0104e-02, 3.0570e-02, 1.2851e-02,
        2.5609e-02, 2.4014e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,677][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([0.0021, 0.0774, 0.1178, 0.0677, 0.0570, 0.0513, 0.0724, 0.0151, 0.0232,
        0.0501, 0.0522, 0.0205, 0.0470, 0.0381, 0.0638, 0.0508, 0.0773, 0.0376,
        0.0436, 0.0348], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,682][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([0.0049, 0.0316, 0.0755, 0.0308, 0.0577, 0.0925, 0.0994, 0.0238, 0.0353,
        0.0684, 0.0794, 0.0840, 0.0568, 0.0332, 0.0368, 0.0298, 0.0516, 0.0342,
        0.0498, 0.0246], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,686][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([0.0133, 0.0310, 0.0865, 0.0295, 0.0090, 0.0182, 0.0306, 0.0058, 0.0567,
        0.0242, 0.0112, 0.1186, 0.0447, 0.0327, 0.0053, 0.0855, 0.1111, 0.0115,
        0.1756, 0.0989], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,691][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([0.0295, 0.1283, 0.1461, 0.0341, 0.0247, 0.1032, 0.0210, 0.0008, 0.0234,
        0.0168, 0.0056, 0.0124, 0.0202, 0.0100, 0.0023, 0.0190, 0.0569, 0.0374,
        0.1466, 0.1617], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,696][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([0.0004, 0.0303, 0.0183, 0.0157, 0.0290, 0.0649, 0.2624, 0.0587, 0.0860,
        0.0843, 0.0574, 0.0368, 0.0460, 0.0309, 0.0176, 0.0417, 0.0607, 0.0221,
        0.0194, 0.0175], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,699][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0029, 0.0157, 0.0513, 0.0372, 0.0462, 0.0374, 0.4208, 0.0246, 0.0141,
        0.0205, 0.0165, 0.0432, 0.0201, 0.0268, 0.0493, 0.0247, 0.0760, 0.0125,
        0.0416, 0.0188], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,700][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0089, 0.0392, 0.0362, 0.0463, 0.0457, 0.1215, 0.0977, 0.0772, 0.0837,
        0.0374, 0.0396, 0.0421, 0.0573, 0.0245, 0.0419, 0.0503, 0.0471, 0.0373,
        0.0313, 0.0350], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,701][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0032, 0.1098, 0.0719, 0.0121, 0.0074, 0.1466, 0.0322, 0.0014, 0.0161,
        0.0136, 0.0031, 0.0069, 0.0126, 0.0031, 0.0009, 0.0277, 0.0414, 0.0290,
        0.2690, 0.1919], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,702][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0120, 0.0111, 0.0414, 0.0608, 0.0752, 0.2622, 0.1707, 0.0246, 0.0128,
        0.0236, 0.0282, 0.0509, 0.0193, 0.0036, 0.0186, 0.0131, 0.0166, 0.0537,
        0.0727, 0.0290], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,704][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0016, 0.0087, 0.0132, 0.0584, 0.0588, 0.0956, 0.1795, 0.1269, 0.0829,
        0.0470, 0.0997, 0.0810, 0.0231, 0.0037, 0.0098, 0.0288, 0.0119, 0.0093,
        0.0181, 0.0418], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:50,707][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0002, 0.1643, 0.1004, 0.0323, 0.1055, 0.0625, 0.1106, 0.0062, 0.0388,
        0.0338, 0.0152, 0.0172, 0.0263, 0.0196, 0.0081, 0.0296, 0.0396, 0.0195,
        0.0183, 0.0094, 0.1423], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,710][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([4.3003e-05, 5.7970e-02, 9.0393e-02, 1.4438e-01, 6.6632e-02, 7.7125e-02,
        2.3709e-01, 1.8437e-02, 2.0614e-02, 3.6056e-02, 5.3213e-02, 2.9410e-02,
        2.1943e-02, 6.7114e-03, 2.2504e-02, 1.9607e-02, 2.0341e-02, 1.1552e-02,
        1.4945e-02, 1.7517e-02, 3.3514e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,714][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0052, 0.1418, 0.1880, 0.1027, 0.0666, 0.0514, 0.0571, 0.0090, 0.0180,
        0.0310, 0.0236, 0.0177, 0.0301, 0.0256, 0.0295, 0.0285, 0.0461, 0.0407,
        0.0235, 0.0203, 0.0436], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,719][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0080, 0.0623, 0.1080, 0.0436, 0.0795, 0.1128, 0.1009, 0.0184, 0.0330,
        0.0583, 0.0537, 0.0642, 0.0429, 0.0227, 0.0227, 0.0174, 0.0323, 0.0353,
        0.0230, 0.0152, 0.0458], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,724][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0359, 0.0584, 0.1384, 0.0300, 0.0063, 0.0189, 0.0218, 0.0017, 0.0244,
        0.0128, 0.0037, 0.0590, 0.0291, 0.0255, 0.0018, 0.0527, 0.0583, 0.0124,
        0.1172, 0.0628, 0.2289], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,726][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.2680e-02, 1.8877e-01, 2.0568e-01, 2.5860e-02, 1.4882e-02, 6.9465e-02,
        5.0342e-03, 7.1146e-05, 6.0837e-03, 4.5489e-03, 8.5592e-04, 2.8940e-03,
        5.6300e-03, 1.7242e-03, 3.4104e-04, 6.7709e-03, 1.3871e-02, 1.6064e-02,
        5.5444e-02, 5.0909e-02, 2.3243e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,729][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0008, 0.0575, 0.0239, 0.0188, 0.0347, 0.0674, 0.2427, 0.0443, 0.0877,
        0.0638, 0.0434, 0.0341, 0.0351, 0.0333, 0.0088, 0.0313, 0.0437, 0.0235,
        0.0114, 0.0117, 0.0821], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,730][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0063, 0.0256, 0.0756, 0.0520, 0.0513, 0.0509, 0.3478, 0.0195, 0.0155,
        0.0184, 0.0112, 0.0465, 0.0217, 0.0209, 0.0332, 0.0201, 0.0692, 0.0145,
        0.0306, 0.0189, 0.0501], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,731][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0150, 0.0498, 0.0671, 0.0594, 0.0543, 0.1177, 0.0747, 0.0519, 0.0959,
        0.0365, 0.0283, 0.0442, 0.0430, 0.0200, 0.0356, 0.0421, 0.0473, 0.0349,
        0.0265, 0.0289, 0.0268], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,733][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([4.3119e-03, 1.2450e-01, 8.3991e-02, 7.3901e-03, 3.5568e-03, 1.0623e-01,
        9.4546e-03, 7.2094e-05, 1.9127e-03, 2.5163e-03, 3.3056e-04, 1.2061e-03,
        2.8168e-03, 4.1133e-04, 6.6041e-05, 5.9005e-03, 8.6403e-03, 1.0282e-02,
        1.3071e-01, 6.3709e-02, 4.3198e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,736][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0189, 0.0212, 0.0538, 0.0730, 0.0916, 0.2911, 0.1558, 0.0132, 0.0095,
        0.0210, 0.0200, 0.0381, 0.0154, 0.0032, 0.0125, 0.0080, 0.0123, 0.0612,
        0.0451, 0.0199, 0.0151], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,740][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0041, 0.0158, 0.0426, 0.0949, 0.0592, 0.1080, 0.1130, 0.1192, 0.0745,
        0.0317, 0.0582, 0.0991, 0.0233, 0.0022, 0.0077, 0.0246, 0.0142, 0.0094,
        0.0145, 0.0230, 0.0607], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:50,744][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:50,747][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[796],
        [ 16],
        [ 29],
        [ 22],
        [  6],
        [  2],
        [  3],
        [  3],
        [  2],
        [  2],
        [  2],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  2],
        [  4],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:06:50,749][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[604],
        [ 12],
        [ 25],
        [ 14],
        [  3],
        [  2],
        [  2],
        [  3],
        [  2],
        [  2],
        [  2],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  2],
        [  4],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:06:50,752][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[48029],
        [18750],
        [16765],
        [14768],
        [41889],
        [33655],
        [30099],
        [35684],
        [33773],
        [23956],
        [26973],
        [27099],
        [23266],
        [25025],
        [20419],
        [21784],
        [23149],
        [20827],
        [23175],
        [23936],
        [24094]], device='cuda:0')
[2024-07-23 21:06:50,756][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[10864],
        [28866],
        [35963],
        [29535],
        [32967],
        [32665],
        [30639],
        [32152],
        [32746],
        [33112],
        [33091],
        [31552],
        [31455],
        [32295],
        [31869],
        [31762],
        [31605],
        [32103],
        [32443],
        [31792],
        [31755]], device='cuda:0')
[2024-07-23 21:06:50,759][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[27971],
        [29938],
        [28783],
        [26578],
        [20821],
        [20278],
        [19863],
        [17763],
        [12994],
        [12416],
        [11961],
        [13288],
        [12728],
        [12793],
        [10651],
        [ 9474],
        [10553],
        [ 7920],
        [ 9151],
        [ 6280],
        [ 9982]], device='cuda:0')
[2024-07-23 21:06:50,762][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 5397],
        [39612],
        [42684],
        [45889],
        [49128],
        [48766],
        [48710],
        [48431],
        [48992],
        [48837],
        [47063],
        [46394],
        [46659],
        [43911],
        [44339],
        [45531],
        [46296],
        [44478],
        [45016],
        [45081],
        [46559]], device='cuda:0')
[2024-07-23 21:06:50,763][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[16359],
        [16451],
        [22096],
        [22438],
        [24741],
        [24901],
        [28329],
        [27740],
        [30322],
        [28810],
        [28652],
        [26696],
        [25191],
        [25708],
        [27470],
        [25202],
        [22249],
        [21830],
        [21852],
        [20835],
        [22215]], device='cuda:0')
[2024-07-23 21:06:50,765][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[15322],
        [11238],
        [11401],
        [11895],
        [12415],
        [12118],
        [12600],
        [14486],
        [14328],
        [13475],
        [14030],
        [13727],
        [14675],
        [15822],
        [16607],
        [14434],
        [14912],
        [16172],
        [16522],
        [16294],
        [17043]], device='cuda:0')
[2024-07-23 21:06:50,767][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[17403],
        [12704],
        [13530],
        [12306],
        [10412],
        [ 8471],
        [ 7123],
        [ 6024],
        [ 4939],
        [ 4697],
        [ 4127],
        [ 3895],
        [ 3647],
        [ 3633],
        [ 3625],
        [ 3528],
        [ 3582],
        [ 2987],
        [ 3263],
        [ 3041],
        [ 3339]], device='cuda:0')
[2024-07-23 21:06:50,770][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[29500],
        [30190],
        [30342],
        [42330],
        [31177],
        [37191],
        [35920],
        [35780],
        [34486],
        [42108],
        [40575],
        [46100],
        [47492],
        [43852],
        [47110],
        [48516],
        [47307],
        [49686],
        [45123],
        [49360],
        [47747]], device='cuda:0')
[2024-07-23 21:06:50,773][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 6154],
        [32427],
        [33360],
        [26922],
        [21104],
        [16029],
        [22402],
        [22183],
        [20774],
        [20501],
        [19519],
        [19246],
        [20271],
        [19620],
        [19914],
        [18703],
        [18513],
        [20112],
        [19986],
        [19440],
        [19342]], device='cuda:0')
[2024-07-23 21:06:50,777][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[22148],
        [34938],
        [37185],
        [35023],
        [32747],
        [34476],
        [34204],
        [39521],
        [34802],
        [34371],
        [35318],
        [35024],
        [35246],
        [35247],
        [35929],
        [35319],
        [34333],
        [34867],
        [33793],
        [33888],
        [33045]], device='cuda:0')
[2024-07-23 21:06:50,780][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[46094],
        [46526],
        [14681],
        [ 6282],
        [ 6141],
        [ 5400],
        [ 5198],
        [ 5316],
        [ 4828],
        [ 4512],
        [ 4724],
        [ 4520],
        [ 3537],
        [ 3779],
        [ 3722],
        [ 3552],
        [ 3219],
        [ 3320],
        [ 3080],
        [ 2943],
        [ 2978]], device='cuda:0')
[2024-07-23 21:06:50,783][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[11494],
        [43501],
        [40915],
        [39829],
        [36411],
        [39660],
        [35587],
        [35147],
        [34121],
        [34027],
        [32987],
        [32299],
        [32135],
        [32416],
        [31749],
        [32871],
        [33705],
        [32528],
        [31790],
        [32350],
        [33616]], device='cuda:0')
[2024-07-23 21:06:50,786][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[6472],
        [1912],
        [2556],
        [3199],
        [1990],
        [3336],
        [4366],
        [2209],
        [3112],
        [1156],
        [2252],
        [1816],
        [2191],
        [ 995],
        [1203],
        [1143],
        [2036],
        [1591],
        [1739],
        [1223],
        [1617]], device='cuda:0')
[2024-07-23 21:06:50,789][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 4834],
        [14695],
        [16694],
        [20864],
        [26591],
        [27356],
        [28107],
        [29711],
        [31341],
        [31975],
        [32847],
        [30417],
        [31407],
        [31416],
        [29564],
        [30794],
        [30490],
        [30773],
        [29817],
        [30186],
        [30210]], device='cuda:0')
[2024-07-23 21:06:50,793][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[9747],
        [7421],
        [7349],
        [7472],
        [5849],
        [6963],
        [4826],
        [4792],
        [4136],
        [3955],
        [3921],
        [3898],
        [3656],
        [3421],
        [3573],
        [3769],
        [3689],
        [3451],
        [3649],
        [3749],
        [4136]], device='cuda:0')
[2024-07-23 21:06:50,796][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[15645],
        [ 4156],
        [ 2129],
        [ 4387],
        [ 6643],
        [ 7018],
        [ 5752],
        [ 6514],
        [ 6751],
        [ 6836],
        [ 6367],
        [ 5888],
        [ 5575],
        [ 5279],
        [ 6085],
        [ 6411],
        [ 6072],
        [ 6225],
        [ 4314],
        [ 5624],
        [ 5530]], device='cuda:0')
[2024-07-23 21:06:50,797][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[17928],
        [13387],
        [17764],
        [15767],
        [ 7841],
        [ 8560],
        [11107],
        [11559],
        [ 9486],
        [ 8910],
        [ 9911],
        [10100],
        [ 9715],
        [11291],
        [11154],
        [10361],
        [10062],
        [10305],
        [10249],
        [10032],
        [ 9733]], device='cuda:0')
[2024-07-23 21:06:50,799][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[8392],
        [5206],
        [5857],
        [4965],
        [4348],
        [4878],
        [5343],
        [4783],
        [3888],
        [4703],
        [4911],
        [4571],
        [4223],
        [4275],
        [4507],
        [4663],
        [4928],
        [5053],
        [5636],
        [6066],
        [6322]], device='cuda:0')
[2024-07-23 21:06:50,801][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[8502],
        [9748],
        [7904],
        [5899],
        [6250],
        [7338],
        [6343],
        [5693],
        [6255],
        [7004],
        [6385],
        [6420],
        [6621],
        [5869],
        [5534],
        [6091],
        [5782],
        [5340],
        [5772],
        [5694],
        [4269]], device='cuda:0')
[2024-07-23 21:06:50,804][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[6453],
        [9075],
        [5917],
        [4728],
        [1856],
        [3032],
        [ 666],
        [ 703],
        [1610],
        [1608],
        [1862],
        [1802],
        [2084],
        [2481],
        [1979],
        [1868],
        [1963],
        [2938],
        [2180],
        [2145],
        [2528]], device='cuda:0')
[2024-07-23 21:06:50,807][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 5372],
        [ 5148],
        [ 8841],
        [15469],
        [ 8258],
        [21922],
        [18960],
        [20809],
        [20774],
        [19505],
        [21267],
        [21226],
        [21189],
        [20029],
        [19408],
        [18631],
        [17248],
        [18252],
        [17706],
        [18631],
        [18298]], device='cuda:0')
[2024-07-23 21:06:50,810][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[4075],
        [1511],
        [1274],
        [2892],
        [3451],
        [3624],
        [4820],
        [3709],
        [5632],
        [4593],
        [4355],
        [4764],
        [4325],
        [4352],
        [4087],
        [4102],
        [4398],
        [3888],
        [3823],
        [4289],
        [4153]], device='cuda:0')
[2024-07-23 21:06:50,813][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[23616],
        [16702],
        [ 7259],
        [13187],
        [19300],
        [17370],
        [26268],
        [28276],
        [30093],
        [28677],
        [30708],
        [28127],
        [28392],
        [28427],
        [28237],
        [28392],
        [29015],
        [27329],
        [21327],
        [20998],
        [23204]], device='cuda:0')
[2024-07-23 21:06:50,817][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[2497],
        [1101],
        [1425],
        [2773],
        [1100],
        [5409],
        [5746],
        [6963],
        [6267],
        [3733],
        [4400],
        [5500],
        [4486],
        [4964],
        [4393],
        [4259],
        [5346],
        [4767],
        [4368],
        [5440],
        [5237]], device='cuda:0')
[2024-07-23 21:06:50,820][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[ 1639],
        [ 5044],
        [10195],
        [14136],
        [ 7746],
        [ 9828],
        [ 6344],
        [ 6651],
        [ 7600],
        [ 7194],
        [ 7040],
        [ 7059],
        [ 6725],
        [ 6819],
        [ 6548],
        [ 6548],
        [ 6923],
        [ 5859],
        [ 6338],
        [ 6404],
        [ 6325]], device='cuda:0')
[2024-07-23 21:06:50,823][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[33617],
        [31748],
        [34556],
        [25694],
        [30672],
        [21799],
        [26798],
        [24912],
        [21750],
        [23342],
        [21933],
        [22415],
        [23227],
        [23057],
        [25004],
        [23799],
        [22702],
        [23907],
        [26383],
        [24670],
        [24325]], device='cuda:0')
[2024-07-23 21:06:50,826][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[37164],
        [44199],
        [45239],
        [40433],
        [40982],
        [36160],
        [29292],
        [38778],
        [35508],
        [41822],
        [36989],
        [38421],
        [38184],
        [41381],
        [40501],
        [41939],
        [37895],
        [38682],
        [39342],
        [40678],
        [38334]], device='cuda:0')
[2024-07-23 21:06:50,829][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184],
        [2184]], device='cuda:0')
[2024-07-23 21:06:50,957][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:50,959][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,959][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,960][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,961][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,961][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,963][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,963][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,964][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,965][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,965][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,966][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,967][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:50,967][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.7720, 0.2280], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,970][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0766, 0.9234], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,974][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0016, 0.9984], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,975][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0860, 0.9140], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,977][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ language] are: tensor([1.3175e-04, 9.9987e-01], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,981][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0019, 0.9981], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,986][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0037, 0.9963], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,991][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0240, 0.9760], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,994][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.4615, 0.5385], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,995][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0087, 0.9913], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,996][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0740, 0.9260], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,996][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.3165, 0.6835], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:50,997][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.2629, 0.0914, 0.6457], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,000][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0045, 0.5248, 0.4707], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,002][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ used] are: tensor([3.9678e-05, 5.6751e-01, 4.3245e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,004][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ used] are: tensor([3.7488e-04, 3.7695e-01, 6.2267e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,007][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ used] are: tensor([1.9644e-05, 5.9393e-01, 4.0606e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,010][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ used] are: tensor([1.0585e-04, 5.0522e-01, 4.9467e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,013][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ used] are: tensor([5.6646e-05, 4.6234e-01, 5.3761e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,018][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0079, 0.7463, 0.2458], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,022][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.3193, 0.4219, 0.2588], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,024][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ used] are: tensor([1.0911e-04, 7.6530e-01, 2.3459e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,024][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0035, 0.7691, 0.2274], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,025][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.4019, 0.4821, 0.1160], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,026][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.3820, 0.0849, 0.3725, 0.1606], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,027][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0069, 0.2039, 0.5057, 0.2835], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,029][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ by] are: tensor([1.5815e-04, 6.4955e-01, 2.7719e-01, 7.3100e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,030][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ by] are: tensor([4.5751e-04, 2.0142e-01, 7.0363e-01, 9.4490e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,033][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ by] are: tensor([3.5762e-05, 5.7836e-01, 2.4883e-01, 1.7278e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,036][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ by] are: tensor([1.2382e-04, 2.7079e-01, 4.5589e-01, 2.7320e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,038][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ by] are: tensor([1.6157e-04, 2.4215e-01, 5.1987e-01, 2.3783e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,043][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ by] are: tensor([0.0049, 0.4781, 0.1358, 0.3812], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,048][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ by] are: tensor([0.0060, 0.3686, 0.2102, 0.4152], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,052][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ by] are: tensor([0.0009, 0.4177, 0.3321, 0.2493], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,053][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0090, 0.5821, 0.2449, 0.1640], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,054][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.7353, 0.0661, 0.0522, 0.1463], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,055][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ Juan] are: tensor([0.4906, 0.1153, 0.2447, 0.0951, 0.0543], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,055][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ Juan] are: tensor([0.0687, 0.3593, 0.1667, 0.1573, 0.2480], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,058][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ Juan] are: tensor([0.0024, 0.4905, 0.1091, 0.1107, 0.2874], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,061][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ Juan] are: tensor([0.0015, 0.3106, 0.5804, 0.0672, 0.0402], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,063][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ Juan] are: tensor([2.7195e-04, 5.5995e-01, 2.3866e-01, 9.5593e-02, 1.0552e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,068][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ Juan] are: tensor([0.0005, 0.2284, 0.1805, 0.0919, 0.4986], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,072][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ Juan] are: tensor([0.0026, 0.2730, 0.2460, 0.1258, 0.3527], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,077][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ Juan] are: tensor([0.0128, 0.3497, 0.1085, 0.1915, 0.3375], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,082][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ Juan] are: tensor([0.2765, 0.1683, 0.1011, 0.1609, 0.2931], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,083][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ Juan] are: tensor([0.0040, 0.3987, 0.0854, 0.1538, 0.3581], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,083][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ Juan] are: tensor([0.0692, 0.5325, 0.1996, 0.1060, 0.0927], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,084][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ Juan] are: tensor([0.6089, 0.0542, 0.0589, 0.1990, 0.0790], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,085][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ B] are: tensor([0.5752, 0.0661, 0.1554, 0.0917, 0.0186, 0.0930], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,087][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ B] are: tensor([0.0212, 0.2848, 0.0817, 0.0876, 0.0584, 0.4663], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,090][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ B] are: tensor([0.0007, 0.4983, 0.0445, 0.0597, 0.1303, 0.2664], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,095][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ B] are: tensor([0.0033, 0.4167, 0.2801, 0.0367, 0.0139, 0.2493], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,097][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ B] are: tensor([2.6867e-05, 5.2041e-01, 2.6487e-01, 1.0214e-01, 6.0152e-02, 5.2395e-02],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,100][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ B] are: tensor([1.7204e-04, 1.1565e-01, 1.2124e-01, 3.9821e-02, 2.3569e-01, 4.8742e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,105][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ B] are: tensor([0.0008, 0.1846, 0.1481, 0.0886, 0.0990, 0.4789], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,109][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ B] are: tensor([0.0017, 0.1876, 0.0574, 0.1225, 0.0537, 0.5770], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,112][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ B] are: tensor([0.5177, 0.0838, 0.0595, 0.1616, 0.1489, 0.0286], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,113][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ B] are: tensor([0.0026, 0.1362, 0.0957, 0.0670, 0.1849, 0.5136], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,113][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ B] are: tensor([0.0241, 0.5432, 0.2012, 0.0701, 0.0437, 0.1177], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,114][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ B] are: tensor([0.4780, 0.0618, 0.0384, 0.2762, 0.0479, 0.0976], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,116][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [aut] are: tensor([0.5792, 0.0387, 0.1680, 0.0453, 0.0115, 0.1392, 0.0183],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,119][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [aut] are: tensor([0.0116, 0.0457, 0.0942, 0.0496, 0.0393, 0.4899, 0.2698],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,122][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [aut] are: tensor([3.4707e-04, 1.2464e-01, 4.2129e-02, 1.9246e-02, 5.9984e-02, 4.6067e-01,
        2.9298e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,125][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [aut] are: tensor([1.0799e-04, 1.5323e-01, 3.0950e-01, 1.4208e-02, 5.3699e-03, 4.2042e-01,
        9.7161e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,128][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [aut] are: tensor([4.3899e-05, 2.9272e-01, 1.7023e-01, 6.2377e-02, 6.1887e-02, 1.0486e-01,
        3.0789e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,130][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [aut] are: tensor([1.8865e-04, 4.1788e-02, 6.3724e-02, 1.5672e-02, 7.9030e-02, 2.7866e-01,
        5.2094e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,133][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [aut] are: tensor([4.1583e-04, 5.4250e-02, 8.6184e-02, 3.2407e-02, 6.5076e-02, 3.4218e-01,
        4.1949e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,138][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [aut] are: tensor([0.0019, 0.0740, 0.0272, 0.0505, 0.0186, 0.3886, 0.4392],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,141][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [aut] are: tensor([0.1306, 0.1930, 0.1112, 0.1727, 0.2150, 0.1421, 0.0353],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,142][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [aut] are: tensor([0.0019, 0.1134, 0.0425, 0.0269, 0.0442, 0.4685, 0.3025],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,143][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [aut] are: tensor([0.0167, 0.5106, 0.2103, 0.0627, 0.0423, 0.1101, 0.0473],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,144][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [aut] are: tensor([0.2631, 0.0501, 0.0342, 0.1982, 0.0213, 0.0501, 0.3829],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,145][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ista] are: tensor([0.1868, 0.0542, 0.3158, 0.0802, 0.0378, 0.2800, 0.0363, 0.0088],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,148][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ista] are: tensor([0.0018, 0.0612, 0.0900, 0.0230, 0.0431, 0.5000, 0.2742, 0.0066],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,151][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ista] are: tensor([2.6684e-05, 8.3577e-02, 6.6865e-02, 1.9084e-02, 5.8888e-02, 4.6371e-01,
        3.0480e-01, 3.0429e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,154][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ista] are: tensor([1.4895e-05, 5.4681e-02, 1.6920e-01, 1.8462e-02, 8.9610e-03, 5.2310e-01,
        2.2513e-01, 4.4177e-04], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,157][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ista] are: tensor([1.3381e-05, 3.0584e-01, 2.4530e-01, 4.2764e-02, 6.7159e-02, 1.5415e-01,
        1.7236e-01, 1.2404e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,160][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ista] are: tensor([2.9726e-04, 1.0187e-01, 1.4140e-01, 2.0130e-02, 1.2023e-01, 3.2131e-01,
        2.7116e-01, 2.3610e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,163][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ista] are: tensor([2.3489e-05, 2.3628e-02, 1.1083e-01, 3.5863e-02, 5.9085e-02, 4.1831e-01,
        3.4985e-01, 2.4067e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,167][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ista] are: tensor([0.0009, 0.0771, 0.0364, 0.0372, 0.0331, 0.3661, 0.4373, 0.0118],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,172][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ista] are: tensor([0.0012, 0.1503, 0.1457, 0.1470, 0.2635, 0.2008, 0.0429, 0.0486],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,172][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ista] are: tensor([3.8733e-04, 1.2487e-01, 7.4769e-02, 1.4277e-02, 4.4445e-02, 4.8912e-01,
        2.4374e-01, 8.3923e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,173][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ista] are: tensor([0.0089, 0.4473, 0.1897, 0.0607, 0.0487, 0.1384, 0.0736, 0.0326],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,174][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ista] are: tensor([0.3301, 0.0236, 0.0204, 0.1286, 0.0141, 0.0267, 0.2347, 0.2218],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,175][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ de] are: tensor([0.1698, 0.0524, 0.3068, 0.0560, 0.0469, 0.2428, 0.0445, 0.0086, 0.0722],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,178][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ de] are: tensor([0.0035, 0.0634, 0.1201, 0.0335, 0.0406, 0.5364, 0.1670, 0.0163, 0.0191],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,181][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ de] are: tensor([1.5691e-04, 7.6641e-02, 6.6270e-02, 2.2972e-02, 6.0074e-02, 5.1026e-01,
        2.5026e-01, 5.0930e-03, 8.2719e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,184][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ de] are: tensor([1.7912e-04, 1.1250e-01, 1.9918e-01, 3.2299e-02, 2.0111e-02, 5.4249e-01,
        8.5488e-02, 1.3848e-03, 6.3664e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,187][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ de] are: tensor([6.9024e-05, 2.1342e-01, 9.4633e-02, 5.6670e-02, 9.1616e-02, 1.9448e-01,
        2.4670e-01, 2.3475e-02, 7.8935e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,191][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ de] are: tensor([0.0007, 0.0994, 0.0662, 0.0226, 0.1244, 0.2785, 0.2366, 0.0285, 0.1430],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,194][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ de] are: tensor([1.9568e-04, 2.6955e-02, 6.6657e-02, 3.6196e-02, 9.1871e-02, 4.5835e-01,
        2.7491e-01, 4.8629e-03, 4.0002e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,197][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ de] are: tensor([6.9026e-04, 2.7779e-02, 1.1161e-02, 2.3999e-02, 3.0541e-02, 6.9317e-01,
        1.5386e-01, 1.6390e-02, 4.2408e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,202][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ de] are: tensor([0.0175, 0.1544, 0.1233, 0.1486, 0.1993, 0.1253, 0.0257, 0.0435, 0.1624],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,203][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ de] are: tensor([0.0025, 0.1473, 0.0693, 0.0281, 0.0844, 0.4603, 0.1136, 0.0107, 0.0839],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,204][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ de] are: tensor([0.0096, 0.3670, 0.1637, 0.0821, 0.0705, 0.1749, 0.0603, 0.0300, 0.0421],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,204][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ de] are: tensor([0.3186, 0.0153, 0.0194, 0.1836, 0.0133, 0.0313, 0.1648, 0.1725, 0.0812],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,207][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ An] are: tensor([0.1741, 0.0852, 0.2249, 0.0827, 0.0474, 0.2109, 0.0466, 0.0091, 0.0581,
        0.0611], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,210][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ An] are: tensor([0.0085, 0.5016, 0.0334, 0.0355, 0.0321, 0.3100, 0.0402, 0.0149, 0.0030,
        0.0209], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,213][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ An] are: tensor([3.4368e-04, 4.9792e-01, 4.8786e-02, 5.2312e-02, 1.0689e-01, 1.6067e-01,
        3.4354e-02, 7.4198e-03, 1.2713e-02, 7.8599e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,218][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ An] are: tensor([0.0025, 0.3235, 0.0773, 0.0309, 0.0163, 0.4671, 0.0401, 0.0036, 0.0085,
        0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,220][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ An] are: tensor([9.9272e-05, 4.8077e-01, 7.7949e-02, 9.8239e-02, 7.1431e-02, 6.8867e-02,
        7.6777e-02, 9.3936e-03, 1.6907e-02, 9.9568e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,223][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ An] are: tensor([1.6163e-04, 1.5856e-01, 6.8575e-02, 2.8586e-02, 1.9762e-01, 2.5656e-01,
        1.4565e-01, 1.4254e-02, 6.5181e-02, 6.4851e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,228][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ An] are: tensor([0.0018, 0.2702, 0.0342, 0.0248, 0.0910, 0.3041, 0.1613, 0.0039, 0.0165,
        0.0922], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,232][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ An] are: tensor([0.0014, 0.2140, 0.0290, 0.0809, 0.0304, 0.4114, 0.1308, 0.0325, 0.0296,
        0.0401], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,233][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ An] are: tensor([0.7425, 0.0282, 0.0350, 0.0903, 0.0420, 0.0075, 0.0037, 0.0043, 0.0276,
        0.0189], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,234][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ An] are: tensor([0.0049, 0.1113, 0.0254, 0.0301, 0.1046, 0.4877, 0.1051, 0.0096, 0.0347,
        0.0865], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,235][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ An] are: tensor([0.0286, 0.4578, 0.1753, 0.0496, 0.0543, 0.0892, 0.0304, 0.0162, 0.0187,
        0.0801], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,237][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ An] are: tensor([0.1038, 0.0146, 0.0141, 0.1519, 0.0176, 0.0439, 0.2132, 0.1388, 0.0751,
        0.2271], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,241][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [za] are: tensor([0.1756, 0.0831, 0.2023, 0.0764, 0.0524, 0.2214, 0.0382, 0.0086, 0.0596,
        0.0457, 0.0367], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,246][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [za] are: tensor([0.0057, 0.4059, 0.0272, 0.0262, 0.0378, 0.3930, 0.0453, 0.0101, 0.0024,
        0.0236, 0.0228], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,249][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [za] are: tensor([8.6029e-05, 3.1134e-01, 2.8087e-02, 4.0441e-02, 1.2836e-01, 3.0217e-01,
        4.7853e-02, 7.2061e-03, 1.1497e-02, 1.0166e-01, 2.1287e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,252][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [za] are: tensor([5.1773e-04, 1.9286e-01, 7.6908e-02, 2.9001e-02, 2.5640e-02, 5.3908e-01,
        6.2106e-02, 3.7458e-03, 1.2598e-02, 5.2648e-02, 4.8952e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,255][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [za] are: tensor([4.8423e-05, 4.3539e-01, 8.6314e-02, 9.5599e-02, 6.7566e-02, 8.5673e-02,
        8.7550e-02, 9.2433e-03, 1.6846e-02, 9.2516e-02, 2.3252e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,257][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [za] are: tensor([6.6565e-05, 1.5032e-01, 5.2072e-02, 1.8048e-02, 1.5876e-01, 3.2888e-01,
        1.3851e-01, 1.1460e-02, 3.5153e-02, 5.1564e-02, 5.5168e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,263][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [za] are: tensor([0.0004, 0.1687, 0.0260, 0.0208, 0.0724, 0.3325, 0.2240, 0.0022, 0.0107,
        0.1114, 0.0309], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,264][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [za] are: tensor([0.0008, 0.2475, 0.0287, 0.0805, 0.0356, 0.3639, 0.1370, 0.0303, 0.0179,
        0.0444, 0.0135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,264][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [za] are: tensor([0.4361, 0.0620, 0.0737, 0.1566, 0.0757, 0.0238, 0.0113, 0.0112, 0.0569,
        0.0527, 0.0400], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,265][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [za] are: tensor([0.0021, 0.0776, 0.0157, 0.0136, 0.1113, 0.5221, 0.0903, 0.0059, 0.0262,
        0.0959, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,266][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [za] are: tensor([0.0133, 0.4258, 0.1632, 0.0469, 0.0618, 0.1038, 0.0330, 0.0182, 0.0180,
        0.0783, 0.0375], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,270][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [za] are: tensor([0.0109, 0.0089, 0.0144, 0.2064, 0.0140, 0.0304, 0.1107, 0.1806, 0.0814,
        0.3141, 0.0282], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:51,274][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1824, 0.0423, 0.2961, 0.0464, 0.0304, 0.2403, 0.0349, 0.0052, 0.0346,
        0.0317, 0.0162, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,279][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0027, 0.0862, 0.1243, 0.0573, 0.0268, 0.4113, 0.1894, 0.0102, 0.0137,
        0.0237, 0.0253, 0.0291], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,282][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ is] are: tensor([2.2843e-04, 1.5431e-01, 5.9003e-02, 3.0276e-02, 4.8811e-02, 4.3530e-01,
        1.8613e-01, 4.1806e-03, 7.8251e-03, 3.5255e-02, 1.6075e-02, 2.2600e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,285][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ is] are: tensor([7.6040e-05, 8.3719e-02, 1.1639e-01, 5.7362e-02, 2.0157e-02, 5.5993e-01,
        9.8845e-02, 8.4823e-04, 4.9835e-03, 2.9199e-02, 2.5218e-03, 2.5971e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,288][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ is] are: tensor([1.7034e-05, 3.5673e-01, 1.2355e-01, 6.8135e-02, 4.7079e-02, 1.2221e-01,
        1.1712e-01, 1.3346e-02, 2.3191e-02, 8.2628e-02, 2.2751e-02, 2.3238e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,291][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ is] are: tensor([1.0692e-04, 7.8202e-02, 7.1131e-02, 2.4786e-02, 8.1515e-02, 3.2770e-01,
        1.8545e-01, 2.0862e-02, 6.9922e-02, 6.8129e-02, 3.1057e-02, 4.1137e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,293][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ is] are: tensor([8.0389e-05, 8.7948e-02, 9.0327e-02, 5.0583e-02, 4.7316e-02, 3.8583e-01,
        2.0792e-01, 3.1596e-03, 3.4913e-02, 3.4604e-02, 1.7606e-02, 3.9711e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,294][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ is] are: tensor([5.2225e-04, 4.6029e-02, 1.6953e-02, 5.8992e-02, 1.9268e-02, 5.9840e-01,
        1.6012e-01, 1.0183e-02, 2.0737e-02, 2.3856e-02, 6.4680e-03, 3.8477e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,295][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ is] are: tensor([1.3116e-04, 3.9192e-02, 3.3035e-02, 5.6787e-02, 8.4357e-02, 1.0465e-01,
        1.8369e-02, 1.7998e-02, 5.0463e-02, 2.6002e-01, 3.0407e-01, 3.0932e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,296][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0014, 0.1633, 0.0547, 0.0338, 0.0464, 0.3850, 0.1504, 0.0109, 0.0536,
        0.0583, 0.0166, 0.0256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,298][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0070, 0.3500, 0.1602, 0.0782, 0.0485, 0.1211, 0.0509, 0.0203, 0.0280,
        0.0693, 0.0339, 0.0326], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,303][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0429, 0.0105, 0.0169, 0.1722, 0.0093, 0.0216, 0.0943, 0.1551, 0.1064,
        0.1797, 0.0325, 0.1587], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:51,307][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ a] are: tensor([0.1879, 0.0496, 0.2172, 0.0633, 0.0346, 0.2474, 0.0296, 0.0047, 0.0337,
        0.0327, 0.0158, 0.0364, 0.0472], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,312][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ a] are: tensor([0.0022, 0.1342, 0.0987, 0.0516, 0.0292, 0.3858, 0.1630, 0.0119, 0.0084,
        0.0408, 0.0307, 0.0163, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,315][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ a] are: tensor([1.1403e-04, 1.9431e-01, 5.3331e-02, 2.9596e-02, 7.4891e-02, 3.4559e-01,
        1.7047e-01, 5.9495e-03, 1.1574e-02, 6.6665e-02, 1.9812e-02, 1.6069e-02,
        1.1630e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,318][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ a] are: tensor([2.0978e-04, 1.1028e-01, 1.2147e-01, 3.8095e-02, 1.7705e-02, 5.7617e-01,
        6.9288e-02, 1.3820e-03, 6.6925e-03, 2.8236e-02, 3.2078e-03, 1.9733e-02,
        7.5286e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,321][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ a] are: tensor([4.2303e-05, 4.4029e-01, 9.7569e-02, 6.2687e-02, 4.5853e-02, 9.1139e-02,
        9.5388e-02, 9.1206e-03, 1.7089e-02, 8.5117e-02, 2.1273e-02, 1.3215e-02,
        2.1213e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,323][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ a] are: tensor([8.5820e-05, 9.2641e-02, 4.8878e-02, 2.7370e-02, 1.3883e-01, 2.5406e-01,
        1.8191e-01, 2.1557e-02, 6.9749e-02, 7.1147e-02, 4.2881e-02, 2.6565e-02,
        2.4318e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,324][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ a] are: tensor([1.1103e-04, 1.0143e-01, 5.6424e-02, 3.3564e-02, 6.8784e-02, 3.0888e-01,
        2.5272e-01, 3.7877e-03, 2.6749e-02, 7.2524e-02, 2.9546e-02, 2.8428e-02,
        1.7057e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,325][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ a] are: tensor([0.0007, 0.0956, 0.0169, 0.0689, 0.0287, 0.4681, 0.1545, 0.0183, 0.0311,
        0.0536, 0.0123, 0.0320, 0.0193], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,326][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ a] are: tensor([0.0028, 0.0833, 0.0632, 0.1086, 0.1012, 0.0699, 0.0229, 0.0194, 0.0604,
        0.1887, 0.1746, 0.0328, 0.0722], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,329][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ a] are: tensor([0.0005, 0.1624, 0.0372, 0.0432, 0.0672, 0.3662, 0.1668, 0.0096, 0.0397,
        0.0630, 0.0194, 0.0182, 0.0067], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,333][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ a] are: tensor([0.0032, 0.4053, 0.1279, 0.0654, 0.0460, 0.1067, 0.0490, 0.0178, 0.0219,
        0.0703, 0.0347, 0.0233, 0.0286], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,337][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ a] are: tensor([0.0326, 0.0085, 0.0115, 0.1209, 0.0079, 0.0165, 0.0985, 0.1060, 0.0644,
        0.1413, 0.0225, 0.0646, 0.3047], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:51,342][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ bit] are: tensor([0.2061, 0.0429, 0.2353, 0.0745, 0.0312, 0.2650, 0.0191, 0.0034, 0.0230,
        0.0250, 0.0140, 0.0269, 0.0292, 0.0044], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,347][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ bit] are: tensor([0.0027, 0.0999, 0.1374, 0.0581, 0.0279, 0.3603, 0.1381, 0.0076, 0.0120,
        0.0357, 0.0380, 0.0215, 0.0355, 0.0255], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,350][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ bit] are: tensor([7.3632e-05, 7.6432e-02, 3.7143e-02, 1.6113e-02, 4.3806e-02, 5.7617e-01,
        1.4862e-01, 3.6846e-03, 7.7219e-03, 4.2435e-02, 1.5135e-02, 2.0531e-02,
        1.0144e-02, 1.9908e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,353][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ bit] are: tensor([3.8198e-05, 5.0529e-02, 1.0079e-01, 2.5681e-02, 1.1618e-02, 6.1925e-01,
        1.1768e-01, 1.2164e-03, 5.5102e-03, 2.3843e-02, 3.4483e-03, 2.7512e-02,
        1.0844e-02, 2.0498e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,354][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ bit] are: tensor([3.6749e-05, 2.8545e-01, 1.0587e-01, 6.1564e-02, 4.1812e-02, 1.1354e-01,
        1.3452e-01, 1.0968e-02, 2.9455e-02, 1.0082e-01, 2.6768e-02, 2.2730e-02,
        3.2248e-02, 3.4216e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,355][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ bit] are: tensor([7.5048e-05, 8.7107e-02, 7.3645e-02, 1.8883e-02, 9.5916e-02, 2.4906e-01,
        1.9009e-01, 1.7661e-02, 7.1690e-02, 7.2191e-02, 3.8859e-02, 3.6793e-02,
        4.0706e-02, 7.3251e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,356][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ bit] are: tensor([1.1091e-04, 4.0911e-02, 7.8639e-02, 3.3962e-02, 3.7319e-02, 3.4347e-01,
        2.6910e-01, 3.3086e-03, 4.8132e-02, 4.3913e-02, 2.7310e-02, 4.8257e-02,
        1.9627e-02, 5.9479e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,358][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ bit] are: tensor([0.0013, 0.0850, 0.0266, 0.0686, 0.0227, 0.3915, 0.1898, 0.0153, 0.0301,
        0.0513, 0.0136, 0.0511, 0.0358, 0.0173], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,361][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ bit] are: tensor([0.0040, 0.0765, 0.0572, 0.0966, 0.0878, 0.0549, 0.0183, 0.0194, 0.0613,
        0.1843, 0.2014, 0.0279, 0.0706, 0.0398], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,366][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ bit] are: tensor([0.0007, 0.1357, 0.0366, 0.0206, 0.0340, 0.4890, 0.1194, 0.0066, 0.0409,
        0.0668, 0.0199, 0.0173, 0.0082, 0.0042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,370][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ bit] are: tensor([0.0038, 0.3388, 0.1307, 0.0623, 0.0520, 0.1174, 0.0588, 0.0225, 0.0290,
        0.0787, 0.0378, 0.0238, 0.0274, 0.0173], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,375][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ bit] are: tensor([0.0242, 0.0094, 0.0149, 0.1925, 0.0111, 0.0194, 0.0910, 0.1167, 0.0640,
        0.1207, 0.0163, 0.0616, 0.1938, 0.0643], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:51,380][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ different] are: tensor([0.1940, 0.0428, 0.1953, 0.0682, 0.0282, 0.2486, 0.0292, 0.0036, 0.0201,
        0.0276, 0.0202, 0.0493, 0.0510, 0.0066, 0.0152], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,383][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ different] are: tensor([0.0033, 0.1287, 0.0544, 0.0401, 0.0294, 0.4991, 0.0974, 0.0058, 0.0045,
        0.0345, 0.0383, 0.0100, 0.0202, 0.0125, 0.0219], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,384][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ different] are: tensor([2.5268e-05, 8.3668e-02, 2.4788e-02, 1.9825e-02, 5.3262e-02, 4.9500e-01,
        1.6706e-01, 4.5328e-03, 1.1205e-02, 7.6712e-02, 2.0485e-02, 1.6229e-02,
        1.3629e-02, 2.5325e-03, 1.1049e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,385][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ different] are: tensor([3.1713e-05, 4.9895e-02, 6.6592e-02, 1.4116e-02, 8.5488e-03, 6.1820e-01,
        1.8357e-01, 1.7887e-03, 4.5949e-03, 2.3850e-02, 5.0827e-03, 1.3824e-02,
        6.7012e-03, 2.3342e-03, 8.6754e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,386][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ different] are: tensor([2.3541e-05, 3.3678e-01, 9.3841e-02, 5.7464e-02, 4.9923e-02, 7.2981e-02,
        1.0585e-01, 6.4094e-03, 1.8967e-02, 1.0957e-01, 2.7010e-02, 1.5183e-02,
        3.1595e-02, 1.9537e-02, 5.4862e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,387][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ different] are: tensor([4.8555e-05, 5.4934e-02, 4.3912e-02, 1.0474e-02, 1.2976e-01, 3.5657e-01,
        1.6814e-01, 6.2952e-03, 3.5870e-02, 5.9504e-02, 4.3200e-02, 3.1391e-02,
        3.1179e-02, 3.3827e-03, 2.5348e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,390][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ different] are: tensor([8.7389e-05, 4.9118e-02, 3.4248e-02, 1.8554e-02, 3.6319e-02, 3.7972e-01,
        3.0489e-01, 1.7339e-03, 2.0938e-02, 5.9384e-02, 2.7754e-02, 2.0193e-02,
        1.8749e-02, 4.0540e-03, 2.4261e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,394][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ different] are: tensor([0.0016, 0.1933, 0.0310, 0.0672, 0.0232, 0.2966, 0.1965, 0.0141, 0.0190,
        0.0502, 0.0137, 0.0294, 0.0257, 0.0102, 0.0283], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,399][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ different] are: tensor([0.4320, 0.0373, 0.0534, 0.1165, 0.0480, 0.0266, 0.0114, 0.0115, 0.0513,
        0.0425, 0.0301, 0.0304, 0.0471, 0.0314, 0.0305], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,402][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ different] are: tensor([4.7057e-04, 2.9229e-02, 1.4213e-02, 1.3783e-02, 4.8365e-02, 6.5481e-01,
        9.0320e-02, 2.1600e-03, 2.6982e-02, 5.5574e-02, 2.0191e-02, 2.4633e-02,
        8.3857e-03, 2.4664e-03, 8.4193e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,406][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ different] are: tensor([0.0051, 0.4189, 0.1416, 0.0519, 0.0454, 0.0865, 0.0381, 0.0164, 0.0182,
        0.0664, 0.0313, 0.0222, 0.0266, 0.0154, 0.0159], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,411][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ different] are: tensor([0.0160, 0.0138, 0.0088, 0.2821, 0.0058, 0.0169, 0.0705, 0.0999, 0.0392,
        0.0698, 0.0098, 0.0375, 0.1656, 0.0571, 0.1073], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:51,413][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ from] are: tensor([0.1724, 0.0489, 0.2156, 0.0895, 0.0272, 0.2021, 0.0302, 0.0046, 0.0254,
        0.0291, 0.0167, 0.0430, 0.0417, 0.0078, 0.0118, 0.0339],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,413][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ from] are: tensor([0.0020, 0.1703, 0.0560, 0.0495, 0.0309, 0.4381, 0.1114, 0.0094, 0.0066,
        0.0271, 0.0273, 0.0092, 0.0199, 0.0122, 0.0204, 0.0099],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,414][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ from] are: tensor([8.7846e-05, 1.8635e-01, 4.6819e-02, 3.6370e-02, 9.3757e-02, 3.7349e-01,
        1.2479e-01, 5.7876e-03, 1.0897e-02, 5.5938e-02, 1.7178e-02, 1.6537e-02,
        1.0943e-02, 1.8894e-03, 1.1007e-02, 8.1587e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,415][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ from] are: tensor([1.2500e-04, 9.6328e-02, 8.6668e-02, 2.0051e-02, 1.4939e-02, 5.8822e-01,
        1.4075e-01, 1.9447e-03, 4.3003e-03, 2.2495e-02, 4.3093e-03, 9.1589e-03,
        4.5099e-03, 2.4896e-03, 1.2827e-03, 2.4306e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,417][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ from] are: tensor([2.3422e-05, 3.5335e-01, 1.0160e-01, 6.0375e-02, 4.5651e-02, 8.6129e-02,
        1.0410e-01, 8.1698e-03, 1.5939e-02, 8.2273e-02, 2.4468e-02, 1.5718e-02,
        2.2026e-02, 1.8731e-02, 3.9553e-02, 2.1895e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,419][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ from] are: tensor([2.3598e-05, 8.2128e-02, 4.3662e-02, 2.1887e-02, 1.1349e-01, 3.2688e-01,
        1.7667e-01, 1.3277e-02, 5.3010e-02, 4.9520e-02, 3.7419e-02, 2.8563e-02,
        2.2294e-02, 3.2962e-03, 1.7931e-02, 9.9461e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,422][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ from] are: tensor([6.2251e-05, 7.1342e-02, 3.2535e-02, 2.4229e-02, 5.0003e-02, 3.8329e-01,
        2.7690e-01, 2.8736e-03, 2.1545e-02, 4.9709e-02, 2.0346e-02, 2.1077e-02,
        1.3433e-02, 3.6053e-03, 1.5734e-02, 1.3317e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,425][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ from] are: tensor([3.4210e-04, 1.2843e-01, 2.5182e-02, 7.8229e-02, 2.3526e-02, 4.0224e-01,
        1.8008e-01, 1.7167e-02, 2.0398e-02, 3.0578e-02, 8.8661e-03, 2.3234e-02,
        1.3339e-02, 7.3211e-03, 2.2518e-02, 1.8552e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,429][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ from] are: tensor([0.0096, 0.0631, 0.0628, 0.0940, 0.0755, 0.0607, 0.0127, 0.0151, 0.0548,
        0.1558, 0.1032, 0.0341, 0.0741, 0.0397, 0.0734, 0.0717],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,432][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ from] are: tensor([9.4305e-05, 3.5802e-02, 2.1280e-02, 2.5487e-02, 4.7912e-02, 6.1499e-01,
        1.4636e-01, 6.4200e-03, 1.8146e-02, 2.6791e-02, 1.4780e-02, 2.2001e-02,
        5.0321e-03, 3.0091e-03, 5.4869e-03, 6.4051e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,437][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ from] are: tensor([0.0073, 0.3811, 0.1540, 0.0576, 0.0452, 0.0908, 0.0407, 0.0175, 0.0179,
        0.0634, 0.0270, 0.0244, 0.0256, 0.0124, 0.0131, 0.0218],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,442][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ from] are: tensor([0.0164, 0.0078, 0.0097, 0.1110, 0.0126, 0.0167, 0.0914, 0.0976, 0.0400,
        0.0921, 0.0195, 0.0382, 0.1206, 0.0596, 0.1301, 0.1365],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:51,443][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2315, 0.0376, 0.1977, 0.0599, 0.0227, 0.2470, 0.0223, 0.0023, 0.0203,
        0.0215, 0.0098, 0.0246, 0.0311, 0.0037, 0.0070, 0.0272, 0.0340],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,443][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0014, 0.0559, 0.1278, 0.0495, 0.0220, 0.3923, 0.1382, 0.0060, 0.0090,
        0.0293, 0.0212, 0.0172, 0.0308, 0.0187, 0.0148, 0.0130, 0.0530],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,444][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([7.8278e-05, 1.4983e-01, 5.6796e-02, 1.6985e-02, 4.8495e-02, 3.4509e-01,
        2.7689e-01, 3.6856e-03, 7.8491e-03, 3.1733e-02, 1.1213e-02, 1.4305e-02,
        8.6486e-03, 3.6184e-03, 7.4576e-03, 4.1463e-03, 1.3181e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,445][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([7.3635e-05, 7.3924e-02, 1.9886e-01, 1.5275e-02, 9.9076e-03, 5.6120e-01,
        9.3099e-02, 3.8194e-04, 1.5955e-03, 1.1801e-02, 1.4743e-03, 1.4233e-02,
        5.1828e-03, 1.2960e-03, 4.6517e-04, 1.1881e-03, 1.0047e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,447][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([1.4221e-05, 2.5674e-01, 1.1175e-01, 5.6802e-02, 4.4573e-02, 1.4104e-01,
        1.3897e-01, 7.1291e-03, 1.6779e-02, 8.1920e-02, 1.5200e-02, 1.5271e-02,
        2.0749e-02, 2.5252e-02, 3.4668e-02, 1.5273e-02, 1.7863e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,449][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([2.3805e-05, 3.8875e-02, 4.3735e-02, 1.9449e-02, 9.4135e-02, 2.7369e-01,
        2.2816e-01, 1.6803e-02, 7.1658e-02, 6.2311e-02, 2.8035e-02, 3.1182e-02,
        2.3376e-02, 5.3434e-03, 1.4646e-02, 1.1923e-02, 3.6646e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,452][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([3.2901e-05, 4.4506e-02, 5.3762e-02, 2.8400e-02, 4.5443e-02, 3.2076e-01,
        2.9987e-01, 2.7019e-03, 2.6547e-02, 4.0739e-02, 1.7985e-02, 3.7436e-02,
        1.2004e-02, 5.2798e-03, 1.0219e-02, 9.3719e-03, 4.4942e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,456][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0007, 0.0480, 0.0121, 0.0534, 0.0279, 0.4979, 0.1871, 0.0086, 0.0171,
        0.0333, 0.0064, 0.0294, 0.0126, 0.0064, 0.0096, 0.0127, 0.0366],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,461][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0002, 0.0371, 0.0240, 0.0332, 0.0369, 0.0977, 0.0118, 0.0123, 0.0336,
        0.2309, 0.2065, 0.0216, 0.0711, 0.0332, 0.0596, 0.0431, 0.0471],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,464][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.0126e-04, 7.3817e-02, 4.2409e-02, 2.1719e-02, 4.5702e-02, 4.3669e-01,
        2.3746e-01, 7.1241e-03, 2.7688e-02, 3.7329e-02, 1.1255e-02, 1.7414e-02,
        5.3410e-03, 5.8056e-03, 3.3294e-03, 6.0737e-03, 2.0744e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,469][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0050, 0.3815, 0.1171, 0.0621, 0.0344, 0.1097, 0.0379, 0.0152, 0.0213,
        0.0619, 0.0275, 0.0224, 0.0257, 0.0143, 0.0194, 0.0202, 0.0245],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,472][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0145, 0.0028, 0.0037, 0.0216, 0.0023, 0.0051, 0.0272, 0.0367, 0.0151,
        0.0400, 0.0097, 0.0175, 0.0778, 0.0370, 0.0900, 0.0813, 0.5175],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:51,473][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.1524, 0.0552, 0.1076, 0.0799, 0.0286, 0.2283, 0.0173, 0.0051, 0.0387,
        0.0426, 0.0177, 0.0350, 0.0459, 0.0083, 0.0116, 0.0496, 0.0512, 0.0250],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,474][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0019, 0.1224, 0.0620, 0.0483, 0.0364, 0.3823, 0.0968, 0.0116, 0.0089,
        0.0474, 0.0331, 0.0099, 0.0252, 0.0193, 0.0214, 0.0128, 0.0351, 0.0251],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,475][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ language] are: tensor([2.4039e-05, 1.0175e-01, 3.2632e-02, 2.1432e-02, 5.7926e-02, 3.7544e-01,
        1.8922e-01, 6.7321e-03, 1.6296e-02, 6.1440e-02, 1.7198e-02, 2.0414e-02,
        1.4046e-02, 7.5017e-03, 1.5884e-02, 1.0182e-02, 2.4434e-02, 2.7455e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,476][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ language] are: tensor([1.0629e-04, 6.1835e-02, 9.6555e-02, 3.2511e-02, 1.3871e-02, 5.2192e-01,
        1.3710e-01, 1.8290e-03, 6.2526e-03, 3.5340e-02, 4.7671e-03, 2.7966e-02,
        1.0417e-02, 3.7339e-03, 1.6072e-03, 6.9500e-03, 3.2114e-02, 5.1238e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,478][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ language] are: tensor([3.0528e-05, 3.5358e-01, 8.6725e-02, 5.7414e-02, 4.1848e-02, 1.0056e-01,
        6.9161e-02, 8.0701e-03, 2.5004e-02, 7.0109e-02, 1.7107e-02, 1.5396e-02,
        2.4694e-02, 2.4744e-02, 5.1796e-02, 1.7841e-02, 1.3974e-02, 2.1946e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,481][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ language] are: tensor([4.8689e-05, 1.2311e-01, 5.4275e-02, 2.7742e-02, 1.0920e-01, 2.5360e-01,
        1.0274e-01, 1.8690e-02, 5.2414e-02, 7.0162e-02, 3.5571e-02, 1.7931e-02,
        2.3429e-02, 7.2271e-03, 2.6055e-02, 1.6718e-02, 3.3125e-02, 2.7966e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,484][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ language] are: tensor([2.8530e-05, 6.4268e-02, 6.3276e-02, 3.4913e-02, 4.7816e-02, 3.3149e-01,
        1.5551e-01, 3.0584e-03, 2.9143e-02, 4.4321e-02, 1.9920e-02, 2.5989e-02,
        1.6362e-02, 9.2803e-03, 2.7333e-02, 1.6142e-02, 6.5601e-02, 4.5555e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,488][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0009, 0.1297, 0.0202, 0.0461, 0.0323, 0.3105, 0.1014, 0.0235, 0.0366,
        0.0652, 0.0148, 0.0328, 0.0207, 0.0128, 0.0329, 0.0203, 0.0427, 0.0567],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,493][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0023, 0.0964, 0.0768, 0.0699, 0.0599, 0.0498, 0.0088, 0.0105, 0.0270,
        0.1224, 0.0939, 0.0242, 0.0496, 0.0446, 0.0628, 0.0507, 0.0456, 0.1048],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,496][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ language] are: tensor([2.5931e-04, 1.6907e-01, 3.7564e-02, 2.6749e-02, 6.4426e-02, 3.4573e-01,
        8.7609e-02, 6.7496e-03, 3.4173e-02, 7.1059e-02, 1.8818e-02, 2.5447e-02,
        9.3272e-03, 9.5486e-03, 7.0089e-03, 9.8876e-03, 1.6187e-02, 6.0379e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,501][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0068, 0.3711, 0.1258, 0.0523, 0.0382, 0.0851, 0.0351, 0.0168, 0.0181,
        0.0603, 0.0280, 0.0209, 0.0274, 0.0155, 0.0165, 0.0205, 0.0260, 0.0354],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,502][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0040, 0.0040, 0.0040, 0.0557, 0.0026, 0.0066, 0.0148, 0.0428, 0.0127,
        0.0455, 0.0052, 0.0173, 0.0631, 0.0263, 0.0709, 0.0577, 0.3869, 0.1800],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:51,503][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ used] are: tensor([0.2349, 0.0237, 0.2004, 0.0465, 0.0144, 0.1627, 0.0140, 0.0018, 0.0141,
        0.0171, 0.0062, 0.0189, 0.0233, 0.0036, 0.0044, 0.0215, 0.0229, 0.0094,
        0.1603], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,504][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ used] are: tensor([0.0013, 0.0341, 0.1035, 0.0503, 0.0246, 0.3475, 0.1509, 0.0058, 0.0114,
        0.0306, 0.0209, 0.0232, 0.0360, 0.0206, 0.0164, 0.0146, 0.0549, 0.0264,
        0.0269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,506][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ used] are: tensor([4.7640e-05, 6.1834e-02, 6.7788e-02, 1.6080e-02, 3.0693e-02, 4.0340e-01,
        2.6862e-01, 3.0170e-03, 6.5844e-03, 3.1186e-02, 8.9576e-03, 1.4815e-02,
        8.6557e-03, 4.4022e-03, 8.8280e-03, 6.1805e-03, 2.0082e-02, 2.9132e-02,
        9.6904e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,508][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ used] are: tensor([2.3566e-04, 7.5529e-02, 2.0561e-01, 2.6403e-02, 1.1913e-02, 4.9774e-01,
        1.1243e-01, 5.0091e-04, 2.0236e-03, 1.4139e-02, 1.6575e-03, 1.4573e-02,
        5.5831e-03, 1.5839e-03, 3.4926e-04, 2.2138e-03, 1.8322e-02, 4.5727e-03,
        4.6133e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,511][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ used] are: tensor([2.3359e-05, 1.9823e-01, 1.6498e-01, 5.2308e-02, 3.2143e-02, 1.0418e-01,
        6.1484e-02, 9.4672e-03, 2.3114e-02, 6.8253e-02, 1.9359e-02, 2.1388e-02,
        2.7981e-02, 2.9745e-02, 5.8818e-02, 2.0608e-02, 1.8521e-02, 3.7261e-02,
        5.2140e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,512][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ used] are: tensor([3.8836e-05, 6.5258e-02, 5.8381e-02, 2.4713e-02, 7.9660e-02, 2.1188e-01,
        1.4579e-01, 1.4538e-02, 6.2238e-02, 6.7792e-02, 3.4094e-02, 3.2087e-02,
        2.7676e-02, 8.3794e-03, 2.3999e-02, 2.1934e-02, 6.0941e-02, 3.5520e-02,
        2.5081e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,515][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ used] are: tensor([1.6189e-05, 3.3480e-02, 7.9529e-02, 3.1201e-02, 3.5771e-02, 2.8408e-01,
        2.4645e-01, 2.3036e-03, 2.6921e-02, 2.7591e-02, 1.3967e-02, 3.1005e-02,
        1.3889e-02, 7.9132e-03, 1.5830e-02, 1.1753e-02, 5.6193e-02, 4.4860e-02,
        3.7248e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,520][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ used] are: tensor([0.0014, 0.0530, 0.0212, 0.0423, 0.0245, 0.3743, 0.1764, 0.0098, 0.0250,
        0.0433, 0.0080, 0.0449, 0.0180, 0.0081, 0.0171, 0.0186, 0.0456, 0.0493,
        0.0192], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,525][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ used] are: tensor([0.0057, 0.0575, 0.0279, 0.0319, 0.0429, 0.0590, 0.0128, 0.0169, 0.0353,
        0.1325, 0.1738, 0.0223, 0.0657, 0.0348, 0.0552, 0.0377, 0.0605, 0.1127,
        0.0150], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,528][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ used] are: tensor([7.7082e-05, 7.3830e-02, 4.7895e-02, 3.1517e-02, 4.3452e-02, 4.2289e-01,
        1.6127e-01, 5.7622e-03, 3.3091e-02, 3.5296e-02, 1.3938e-02, 1.5460e-02,
        5.0693e-03, 5.2453e-03, 5.8321e-03, 8.0222e-03, 1.8395e-02, 4.6363e-02,
        2.6594e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,532][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ used] are: tensor([0.0068, 0.3695, 0.1311, 0.0601, 0.0377, 0.0753, 0.0401, 0.0145, 0.0203,
        0.0573, 0.0257, 0.0192, 0.0219, 0.0128, 0.0145, 0.0209, 0.0205, 0.0269,
        0.0248], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,534][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ used] are: tensor([0.0100, 0.0047, 0.0057, 0.0492, 0.0028, 0.0070, 0.0175, 0.0191, 0.0097,
        0.0292, 0.0041, 0.0117, 0.0463, 0.0260, 0.0481, 0.0619, 0.3071, 0.1338,
        0.2061], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:51,535][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ by] are: tensor([0.1200, 0.0302, 0.1530, 0.0700, 0.0177, 0.2022, 0.0131, 0.0035, 0.0275,
        0.0255, 0.0145, 0.0216, 0.0288, 0.0034, 0.0061, 0.0316, 0.0351, 0.0114,
        0.1496, 0.0349], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,536][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ by] are: tensor([0.0006, 0.0325, 0.1321, 0.0406, 0.0261, 0.2843, 0.1526, 0.0088, 0.0087,
        0.0244, 0.0251, 0.0173, 0.0324, 0.0279, 0.0192, 0.0144, 0.0596, 0.0389,
        0.0369, 0.0177], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,537][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ by] are: tensor([2.7742e-05, 5.0935e-02, 6.4281e-02, 1.6158e-02, 4.6207e-02, 3.3311e-01,
        3.0641e-01, 7.0474e-03, 1.0242e-02, 3.5897e-02, 1.4882e-02, 1.5419e-02,
        1.1375e-02, 7.3359e-03, 1.3576e-02, 6.0470e-03, 1.7200e-02, 2.7794e-02,
        1.3720e-02, 2.3355e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,539][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ by] are: tensor([1.2561e-04, 6.2954e-02, 1.5867e-01, 1.6276e-02, 2.1450e-02, 4.9791e-01,
        1.4627e-01, 8.8200e-04, 2.9733e-03, 2.1355e-02, 3.7305e-03, 1.5449e-02,
        9.5803e-03, 2.1591e-03, 1.2980e-03, 2.6556e-03, 2.2208e-02, 3.9462e-03,
        8.6949e-03, 1.4138e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,541][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ by] are: tensor([1.9111e-05, 2.0772e-01, 1.5173e-01, 6.8621e-02, 3.6698e-02, 1.1997e-01,
        9.5285e-02, 1.2569e-02, 1.9087e-02, 6.3485e-02, 1.5507e-02, 1.7622e-02,
        2.1739e-02, 3.2125e-02, 3.0312e-02, 1.5813e-02, 1.8682e-02, 3.3812e-02,
        3.4369e-02, 4.8356e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,543][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ by] are: tensor([1.4137e-05, 3.9223e-02, 4.3560e-02, 2.4277e-02, 6.4291e-02, 2.5975e-01,
        2.1233e-01, 2.7388e-02, 6.2173e-02, 5.5517e-02, 2.8999e-02, 2.1210e-02,
        2.0913e-02, 6.8962e-03, 2.0631e-02, 1.6743e-02, 3.0493e-02, 2.3842e-02,
        2.5671e-02, 1.6078e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,546][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ by] are: tensor([7.1575e-06, 2.5301e-02, 5.7061e-02, 2.9020e-02, 4.5091e-02, 2.6531e-01,
        2.5347e-01, 3.8005e-03, 2.8520e-02, 2.9965e-02, 1.8358e-02, 2.9903e-02,
        1.2774e-02, 1.0467e-02, 1.7735e-02, 1.2598e-02, 4.9721e-02, 4.9849e-02,
        4.9395e-02, 1.1648e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,549][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ by] are: tensor([2.6263e-04, 6.3925e-02, 1.3522e-02, 5.2557e-02, 3.2521e-02, 3.2386e-01,
        1.6794e-01, 1.6379e-02, 2.9287e-02, 4.4013e-02, 1.4021e-02, 3.9055e-02,
        1.8597e-02, 1.6405e-02, 2.5510e-02, 2.0524e-02, 3.1717e-02, 5.4791e-02,
        2.2063e-02, 1.3052e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,552][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ by] are: tensor([3.6974e-05, 5.9629e-02, 3.1820e-02, 3.9476e-02, 4.2097e-02, 8.3008e-02,
        9.9020e-03, 1.0049e-02, 1.5855e-02, 1.9778e-01, 1.6336e-01, 1.6194e-02,
        3.8367e-02, 2.5022e-02, 6.1765e-02, 4.2263e-02, 3.0015e-02, 1.0750e-01,
        1.3877e-02, 1.1996e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,554][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ by] are: tensor([7.1536e-05, 8.4202e-02, 4.2844e-02, 3.4232e-02, 3.6401e-02, 3.4856e-01,
        1.8457e-01, 1.1793e-02, 2.5885e-02, 3.7373e-02, 1.4697e-02, 1.3935e-02,
        7.0557e-03, 7.4480e-03, 6.6597e-03, 8.9367e-03, 1.9814e-02, 6.1121e-02,
        3.7830e-02, 1.6575e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,559][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ by] are: tensor([0.0037, 0.2884, 0.1114, 0.0700, 0.0430, 0.0914, 0.0488, 0.0193, 0.0237,
        0.0609, 0.0328, 0.0206, 0.0243, 0.0162, 0.0210, 0.0228, 0.0232, 0.0328,
        0.0325, 0.0133], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,563][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ by] are: tensor([0.0312, 0.0067, 0.0082, 0.0607, 0.0070, 0.0078, 0.0185, 0.0168, 0.0075,
        0.0217, 0.0043, 0.0076, 0.0209, 0.0141, 0.0226, 0.0325, 0.1438, 0.0711,
        0.1752, 0.3220], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:51,564][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.2631, 0.0273, 0.1592, 0.0519, 0.0145, 0.1840, 0.0122, 0.0012, 0.0135,
        0.0126, 0.0051, 0.0162, 0.0180, 0.0018, 0.0032, 0.0152, 0.0197, 0.0074,
        0.1099, 0.0192, 0.0450], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,565][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0022, 0.0961, 0.1801, 0.0510, 0.0241, 0.3111, 0.1099, 0.0043, 0.0055,
        0.0226, 0.0171, 0.0121, 0.0185, 0.0147, 0.0085, 0.0078, 0.0274, 0.0341,
        0.0199, 0.0093, 0.0237], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,566][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([1.5152e-04, 1.4659e-01, 6.8479e-02, 1.7563e-02, 4.7706e-02, 3.0118e-01,
        2.7091e-01, 2.5816e-03, 6.9616e-03, 3.2102e-02, 8.7649e-03, 1.1728e-02,
        8.2763e-03, 4.5969e-03, 8.3020e-03, 3.9765e-03, 1.3131e-02, 2.6338e-02,
        5.0183e-03, 1.1917e-03, 1.4451e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,568][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.9207e-04, 1.0372e-01, 2.3989e-01, 8.7516e-03, 9.9680e-03, 5.1047e-01,
        8.0314e-02, 1.4245e-04, 5.9834e-04, 5.3960e-03, 6.3859e-04, 5.0445e-03,
        2.3989e-03, 4.8130e-04, 1.5939e-04, 3.6849e-04, 4.2144e-03, 9.4840e-04,
        2.8713e-03, 2.3001e-04, 2.3205e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,570][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([2.6152e-05, 2.8519e-01, 1.2623e-01, 6.8471e-02, 4.0919e-02, 1.1742e-01,
        9.0883e-02, 6.8930e-03, 1.4827e-02, 5.9365e-02, 1.2869e-02, 1.2085e-02,
        1.5005e-02, 1.9658e-02, 2.6329e-02, 1.3431e-02, 1.5086e-02, 3.5534e-02,
        2.2233e-02, 3.2653e-03, 1.4272e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,572][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([3.4004e-05, 3.7251e-02, 4.4106e-02, 2.4313e-02, 1.1013e-01, 2.2149e-01,
        2.0323e-01, 1.5191e-02, 5.4644e-02, 5.9214e-02, 2.6431e-02, 2.3851e-02,
        2.0046e-02, 4.8851e-03, 1.3146e-02, 9.4300e-03, 2.5439e-02, 2.1191e-02,
        1.3374e-02, 8.9845e-03, 6.3620e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,575][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([3.4985e-05, 4.6543e-02, 7.5705e-02, 2.9826e-02, 4.0788e-02, 2.7910e-01,
        2.4575e-01, 1.7492e-03, 2.1975e-02, 3.0373e-02, 1.2381e-02, 2.8653e-02,
        9.2277e-03, 5.1954e-03, 7.7865e-03, 7.3027e-03, 4.0403e-02, 4.6865e-02,
        1.8607e-02, 6.3429e-03, 4.5387e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,580][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0009, 0.0699, 0.0096, 0.0504, 0.0305, 0.5065, 0.1307, 0.0071, 0.0118,
        0.0301, 0.0061, 0.0223, 0.0107, 0.0061, 0.0091, 0.0092, 0.0231, 0.0372,
        0.0079, 0.0061, 0.0148], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,582][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([1.4567e-04, 4.2778e-02, 1.9740e-02, 2.9165e-02, 3.4461e-02, 8.2270e-02,
        9.5758e-03, 9.0961e-03, 2.7597e-02, 1.9744e-01, 2.0469e-01, 1.4524e-02,
        4.7457e-02, 2.4273e-02, 4.6947e-02, 3.7237e-02, 3.1293e-02, 1.0396e-01,
        8.9163e-03, 9.3606e-03, 1.9072e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,585][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([1.0088e-04, 6.0786e-02, 4.1483e-02, 2.8111e-02, 5.0535e-02, 4.4260e-01,
        1.9502e-01, 5.1373e-03, 2.4064e-02, 2.4564e-02, 8.8514e-03, 1.1585e-02,
        4.1394e-03, 4.5569e-03, 2.8846e-03, 4.3566e-03, 1.3150e-02, 3.7479e-02,
        1.5319e-02, 8.8348e-03, 1.6438e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,590][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0060, 0.3709, 0.1176, 0.0699, 0.0336, 0.0983, 0.0397, 0.0111, 0.0168,
        0.0467, 0.0205, 0.0168, 0.0204, 0.0110, 0.0142, 0.0143, 0.0185, 0.0288,
        0.0220, 0.0079, 0.0152], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,593][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0125, 0.0019, 0.0022, 0.0109, 0.0017, 0.0022, 0.0109, 0.0112, 0.0053,
        0.0116, 0.0040, 0.0045, 0.0180, 0.0102, 0.0254, 0.0241, 0.1230, 0.0954,
        0.1109, 0.2198, 0.2940], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:51,723][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:06:51,727][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,731][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,735][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,738][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,740][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,741][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,743][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,744][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,745][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,746][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,746][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,747][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:06:51,749][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7720, 0.2280], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,753][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0766, 0.9234], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,757][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0016, 0.9984], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,761][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0860, 0.9140], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,764][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([1.3175e-04, 9.9987e-01], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,769][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0019, 0.9981], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,773][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0037, 0.9963], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,774][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0240, 0.9760], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,775][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.9807, 0.0193], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,776][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0087, 0.9913], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,776][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0310, 0.9690], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,778][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0353, 0.9647], device='cuda:0') for source tokens [The language]
[2024-07-23 21:06:51,781][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.2629, 0.0914, 0.6457], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,786][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0045, 0.5248, 0.4707], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,789][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([3.9678e-05, 5.6751e-01, 4.3245e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,792][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([3.7488e-04, 3.7695e-01, 6.2267e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,794][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([1.9644e-05, 5.9393e-01, 4.0606e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,797][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([1.0585e-04, 5.0522e-01, 4.9467e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,800][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([5.6646e-05, 4.6234e-01, 5.3761e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,803][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0079, 0.7463, 0.2458], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,804][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.8695, 0.0685, 0.0620], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,805][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([1.0911e-04, 7.6530e-01, 2.3459e-01], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,805][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([1.5561e-04, 9.4198e-01, 5.7866e-02], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,806][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([3.9455e-04, 9.2158e-01, 7.8026e-02], device='cuda:0') for source tokens [The language used]
[2024-07-23 21:06:51,809][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.3820, 0.0849, 0.3725, 0.1606], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,813][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0069, 0.2039, 0.5057, 0.2835], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,815][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([1.5815e-04, 6.4955e-01, 2.7719e-01, 7.3100e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,818][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([4.5751e-04, 2.0142e-01, 7.0363e-01, 9.4490e-02], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,821][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([3.5762e-05, 5.7836e-01, 2.4883e-01, 1.7278e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,824][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([1.2382e-04, 2.7079e-01, 4.5589e-01, 2.7320e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,827][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([1.6157e-04, 2.4215e-01, 5.1987e-01, 2.3783e-01], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,831][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([0.0049, 0.4781, 0.1358, 0.3812], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,833][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.5984, 0.2022, 0.0918, 0.1076], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,833][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([0.0009, 0.4177, 0.3321, 0.2493], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,834][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([0.0017, 0.8408, 0.1222, 0.0353], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,835][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0023, 0.5672, 0.3622, 0.0683], device='cuda:0') for source tokens [The language used by]
[2024-07-23 21:06:51,836][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ Juan] are: tensor([0.4906, 0.1153, 0.2447, 0.0951, 0.0543], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,838][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ Juan] are: tensor([0.0687, 0.3593, 0.1667, 0.1573, 0.2480], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,841][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ Juan] are: tensor([0.0024, 0.4905, 0.1091, 0.1107, 0.2874], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,845][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ Juan] are: tensor([0.0015, 0.3106, 0.5804, 0.0672, 0.0402], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,848][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ Juan] are: tensor([2.7195e-04, 5.5995e-01, 2.3866e-01, 9.5593e-02, 1.0552e-01],
       device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,853][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ Juan] are: tensor([0.0005, 0.2284, 0.1805, 0.0919, 0.4986], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,857][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ Juan] are: tensor([0.0026, 0.2730, 0.2460, 0.1258, 0.3527], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,862][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ Juan] are: tensor([0.0128, 0.3497, 0.1085, 0.1915, 0.3375], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,863][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ Juan] are: tensor([0.9945, 0.0016, 0.0017, 0.0013, 0.0010], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,864][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ Juan] are: tensor([0.0040, 0.3987, 0.0854, 0.1538, 0.3581], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,864][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ Juan] are: tensor([0.0429, 0.8200, 0.1151, 0.0164, 0.0055], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,866][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ Juan] are: tensor([0.0031, 0.5952, 0.2459, 0.0387, 0.1171], device='cuda:0') for source tokens [The language used by Juan]
[2024-07-23 21:06:51,869][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ B] are: tensor([0.5752, 0.0661, 0.1554, 0.0917, 0.0186, 0.0930], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,874][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ B] are: tensor([0.0212, 0.2848, 0.0817, 0.0876, 0.0584, 0.4663], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,878][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ B] are: tensor([0.0007, 0.4983, 0.0445, 0.0597, 0.1303, 0.2664], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,883][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ B] are: tensor([0.0033, 0.4167, 0.2801, 0.0367, 0.0139, 0.2493], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,886][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ B] are: tensor([2.6867e-05, 5.2041e-01, 2.6487e-01, 1.0214e-01, 6.0152e-02, 5.2395e-02],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,889][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ B] are: tensor([1.7204e-04, 1.1565e-01, 1.2124e-01, 3.9821e-02, 2.3569e-01, 4.8742e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,891][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ B] are: tensor([0.0008, 0.1846, 0.1481, 0.0886, 0.0990, 0.4789], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,892][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ B] are: tensor([0.0017, 0.1876, 0.0574, 0.1225, 0.0537, 0.5770], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,893][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ B] are: tensor([9.9526e-01, 1.0045e-03, 1.2160e-03, 2.4047e-03, 7.0309e-05, 4.5516e-05],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,894][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ B] are: tensor([0.0026, 0.1362, 0.0957, 0.0670, 0.1849, 0.5136], device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,895][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ B] are: tensor([6.4564e-03, 8.7578e-01, 1.0671e-01, 3.8271e-03, 4.9106e-04, 6.7355e-03],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,896][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ B] are: tensor([2.8129e-04, 5.6741e-01, 5.9020e-02, 1.1738e-02, 2.9231e-02, 3.3232e-01],
       device='cuda:0') for source tokens [The language used by Juan B]
[2024-07-23 21:06:51,900][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [aut] are: tensor([0.5792, 0.0387, 0.1680, 0.0453, 0.0115, 0.1392, 0.0183],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,904][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [aut] are: tensor([0.0116, 0.0457, 0.0942, 0.0496, 0.0393, 0.4899, 0.2698],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,907][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [aut] are: tensor([3.4707e-04, 1.2464e-01, 4.2129e-02, 1.9246e-02, 5.9984e-02, 4.6067e-01,
        2.9298e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,909][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [aut] are: tensor([1.0799e-04, 1.5323e-01, 3.0950e-01, 1.4208e-02, 5.3699e-03, 4.2042e-01,
        9.7161e-02], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,912][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [aut] are: tensor([4.3899e-05, 2.9272e-01, 1.7023e-01, 6.2377e-02, 6.1887e-02, 1.0486e-01,
        3.0789e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,915][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [aut] are: tensor([1.8865e-04, 4.1788e-02, 6.3724e-02, 1.5672e-02, 7.9030e-02, 2.7866e-01,
        5.2094e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,918][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [aut] are: tensor([4.1583e-04, 5.4250e-02, 8.6184e-02, 3.2407e-02, 6.5076e-02, 3.4218e-01,
        4.1949e-01], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,921][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [aut] are: tensor([0.0019, 0.0740, 0.0272, 0.0505, 0.0186, 0.3886, 0.4392],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,922][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [aut] are: tensor([0.9529, 0.0127, 0.0173, 0.0084, 0.0023, 0.0052, 0.0011],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,923][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [aut] are: tensor([0.0019, 0.1134, 0.0425, 0.0269, 0.0442, 0.4685, 0.3025],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,923][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [aut] are: tensor([3.9574e-03, 8.8244e-01, 1.0361e-01, 3.0374e-03, 4.5745e-04, 5.9823e-03,
        5.1139e-04], device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,925][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [aut] are: tensor([0.0012, 0.4162, 0.1407, 0.0146, 0.0171, 0.2674, 0.1429],
       device='cuda:0') for source tokens [The language used by Juan Baut]
[2024-07-23 21:06:51,928][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ista] are: tensor([0.1868, 0.0542, 0.3158, 0.0802, 0.0378, 0.2800, 0.0363, 0.0088],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,933][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ista] are: tensor([0.0018, 0.0612, 0.0900, 0.0230, 0.0431, 0.5000, 0.2742, 0.0066],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,936][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ista] are: tensor([2.6684e-05, 8.3577e-02, 6.6865e-02, 1.9084e-02, 5.8888e-02, 4.6371e-01,
        3.0480e-01, 3.0429e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,939][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ista] are: tensor([1.4895e-05, 5.4681e-02, 1.6920e-01, 1.8462e-02, 8.9610e-03, 5.2310e-01,
        2.2513e-01, 4.4177e-04], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,941][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ista] are: tensor([1.3381e-05, 3.0584e-01, 2.4530e-01, 4.2764e-02, 6.7159e-02, 1.5415e-01,
        1.7236e-01, 1.2404e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,944][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ista] are: tensor([2.9726e-04, 1.0187e-01, 1.4140e-01, 2.0130e-02, 1.2023e-01, 3.2131e-01,
        2.7116e-01, 2.3610e-02], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,947][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ista] are: tensor([2.3489e-05, 2.3628e-02, 1.1083e-01, 3.5863e-02, 5.9085e-02, 4.1831e-01,
        3.4985e-01, 2.4067e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,950][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ista] are: tensor([0.0009, 0.0771, 0.0364, 0.0372, 0.0331, 0.3661, 0.4373, 0.0118],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,951][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ista] are: tensor([0.1069, 0.2764, 0.3571, 0.0807, 0.0548, 0.1008, 0.0216, 0.0017],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,952][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ista] are: tensor([3.8733e-04, 1.2487e-01, 7.4769e-02, 1.4277e-02, 4.4445e-02, 4.8912e-01,
        2.4374e-01, 8.3923e-03], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,953][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ista] are: tensor([1.1965e-03, 8.8393e-01, 9.4943e-02, 2.9215e-03, 7.6906e-04, 1.3886e-02,
        2.1601e-03, 1.9859e-04], device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,955][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ista] are: tensor([0.0004, 0.1969, 0.1572, 0.0111, 0.0285, 0.2852, 0.3131, 0.0076],
       device='cuda:0') for source tokens [The language used by Juan Bautista]
[2024-07-23 21:06:51,958][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ de] are: tensor([0.1698, 0.0524, 0.3068, 0.0560, 0.0469, 0.2428, 0.0445, 0.0086, 0.0722],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,958][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ de] are: tensor([0.0035, 0.0634, 0.1201, 0.0335, 0.0406, 0.5364, 0.1670, 0.0163, 0.0191],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,961][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ de] are: tensor([1.5691e-04, 7.6641e-02, 6.6270e-02, 2.2972e-02, 6.0074e-02, 5.1026e-01,
        2.5026e-01, 5.0930e-03, 8.2719e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,964][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ de] are: tensor([1.7912e-04, 1.1250e-01, 1.9918e-01, 3.2299e-02, 2.0111e-02, 5.4249e-01,
        8.5488e-02, 1.3848e-03, 6.3664e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,967][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ de] are: tensor([6.9024e-05, 2.1342e-01, 9.4633e-02, 5.6670e-02, 9.1616e-02, 1.9448e-01,
        2.4670e-01, 2.3475e-02, 7.8935e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,971][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ de] are: tensor([0.0007, 0.0994, 0.0662, 0.0226, 0.1244, 0.2785, 0.2366, 0.0285, 0.1430],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,974][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ de] are: tensor([1.9568e-04, 2.6955e-02, 6.6657e-02, 3.6196e-02, 9.1871e-02, 4.5835e-01,
        2.7491e-01, 4.8629e-03, 4.0002e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,976][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ de] are: tensor([6.9026e-04, 2.7779e-02, 1.1161e-02, 2.3999e-02, 3.0541e-02, 6.9317e-01,
        1.5386e-01, 1.6390e-02, 4.2408e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,981][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ de] are: tensor([0.1589, 0.1942, 0.2104, 0.1066, 0.0718, 0.1769, 0.0340, 0.0069, 0.0402],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,982][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ de] are: tensor([0.0025, 0.1473, 0.0693, 0.0281, 0.0844, 0.4603, 0.1136, 0.0107, 0.0839],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,983][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ de] are: tensor([3.6423e-03, 8.1564e-01, 1.1932e-01, 1.3411e-02, 4.0718e-03, 4.0054e-02,
        2.5772e-03, 4.2489e-04, 8.5892e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,983][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ de] are: tensor([0.0035, 0.2731, 0.1770, 0.0283, 0.0418, 0.2458, 0.1386, 0.0088, 0.0830],
       device='cuda:0') for source tokens [The language used by Juan Bautista de]
[2024-07-23 21:06:51,985][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ An] are: tensor([0.1741, 0.0852, 0.2249, 0.0827, 0.0474, 0.2109, 0.0466, 0.0091, 0.0581,
        0.0611], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,988][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ An] are: tensor([0.0085, 0.5016, 0.0334, 0.0355, 0.0321, 0.3100, 0.0402, 0.0149, 0.0030,
        0.0209], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,991][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ An] are: tensor([3.4368e-04, 4.9792e-01, 4.8786e-02, 5.2312e-02, 1.0689e-01, 1.6067e-01,
        3.4354e-02, 7.4198e-03, 1.2713e-02, 7.8599e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,995][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ An] are: tensor([0.0025, 0.3235, 0.0773, 0.0309, 0.0163, 0.4671, 0.0401, 0.0036, 0.0085,
        0.0303], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:51,998][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ An] are: tensor([9.9272e-05, 4.8077e-01, 7.7949e-02, 9.8239e-02, 7.1431e-02, 6.8867e-02,
        7.6777e-02, 9.3936e-03, 1.6907e-02, 9.9568e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,000][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ An] are: tensor([1.6163e-04, 1.5856e-01, 6.8575e-02, 2.8586e-02, 1.9762e-01, 2.5656e-01,
        1.4565e-01, 1.4254e-02, 6.5181e-02, 6.4851e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,005][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ An] are: tensor([0.0018, 0.2702, 0.0342, 0.0248, 0.0910, 0.3041, 0.1613, 0.0039, 0.0165,
        0.0922], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,010][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ An] are: tensor([0.0014, 0.2140, 0.0290, 0.0809, 0.0304, 0.4114, 0.1308, 0.0325, 0.0296,
        0.0401], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,011][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ An] are: tensor([9.9799e-01, 1.5513e-04, 5.8324e-04, 1.2270e-03, 2.0348e-05, 7.2944e-06,
        3.5129e-06, 2.0999e-06, 1.0025e-05, 1.6479e-07], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,012][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ An] are: tensor([0.0049, 0.1113, 0.0254, 0.0301, 0.1046, 0.4877, 0.1051, 0.0096, 0.0347,
        0.0865], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,012][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ An] are: tensor([7.3046e-03, 8.4666e-01, 1.2906e-01, 3.2322e-03, 1.9028e-03, 7.2401e-03,
        5.8982e-04, 1.1366e-04, 1.0686e-04, 3.7908e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,013][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ An] are: tensor([1.1972e-04, 2.5018e-01, 7.1636e-02, 1.3214e-02, 2.1861e-02, 3.1561e-01,
        4.9823e-02, 2.0454e-02, 6.2887e-02, 1.9422e-01], device='cuda:0') for source tokens [The language used by Juan Bautista de An]
[2024-07-23 21:06:52,016][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [za] are: tensor([0.1756, 0.0831, 0.2023, 0.0764, 0.0524, 0.2214, 0.0382, 0.0086, 0.0596,
        0.0457, 0.0367], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,020][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [za] are: tensor([0.0057, 0.4059, 0.0272, 0.0262, 0.0378, 0.3930, 0.0453, 0.0101, 0.0024,
        0.0236, 0.0228], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,023][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [za] are: tensor([8.6029e-05, 3.1134e-01, 2.8087e-02, 4.0441e-02, 1.2836e-01, 3.0217e-01,
        4.7853e-02, 7.2061e-03, 1.1497e-02, 1.0166e-01, 2.1287e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,026][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [za] are: tensor([5.1773e-04, 1.9286e-01, 7.6908e-02, 2.9001e-02, 2.5640e-02, 5.3908e-01,
        6.2106e-02, 3.7458e-03, 1.2598e-02, 5.2648e-02, 4.8952e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,028][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [za] are: tensor([4.8423e-05, 4.3539e-01, 8.6314e-02, 9.5599e-02, 6.7566e-02, 8.5673e-02,
        8.7550e-02, 9.2433e-03, 1.6846e-02, 9.2516e-02, 2.3252e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,031][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [za] are: tensor([6.6565e-05, 1.5032e-01, 5.2072e-02, 1.8048e-02, 1.5876e-01, 3.2888e-01,
        1.3851e-01, 1.1460e-02, 3.5153e-02, 5.1564e-02, 5.5168e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,035][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [za] are: tensor([0.0004, 0.1687, 0.0260, 0.0208, 0.0724, 0.3325, 0.2240, 0.0022, 0.0107,
        0.1114, 0.0309], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,040][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [za] are: tensor([0.0008, 0.2475, 0.0287, 0.0805, 0.0356, 0.3639, 0.1370, 0.0303, 0.0179,
        0.0444, 0.0135], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,041][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [za] are: tensor([9.9910e-01, 4.5929e-05, 2.3713e-04, 6.0414e-04, 4.7726e-06, 1.7459e-06,
        1.4040e-06, 4.0165e-07, 3.1157e-06, 2.4345e-08, 5.3930e-09],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,042][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [za] are: tensor([0.0021, 0.0776, 0.0157, 0.0136, 0.1113, 0.5221, 0.0903, 0.0059, 0.0262,
        0.0959, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,042][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [za] are: tensor([1.3606e-03, 8.4972e-01, 1.2420e-01, 2.7703e-03, 2.7007e-03, 1.3423e-02,
        7.9320e-04, 1.4840e-04, 9.6861e-05, 4.2467e-03, 5.3966e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,044][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [za] are: tensor([1.3618e-05, 1.2644e-01, 5.7522e-02, 6.0854e-03, 2.0046e-02, 3.3711e-01,
        3.9213e-02, 1.6401e-02, 3.7752e-02, 2.9413e-01, 6.5279e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza]
[2024-07-23 21:06:52,047][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.1824, 0.0423, 0.2961, 0.0464, 0.0304, 0.2403, 0.0349, 0.0052, 0.0346,
        0.0317, 0.0162, 0.0394], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,051][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0027, 0.0862, 0.1243, 0.0573, 0.0268, 0.4113, 0.1894, 0.0102, 0.0137,
        0.0237, 0.0253, 0.0291], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,054][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([2.2843e-04, 1.5431e-01, 5.9003e-02, 3.0276e-02, 4.8811e-02, 4.3530e-01,
        1.8613e-01, 4.1806e-03, 7.8251e-03, 3.5255e-02, 1.6075e-02, 2.2600e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,056][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([7.6040e-05, 8.3719e-02, 1.1639e-01, 5.7362e-02, 2.0157e-02, 5.5993e-01,
        9.8845e-02, 8.4823e-04, 4.9835e-03, 2.9199e-02, 2.5218e-03, 2.5971e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,059][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([1.7034e-05, 3.5673e-01, 1.2355e-01, 6.8135e-02, 4.7079e-02, 1.2221e-01,
        1.1712e-01, 1.3346e-02, 2.3191e-02, 8.2628e-02, 2.2751e-02, 2.3238e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,062][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([1.0692e-04, 7.8202e-02, 7.1131e-02, 2.4786e-02, 8.1515e-02, 3.2770e-01,
        1.8545e-01, 2.0862e-02, 6.9922e-02, 6.8129e-02, 3.1057e-02, 4.1137e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,065][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([8.0389e-05, 8.7948e-02, 9.0327e-02, 5.0583e-02, 4.7316e-02, 3.8583e-01,
        2.0792e-01, 3.1596e-03, 3.4913e-02, 3.4604e-02, 1.7606e-02, 3.9711e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,068][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([5.2225e-04, 4.6029e-02, 1.6953e-02, 5.8992e-02, 1.9268e-02, 5.9840e-01,
        1.6012e-01, 1.0183e-02, 2.0737e-02, 2.3856e-02, 6.4680e-03, 3.8477e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,069][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0388, 0.1224, 0.2798, 0.1695, 0.0846, 0.1787, 0.0551, 0.0036, 0.0153,
        0.0207, 0.0074, 0.0241], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,070][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0014, 0.1633, 0.0547, 0.0338, 0.0464, 0.3850, 0.1504, 0.0109, 0.0536,
        0.0583, 0.0166, 0.0256], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,071][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([1.1802e-03, 8.6135e-01, 1.0386e-01, 1.0838e-02, 1.1344e-03, 1.6325e-02,
        1.1764e-03, 8.5730e-05, 2.2954e-04, 3.1540e-03, 2.6305e-04, 4.0211e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,072][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0009, 0.2845, 0.2010, 0.0505, 0.0340, 0.1720, 0.1189, 0.0041, 0.0329,
        0.0394, 0.0125, 0.0492], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is]
[2024-07-23 21:06:52,074][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ a] are: tensor([0.1879, 0.0496, 0.2172, 0.0633, 0.0346, 0.2474, 0.0296, 0.0047, 0.0337,
        0.0327, 0.0158, 0.0364, 0.0472], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,078][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ a] are: tensor([0.0022, 0.1342, 0.0987, 0.0516, 0.0292, 0.3858, 0.1630, 0.0119, 0.0084,
        0.0408, 0.0307, 0.0163, 0.0271], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,080][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ a] are: tensor([1.1403e-04, 1.9431e-01, 5.3331e-02, 2.9596e-02, 7.4891e-02, 3.4559e-01,
        1.7047e-01, 5.9495e-03, 1.1574e-02, 6.6665e-02, 1.9812e-02, 1.6069e-02,
        1.1630e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,083][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ a] are: tensor([2.0978e-04, 1.1028e-01, 1.2147e-01, 3.8095e-02, 1.7705e-02, 5.7617e-01,
        6.9288e-02, 1.3820e-03, 6.6925e-03, 2.8236e-02, 3.2078e-03, 1.9733e-02,
        7.5286e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,085][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ a] are: tensor([4.2303e-05, 4.4029e-01, 9.7569e-02, 6.2687e-02, 4.5853e-02, 9.1139e-02,
        9.5388e-02, 9.1206e-03, 1.7089e-02, 8.5117e-02, 2.1273e-02, 1.3215e-02,
        2.1213e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,088][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ a] are: tensor([8.5820e-05, 9.2641e-02, 4.8878e-02, 2.7370e-02, 1.3883e-01, 2.5406e-01,
        1.8191e-01, 2.1557e-02, 6.9749e-02, 7.1147e-02, 4.2881e-02, 2.6565e-02,
        2.4318e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,091][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ a] are: tensor([1.1103e-04, 1.0143e-01, 5.6424e-02, 3.3564e-02, 6.8784e-02, 3.0888e-01,
        2.5272e-01, 3.7877e-03, 2.6749e-02, 7.2524e-02, 2.9546e-02, 2.8428e-02,
        1.7057e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,096][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ a] are: tensor([0.0007, 0.0956, 0.0169, 0.0689, 0.0287, 0.4681, 0.1545, 0.0183, 0.0311,
        0.0536, 0.0123, 0.0320, 0.0193], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,099][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ a] are: tensor([0.1944, 0.1737, 0.2681, 0.2345, 0.0364, 0.0540, 0.0164, 0.0018, 0.0057,
        0.0033, 0.0010, 0.0063, 0.0042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,100][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ a] are: tensor([0.0005, 0.1624, 0.0372, 0.0432, 0.0672, 0.3662, 0.1668, 0.0096, 0.0397,
        0.0630, 0.0194, 0.0182, 0.0067], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,101][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ a] are: tensor([5.0792e-04, 9.1843e-01, 5.8662e-02, 5.8929e-03, 1.1432e-03, 1.0048e-02,
        1.2547e-03, 8.4861e-05, 1.1847e-04, 2.9842e-03, 3.4137e-04, 2.0910e-04,
        3.2358e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,102][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ a] are: tensor([1.6333e-04, 2.4995e-01, 1.2754e-01, 4.7223e-02, 2.6811e-02, 2.2767e-01,
        1.0367e-01, 8.3674e-03, 3.9628e-02, 8.7054e-02, 2.2475e-02, 2.7550e-02,
        3.1894e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a]
[2024-07-23 21:06:52,104][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ bit] are: tensor([0.2061, 0.0429, 0.2353, 0.0745, 0.0312, 0.2650, 0.0191, 0.0034, 0.0230,
        0.0250, 0.0140, 0.0269, 0.0292, 0.0044], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,107][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ bit] are: tensor([0.0027, 0.0999, 0.1374, 0.0581, 0.0279, 0.3603, 0.1381, 0.0076, 0.0120,
        0.0357, 0.0380, 0.0215, 0.0355, 0.0255], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,109][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ bit] are: tensor([7.3632e-05, 7.6432e-02, 3.7143e-02, 1.6113e-02, 4.3806e-02, 5.7617e-01,
        1.4862e-01, 3.6846e-03, 7.7219e-03, 4.2435e-02, 1.5135e-02, 2.0531e-02,
        1.0144e-02, 1.9908e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,112][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ bit] are: tensor([3.8198e-05, 5.0529e-02, 1.0079e-01, 2.5681e-02, 1.1618e-02, 6.1925e-01,
        1.1768e-01, 1.2164e-03, 5.5102e-03, 2.3843e-02, 3.4483e-03, 2.7512e-02,
        1.0844e-02, 2.0498e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,115][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ bit] are: tensor([3.6749e-05, 2.8545e-01, 1.0587e-01, 6.1564e-02, 4.1812e-02, 1.1354e-01,
        1.3452e-01, 1.0968e-02, 2.9455e-02, 1.0082e-01, 2.6768e-02, 2.2730e-02,
        3.2248e-02, 3.4216e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,118][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ bit] are: tensor([7.5048e-05, 8.7107e-02, 7.3645e-02, 1.8883e-02, 9.5916e-02, 2.4906e-01,
        1.9009e-01, 1.7661e-02, 7.1690e-02, 7.2191e-02, 3.8859e-02, 3.6793e-02,
        4.0706e-02, 7.3251e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,120][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ bit] are: tensor([1.1091e-04, 4.0911e-02, 7.8639e-02, 3.3962e-02, 3.7319e-02, 3.4347e-01,
        2.6910e-01, 3.3086e-03, 4.8132e-02, 4.3913e-02, 2.7310e-02, 4.8257e-02,
        1.9627e-02, 5.9479e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,125][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ bit] are: tensor([0.0013, 0.0850, 0.0266, 0.0686, 0.0227, 0.3915, 0.1898, 0.0153, 0.0301,
        0.0513, 0.0136, 0.0511, 0.0358, 0.0173], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,129][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ bit] are: tensor([0.1024, 0.0908, 0.2891, 0.2377, 0.0388, 0.1296, 0.0440, 0.0025, 0.0088,
        0.0102, 0.0038, 0.0221, 0.0152, 0.0052], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,129][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ bit] are: tensor([0.0007, 0.1357, 0.0366, 0.0206, 0.0340, 0.4890, 0.1194, 0.0066, 0.0409,
        0.0668, 0.0199, 0.0173, 0.0082, 0.0042], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,130][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ bit] are: tensor([4.2221e-04, 8.8423e-01, 7.6470e-02, 7.6076e-03, 1.5842e-03, 2.0789e-02,
        1.9964e-03, 1.5046e-04, 3.3414e-04, 5.2814e-03, 4.8130e-04, 2.5156e-04,
        3.1714e-04, 8.7505e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,131][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ bit] are: tensor([0.0003, 0.1942, 0.1418, 0.0261, 0.0264, 0.2599, 0.1409, 0.0062, 0.0356,
        0.0657, 0.0248, 0.0359, 0.0341, 0.0081], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit]
[2024-07-23 21:06:52,134][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ different] are: tensor([0.1940, 0.0428, 0.1953, 0.0682, 0.0282, 0.2486, 0.0292, 0.0036, 0.0201,
        0.0276, 0.0202, 0.0493, 0.0510, 0.0066, 0.0152], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,137][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ different] are: tensor([0.0033, 0.1287, 0.0544, 0.0401, 0.0294, 0.4991, 0.0974, 0.0058, 0.0045,
        0.0345, 0.0383, 0.0100, 0.0202, 0.0125, 0.0219], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,139][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ different] are: tensor([2.5268e-05, 8.3668e-02, 2.4788e-02, 1.9825e-02, 5.3262e-02, 4.9500e-01,
        1.6706e-01, 4.5328e-03, 1.1205e-02, 7.6712e-02, 2.0485e-02, 1.6229e-02,
        1.3629e-02, 2.5325e-03, 1.1049e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,142][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ different] are: tensor([3.1713e-05, 4.9895e-02, 6.6592e-02, 1.4116e-02, 8.5488e-03, 6.1820e-01,
        1.8357e-01, 1.7887e-03, 4.5949e-03, 2.3850e-02, 5.0827e-03, 1.3824e-02,
        6.7012e-03, 2.3342e-03, 8.6754e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,145][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ different] are: tensor([2.3541e-05, 3.3678e-01, 9.3841e-02, 5.7464e-02, 4.9923e-02, 7.2981e-02,
        1.0585e-01, 6.4094e-03, 1.8967e-02, 1.0957e-01, 2.7010e-02, 1.5183e-02,
        3.1595e-02, 1.9537e-02, 5.4862e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,147][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ different] are: tensor([4.8555e-05, 5.4934e-02, 4.3912e-02, 1.0474e-02, 1.2976e-01, 3.5657e-01,
        1.6814e-01, 6.2952e-03, 3.5870e-02, 5.9504e-02, 4.3200e-02, 3.1391e-02,
        3.1179e-02, 3.3827e-03, 2.5348e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,150][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ different] are: tensor([8.7389e-05, 4.9118e-02, 3.4248e-02, 1.8554e-02, 3.6319e-02, 3.7972e-01,
        3.0489e-01, 1.7339e-03, 2.0938e-02, 5.9384e-02, 2.7754e-02, 2.0193e-02,
        1.8749e-02, 4.0540e-03, 2.4261e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,155][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ different] are: tensor([0.0016, 0.1933, 0.0310, 0.0672, 0.0232, 0.2966, 0.1965, 0.0141, 0.0190,
        0.0502, 0.0137, 0.0294, 0.0257, 0.0102, 0.0283], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,158][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ different] are: tensor([9.9594e-01, 1.8487e-04, 1.2708e-03, 2.3290e-03, 4.3851e-05, 5.7170e-05,
        7.7506e-05, 9.0882e-06, 2.5315e-05, 5.4310e-07, 1.4786e-07, 4.1066e-05,
        9.2580e-06, 1.1121e-05, 1.3185e-06], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,159][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ different] are: tensor([4.7057e-04, 2.9229e-02, 1.4213e-02, 1.3783e-02, 4.8365e-02, 6.5481e-01,
        9.0320e-02, 2.1600e-03, 2.6982e-02, 5.5574e-02, 2.0191e-02, 2.4633e-02,
        8.3857e-03, 2.4664e-03, 8.4193e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,160][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ different] are: tensor([4.8758e-04, 8.9086e-01, 9.2121e-02, 3.5129e-03, 1.1053e-03, 7.4216e-03,
        8.7681e-04, 7.5184e-05, 8.7381e-05, 2.4620e-03, 2.8640e-04, 2.4897e-04,
        2.7712e-04, 8.6216e-05, 8.8102e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,160][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ different] are: tensor([9.8662e-06, 9.9348e-02, 9.1803e-02, 1.0511e-02, 1.4520e-02, 3.2321e-01,
        1.1730e-01, 7.7767e-03, 1.4144e-02, 1.2960e-01, 5.5826e-02, 2.5799e-02,
        3.3619e-02, 7.7661e-03, 6.8765e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different]
[2024-07-23 21:06:52,163][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ from] are: tensor([0.1724, 0.0489, 0.2156, 0.0895, 0.0272, 0.2021, 0.0302, 0.0046, 0.0254,
        0.0291, 0.0167, 0.0430, 0.0417, 0.0078, 0.0118, 0.0339],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,166][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ from] are: tensor([0.0020, 0.1703, 0.0560, 0.0495, 0.0309, 0.4381, 0.1114, 0.0094, 0.0066,
        0.0271, 0.0273, 0.0092, 0.0199, 0.0122, 0.0204, 0.0099],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,168][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ from] are: tensor([8.7846e-05, 1.8635e-01, 4.6819e-02, 3.6370e-02, 9.3757e-02, 3.7349e-01,
        1.2479e-01, 5.7876e-03, 1.0897e-02, 5.5938e-02, 1.7178e-02, 1.6537e-02,
        1.0943e-02, 1.8894e-03, 1.1007e-02, 8.1587e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,171][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ from] are: tensor([1.2500e-04, 9.6328e-02, 8.6668e-02, 2.0051e-02, 1.4939e-02, 5.8822e-01,
        1.4075e-01, 1.9447e-03, 4.3003e-03, 2.2495e-02, 4.3093e-03, 9.1589e-03,
        4.5099e-03, 2.4896e-03, 1.2827e-03, 2.4306e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,174][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ from] are: tensor([2.3422e-05, 3.5335e-01, 1.0160e-01, 6.0375e-02, 4.5651e-02, 8.6129e-02,
        1.0410e-01, 8.1698e-03, 1.5939e-02, 8.2273e-02, 2.4468e-02, 1.5718e-02,
        2.2026e-02, 1.8731e-02, 3.9553e-02, 2.1895e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,177][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ from] are: tensor([2.3598e-05, 8.2128e-02, 4.3662e-02, 2.1887e-02, 1.1349e-01, 3.2688e-01,
        1.7667e-01, 1.3277e-02, 5.3010e-02, 4.9520e-02, 3.7419e-02, 2.8563e-02,
        2.2294e-02, 3.2962e-03, 1.7931e-02, 9.9461e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,180][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ from] are: tensor([6.2251e-05, 7.1342e-02, 3.2535e-02, 2.4229e-02, 5.0003e-02, 3.8329e-01,
        2.7690e-01, 2.8736e-03, 2.1545e-02, 4.9709e-02, 2.0346e-02, 2.1077e-02,
        1.3433e-02, 3.6053e-03, 1.5734e-02, 1.3317e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,182][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ from] are: tensor([3.4210e-04, 1.2843e-01, 2.5182e-02, 7.8229e-02, 2.3526e-02, 4.0224e-01,
        1.8008e-01, 1.7167e-02, 2.0398e-02, 3.0578e-02, 8.8661e-03, 2.3234e-02,
        1.3339e-02, 7.3211e-03, 2.2518e-02, 1.8552e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,185][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ from] are: tensor([7.7431e-01, 1.9503e-02, 5.7787e-02, 1.2526e-01, 4.1037e-03, 8.0572e-03,
        3.6949e-03, 4.7915e-04, 1.5617e-03, 2.0977e-04, 4.0160e-05, 2.4223e-03,
        8.3937e-04, 6.4698e-04, 1.6397e-04, 9.2223e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,188][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ from] are: tensor([9.4305e-05, 3.5802e-02, 2.1280e-02, 2.5487e-02, 4.7912e-02, 6.1499e-01,
        1.4636e-01, 6.4200e-03, 1.8146e-02, 2.6791e-02, 1.4780e-02, 2.2001e-02,
        5.0321e-03, 3.0091e-03, 5.4869e-03, 6.4051e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,189][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ from] are: tensor([9.2254e-04, 8.4789e-01, 1.2803e-01, 5.3118e-03, 1.3053e-03, 1.1267e-02,
        1.2210e-03, 1.2360e-04, 9.7101e-05, 2.6547e-03, 2.4954e-04, 3.6680e-04,
        2.9181e-04, 5.3962e-05, 6.6364e-05, 1.5347e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,189][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ from] are: tensor([4.9386e-05, 1.6876e-01, 1.2786e-01, 2.6510e-02, 3.4727e-02, 3.0500e-01,
        1.0898e-01, 7.1695e-03, 2.0934e-02, 7.1941e-02, 2.7798e-02, 2.4722e-02,
        2.5613e-02, 5.0500e-03, 2.8476e-02, 1.6407e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from]
[2024-07-23 21:06:52,190][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.2315, 0.0376, 0.1977, 0.0599, 0.0227, 0.2470, 0.0223, 0.0023, 0.0203,
        0.0215, 0.0098, 0.0246, 0.0311, 0.0037, 0.0070, 0.0272, 0.0340],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,193][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0014, 0.0559, 0.1278, 0.0495, 0.0220, 0.3923, 0.1382, 0.0060, 0.0090,
        0.0293, 0.0212, 0.0172, 0.0308, 0.0187, 0.0148, 0.0130, 0.0530],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,195][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([7.8278e-05, 1.4983e-01, 5.6796e-02, 1.6985e-02, 4.8495e-02, 3.4509e-01,
        2.7689e-01, 3.6856e-03, 7.8491e-03, 3.1733e-02, 1.1213e-02, 1.4305e-02,
        8.6486e-03, 3.6184e-03, 7.4576e-03, 4.1463e-03, 1.3181e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,198][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([7.3635e-05, 7.3924e-02, 1.9886e-01, 1.5275e-02, 9.9076e-03, 5.6120e-01,
        9.3099e-02, 3.8194e-04, 1.5955e-03, 1.1801e-02, 1.4743e-03, 1.4233e-02,
        5.1828e-03, 1.2960e-03, 4.6517e-04, 1.1881e-03, 1.0047e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,201][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([1.4221e-05, 2.5674e-01, 1.1175e-01, 5.6802e-02, 4.4573e-02, 1.4104e-01,
        1.3897e-01, 7.1291e-03, 1.6779e-02, 8.1920e-02, 1.5200e-02, 1.5271e-02,
        2.0749e-02, 2.5252e-02, 3.4668e-02, 1.5273e-02, 1.7863e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,203][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([2.3805e-05, 3.8875e-02, 4.3735e-02, 1.9449e-02, 9.4135e-02, 2.7369e-01,
        2.2816e-01, 1.6803e-02, 7.1658e-02, 6.2311e-02, 2.8035e-02, 3.1182e-02,
        2.3376e-02, 5.3434e-03, 1.4646e-02, 1.1923e-02, 3.6646e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,206][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([3.2901e-05, 4.4506e-02, 5.3762e-02, 2.8400e-02, 4.5443e-02, 3.2076e-01,
        2.9987e-01, 2.7019e-03, 2.6547e-02, 4.0739e-02, 1.7985e-02, 3.7436e-02,
        1.2004e-02, 5.2798e-03, 1.0219e-02, 9.3719e-03, 4.4942e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,211][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0007, 0.0480, 0.0121, 0.0534, 0.0279, 0.4979, 0.1871, 0.0086, 0.0171,
        0.0333, 0.0064, 0.0294, 0.0126, 0.0064, 0.0096, 0.0127, 0.0366],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,215][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0280, 0.1544, 0.1740, 0.0929, 0.0514, 0.3439, 0.0477, 0.0034, 0.0076,
        0.0243, 0.0072, 0.0138, 0.0142, 0.0048, 0.0038, 0.0105, 0.0180],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,217][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.0126e-04, 7.3817e-02, 4.2409e-02, 2.1719e-02, 4.5702e-02, 4.3669e-01,
        2.3746e-01, 7.1241e-03, 2.7688e-02, 3.7329e-02, 1.1255e-02, 1.7414e-02,
        5.3410e-03, 5.8056e-03, 3.3294e-03, 6.0737e-03, 2.0744e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,218][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.1749e-03, 9.2831e-01, 5.0986e-02, 4.9558e-03, 4.9148e-04, 1.0634e-02,
        6.2320e-04, 4.6262e-05, 9.4036e-05, 1.7888e-03, 1.5306e-04, 1.7395e-04,
        2.0048e-04, 5.0008e-05, 1.0677e-04, 8.5939e-05, 1.2764e-04],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,219][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0007, 0.2015, 0.1679, 0.0332, 0.0210, 0.2320, 0.1150, 0.0034, 0.0173,
        0.0279, 0.0119, 0.0204, 0.0253, 0.0056, 0.0134, 0.0103, 0.0932],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the]
[2024-07-23 21:06:52,220][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.1524, 0.0552, 0.1076, 0.0799, 0.0286, 0.2283, 0.0173, 0.0051, 0.0387,
        0.0426, 0.0177, 0.0350, 0.0459, 0.0083, 0.0116, 0.0496, 0.0512, 0.0250],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,223][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0019, 0.1224, 0.0620, 0.0483, 0.0364, 0.3823, 0.0968, 0.0116, 0.0089,
        0.0474, 0.0331, 0.0099, 0.0252, 0.0193, 0.0214, 0.0128, 0.0351, 0.0251],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,224][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([2.4039e-05, 1.0175e-01, 3.2632e-02, 2.1432e-02, 5.7926e-02, 3.7544e-01,
        1.8922e-01, 6.7321e-03, 1.6296e-02, 6.1440e-02, 1.7198e-02, 2.0414e-02,
        1.4046e-02, 7.5017e-03, 1.5884e-02, 1.0182e-02, 2.4434e-02, 2.7455e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,227][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([1.0629e-04, 6.1835e-02, 9.6555e-02, 3.2511e-02, 1.3871e-02, 5.2192e-01,
        1.3710e-01, 1.8290e-03, 6.2526e-03, 3.5340e-02, 4.7671e-03, 2.7966e-02,
        1.0417e-02, 3.7339e-03, 1.6072e-03, 6.9500e-03, 3.2114e-02, 5.1238e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,230][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([3.0528e-05, 3.5358e-01, 8.6725e-02, 5.7414e-02, 4.1848e-02, 1.0056e-01,
        6.9161e-02, 8.0701e-03, 2.5004e-02, 7.0109e-02, 1.7107e-02, 1.5396e-02,
        2.4694e-02, 2.4744e-02, 5.1796e-02, 1.7841e-02, 1.3974e-02, 2.1946e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,233][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([4.8689e-05, 1.2311e-01, 5.4275e-02, 2.7742e-02, 1.0920e-01, 2.5360e-01,
        1.0274e-01, 1.8690e-02, 5.2414e-02, 7.0162e-02, 3.5571e-02, 1.7931e-02,
        2.3429e-02, 7.2271e-03, 2.6055e-02, 1.6718e-02, 3.3125e-02, 2.7966e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,236][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([2.8530e-05, 6.4268e-02, 6.3276e-02, 3.4913e-02, 4.7816e-02, 3.3149e-01,
        1.5551e-01, 3.0584e-03, 2.9143e-02, 4.4321e-02, 1.9920e-02, 2.5989e-02,
        1.6362e-02, 9.2803e-03, 2.7333e-02, 1.6142e-02, 6.5601e-02, 4.5555e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,240][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0009, 0.1297, 0.0202, 0.0461, 0.0323, 0.3105, 0.1014, 0.0235, 0.0366,
        0.0652, 0.0148, 0.0328, 0.0207, 0.0128, 0.0329, 0.0203, 0.0427, 0.0567],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,245][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0399, 0.3211, 0.2719, 0.1800, 0.0306, 0.0814, 0.0075, 0.0016, 0.0061,
        0.0062, 0.0013, 0.0099, 0.0071, 0.0024, 0.0013, 0.0077, 0.0117, 0.0123],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,247][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([2.5931e-04, 1.6907e-01, 3.7564e-02, 2.6749e-02, 6.4426e-02, 3.4573e-01,
        8.7609e-02, 6.7496e-03, 3.4173e-02, 7.1059e-02, 1.8818e-02, 2.5447e-02,
        9.3272e-03, 9.5486e-03, 7.0089e-03, 9.8876e-03, 1.6187e-02, 6.0379e-02],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,247][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([1.3832e-03, 8.9646e-01, 7.5150e-02, 5.1907e-03, 1.0727e-03, 1.1627e-02,
        1.1736e-03, 1.7257e-04, 1.6125e-04, 3.2766e-03, 3.9532e-04, 3.0630e-04,
        5.0883e-04, 1.5752e-04, 1.8521e-04, 2.1945e-04, 3.8590e-04, 2.1765e-03],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,248][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0003, 0.2375, 0.0862, 0.0136, 0.0202, 0.1878, 0.0583, 0.0080, 0.0132,
        0.0657, 0.0211, 0.0178, 0.0211, 0.0075, 0.0210, 0.0155, 0.0867, 0.1183],
       device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language]
[2024-07-23 21:06:52,249][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ used] are: tensor([0.2349, 0.0237, 0.2004, 0.0465, 0.0144, 0.1627, 0.0140, 0.0018, 0.0141,
        0.0171, 0.0062, 0.0189, 0.0233, 0.0036, 0.0044, 0.0215, 0.0229, 0.0094,
        0.1603], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,252][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ used] are: tensor([0.0013, 0.0341, 0.1035, 0.0503, 0.0246, 0.3475, 0.1509, 0.0058, 0.0114,
        0.0306, 0.0209, 0.0232, 0.0360, 0.0206, 0.0164, 0.0146, 0.0549, 0.0264,
        0.0269], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,254][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ used] are: tensor([4.7640e-05, 6.1834e-02, 6.7788e-02, 1.6080e-02, 3.0693e-02, 4.0340e-01,
        2.6862e-01, 3.0170e-03, 6.5844e-03, 3.1186e-02, 8.9576e-03, 1.4815e-02,
        8.6557e-03, 4.4022e-03, 8.8280e-03, 6.1805e-03, 2.0082e-02, 2.9132e-02,
        9.6904e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,257][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ used] are: tensor([2.3566e-04, 7.5529e-02, 2.0561e-01, 2.6403e-02, 1.1913e-02, 4.9774e-01,
        1.1243e-01, 5.0091e-04, 2.0236e-03, 1.4139e-02, 1.6575e-03, 1.4573e-02,
        5.5831e-03, 1.5839e-03, 3.4926e-04, 2.2138e-03, 1.8322e-02, 4.5727e-03,
        4.6133e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,259][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ used] are: tensor([2.3359e-05, 1.9823e-01, 1.6498e-01, 5.2308e-02, 3.2143e-02, 1.0418e-01,
        6.1484e-02, 9.4672e-03, 2.3114e-02, 6.8253e-02, 1.9359e-02, 2.1388e-02,
        2.7981e-02, 2.9745e-02, 5.8818e-02, 2.0608e-02, 1.8521e-02, 3.7261e-02,
        5.2140e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,262][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ used] are: tensor([3.8836e-05, 6.5258e-02, 5.8381e-02, 2.4713e-02, 7.9660e-02, 2.1188e-01,
        1.4579e-01, 1.4538e-02, 6.2238e-02, 6.7792e-02, 3.4094e-02, 3.2087e-02,
        2.7676e-02, 8.3794e-03, 2.3999e-02, 2.1934e-02, 6.0941e-02, 3.5520e-02,
        2.5081e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,265][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ used] are: tensor([1.6189e-05, 3.3480e-02, 7.9529e-02, 3.1201e-02, 3.5771e-02, 2.8408e-01,
        2.4645e-01, 2.3036e-03, 2.6921e-02, 2.7591e-02, 1.3967e-02, 3.1005e-02,
        1.3889e-02, 7.9132e-03, 1.5830e-02, 1.1753e-02, 5.6193e-02, 4.4860e-02,
        3.7248e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,270][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ used] are: tensor([0.0014, 0.0530, 0.0212, 0.0423, 0.0245, 0.3743, 0.1764, 0.0098, 0.0250,
        0.0433, 0.0080, 0.0449, 0.0180, 0.0081, 0.0171, 0.0186, 0.0456, 0.0493,
        0.0192], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,274][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ used] are: tensor([0.2631, 0.2101, 0.1583, 0.0600, 0.0306, 0.1240, 0.0336, 0.0039, 0.0081,
        0.0108, 0.0045, 0.0134, 0.0144, 0.0055, 0.0044, 0.0085, 0.0178, 0.0183,
        0.0107], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,276][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ used] are: tensor([7.7082e-05, 7.3830e-02, 4.7895e-02, 3.1517e-02, 4.3452e-02, 4.2289e-01,
        1.6127e-01, 5.7622e-03, 3.3091e-02, 3.5296e-02, 1.3938e-02, 1.5460e-02,
        5.0693e-03, 5.2453e-03, 5.8321e-03, 8.0222e-03, 1.8395e-02, 4.6363e-02,
        2.6594e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,277][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ used] are: tensor([8.2437e-04, 9.2614e-01, 5.9666e-02, 4.6937e-03, 5.0322e-04, 4.6262e-03,
        6.4556e-04, 3.0791e-05, 6.8795e-05, 1.5164e-03, 1.1572e-04, 7.6193e-05,
        1.0982e-04, 2.7260e-05, 4.9862e-05, 7.4295e-05, 7.4398e-05, 4.9582e-04,
        2.6525e-04], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,278][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ used] are: tensor([0.0004, 0.2090, 0.1787, 0.0177, 0.0135, 0.0864, 0.0767, 0.0019, 0.0114,
        0.0212, 0.0054, 0.0150, 0.0214, 0.0112, 0.0112, 0.0134, 0.1004, 0.1267,
        0.0785], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used]
[2024-07-23 21:06:52,279][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ by] are: tensor([0.1200, 0.0302, 0.1530, 0.0700, 0.0177, 0.2022, 0.0131, 0.0035, 0.0275,
        0.0255, 0.0145, 0.0216, 0.0288, 0.0034, 0.0061, 0.0316, 0.0351, 0.0114,
        0.1496, 0.0349], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,282][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ by] are: tensor([0.0006, 0.0325, 0.1321, 0.0406, 0.0261, 0.2843, 0.1526, 0.0088, 0.0087,
        0.0244, 0.0251, 0.0173, 0.0324, 0.0279, 0.0192, 0.0144, 0.0596, 0.0389,
        0.0369, 0.0177], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,285][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ by] are: tensor([2.7742e-05, 5.0935e-02, 6.4281e-02, 1.6158e-02, 4.6207e-02, 3.3311e-01,
        3.0641e-01, 7.0474e-03, 1.0242e-02, 3.5897e-02, 1.4882e-02, 1.5419e-02,
        1.1375e-02, 7.3359e-03, 1.3576e-02, 6.0470e-03, 1.7200e-02, 2.7794e-02,
        1.3720e-02, 2.3355e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,288][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ by] are: tensor([1.2561e-04, 6.2954e-02, 1.5867e-01, 1.6276e-02, 2.1450e-02, 4.9791e-01,
        1.4627e-01, 8.8200e-04, 2.9733e-03, 2.1355e-02, 3.7305e-03, 1.5449e-02,
        9.5803e-03, 2.1591e-03, 1.2980e-03, 2.6556e-03, 2.2208e-02, 3.9462e-03,
        8.6949e-03, 1.4138e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,291][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ by] are: tensor([1.9111e-05, 2.0772e-01, 1.5173e-01, 6.8621e-02, 3.6698e-02, 1.1997e-01,
        9.5285e-02, 1.2569e-02, 1.9087e-02, 6.3485e-02, 1.5507e-02, 1.7622e-02,
        2.1739e-02, 3.2125e-02, 3.0312e-02, 1.5813e-02, 1.8682e-02, 3.3812e-02,
        3.4369e-02, 4.8356e-03], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,294][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ by] are: tensor([1.4137e-05, 3.9223e-02, 4.3560e-02, 2.4277e-02, 6.4291e-02, 2.5975e-01,
        2.1233e-01, 2.7388e-02, 6.2173e-02, 5.5517e-02, 2.8999e-02, 2.1210e-02,
        2.0913e-02, 6.8962e-03, 2.0631e-02, 1.6743e-02, 3.0493e-02, 2.3842e-02,
        2.5671e-02, 1.6078e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,296][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ by] are: tensor([7.1575e-06, 2.5301e-02, 5.7061e-02, 2.9020e-02, 4.5091e-02, 2.6531e-01,
        2.5347e-01, 3.8005e-03, 2.8520e-02, 2.9965e-02, 1.8358e-02, 2.9903e-02,
        1.2774e-02, 1.0467e-02, 1.7735e-02, 1.2598e-02, 4.9721e-02, 4.9849e-02,
        4.9395e-02, 1.1648e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,299][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ by] are: tensor([2.6263e-04, 6.3925e-02, 1.3522e-02, 5.2557e-02, 3.2521e-02, 3.2386e-01,
        1.6794e-01, 1.6379e-02, 2.9287e-02, 4.4013e-02, 1.4021e-02, 3.9055e-02,
        1.8597e-02, 1.6405e-02, 2.5510e-02, 2.0524e-02, 3.1717e-02, 5.4791e-02,
        2.2063e-02, 1.3052e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,303][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ by] are: tensor([0.0052, 0.4099, 0.1693, 0.0722, 0.0338, 0.1837, 0.0228, 0.0020, 0.0053,
        0.0288, 0.0054, 0.0058, 0.0077, 0.0022, 0.0033, 0.0079, 0.0077, 0.0178,
        0.0059, 0.0031], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,305][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ by] are: tensor([7.1536e-05, 8.4202e-02, 4.2844e-02, 3.4232e-02, 3.6401e-02, 3.4856e-01,
        1.8457e-01, 1.1793e-02, 2.5885e-02, 3.7373e-02, 1.4697e-02, 1.3935e-02,
        7.0557e-03, 7.4480e-03, 6.6597e-03, 8.9367e-03, 1.9814e-02, 6.1121e-02,
        3.7830e-02, 1.6575e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,306][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ by] are: tensor([5.0849e-04, 8.8848e-01, 6.8637e-02, 1.4493e-02, 1.4279e-03, 1.5637e-02,
        2.4630e-03, 1.8144e-04, 2.2782e-04, 3.3909e-03, 4.6081e-04, 2.1326e-04,
        3.3201e-04, 1.1645e-04, 2.8859e-04, 2.0975e-04, 2.4892e-04, 1.6516e-03,
        9.4616e-04, 8.8048e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,307][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ by] are: tensor([0.0004, 0.2009, 0.1616, 0.0336, 0.0279, 0.1210, 0.0932, 0.0031, 0.0119,
        0.0197, 0.0078, 0.0107, 0.0188, 0.0073, 0.0100, 0.0095, 0.0759, 0.0796,
        0.0798, 0.0273], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by]
[2024-07-23 21:06:52,308][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.2631, 0.0273, 0.1592, 0.0519, 0.0145, 0.1840, 0.0122, 0.0012, 0.0135,
        0.0126, 0.0051, 0.0162, 0.0180, 0.0018, 0.0032, 0.0152, 0.0197, 0.0074,
        0.1099, 0.0192, 0.0450], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,311][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0022, 0.0961, 0.1801, 0.0510, 0.0241, 0.3111, 0.1099, 0.0043, 0.0055,
        0.0226, 0.0171, 0.0121, 0.0185, 0.0147, 0.0085, 0.0078, 0.0274, 0.0341,
        0.0199, 0.0093, 0.0237], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,314][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([1.5152e-04, 1.4659e-01, 6.8479e-02, 1.7563e-02, 4.7706e-02, 3.0118e-01,
        2.7091e-01, 2.5816e-03, 6.9616e-03, 3.2102e-02, 8.7649e-03, 1.1728e-02,
        8.2763e-03, 4.5969e-03, 8.3020e-03, 3.9765e-03, 1.3131e-02, 2.6338e-02,
        5.0183e-03, 1.1917e-03, 1.4451e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,317][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.9207e-04, 1.0372e-01, 2.3989e-01, 8.7516e-03, 9.9680e-03, 5.1047e-01,
        8.0314e-02, 1.4245e-04, 5.9834e-04, 5.3960e-03, 6.3859e-04, 5.0445e-03,
        2.3989e-03, 4.8130e-04, 1.5939e-04, 3.6849e-04, 4.2144e-03, 9.4840e-04,
        2.8713e-03, 2.3001e-04, 2.3205e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,320][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.6152e-05, 2.8519e-01, 1.2623e-01, 6.8471e-02, 4.0919e-02, 1.1742e-01,
        9.0883e-02, 6.8930e-03, 1.4827e-02, 5.9365e-02, 1.2869e-02, 1.2085e-02,
        1.5005e-02, 1.9658e-02, 2.6329e-02, 1.3431e-02, 1.5086e-02, 3.5534e-02,
        2.2233e-02, 3.2653e-03, 1.4272e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,323][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([3.4004e-05, 3.7251e-02, 4.4106e-02, 2.4313e-02, 1.1013e-01, 2.2149e-01,
        2.0323e-01, 1.5191e-02, 5.4644e-02, 5.9214e-02, 2.6431e-02, 2.3851e-02,
        2.0046e-02, 4.8851e-03, 1.3146e-02, 9.4300e-03, 2.5439e-02, 2.1191e-02,
        1.3374e-02, 8.9845e-03, 6.3620e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,326][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([3.4985e-05, 4.6543e-02, 7.5705e-02, 2.9826e-02, 4.0788e-02, 2.7910e-01,
        2.4575e-01, 1.7492e-03, 2.1975e-02, 3.0373e-02, 1.2381e-02, 2.8653e-02,
        9.2277e-03, 5.1954e-03, 7.7865e-03, 7.3027e-03, 4.0403e-02, 4.6865e-02,
        1.8607e-02, 6.3429e-03, 4.5387e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,330][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0009, 0.0699, 0.0096, 0.0504, 0.0305, 0.5065, 0.1307, 0.0071, 0.0118,
        0.0301, 0.0061, 0.0223, 0.0107, 0.0061, 0.0091, 0.0092, 0.0231, 0.0372,
        0.0079, 0.0061, 0.0148], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,334][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0533, 0.2646, 0.1635, 0.0794, 0.0547, 0.2594, 0.0380, 0.0020, 0.0060,
        0.0175, 0.0052, 0.0059, 0.0068, 0.0019, 0.0022, 0.0061, 0.0079, 0.0126,
        0.0044, 0.0015, 0.0072], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,335][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.0088e-04, 6.0786e-02, 4.1483e-02, 2.8111e-02, 5.0535e-02, 4.4260e-01,
        1.9502e-01, 5.1373e-03, 2.4064e-02, 2.4564e-02, 8.8514e-03, 1.1585e-02,
        4.1394e-03, 4.5569e-03, 2.8846e-03, 4.3566e-03, 1.3150e-02, 3.7479e-02,
        1.5319e-02, 8.8348e-03, 1.6438e-02], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,336][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([1.5608e-03, 9.3137e-01, 4.8924e-02, 7.1683e-03, 4.5718e-04, 7.7444e-03,
        6.5532e-04, 1.9075e-05, 4.9159e-05, 8.0583e-04, 6.6835e-05, 6.6055e-05,
        9.5536e-05, 2.2191e-05, 4.7264e-05, 3.4211e-05, 5.3938e-05, 6.3544e-04,
        1.7921e-04, 1.2171e-05, 3.2766e-05], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,337][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0008, 0.2858, 0.1639, 0.0375, 0.0152, 0.1544, 0.0697, 0.0013, 0.0081,
        0.0116, 0.0055, 0.0065, 0.0093, 0.0018, 0.0046, 0.0039, 0.0301, 0.0601,
        0.0315, 0.0129, 0.0855], device='cuda:0') for source tokens [The language used by Juan Bautista de Anza is a bit different from the language used by the]
[2024-07-23 21:06:52,341][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:06:52,343][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[480],
        [ 73],
        [ 80],
        [ 19],
        [  8],
        [  8],
        [  5],
        [  5],
        [  2],
        [ 16],
        [  2],
        [  1],
        [  1],
        [  3],
        [ 10],
        [  1],
        [  2],
        [  9],
        [ 10],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:06:52,346][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[546],
        [ 47],
        [ 57],
        [ 11],
        [  4],
        [  3],
        [  4],
        [  3],
        [  1],
        [  1],
        [  1],
        [  1],
        [  1],
        [  2],
        [  2],
        [  1],
        [  1],
        [  5],
        [  4],
        [  1],
        [  1]], device='cuda:0')
[2024-07-23 21:06:52,350][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[  816],
        [ 3205],
        [15351],
        [13058],
        [11095],
        [ 8998],
        [ 8859],
        [19768],
        [19068],
        [19193],
        [18839],
        [18012],
        [17246],
        [17733],
        [16491],
        [16358],
        [15603],
        [15522],
        [13890],
        [15266],
        [13033]], device='cuda:0')
[2024-07-23 21:06:52,353][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[24609],
        [15652],
        [28324],
        [33653],
        [31248],
        [32817],
        [33795],
        [33686],
        [34795],
        [26913],
        [28920],
        [32922],
        [31828],
        [31989],
        [31805],
        [31157],
        [31537],
        [30102],
        [30441],
        [30046],
        [31627]], device='cuda:0')
[2024-07-23 21:06:52,356][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[10367],
        [18754],
        [43071],
        [40951],
        [48995],
        [45707],
        [47720],
        [47951],
        [47871],
        [45587],
        [47451],
        [47586],
        [47827],
        [47260],
        [47452],
        [47856],
        [47813],
        [47646],
        [47710],
        [48071],
        [47812]], device='cuda:0')
[2024-07-23 21:06:52,359][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[35395],
        [41735],
        [25426],
        [22476],
        [26060],
        [32504],
        [27837],
        [27808],
        [29432],
        [34053],
        [31824],
        [29606],
        [29949],
        [28829],
        [28798],
        [29675],
        [28156],
        [28224],
        [27668],
        [27687],
        [28173]], device='cuda:0')
[2024-07-23 21:06:52,362][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[15102],
        [ 6245],
        [ 4320],
        [ 4742],
        [ 5221],
        [ 4979],
        [ 7430],
        [ 6250],
        [ 8172],
        [ 7055],
        [ 6972],
        [ 6971],
        [ 6999],
        [ 7653],
        [ 7154],
        [ 7064],
        [ 7576],
        [ 7169],
        [ 6990],
        [ 7262],
        [ 7387]], device='cuda:0')
[2024-07-23 21:06:52,366][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[37227],
        [ 5366],
        [ 1467],
        [ 1414],
        [ 4291],
        [ 5359],
        [ 6926],
        [ 5148],
        [ 6691],
        [ 6526],
        [ 6737],
        [ 6601],
        [ 6955],
        [ 6408],
        [ 6861],
        [ 6677],
        [ 7031],
        [ 6065],
        [ 5850],
        [ 6241],
        [ 6331]], device='cuda:0')
[2024-07-23 21:06:52,369][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[15531],
        [21235],
        [21832],
        [18465],
        [16628],
        [11170],
        [11273],
        [10734],
        [10318],
        [13048],
        [12354],
        [11579],
        [12305],
        [11915],
        [11656],
        [11555],
        [12141],
        [13098],
        [13174],
        [13313],
        [12950]], device='cuda:0')
[2024-07-23 21:06:52,370][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[22508],
        [49716],
        [49937],
        [50206],
        [50188],
        [50235],
        [50233],
        [50234],
        [50239],
        [50235],
        [50232],
        [50238],
        [50238],
        [50236],
        [50229],
        [50236],
        [50237],
        [50224],
        [50233],
        [50229],
        [50235]], device='cuda:0')
[2024-07-23 21:06:52,372][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[3032],
        [2866],
        [2684],
        [1543],
        [2581],
        [2910],
        [ 778],
        [1857],
        [1689],
        [3002],
        [2844],
        [2105],
        [2010],
        [2019],
        [2852],
        [1995],
        [2102],
        [2015],
        [2059],
        [2091],
        [2095]], device='cuda:0')
[2024-07-23 21:06:52,374][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[33148],
        [29854],
        [26786],
        [20707],
        [23215],
        [23487],
        [20547],
        [21319],
        [21339],
        [22640],
        [23065],
        [21088],
        [21205],
        [22044],
        [22111],
        [21428],
        [20018],
        [22534],
        [20929],
        [20710],
        [20573]], device='cuda:0')
[2024-07-23 21:06:52,376][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[38534],
        [15012],
        [16681],
        [16210],
        [17114],
        [17523],
        [17243],
        [17297],
        [17230],
        [18402],
        [18757],
        [18678],
        [18304],
        [18927],
        [18761],
        [19417],
        [19566],
        [20296],
        [20402],
        [21154],
        [20325]], device='cuda:0')
[2024-07-23 21:06:52,380][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 2524],
        [34248],
        [28860],
        [ 6178],
        [ 6602],
        [ 7643],
        [ 7341],
        [ 7817],
        [ 9372],
        [12416],
        [14213],
        [13857],
        [13401],
        [13055],
        [11221],
        [11436],
        [ 8021],
        [ 9826],
        [10176],
        [ 9235],
        [ 8239]], device='cuda:0')
[2024-07-23 21:06:52,383][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[16088],
        [13258],
        [10261],
        [16075],
        [12704],
        [ 6456],
        [ 8154],
        [ 9456],
        [10027],
        [ 6222],
        [ 8989],
        [11672],
        [11521],
        [11329],
        [ 9687],
        [11953],
        [12966],
        [13152],
        [13337],
        [15301],
        [13776]], device='cuda:0')
[2024-07-23 21:06:52,386][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[18725],
        [24869],
        [25935],
        [26467],
        [27565],
        [28796],
        [30040],
        [25252],
        [25616],
        [25221],
        [25527],
        [25742],
        [24699],
        [25077],
        [24473],
        [24874],
        [25628],
        [23932],
        [25865],
        [24489],
        [26422]], device='cuda:0')
[2024-07-23 21:06:52,389][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 8568],
        [14087],
        [16202],
        [11711],
        [17274],
        [14971],
        [ 9686],
        [ 9817],
        [10755],
        [15526],
        [15538],
        [11065],
        [11979],
        [11936],
        [12590],
        [12945],
        [12133],
        [13555],
        [12557],
        [12595],
        [12887]], device='cuda:0')
[2024-07-23 21:06:52,393][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[43957],
        [36195],
        [39263],
        [38319],
        [37083],
        [37414],
        [37113],
        [37235],
        [37592],
        [37038],
        [35817],
        [36502],
        [35476],
        [36956],
        [35670],
        [35625],
        [35579],
        [34344],
        [35403],
        [34528],
        [35076]], device='cuda:0')
[2024-07-23 21:06:52,396][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[11791],
        [ 6235],
        [ 3407],
        [ 3347],
        [ 3307],
        [ 3582],
        [ 3231],
        [ 3220],
        [ 3220],
        [ 3605],
        [ 3415],
        [ 3208],
        [ 3287],
        [ 3294],
        [ 3358],
        [ 3344],
        [ 3237],
        [ 3272],
        [ 3202],
        [ 3215],
        [ 3189]], device='cuda:0')
[2024-07-23 21:06:52,399][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[2573],
        [4527],
        [6931],
        [4562],
        [3851],
        [4076],
        [1730],
        [2152],
        [1309],
        [2119],
        [1996],
        [1781],
        [1964],
        [1528],
        [1662],
        [1718],
        [1489],
        [1718],
        [1825],
        [1700],
        [1709]], device='cuda:0')
[2024-07-23 21:06:52,402][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[1916],
        [2899],
        [4057],
        [3523],
        [2295],
        [2582],
        [2158],
        [2370],
        [2239],
        [2287],
        [2244],
        [2198],
        [2158],
        [2251],
        [2204],
        [2197],
        [2129],
        [2326],
        [2347],
        [2206],
        [2264]], device='cuda:0')
[2024-07-23 21:06:52,405][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 3308],
        [12996],
        [14534],
        [14246],
        [ 9177],
        [ 5933],
        [ 7260],
        [ 6141],
        [ 5917],
        [ 6930],
        [ 7080],
        [ 6645],
        [ 7602],
        [ 7601],
        [ 7304],
        [ 7080],
        [ 7967],
        [ 7256],
        [ 8278],
        [ 8788],
        [ 8097]], device='cuda:0')
[2024-07-23 21:06:52,407][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[1850],
        [2569],
        [3402],
        [4231],
        [5512],
        [5092],
        [6026],
        [6066],
        [5931],
        [5308],
        [5260],
        [5998],
        [5995],
        [6182],
        [5795],
        [5947],
        [6219],
        [6084],
        [6391],
        [6441],
        [6006]], device='cuda:0')
[2024-07-23 21:06:52,408][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[14590],
        [14575],
        [14419],
        [13592],
        [14585],
        [14586],
        [14535],
        [ 7037],
        [ 4070],
        [14589],
        [14590],
        [ 8654],
        [ 3307],
        [ 5697],
        [14587],
        [14197],
        [ 9509],
        [ 8912],
        [ 2590],
        [10158],
        [ 9245]], device='cuda:0')
[2024-07-23 21:06:52,411][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 6026],
        [ 3192],
        [ 3518],
        [ 5903],
        [ 6821],
        [ 9389],
        [11139],
        [10798],
        [10535],
        [10264],
        [10396],
        [10095],
        [10077],
        [10033],
        [10707],
        [11016],
        [11251],
        [ 8573],
        [10303],
        [10049],
        [10751]], device='cuda:0')
[2024-07-23 21:06:52,413][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[1870],
        [ 736],
        [ 658],
        [ 574],
        [ 576],
        [ 605],
        [ 608],
        [ 607],
        [ 566],
        [ 575],
        [ 573],
        [ 586],
        [ 635],
        [ 605],
        [ 609],
        [ 572],
        [ 645],
        [ 612],
        [ 642],
        [ 606],
        [ 650]], device='cuda:0')
[2024-07-23 21:06:52,416][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[6245],
        [8106],
        [7936],
        [6875],
        [6995],
        [6598],
        [5945],
        [5077],
        [5849],
        [5425],
        [4553],
        [6169],
        [5820],
        [5501],
        [4772],
        [5330],
        [5795],
        [6142],
        [6600],
        [6396],
        [6665]], device='cuda:0')
[2024-07-23 21:06:52,420][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[21811],
        [22659],
        [21040],
        [22126],
        [21064],
        [20050],
        [23333],
        [25042],
        [26688],
        [21847],
        [22942],
        [24955],
        [27080],
        [24828],
        [23571],
        [23176],
        [23821],
        [24761],
        [23842],
        [24083],
        [23549]], device='cuda:0')
[2024-07-23 21:06:52,423][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[29380],
        [27196],
        [30409],
        [25191],
        [26149],
        [31757],
        [32161],
        [31528],
        [31508],
        [32833],
        [29396],
        [29300],
        [29990],
        [29617],
        [30488],
        [28552],
        [29125],
        [28153],
        [28202],
        [26826],
        [28461]], device='cuda:0')
[2024-07-23 21:06:52,426][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159],
        [6159]], device='cuda:0')
