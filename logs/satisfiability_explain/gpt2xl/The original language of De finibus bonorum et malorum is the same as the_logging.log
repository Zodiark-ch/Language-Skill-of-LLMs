[2024-07-23 21:05:21,940][explain_satisfiability.py][line:85][INFO] ############ CASE TEXT isThe original language of De finibus bonorum et malorum is the same as the
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:86][INFO] ############ CASE Prediction is  original
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:87][INFO] ############ Refined Forward Graph
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:88][INFO] ****** Layer 1
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 0
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 1
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 2
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 3
[2024-07-23 21:05:21,940][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 4
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 5
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 6
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 7
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit5', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 8
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 9
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 10
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit26']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 11
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 12
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,941][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 13
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 14
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit26']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 15
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 16
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit3', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 17
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit10', 'circuit23', 'circuit27']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 18
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 19
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 20
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit27']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 21
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,942][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 22
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 23
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit15', 'circuit27']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 24
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 25
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 26
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 27
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 1 and circuit 28
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 0
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 1
[2024-07-23 21:05:21,943][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit3', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,944][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,944][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 2
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit10', 'circuit11', 'circuit16', 'circuit27']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 3
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit9', 'circuit10']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 4
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 5
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 6
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit20', 'circuit22']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit23']
[2024-07-23 21:05:21,948][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 7
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 8
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit3', 'circuit4', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 9
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 10
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 11
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 12
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit26']
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,949][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 13
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 14
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 15
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit14', 'circuit22', 'circuit23', 'circuit27']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 16
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 17
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 18
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 19
[2024-07-23 21:05:21,950][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 20
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 21
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 22
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 23
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 24
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 25
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 26
[2024-07-23 21:05:21,951][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 27
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 2 and circuit 28
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 0
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit3', 'circuit5', 'circuit6', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 1
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 2
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21']
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 3
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 4
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,952][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 5
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit19', 'circuit23']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit19', 'circuit23', 'circuit25']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 6
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 7
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit24']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 8
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 9
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit2']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 10
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 11
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit8', 'circuit9']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit5', 'circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 12
[2024-07-23 21:05:21,953][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 13
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit7', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 14
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit17', 'circuit19']
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 15
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 16
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 17
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 18
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit11', 'circuit12', 'circuit27']
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 19
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,954][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 20
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 21
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 22
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 23
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 24
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 25
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 26
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,955][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 27
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 3 and circuit 28
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 0
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit6', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 1
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 2
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 3
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit9', 'circuit14', 'circuit15', 'circuit16', 'circuit23']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,956][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 4
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 5
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit8', 'circuit15', 'circuit18', 'circuit22', 'circuit25']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 6
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit1', 'circuit2', 'circuit3', 'circuit4']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit2', 'circuit5']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 7
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 8
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit10', 'circuit11', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit8', 'circuit12', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 9
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit12', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit6', 'circuit7', 'circuit13', 'circuit16']
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,957][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit14']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 10
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit1', 'circuit7', 'circuit9', 'circuit13', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 11
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit5', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 12
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 13
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit8', 'circuit10', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit6', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 14
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 15
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22', 'circuit26']
[2024-07-23 21:05:21,958][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 16
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 17
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit19']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 18
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit10', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit17', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 19
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 20
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit3', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit17']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 21
[2024-07-23 21:05:21,959][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit3', 'circuit4', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit26']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit16', 'circuit17']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 22
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 23
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 24
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 25
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit6', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit6', 'circuit7', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 26
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,960][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 27
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:94][INFO] Layer 4 and circuit 28
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 0
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit6', 'circuit7', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 1
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 2
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit25']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,961][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 3
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 4
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 5
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 6
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit20', 'circuit22']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit18', 'circuit19', 'circuit20', 'circuit24']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 7
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit26']
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 8
[2024-07-23 21:05:21,962][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit26']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 9
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit16', 'circuit19', 'circuit24']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 10
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit23', 'circuit28']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit2']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 11
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit24']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 12
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit8']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0']
[2024-07-23 21:05:21,963][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 13
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit9', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 14
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 15
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 16
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 17
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,964][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 18
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 19
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 20
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 21
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 22
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,965][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 23
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 24
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 25
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 26
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 27
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,966][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:94][INFO] Layer 5 and circuit 28
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 0
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit2', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 1
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 2
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 3
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,967][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit7', 'circuit8', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit21', 'circuit22']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit25']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit25']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 4
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit20', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 5
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 6
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit13', 'circuit15']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 7
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit19']
[2024-07-23 21:05:21,968][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16', 'circuit17', 'circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit22']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 8
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit16']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit3']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 9
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 10
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit14', 'circuit17', 'circuit27']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 11
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit22']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit9', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,969][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 12
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 13
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit7', 'circuit8', 'circuit9', 'circuit12', 'circuit13', 'circuit15', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit8', 'circuit14', 'circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 14
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 15
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,970][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 16
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 17
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 18
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 19
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 20
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,971][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 21
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 22
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 23
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 24
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,972][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 25
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 26
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 27
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:94][INFO] Layer 6 and circuit 28
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,973][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 0
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit6', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit5', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit2', 'circuit6', 'circuit7', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 1
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit18', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit25']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 2
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit7', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit26']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 3
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit20', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,974][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit14']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 4
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit15', 'circuit17', 'circuit18', 'circuit27']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit15', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit17', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit20', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 5
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit7']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit2']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 6
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit14', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit26']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit13', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 7
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit3', 'circuit7', 'circuit8', 'circuit9', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,975][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit6', 'circuit8', 'circuit9', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit16', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit20', 'circuit21', 'circuit23']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 8
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit6', 'circuit7', 'circuit9', 'circuit13', 'circuit14', 'circuit17', 'circuit26']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 9
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit18']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 10
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit9', 'circuit13', 'circuit17']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit15']
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,976][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 11
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit22']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 12
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 13
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit2', 'circuit9', 'circuit10', 'circuit12', 'circuit24', 'circuit27']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 14
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,977][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 15
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 16
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 17
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 18
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,978][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 19
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 20
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 21
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 22
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,979][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 23
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 24
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 25
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 26
[2024-07-23 21:05:21,980][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 27
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:94][INFO] Layer 7 and circuit 28
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 0
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit6', 'circuit8', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,981][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit7', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit4', 'circuit10', 'circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit4', 'circuit6', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit3', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 1
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit18', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 2
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 3
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,982][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 4
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit24', 'circuit25']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit15']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 5
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit27']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit16', 'circuit21', 'circuit25']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 6
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit7', 'circuit8', 'circuit9', 'circuit11']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit16', 'circuit27']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 7
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit15', 'circuit17', 'circuit18', 'circuit20']
[2024-07-23 21:05:21,983][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 8
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit21', 'circuit22', 'circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 9
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit23', 'circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit24']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 10
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit15', 'circuit17', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0']
[2024-07-23 21:05:21,984][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit13']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit13', 'circuit15', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit21', 'circuit22']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 11
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 12
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 13
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit5', 'circuit7', 'circuit13', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit17', 'circuit23', 'circuit24', 'circuit26']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit7']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit7', 'circuit14', 'circuit22']
[2024-07-23 21:05:21,985][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit13', 'circuit18', 'circuit19']
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 14
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 15
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 16
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 17
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,986][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 18
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 19
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 20
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,987][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 21
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 22
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 23
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 24
[2024-07-23 21:05:21,988][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 25
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 26
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 27
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,989][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:94][INFO] Layer 8 and circuit 28
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 0
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit5', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit6', 'circuit8', 'circuit9', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit2', 'circuit4', 'circuit7', 'circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit2', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit4', 'circuit5', 'circuit8', 'circuit10', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit4', 'circuit7', 'circuit8', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit4', 'circuit6', 'circuit8', 'circuit9', 'circuit10', 'circuit12', 'circuit13', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 1
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,990][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 2
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 3
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit17', 'circuit20']
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 4
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit14', 'circuit15', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit20', 'circuit23']
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,991][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit18', 'circuit20', 'circuit21']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit10', 'circuit11']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 5
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit18', 'circuit19']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit21', 'circuit22']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit1', 'circuit5', 'circuit6', 'circuit9', 'circuit14', 'circuit15', 'circuit18', 'circuit19']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 6
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 7
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,992][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 8
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 9
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit16']
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit23']
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit13', 'circuit21', 'circuit25']
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 10
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,993][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit22', 'circuit24']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit13', 'circuit16']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 11
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 12
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 13
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit25']
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,994][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit14', 'circuit17']
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 14
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 15
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 16
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,995][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 17
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 18
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 19
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,996][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 20
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 21
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 22
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,997][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 23
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 24
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 25
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:21,998][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 26
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 27
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:94][INFO] Layer 9 and circuit 28
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:21,999][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 0
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit12', 'circuit13', 'circuit14', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit2', 'circuit11', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit5', 'circuit7', 'circuit8', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit11', 'circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit3', 'circuit5', 'circuit8', 'circuit10', 'circuit14', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit2', 'circuit3', 'circuit4', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit13', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit27']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 1
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 2
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,000][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 3
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 4
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 5
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,001][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 6
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 7
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit14', 'circuit15', 'circuit16', 'circuit18']
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 8
[2024-07-23 21:05:22,002][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 9
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit15', 'circuit18', 'circuit21', 'circuit23']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit13', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit25']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit13', 'circuit18', 'circuit19', 'circuit22']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 10
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit21']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit3', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit19']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit13', 'circuit27']
[2024-07-23 21:05:22,003][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit14']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 11
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit19', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 12
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 13
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit25', 'circuit27']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit13', 'circuit16', 'circuit19', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit22', 'circuit23', 'circuit24', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit13', 'circuit16', 'circuit17', 'circuit18', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit8', 'circuit9', 'circuit13', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit9', 'circuit10', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit18', 'circuit20', 'circuit24', 'circuit26']
[2024-07-23 21:05:22,004][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit13', 'circuit16', 'circuit17', 'circuit19', 'circuit20', 'circuit22', 'circuit23', 'circuit24']
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit20']
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit16', 'circuit19', 'circuit20', 'circuit25']
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit13', 'circuit17', 'circuit20', 'circuit21', 'circuit23', 'circuit25']
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 14
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 15
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 16
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,005][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 17
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 18
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 19
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,006][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 20
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 21
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,007][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 22
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 23
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 24
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,008][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 25
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 26
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 27
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,009][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:94][INFO] Layer 10 and circuit 28
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 0
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit13', 'circuit14', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit21', 'circuit22', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit23']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit19', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit18', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit13']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit20', 'circuit21']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit16', 'circuit17', 'circuit25']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit9', 'circuit11', 'circuit14', 'circuit16', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26']
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 1
[2024-07-23 21:05:22,010][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 2
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 3
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,011][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 4
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 5
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 6
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,012][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 7
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 8
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,013][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 9
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit16', 'circuit25', 'circuit27']
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit12', 'circuit13', 'circuit16', 'circuit18', 'circuit21', 'circuit23', 'circuit26']
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0']
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 10
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 11
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,014][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 12
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 13
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit21']
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit23', 'circuit24']
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit23', 'circuit25']
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,015][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25']
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 14
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 15
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 16
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,016][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 17
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 18
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,017][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 19
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 20
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 21
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,018][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 22
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit23', 'circuit26']
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0']
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0']
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 23
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 24
[2024-07-23 21:05:22,019][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 25
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are []
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 26
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,020][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 27
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:94][INFO] Layer 11 and circuit 28
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 0, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 1, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 2, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 3, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 4, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 5, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 6, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 7, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 8, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 9, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:22,021][explain_satisfiability.py][line:100][INFO] for Layer 10, the reserve circuits are ['circuit0', 'circuit1', 'circuit2', 'circuit3', 'circuit4', 'circuit5', 'circuit6', 'circuit7', 'circuit8', 'circuit9', 'circuit10', 'circuit11', 'circuit12', 'circuit13', 'circuit14', 'circuit15', 'circuit16', 'circuit17', 'circuit18', 'circuit19', 'circuit20', 'circuit21', 'circuit22', 'circuit23', 'circuit24', 'circuit25', 'circuit26', 'circuit27', 'circuit28']
[2024-07-23 21:05:23,760][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:23,762][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,764][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,765][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,767][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,768][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,769][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,770][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,771][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,771][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,773][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,775][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,776][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:23,778][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.8760, 0.1240], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,778][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.0013, 0.9987], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,781][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.8840, 0.1160], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,782][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.0567, 0.9433], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,784][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.5982, 0.4018], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,786][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,788][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.8151, 0.1849], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,789][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.9414, 0.0586], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,792][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9035, 0.0965], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,793][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.8480, 0.1520], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,795][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.7101, 0.2899], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,797][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.7987, 0.2013], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:23,799][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.5919, 0.3569, 0.0512], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,801][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0016, 0.0057, 0.9927], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,801][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.6998, 0.1853, 0.1149], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,802][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0228, 0.2130, 0.7642], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,803][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.3379, 0.2728, 0.3893], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,803][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0091, 0.0010, 0.9899], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,804][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.4141, 0.3172, 0.2686], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,806][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.5601, 0.2736, 0.1663], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,808][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.7863, 0.1338, 0.0799], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,809][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.6896, 0.2534, 0.0571], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,811][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.5777, 0.1519, 0.2704], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,813][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.6712, 0.1580, 0.1708], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:23,815][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5658, 0.2750, 0.0975, 0.0617], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,816][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ of] are: tensor([2.7660e-03, 7.1156e-04, 1.6157e-04, 9.9636e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,817][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.5853, 0.1283, 0.1155, 0.1708], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,820][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0460, 0.3025, 0.1767, 0.4748], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,821][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2837, 0.4413, 0.1808, 0.0943], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,823][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.2564, 0.0711, 0.0179, 0.6546], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,825][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2911, 0.3001, 0.3820, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,827][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3290, 0.1464, 0.3082, 0.2164], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,828][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.3944, 0.0866, 0.0580, 0.4609], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,831][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.5378, 0.1552, 0.0942, 0.2128], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,832][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.5119, 0.1236, 0.0501, 0.3144], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,834][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.4950, 0.1927, 0.2060, 0.1064], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:23,836][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.4580, 0.1367, 0.1780, 0.1396, 0.0877], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,837][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ De] are: tensor([4.7577e-04, 1.8430e-03, 1.9662e-03, 7.1858e-04, 9.9500e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,839][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.4633, 0.1429, 0.1551, 0.1451, 0.0936], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,841][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ De] are: tensor([7.3997e-03, 1.5697e-03, 4.9343e-04, 1.9224e-03, 9.8861e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,843][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0848, 0.0304, 0.0194, 0.0221, 0.8434], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,844][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ De] are: tensor([2.5107e-02, 1.8511e-03, 3.3911e-05, 1.8053e-05, 9.7299e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,846][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.4389, 0.1573, 0.2067, 0.0502, 0.1469], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,848][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2565, 0.0871, 0.0704, 0.3043, 0.2817], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,850][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.4489, 0.0990, 0.0623, 0.2324, 0.1574], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,852][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.5173, 0.1428, 0.0882, 0.1776, 0.0742], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,854][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.4806, 0.1108, 0.0556, 0.1029, 0.2502], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,855][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.3248, 0.2346, 0.3069, 0.0896, 0.0441], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:23,857][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.3910, 0.0952, 0.1772, 0.1601, 0.1248, 0.0517], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,859][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([8.9033e-05, 4.7931e-03, 1.5460e-04, 1.4006e-03, 4.8277e-04, 9.9308e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,860][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.3253, 0.1499, 0.2778, 0.0976, 0.1197, 0.0297], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,862][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([1.3443e-03, 5.2498e-04, 2.4231e-05, 4.8777e-05, 9.8004e-04, 9.9708e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,863][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([9.6387e-03, 2.0815e-03, 8.9040e-04, 2.3722e-03, 1.8421e-02, 9.6660e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,865][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([4.1795e-03, 6.9115e-04, 3.2217e-05, 3.0980e-06, 3.2352e-04, 9.9477e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,866][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.3693, 0.1463, 0.2547, 0.0666, 0.0952, 0.0678], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,868][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.1704, 0.1249, 0.0505, 0.2912, 0.2461, 0.1170], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,869][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2367, 0.0718, 0.3045, 0.1910, 0.0930, 0.1031], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,869][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.3890, 0.1462, 0.1505, 0.1771, 0.1211, 0.0160], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,870][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.3530, 0.1560, 0.0485, 0.1120, 0.0644, 0.2661], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,871][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.3704, 0.1091, 0.1418, 0.1254, 0.0552, 0.1981], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:23,872][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.2608, 0.1815, 0.1922, 0.0788, 0.0981, 0.1201, 0.0684],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,873][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([1.0192e-06, 8.7755e-06, 3.8803e-04, 2.7981e-06, 3.3821e-05, 1.3176e-05,
        9.9955e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,875][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.3722, 0.1217, 0.1065, 0.0873, 0.2095, 0.0315, 0.0713],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,876][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([6.4776e-05, 1.0527e-04, 1.1847e-04, 9.7527e-06, 2.5299e-03, 2.3382e-03,
        9.9483e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,877][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([2.1835e-03, 9.6791e-04, 1.3731e-03, 5.5387e-04, 2.0302e-03, 4.1934e-01,
        5.7355e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,878][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([3.9244e-05, 4.0222e-06, 1.0783e-05, 3.0826e-09, 6.8012e-07, 4.1481e-06,
        9.9994e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,880][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.3578, 0.0460, 0.1112, 0.0472, 0.1037, 0.2075, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,882][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.1385, 0.0366, 0.0270, 0.0995, 0.1631, 0.5167, 0.0187],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,884][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.3253, 0.0636, 0.0886, 0.1041, 0.2854, 0.0992, 0.0339],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,886][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.3516, 0.1608, 0.1300, 0.1073, 0.0936, 0.1321, 0.0246],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,888][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3575, 0.0932, 0.0864, 0.1007, 0.0738, 0.0333, 0.2552],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,890][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2338, 0.1569, 0.1996, 0.1117, 0.0276, 0.1409, 0.1294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:23,892][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.3167, 0.0847, 0.0889, 0.1721, 0.0527, 0.0848, 0.1027, 0.0974],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,893][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([1.8418e-05, 1.5845e-04, 3.4632e-05, 6.7318e-05, 1.0771e-04, 1.1345e-03,
        4.4688e-05, 9.9843e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,895][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2892, 0.1284, 0.1390, 0.1031, 0.1174, 0.0236, 0.1353, 0.0639],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,896][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([1.5037e-05, 2.4666e-06, 2.9078e-07, 2.2765e-07, 1.2378e-05, 3.5374e-04,
        3.4359e-06, 9.9961e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,898][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0191, 0.0041, 0.0011, 0.0074, 0.0135, 0.0872, 0.0397, 0.8278],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,900][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([1.0356e-03, 6.9173e-05, 2.1573e-06, 1.5790e-07, 1.1900e-05, 8.2333e-05,
        4.3949e-05, 9.9875e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,901][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.2490, 0.0573, 0.2070, 0.0563, 0.0739, 0.0955, 0.2027, 0.0582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,904][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1548, 0.0512, 0.0824, 0.1428, 0.1735, 0.1805, 0.1188, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,905][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.3282, 0.1681, 0.0635, 0.1119, 0.0505, 0.0399, 0.0711, 0.1668],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,907][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.3182, 0.1073, 0.0941, 0.1434, 0.1170, 0.1257, 0.0775, 0.0168],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,909][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.3652, 0.0983, 0.0357, 0.1019, 0.0610, 0.0540, 0.0257, 0.2582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,911][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.2492, 0.1058, 0.1344, 0.1007, 0.0442, 0.1888, 0.0806, 0.0963],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:23,913][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2609, 0.0855, 0.0964, 0.0838, 0.0518, 0.2071, 0.0757, 0.0482, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,914][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([2.1647e-05, 2.2618e-04, 7.5674e-04, 5.2729e-05, 4.7814e-04, 8.4970e-04,
        2.2010e-02, 4.4437e-03, 9.7116e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,916][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2813, 0.1034, 0.1891, 0.0709, 0.0713, 0.0681, 0.0694, 0.0395, 0.1068],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,917][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([3.1611e-04, 1.1498e-05, 6.2957e-06, 6.2300e-06, 1.8404e-04, 1.1954e-03,
        2.0652e-03, 3.0892e-03, 9.9313e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,918][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([3.4240e-03, 6.8455e-04, 1.7888e-03, 1.8692e-03, 5.7352e-03, 1.2699e-01,
        2.1376e-02, 1.0442e-01, 7.3372e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,920][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([2.7098e-03, 6.6089e-05, 4.2695e-05, 1.3363e-06, 1.5544e-05, 4.6581e-05,
        1.4676e-03, 3.3913e-05, 9.9562e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,921][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1964, 0.0446, 0.0970, 0.0334, 0.1440, 0.1716, 0.0924, 0.1504, 0.0703],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,923][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2108, 0.0727, 0.0266, 0.1216, 0.2963, 0.0979, 0.0108, 0.0621, 0.1013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,926][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.3112, 0.0957, 0.0654, 0.1124, 0.1428, 0.1106, 0.0355, 0.1010, 0.0254],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,927][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2871, 0.1045, 0.1239, 0.0954, 0.1186, 0.0917, 0.0766, 0.0803, 0.0218],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,929][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.2810, 0.0887, 0.0638, 0.1092, 0.0574, 0.0646, 0.0869, 0.0316, 0.2168],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,931][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1898, 0.1720, 0.1684, 0.0619, 0.0262, 0.0930, 0.0876, 0.1385, 0.0625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:23,933][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.2441, 0.0991, 0.1640, 0.0518, 0.0870, 0.0434, 0.0966, 0.0902, 0.0689,
        0.0549], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,934][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ et] are: tensor([4.7677e-04, 9.6306e-04, 7.8710e-04, 1.2303e-03, 5.0300e-04, 2.9748e-04,
        5.6700e-04, 1.0168e-04, 1.6024e-03, 9.9347e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,935][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.2322, 0.0570, 0.1523, 0.1388, 0.1114, 0.0279, 0.1442, 0.0560, 0.0526,
        0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,936][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ et] are: tensor([2.3542e-04, 2.7035e-06, 2.9933e-06, 1.7182e-06, 1.9735e-05, 3.0432e-04,
        1.0826e-04, 1.1600e-03, 1.2660e-03, 9.9690e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,937][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0103, 0.0012, 0.0031, 0.0103, 0.0160, 0.0452, 0.0515, 0.0215, 0.0980,
        0.7429], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,938][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ et] are: tensor([6.6580e-04, 9.9814e-05, 1.2676e-05, 1.6858e-06, 2.2680e-05, 5.2646e-06,
        3.2962e-05, 2.5723e-06, 4.2750e-06, 9.9915e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,938][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.1125, 0.0502, 0.1162, 0.0187, 0.0562, 0.0757, 0.1488, 0.0737, 0.1571,
        0.1908], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,940][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1659, 0.0244, 0.0072, 0.0892, 0.0673, 0.0498, 0.0915, 0.0927, 0.2534,
        0.1588], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,942][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.2893, 0.1517, 0.0633, 0.2019, 0.0665, 0.0686, 0.0207, 0.0704, 0.0518,
        0.0158], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,944][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.2545, 0.1006, 0.1058, 0.1149, 0.0908, 0.0798, 0.0611, 0.0640, 0.0951,
        0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,945][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.2180, 0.1248, 0.0577, 0.1130, 0.0591, 0.0506, 0.0317, 0.0346, 0.0288,
        0.2817], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,948][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2041, 0.0920, 0.1192, 0.0695, 0.0307, 0.1229, 0.0598, 0.0869, 0.0556,
        0.1592], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:23,949][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.1812, 0.0553, 0.0874, 0.0538, 0.0374, 0.0381, 0.0958, 0.0798, 0.1372,
        0.1057, 0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,951][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([3.3097e-05, 2.2359e-04, 1.7588e-04, 4.3431e-05, 1.2347e-03, 1.5803e-04,
        4.7333e-04, 3.8590e-04, 1.7457e-04, 1.2392e-05, 9.9709e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,953][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.2732, 0.0515, 0.0984, 0.0803, 0.0509, 0.0455, 0.0462, 0.0600, 0.1690,
        0.0592, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,954][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([4.4071e-04, 1.7670e-05, 2.9868e-06, 1.0578e-06, 3.7284e-04, 6.4737e-04,
        1.1974e-04, 1.1023e-02, 2.9585e-03, 1.4759e-02, 9.6966e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,956][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0062, 0.0013, 0.0012, 0.0016, 0.0095, 0.0076, 0.0021, 0.0359, 0.0057,
        0.0430, 0.8860], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,957][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([1.7796e-04, 4.5544e-06, 1.0613e-06, 5.2177e-08, 1.2367e-05, 3.1378e-06,
        9.0654e-07, 5.1383e-05, 3.7395e-06, 3.3069e-07, 9.9974e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,959][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.1427, 0.0773, 0.1225, 0.0259, 0.0431, 0.0874, 0.1447, 0.0785, 0.1333,
        0.0724, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,961][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0949, 0.0229, 0.0258, 0.0527, 0.0250, 0.0513, 0.0445, 0.0423, 0.0832,
        0.3576, 0.1998], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,963][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.1804, 0.0613, 0.0599, 0.1166, 0.0567, 0.0692, 0.0987, 0.0650, 0.0953,
        0.0497, 0.1473], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,965][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2115, 0.0904, 0.1025, 0.1028, 0.0736, 0.1517, 0.0630, 0.0622, 0.0730,
        0.0602, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,967][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2339, 0.0713, 0.0406, 0.0957, 0.0639, 0.0431, 0.0316, 0.0534, 0.0286,
        0.0444, 0.2935], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,968][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.1835, 0.0823, 0.1299, 0.0683, 0.0266, 0.0865, 0.0563, 0.0819, 0.0699,
        0.0974, 0.1175], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:23,971][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.2061, 0.0661, 0.0753, 0.0620, 0.0414, 0.1676, 0.0582, 0.0389, 0.0737,
        0.0778, 0.0566, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,972][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [orum] are: tensor([8.6567e-06, 7.8134e-05, 2.7261e-04, 1.7985e-05, 1.8909e-04, 3.3536e-04,
        1.1171e-02, 1.7158e-03, 5.1216e-01, 4.4262e-05, 3.1099e-04, 4.7369e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,974][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.2305, 0.0863, 0.1576, 0.0584, 0.0608, 0.0616, 0.0612, 0.0364, 0.0925,
        0.0278, 0.0359, 0.0911], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,975][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.0443e-04, 1.9482e-06, 9.9700e-07, 8.5760e-07, 2.4087e-05, 1.6049e-04,
        3.2396e-04, 4.3607e-04, 1.6716e-01, 6.0149e-03, 3.2648e-02, 7.9313e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,976][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [orum] are: tensor([1.5673e-03, 2.4146e-04, 6.2082e-04, 6.6058e-04, 1.6916e-03, 4.0039e-02,
        7.2441e-03, 3.3730e-02, 2.5429e-01, 7.8436e-03, 1.3365e-01, 5.1842e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,978][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [orum] are: tensor([1.2523e-03, 3.1623e-05, 2.0093e-05, 6.3776e-07, 7.9575e-06, 2.1615e-05,
        7.0141e-04, 1.6131e-05, 5.3550e-01, 6.5836e-06, 4.8697e-05, 4.6239e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,979][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1349, 0.0309, 0.0667, 0.0218, 0.1030, 0.1296, 0.0652, 0.1083, 0.0522,
        0.1469, 0.0874, 0.0530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,982][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1671, 0.0387, 0.0137, 0.0610, 0.1336, 0.0521, 0.0053, 0.0290, 0.0523,
        0.0668, 0.2698, 0.1105], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,983][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2079, 0.0639, 0.0469, 0.0733, 0.0967, 0.0797, 0.0248, 0.0714, 0.0177,
        0.1571, 0.1423, 0.0182], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,985][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2285, 0.0839, 0.1033, 0.0758, 0.0990, 0.0762, 0.0662, 0.0664, 0.0181,
        0.0816, 0.0824, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,987][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.1796, 0.0584, 0.0463, 0.0744, 0.0429, 0.0502, 0.0706, 0.0247, 0.1896,
        0.0402, 0.0444, 0.1786], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,989][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1531, 0.1409, 0.1372, 0.0496, 0.0227, 0.0749, 0.0725, 0.1105, 0.0511,
        0.0794, 0.0538, 0.0543], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:23,991][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1593, 0.0595, 0.0350, 0.0117, 0.0350, 0.1127, 0.0594, 0.1245, 0.1007,
        0.0924, 0.0672, 0.1152, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:23,992][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ is] are: tensor([2.6377e-03, 5.3790e-04, 3.5144e-04, 4.8993e-03, 3.4439e-04, 1.7886e-04,
        1.4119e-04, 2.2006e-04, 3.7787e-04, 5.6773e-04, 4.3503e-04, 3.4606e-04,
        9.8896e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:23,994][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.2076, 0.0671, 0.0584, 0.1016, 0.0632, 0.0229, 0.1146, 0.0289, 0.0556,
        0.0608, 0.0346, 0.0583, 0.1264], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:23,995][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ is] are: tensor([8.9733e-03, 2.5728e-04, 6.3198e-05, 4.2710e-04, 1.2356e-04, 1.8010e-04,
        5.7753e-04, 1.3562e-03, 1.5530e-03, 1.7582e-02, 1.2350e-02, 8.0742e-03,
        9.4848e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:23,997][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0735, 0.0252, 0.0096, 0.0248, 0.0113, 0.0143, 0.0121, 0.0525, 0.0170,
        0.0705, 0.1032, 0.0380, 0.5479], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:23,999][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ is] are: tensor([2.4296e-02, 2.0065e-03, 2.4683e-03, 3.2399e-03, 9.1281e-04, 2.6412e-04,
        1.3690e-04, 2.7982e-04, 1.0724e-03, 3.4749e-04, 8.2915e-04, 8.5929e-04,
        9.6329e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,001][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0442, 0.0710, 0.0912, 0.0062, 0.0433, 0.0565, 0.1127, 0.1228, 0.0924,
        0.1439, 0.0943, 0.1050, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,002][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0382, 0.0055, 0.0063, 0.0105, 0.0089, 0.0068, 0.0116, 0.0223, 0.0553,
        0.1197, 0.0742, 0.1679, 0.4729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,003][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1005, 0.0181, 0.0222, 0.1145, 0.0270, 0.0361, 0.0120, 0.0361, 0.0215,
        0.0662, 0.0443, 0.0249, 0.4767], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,004][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.1968, 0.0875, 0.0598, 0.1077, 0.0675, 0.0532, 0.0483, 0.0489, 0.0532,
        0.0585, 0.0475, 0.0567, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,005][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1773, 0.0605, 0.0461, 0.1201, 0.0489, 0.0366, 0.0381, 0.0462, 0.0291,
        0.0419, 0.0515, 0.0280, 0.2756], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,006][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2686, 0.0734, 0.0622, 0.0596, 0.0286, 0.0763, 0.0518, 0.0510, 0.0488,
        0.0860, 0.0506, 0.0495, 0.0937], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,007][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1187, 0.0530, 0.0231, 0.0090, 0.0339, 0.0879, 0.0726, 0.1919, 0.1105,
        0.0628, 0.0714, 0.1262, 0.0268, 0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,008][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([5.5456e-02, 6.2910e-04, 5.2470e-05, 7.5203e-02, 1.5758e-03, 6.1685e-04,
        7.5570e-05, 2.6828e-04, 3.3991e-04, 5.4090e-04, 9.9822e-04, 3.3152e-04,
        9.3071e-04, 8.6298e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,010][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1933, 0.0688, 0.0495, 0.1293, 0.0759, 0.0232, 0.0593, 0.0433, 0.0392,
        0.0608, 0.0500, 0.0415, 0.1294, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,011][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([1.0327e-02, 5.5598e-04, 1.8978e-04, 8.8268e-04, 7.7357e-04, 1.4620e-03,
        1.3360e-03, 6.9476e-03, 4.0072e-03, 2.6717e-02, 3.9356e-02, 1.8470e-02,
        2.7426e-01, 6.1471e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,013][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0684, 0.0241, 0.0114, 0.0154, 0.0112, 0.0226, 0.0109, 0.0430, 0.0177,
        0.0876, 0.0816, 0.0392, 0.3444, 0.2226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,015][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2424, 0.1698, 0.0204, 0.1024, 0.0214, 0.0151, 0.0028, 0.0094, 0.0044,
        0.0075, 0.0067, 0.0036, 0.1080, 0.2859], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,017][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0360, 0.0823, 0.0722, 0.0017, 0.0304, 0.0477, 0.1088, 0.1206, 0.1416,
        0.0848, 0.1033, 0.1612, 0.0081, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,019][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0279, 0.0032, 0.0043, 0.0076, 0.0076, 0.0078, 0.0108, 0.0210, 0.0345,
        0.0615, 0.0589, 0.0995, 0.3213, 0.3340], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,021][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0320, 0.0061, 0.0078, 0.0554, 0.0104, 0.0231, 0.0051, 0.0258, 0.0086,
        0.0386, 0.0118, 0.0101, 0.3445, 0.4207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,023][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2133, 0.0698, 0.0517, 0.0984, 0.0611, 0.0412, 0.0386, 0.0457, 0.0436,
        0.0513, 0.0421, 0.0463, 0.0945, 0.1025], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,025][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.3014, 0.0698, 0.0431, 0.0809, 0.0521, 0.0322, 0.0262, 0.0402, 0.0215,
        0.0372, 0.0384, 0.0207, 0.0695, 0.1669], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,027][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.2058, 0.0688, 0.0761, 0.0377, 0.0235, 0.0725, 0.0467, 0.0535, 0.0570,
        0.0769, 0.0486, 0.0585, 0.0763, 0.0981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,029][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1031, 0.0711, 0.0270, 0.0167, 0.0285, 0.0588, 0.1160, 0.1381, 0.1326,
        0.0269, 0.0458, 0.1520, 0.0359, 0.0217, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,031][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ same] are: tensor([1.8082e-03, 1.3732e-02, 1.1511e-03, 1.0779e-03, 4.1301e-04, 1.3396e-03,
        2.4368e-04, 3.0300e-03, 7.6835e-05, 1.1669e-04, 5.8620e-04, 6.8874e-05,
        1.7218e-03, 1.2435e-03, 9.7339e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,033][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.1757, 0.0950, 0.0731, 0.0913, 0.0312, 0.0264, 0.0586, 0.0303, 0.0783,
        0.0449, 0.0384, 0.0810, 0.0720, 0.0579, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,034][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ same] are: tensor([8.8967e-04, 1.8280e-05, 6.2767e-06, 6.8885e-06, 2.7967e-05, 9.4842e-04,
        4.9636e-05, 1.8252e-03, 7.9017e-05, 1.3148e-03, 2.8364e-03, 3.4912e-04,
        7.8719e-04, 1.2641e-03, 9.8960e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,036][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0593, 0.0125, 0.0042, 0.0194, 0.0032, 0.0063, 0.0081, 0.0715, 0.0114,
        0.0400, 0.0668, 0.0231, 0.1284, 0.1237, 0.4222], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,038][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ same] are: tensor([1.7116e-02, 2.9655e-02, 3.2967e-04, 4.9665e-05, 3.0534e-04, 5.1293e-05,
        6.2239e-06, 1.0991e-04, 9.5665e-07, 4.3046e-05, 6.8255e-05, 6.4576e-07,
        1.0273e-05, 2.2959e-05, 9.5223e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,040][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0630, 0.1151, 0.1044, 0.0106, 0.0362, 0.0663, 0.0832, 0.0694, 0.0851,
        0.1247, 0.0766, 0.0907, 0.0208, 0.0064, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,042][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0458, 0.0035, 0.0025, 0.0074, 0.0058, 0.0018, 0.0071, 0.0236, 0.0257,
        0.0517, 0.0812, 0.0720, 0.2244, 0.3026, 0.1449], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,044][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.1046, 0.0408, 0.0543, 0.0555, 0.0232, 0.0311, 0.0253, 0.0348, 0.0407,
        0.0942, 0.0394, 0.0451, 0.1734, 0.2067, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,046][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1856, 0.0683, 0.0592, 0.0750, 0.0641, 0.0505, 0.0361, 0.0417, 0.0456,
        0.0462, 0.0577, 0.0486, 0.0854, 0.0831, 0.0527], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,048][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1891, 0.0882, 0.0505, 0.0725, 0.0386, 0.0369, 0.0231, 0.0368, 0.0206,
        0.0392, 0.0364, 0.0198, 0.0608, 0.0761, 0.2115], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,050][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.3156, 0.0679, 0.0640, 0.0382, 0.0263, 0.0611, 0.0326, 0.0359, 0.0412,
        0.0756, 0.0437, 0.0411, 0.0380, 0.0428, 0.0758], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,052][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0864, 0.0634, 0.0462, 0.0080, 0.0477, 0.0565, 0.0851, 0.1121, 0.0804,
        0.0994, 0.0845, 0.0916, 0.0482, 0.0261, 0.0486, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,054][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ as] are: tensor([2.3854e-03, 2.0895e-04, 2.5717e-04, 2.8681e-03, 3.5010e-04, 3.8690e-04,
        2.0201e-04, 2.2578e-04, 2.0202e-04, 2.0501e-04, 1.7903e-03, 1.9313e-04,
        1.9476e-04, 8.5942e-04, 7.8604e-04, 9.8889e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,056][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.1792, 0.0660, 0.0502, 0.0982, 0.0569, 0.0234, 0.0318, 0.0392, 0.0474,
        0.0406, 0.0338, 0.0509, 0.0833, 0.0367, 0.0603, 0.1021],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,057][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ as] are: tensor([5.1346e-04, 6.1543e-06, 4.4379e-06, 1.8338e-05, 4.5401e-06, 1.3863e-05,
        1.5373e-05, 1.1318e-04, 1.4519e-05, 1.5111e-04, 6.9566e-04, 7.1744e-05,
        5.8046e-03, 9.5863e-03, 8.0341e-01, 1.7958e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,059][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0219, 0.0037, 0.0035, 0.0052, 0.0030, 0.0036, 0.0018, 0.0170, 0.0050,
        0.0209, 0.0260, 0.0112, 0.2114, 0.0589, 0.1340, 0.4729],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,061][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ as] are: tensor([2.7994e-02, 6.6583e-03, 4.6749e-03, 4.2371e-03, 1.6441e-03, 4.1065e-04,
        3.0593e-04, 9.3266e-04, 1.3406e-04, 6.6179e-04, 1.5577e-03, 1.0409e-04,
        7.2350e-04, 1.4188e-03, 5.2119e-03, 9.4333e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,063][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0419, 0.0819, 0.0902, 0.0058, 0.0463, 0.0621, 0.0695, 0.1048, 0.0721,
        0.1508, 0.0772, 0.0808, 0.0232, 0.0098, 0.0676, 0.0160],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,065][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0233, 0.0017, 0.0023, 0.0033, 0.0024, 0.0027, 0.0071, 0.0129, 0.0260,
        0.0446, 0.0345, 0.0731, 0.1240, 0.1666, 0.1638, 0.3118],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,067][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0383, 0.0088, 0.0139, 0.0697, 0.0112, 0.0241, 0.0057, 0.0234, 0.0076,
        0.0357, 0.0135, 0.0087, 0.3306, 0.2743, 0.0189, 0.1156],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,069][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.1533, 0.0587, 0.0459, 0.0776, 0.0502, 0.0372, 0.0343, 0.0406, 0.0421,
        0.0474, 0.0489, 0.0451, 0.0847, 0.0851, 0.0574, 0.0913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,070][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1203, 0.0452, 0.0474, 0.0590, 0.0356, 0.0298, 0.0337, 0.0324, 0.0203,
        0.0293, 0.0414, 0.0208, 0.0882, 0.0823, 0.0667, 0.2474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,070][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.2140, 0.0688, 0.0537, 0.0550, 0.0268, 0.0528, 0.0414, 0.0405, 0.0389,
        0.0713, 0.0362, 0.0388, 0.0522, 0.0343, 0.0751, 0.1002],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,071][circuit_model.py][line:1532][INFO] ##0-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1166, 0.0495, 0.0222, 0.0082, 0.0323, 0.0856, 0.0671, 0.1789, 0.1055,
        0.0583, 0.0675, 0.1206, 0.0247, 0.0114, 0.0267, 0.0111, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,072][circuit_model.py][line:1535][INFO] ##0-th layer ##Weight##: The head2 weight for token [ the] are: tensor([2.7131e-02, 2.5542e-04, 2.0961e-05, 3.0705e-02, 6.9835e-04, 2.8151e-04,
        3.8641e-05, 1.1279e-04, 1.7476e-04, 2.5317e-04, 4.2775e-04, 1.7526e-04,
        4.4160e-04, 4.4301e-01, 3.9625e-04, 8.4876e-03, 4.8739e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,074][circuit_model.py][line:1538][INFO] ##0-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.1437, 0.0520, 0.0372, 0.0960, 0.0583, 0.0181, 0.0453, 0.0334, 0.0294,
        0.0500, 0.0411, 0.0312, 0.1009, 0.0272, 0.0481, 0.1595, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,075][circuit_model.py][line:1541][INFO] ##0-th layer ##Weight##: The head4 weight for token [ the] are: tensor([6.2250e-03, 1.9250e-04, 6.1471e-05, 2.4989e-04, 1.8313e-04, 3.1236e-04,
        3.3465e-04, 1.4383e-03, 8.9219e-04, 4.2014e-03, 6.1448e-03, 3.8249e-03,
        3.6687e-02, 8.4951e-02, 9.9260e-02, 1.6062e-01, 5.9442e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,077][circuit_model.py][line:1544][INFO] ##0-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0520, 0.0145, 0.0068, 0.0089, 0.0056, 0.0102, 0.0049, 0.0184, 0.0075,
        0.0337, 0.0293, 0.0156, 0.1236, 0.0799, 0.1484, 0.2184, 0.2223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,079][circuit_model.py][line:1547][INFO] ##0-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.1642, 0.1197, 0.0144, 0.0747, 0.0162, 0.0110, 0.0019, 0.0069, 0.0031,
        0.0051, 0.0049, 0.0026, 0.0883, 0.2206, 0.0479, 0.0179, 0.2006],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,080][circuit_model.py][line:1550][INFO] ##0-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0320, 0.0744, 0.0640, 0.0014, 0.0275, 0.0443, 0.0976, 0.1133, 0.1324,
        0.0751, 0.0966, 0.1514, 0.0070, 0.0011, 0.0749, 0.0056, 0.0013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,082][circuit_model.py][line:1553][INFO] ##0-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0192, 0.0016, 0.0020, 0.0035, 0.0031, 0.0028, 0.0037, 0.0073, 0.0123,
        0.0173, 0.0162, 0.0329, 0.0784, 0.0839, 0.0832, 0.3199, 0.3127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,084][circuit_model.py][line:1556][INFO] ##0-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0202, 0.0038, 0.0051, 0.0343, 0.0067, 0.0150, 0.0033, 0.0169, 0.0057,
        0.0245, 0.0078, 0.0067, 0.2173, 0.2539, 0.0080, 0.0637, 0.3070],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,086][circuit_model.py][line:1559][INFO] ##0-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1614, 0.0528, 0.0403, 0.0739, 0.0477, 0.0318, 0.0309, 0.0350, 0.0342,
        0.0401, 0.0329, 0.0363, 0.0730, 0.0767, 0.0530, 0.0963, 0.0834],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,088][circuit_model.py][line:1562][INFO] ##0-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2087, 0.0473, 0.0313, 0.0565, 0.0391, 0.0250, 0.0208, 0.0314, 0.0179,
        0.0298, 0.0312, 0.0178, 0.0568, 0.1346, 0.0506, 0.0647, 0.1366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,090][circuit_model.py][line:1565][INFO] ##0-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1671, 0.0535, 0.0588, 0.0308, 0.0207, 0.0561, 0.0373, 0.0406, 0.0456,
        0.0567, 0.0356, 0.0459, 0.0560, 0.0735, 0.0735, 0.0735, 0.0749],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,102][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:24,104][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,105][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,107][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,109][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,109][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,110][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,110][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,111][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,112][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,112][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,113][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,114][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,114][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.8760, 0.1240], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,115][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.0013, 0.9987], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,116][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.8840, 0.1160], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,117][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.0567, 0.9433], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,117][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.5982, 0.4018], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,118][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.0194, 0.9806], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,119][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.8151, 0.1849], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,120][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.9414, 0.0586], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,122][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.9035, 0.0965], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,124][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.8480, 0.1520], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,126][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.7101, 0.2899], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,128][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.7987, 0.2013], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,129][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.5919, 0.3569, 0.0512], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,132][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0016, 0.0057, 0.9927], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,133][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.6998, 0.1853, 0.1149], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,135][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0228, 0.2130, 0.7642], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,137][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.3379, 0.2728, 0.3893], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,139][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0091, 0.0010, 0.9899], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,141][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.4141, 0.3172, 0.2686], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,143][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.5601, 0.2736, 0.1663], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,145][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.7863, 0.1338, 0.0799], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,146][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.6896, 0.2534, 0.0571], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,149][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.5777, 0.1519, 0.2704], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,150][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.6712, 0.1580, 0.1708], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,152][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.5658, 0.2750, 0.0975, 0.0617], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,153][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([2.7660e-03, 7.1156e-04, 1.6157e-04, 9.9636e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,155][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.5853, 0.1283, 0.1155, 0.1708], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,157][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0460, 0.3025, 0.1767, 0.4748], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,158][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.2837, 0.4413, 0.1808, 0.0943], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,159][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.2564, 0.0711, 0.0179, 0.6546], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,160][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.2911, 0.3001, 0.3820, 0.0267], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,160][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.3290, 0.1464, 0.3082, 0.2164], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,161][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.3944, 0.0866, 0.0580, 0.4609], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,162][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.5378, 0.1552, 0.0942, 0.2128], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,164][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.5119, 0.1236, 0.0501, 0.3144], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,166][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.4950, 0.1927, 0.2060, 0.1064], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,167][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.4580, 0.1367, 0.1780, 0.1396, 0.0877], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,168][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([4.7577e-04, 1.8430e-03, 1.9662e-03, 7.1858e-04, 9.9500e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,170][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.4633, 0.1429, 0.1551, 0.1451, 0.0936], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,172][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([7.3997e-03, 1.5697e-03, 4.9343e-04, 1.9224e-03, 9.8861e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,173][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0848, 0.0304, 0.0194, 0.0221, 0.8434], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,175][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([2.5107e-02, 1.8511e-03, 3.3911e-05, 1.8053e-05, 9.7299e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,177][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.4389, 0.1573, 0.2067, 0.0502, 0.1469], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,179][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.2565, 0.0871, 0.0704, 0.3043, 0.2817], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,181][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.4489, 0.0990, 0.0623, 0.2324, 0.1574], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,183][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.5173, 0.1428, 0.0882, 0.1776, 0.0742], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,184][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.4806, 0.1108, 0.0556, 0.1029, 0.2502], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,186][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.3248, 0.2346, 0.3069, 0.0896, 0.0441], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,188][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.3910, 0.0952, 0.1772, 0.1601, 0.1248, 0.0517], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,189][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([8.9033e-05, 4.7931e-03, 1.5460e-04, 1.4006e-03, 4.8277e-04, 9.9308e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,191][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.3253, 0.1499, 0.2778, 0.0976, 0.1197, 0.0297], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,193][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([1.3443e-03, 5.2498e-04, 2.4231e-05, 4.8777e-05, 9.8004e-04, 9.9708e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,194][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([9.6387e-03, 2.0815e-03, 8.9040e-04, 2.3722e-03, 1.8421e-02, 9.6660e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,195][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([4.1795e-03, 6.9115e-04, 3.2217e-05, 3.0980e-06, 3.2352e-04, 9.9477e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,197][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.3693, 0.1463, 0.2547, 0.0666, 0.0952, 0.0678], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,199][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.1704, 0.1249, 0.0505, 0.2912, 0.2461, 0.1170], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,201][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.2367, 0.0718, 0.3045, 0.1910, 0.0930, 0.1031], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,202][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.3890, 0.1462, 0.1505, 0.1771, 0.1211, 0.0160], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,205][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.3530, 0.1560, 0.0485, 0.1120, 0.0644, 0.2661], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,206][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.3704, 0.1091, 0.1418, 0.1254, 0.0552, 0.1981], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,208][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.2608, 0.1815, 0.1922, 0.0788, 0.0981, 0.1201, 0.0684],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,209][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([1.0192e-06, 8.7755e-06, 3.8803e-04, 2.7981e-06, 3.3821e-05, 1.3176e-05,
        9.9955e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,211][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.3722, 0.1217, 0.1065, 0.0873, 0.2095, 0.0315, 0.0713],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,212][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([6.4776e-05, 1.0527e-04, 1.1847e-04, 9.7527e-06, 2.5299e-03, 2.3382e-03,
        9.9483e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,213][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([2.1835e-03, 9.6791e-04, 1.3731e-03, 5.5387e-04, 2.0302e-03, 4.1934e-01,
        5.7355e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,215][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([3.9244e-05, 4.0222e-06, 1.0783e-05, 3.0826e-09, 6.8012e-07, 4.1481e-06,
        9.9994e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,217][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.3578, 0.0460, 0.1112, 0.0472, 0.1037, 0.2075, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,218][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.1385, 0.0366, 0.0270, 0.0995, 0.1631, 0.5167, 0.0187],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,221][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.3253, 0.0636, 0.0886, 0.1041, 0.2854, 0.0992, 0.0339],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,223][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.3516, 0.1608, 0.1300, 0.1073, 0.0936, 0.1321, 0.0246],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,224][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.3575, 0.0932, 0.0864, 0.1007, 0.0738, 0.0333, 0.2552],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,227][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.2338, 0.1569, 0.1996, 0.1117, 0.0276, 0.1409, 0.1294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,229][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.3167, 0.0847, 0.0889, 0.1721, 0.0527, 0.0848, 0.1027, 0.0974],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,230][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([1.8418e-05, 1.5845e-04, 3.4632e-05, 6.7318e-05, 1.0771e-04, 1.1345e-03,
        4.4688e-05, 9.9843e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,230][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.2892, 0.1284, 0.1390, 0.1031, 0.1174, 0.0236, 0.1353, 0.0639],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,231][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([1.5037e-05, 2.4666e-06, 2.9078e-07, 2.2765e-07, 1.2378e-05, 3.5374e-04,
        3.4359e-06, 9.9961e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,232][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0191, 0.0041, 0.0011, 0.0074, 0.0135, 0.0872, 0.0397, 0.8278],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,233][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([1.0356e-03, 6.9173e-05, 2.1573e-06, 1.5790e-07, 1.1900e-05, 8.2333e-05,
        4.3949e-05, 9.9875e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,234][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.2490, 0.0573, 0.2070, 0.0563, 0.0739, 0.0955, 0.2027, 0.0582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,236][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.1548, 0.0512, 0.0824, 0.1428, 0.1735, 0.1805, 0.1188, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,238][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.3282, 0.1681, 0.0635, 0.1119, 0.0505, 0.0399, 0.0711, 0.1668],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,240][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.3182, 0.1073, 0.0941, 0.1434, 0.1170, 0.1257, 0.0775, 0.0168],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,242][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.3652, 0.0983, 0.0357, 0.1019, 0.0610, 0.0540, 0.0257, 0.2582],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,243][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.2492, 0.1058, 0.1344, 0.1007, 0.0442, 0.1888, 0.0806, 0.0963],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,245][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.2609, 0.0855, 0.0964, 0.0838, 0.0518, 0.2071, 0.0757, 0.0482, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,247][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([2.1647e-05, 2.2618e-04, 7.5674e-04, 5.2729e-05, 4.7814e-04, 8.4970e-04,
        2.2010e-02, 4.4437e-03, 9.7116e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,249][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.2813, 0.1034, 0.1891, 0.0709, 0.0713, 0.0681, 0.0694, 0.0395, 0.1068],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,250][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([3.1611e-04, 1.1498e-05, 6.2957e-06, 6.2300e-06, 1.8404e-04, 1.1954e-03,
        2.0652e-03, 3.0892e-03, 9.9313e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,251][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([3.4240e-03, 6.8455e-04, 1.7888e-03, 1.8692e-03, 5.7352e-03, 1.2699e-01,
        2.1376e-02, 1.0442e-01, 7.3372e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,253][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([2.7098e-03, 6.6089e-05, 4.2695e-05, 1.3363e-06, 1.5544e-05, 4.6581e-05,
        1.4676e-03, 3.3913e-05, 9.9562e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,255][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.1964, 0.0446, 0.0970, 0.0334, 0.1440, 0.1716, 0.0924, 0.1504, 0.0703],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,257][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.2108, 0.0727, 0.0266, 0.1216, 0.2963, 0.0979, 0.0108, 0.0621, 0.1013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,259][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.3112, 0.0957, 0.0654, 0.1124, 0.1428, 0.1106, 0.0355, 0.1010, 0.0254],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,261][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.2871, 0.1045, 0.1239, 0.0954, 0.1186, 0.0917, 0.0766, 0.0803, 0.0218],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,263][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.2810, 0.0887, 0.0638, 0.1092, 0.0574, 0.0646, 0.0869, 0.0316, 0.2168],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,265][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.1898, 0.1720, 0.1684, 0.0619, 0.0262, 0.0930, 0.0876, 0.1385, 0.0625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,267][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.2441, 0.0991, 0.1640, 0.0518, 0.0870, 0.0434, 0.0966, 0.0902, 0.0689,
        0.0549], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,268][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([4.7677e-04, 9.6306e-04, 7.8710e-04, 1.2303e-03, 5.0300e-04, 2.9748e-04,
        5.6700e-04, 1.0168e-04, 1.6024e-03, 9.9347e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,270][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.2322, 0.0570, 0.1523, 0.1388, 0.1114, 0.0279, 0.1442, 0.0560, 0.0526,
        0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,272][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([2.3542e-04, 2.7035e-06, 2.9933e-06, 1.7182e-06, 1.9735e-05, 3.0432e-04,
        1.0826e-04, 1.1600e-03, 1.2660e-03, 9.9690e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,273][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0103, 0.0012, 0.0031, 0.0103, 0.0160, 0.0452, 0.0515, 0.0215, 0.0980,
        0.7429], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,275][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([6.6580e-04, 9.9814e-05, 1.2676e-05, 1.6858e-06, 2.2680e-05, 5.2646e-06,
        3.2962e-05, 2.5723e-06, 4.2750e-06, 9.9915e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,277][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.1125, 0.0502, 0.1162, 0.0187, 0.0562, 0.0757, 0.1488, 0.0737, 0.1571,
        0.1908], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,279][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.1659, 0.0244, 0.0072, 0.0892, 0.0673, 0.0498, 0.0915, 0.0927, 0.2534,
        0.1588], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,281][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.2893, 0.1517, 0.0633, 0.2019, 0.0665, 0.0686, 0.0207, 0.0704, 0.0518,
        0.0158], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,283][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.2545, 0.1006, 0.1058, 0.1149, 0.0908, 0.0798, 0.0611, 0.0640, 0.0951,
        0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,285][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.2180, 0.1248, 0.0577, 0.1130, 0.0591, 0.0506, 0.0317, 0.0346, 0.0288,
        0.2817], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,287][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.2041, 0.0920, 0.1192, 0.0695, 0.0307, 0.1229, 0.0598, 0.0869, 0.0556,
        0.1592], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,289][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.1812, 0.0553, 0.0874, 0.0538, 0.0374, 0.0381, 0.0958, 0.0798, 0.1372,
        0.1057, 0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,290][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([3.3097e-05, 2.2359e-04, 1.7588e-04, 4.3431e-05, 1.2347e-03, 1.5803e-04,
        4.7333e-04, 3.8590e-04, 1.7457e-04, 1.2392e-05, 9.9709e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,293][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.2732, 0.0515, 0.0984, 0.0803, 0.0509, 0.0455, 0.0462, 0.0600, 0.1690,
        0.0592, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,294][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([4.4071e-04, 1.7670e-05, 2.9868e-06, 1.0578e-06, 3.7284e-04, 6.4737e-04,
        1.1974e-04, 1.1023e-02, 2.9585e-03, 1.4759e-02, 9.6966e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,296][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0062, 0.0013, 0.0012, 0.0016, 0.0095, 0.0076, 0.0021, 0.0359, 0.0057,
        0.0430, 0.8860], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,296][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([1.7796e-04, 4.5544e-06, 1.0613e-06, 5.2177e-08, 1.2367e-05, 3.1378e-06,
        9.0654e-07, 5.1383e-05, 3.7395e-06, 3.3069e-07, 9.9974e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,297][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.1427, 0.0773, 0.1225, 0.0259, 0.0431, 0.0874, 0.1447, 0.0785, 0.1333,
        0.0724, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,298][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0949, 0.0229, 0.0258, 0.0527, 0.0250, 0.0513, 0.0445, 0.0423, 0.0832,
        0.3576, 0.1998], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,299][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1804, 0.0613, 0.0599, 0.1166, 0.0567, 0.0692, 0.0987, 0.0650, 0.0953,
        0.0497, 0.1473], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,300][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.2115, 0.0904, 0.1025, 0.1028, 0.0736, 0.1517, 0.0630, 0.0622, 0.0730,
        0.0602, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,301][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.2339, 0.0713, 0.0406, 0.0957, 0.0639, 0.0431, 0.0316, 0.0534, 0.0286,
        0.0444, 0.2935], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,303][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.1835, 0.0823, 0.1299, 0.0683, 0.0266, 0.0865, 0.0563, 0.0819, 0.0699,
        0.0974, 0.1175], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,305][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.2061, 0.0661, 0.0753, 0.0620, 0.0414, 0.1676, 0.0582, 0.0389, 0.0737,
        0.0778, 0.0566, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,306][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([8.6567e-06, 7.8134e-05, 2.7261e-04, 1.7985e-05, 1.8909e-04, 3.3536e-04,
        1.1171e-02, 1.7158e-03, 5.1216e-01, 4.4262e-05, 3.1099e-04, 4.7369e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,307][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.2305, 0.0863, 0.1576, 0.0584, 0.0608, 0.0616, 0.0612, 0.0364, 0.0925,
        0.0278, 0.0359, 0.0911], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,309][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([1.0443e-04, 1.9482e-06, 9.9700e-07, 8.5760e-07, 2.4087e-05, 1.6049e-04,
        3.2396e-04, 4.3607e-04, 1.6716e-01, 6.0149e-03, 3.2648e-02, 7.9313e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,310][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([1.5673e-03, 2.4146e-04, 6.2082e-04, 6.6058e-04, 1.6916e-03, 4.0039e-02,
        7.2441e-03, 3.3730e-02, 2.5429e-01, 7.8436e-03, 1.3365e-01, 5.1842e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,311][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([1.2523e-03, 3.1623e-05, 2.0093e-05, 6.3776e-07, 7.9575e-06, 2.1615e-05,
        7.0141e-04, 1.6131e-05, 5.3550e-01, 6.5836e-06, 4.8697e-05, 4.6239e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,313][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.1349, 0.0309, 0.0667, 0.0218, 0.1030, 0.1296, 0.0652, 0.1083, 0.0522,
        0.1469, 0.0874, 0.0530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,315][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.1671, 0.0387, 0.0137, 0.0610, 0.1336, 0.0521, 0.0053, 0.0290, 0.0523,
        0.0668, 0.2698, 0.1105], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,317][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.2079, 0.0639, 0.0469, 0.0733, 0.0967, 0.0797, 0.0248, 0.0714, 0.0177,
        0.1571, 0.1423, 0.0182], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,319][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.2285, 0.0839, 0.1033, 0.0758, 0.0990, 0.0762, 0.0662, 0.0664, 0.0181,
        0.0816, 0.0824, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,321][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.1796, 0.0584, 0.0463, 0.0744, 0.0429, 0.0502, 0.0706, 0.0247, 0.1896,
        0.0402, 0.0444, 0.1786], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,323][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.1531, 0.1409, 0.1372, 0.0496, 0.0227, 0.0749, 0.0725, 0.1105, 0.0511,
        0.0794, 0.0538, 0.0543], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,325][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.1593, 0.0595, 0.0350, 0.0117, 0.0350, 0.1127, 0.0594, 0.1245, 0.1007,
        0.0924, 0.0672, 0.1152, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,326][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([2.6377e-03, 5.3790e-04, 3.5144e-04, 4.8993e-03, 3.4439e-04, 1.7886e-04,
        1.4119e-04, 2.2006e-04, 3.7787e-04, 5.6773e-04, 4.3503e-04, 3.4606e-04,
        9.8896e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,328][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.2076, 0.0671, 0.0584, 0.1016, 0.0632, 0.0229, 0.1146, 0.0289, 0.0556,
        0.0608, 0.0346, 0.0583, 0.1264], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,329][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([8.9733e-03, 2.5728e-04, 6.3198e-05, 4.2710e-04, 1.2356e-04, 1.8010e-04,
        5.7753e-04, 1.3562e-03, 1.5530e-03, 1.7582e-02, 1.2350e-02, 8.0742e-03,
        9.4848e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,331][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0735, 0.0252, 0.0096, 0.0248, 0.0113, 0.0143, 0.0121, 0.0525, 0.0170,
        0.0705, 0.1032, 0.0380, 0.5479], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,333][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([2.4296e-02, 2.0065e-03, 2.4683e-03, 3.2399e-03, 9.1281e-04, 2.6412e-04,
        1.3690e-04, 2.7982e-04, 1.0724e-03, 3.4749e-04, 8.2915e-04, 8.5929e-04,
        9.6329e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,335][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0442, 0.0710, 0.0912, 0.0062, 0.0433, 0.0565, 0.1127, 0.1228, 0.0924,
        0.1439, 0.0943, 0.1050, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,337][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0382, 0.0055, 0.0063, 0.0105, 0.0089, 0.0068, 0.0116, 0.0223, 0.0553,
        0.1197, 0.0742, 0.1679, 0.4729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,339][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1005, 0.0181, 0.0222, 0.1145, 0.0270, 0.0361, 0.0120, 0.0361, 0.0215,
        0.0662, 0.0443, 0.0249, 0.4767], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,341][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.1968, 0.0875, 0.0598, 0.1077, 0.0675, 0.0532, 0.0483, 0.0489, 0.0532,
        0.0585, 0.0475, 0.0567, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,343][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.1773, 0.0605, 0.0461, 0.1201, 0.0489, 0.0366, 0.0381, 0.0462, 0.0291,
        0.0419, 0.0515, 0.0280, 0.2756], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,345][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.2686, 0.0734, 0.0622, 0.0596, 0.0286, 0.0763, 0.0518, 0.0510, 0.0488,
        0.0860, 0.0506, 0.0495, 0.0937], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,347][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1187, 0.0530, 0.0231, 0.0090, 0.0339, 0.0879, 0.0726, 0.1919, 0.1105,
        0.0628, 0.0714, 0.1262, 0.0268, 0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,348][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([5.5456e-02, 6.2910e-04, 5.2470e-05, 7.5203e-02, 1.5758e-03, 6.1685e-04,
        7.5570e-05, 2.6828e-04, 3.3991e-04, 5.4090e-04, 9.9822e-04, 3.3152e-04,
        9.3071e-04, 8.6298e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,350][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1933, 0.0688, 0.0495, 0.1293, 0.0759, 0.0232, 0.0593, 0.0433, 0.0392,
        0.0608, 0.0500, 0.0415, 0.1294, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,351][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.0327e-02, 5.5598e-04, 1.8978e-04, 8.8268e-04, 7.7357e-04, 1.4620e-03,
        1.3360e-03, 6.9476e-03, 4.0072e-03, 2.6717e-02, 3.9356e-02, 1.8470e-02,
        2.7426e-01, 6.1471e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,354][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0684, 0.0241, 0.0114, 0.0154, 0.0112, 0.0226, 0.0109, 0.0430, 0.0177,
        0.0876, 0.0816, 0.0392, 0.3444, 0.2226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,356][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2424, 0.1698, 0.0204, 0.1024, 0.0214, 0.0151, 0.0028, 0.0094, 0.0044,
        0.0075, 0.0067, 0.0036, 0.1080, 0.2859], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,357][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0360, 0.0823, 0.0722, 0.0017, 0.0304, 0.0477, 0.1088, 0.1206, 0.1416,
        0.0848, 0.1033, 0.1612, 0.0081, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,360][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0279, 0.0032, 0.0043, 0.0076, 0.0076, 0.0078, 0.0108, 0.0210, 0.0345,
        0.0615, 0.0589, 0.0995, 0.3213, 0.3340], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,362][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0320, 0.0061, 0.0078, 0.0554, 0.0104, 0.0231, 0.0051, 0.0258, 0.0086,
        0.0386, 0.0118, 0.0101, 0.3445, 0.4207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,364][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2133, 0.0698, 0.0517, 0.0984, 0.0611, 0.0412, 0.0386, 0.0457, 0.0436,
        0.0513, 0.0421, 0.0463, 0.0945, 0.1025], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,365][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3014, 0.0698, 0.0431, 0.0809, 0.0521, 0.0322, 0.0262, 0.0402, 0.0215,
        0.0372, 0.0384, 0.0207, 0.0695, 0.1669], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,365][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.2058, 0.0688, 0.0761, 0.0377, 0.0235, 0.0725, 0.0467, 0.0535, 0.0570,
        0.0769, 0.0486, 0.0585, 0.0763, 0.0981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,366][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.1031, 0.0711, 0.0270, 0.0167, 0.0285, 0.0588, 0.1160, 0.1381, 0.1326,
        0.0269, 0.0458, 0.1520, 0.0359, 0.0217, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,367][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([1.8082e-03, 1.3732e-02, 1.1511e-03, 1.0779e-03, 4.1301e-04, 1.3396e-03,
        2.4368e-04, 3.0300e-03, 7.6835e-05, 1.1669e-04, 5.8620e-04, 6.8874e-05,
        1.7218e-03, 1.2435e-03, 9.7339e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,369][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.1757, 0.0950, 0.0731, 0.0913, 0.0312, 0.0264, 0.0586, 0.0303, 0.0783,
        0.0449, 0.0384, 0.0810, 0.0720, 0.0579, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,371][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([8.8967e-04, 1.8280e-05, 6.2767e-06, 6.8885e-06, 2.7967e-05, 9.4842e-04,
        4.9636e-05, 1.8252e-03, 7.9017e-05, 1.3148e-03, 2.8364e-03, 3.4912e-04,
        7.8719e-04, 1.2641e-03, 9.8960e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,373][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.0593, 0.0125, 0.0042, 0.0194, 0.0032, 0.0063, 0.0081, 0.0715, 0.0114,
        0.0400, 0.0668, 0.0231, 0.1284, 0.1237, 0.4222], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,374][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([1.7116e-02, 2.9655e-02, 3.2967e-04, 4.9665e-05, 3.0534e-04, 5.1293e-05,
        6.2239e-06, 1.0991e-04, 9.5665e-07, 4.3046e-05, 6.8255e-05, 6.4576e-07,
        1.0273e-05, 2.2959e-05, 9.5223e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,376][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0630, 0.1151, 0.1044, 0.0106, 0.0362, 0.0663, 0.0832, 0.0694, 0.0851,
        0.1247, 0.0766, 0.0907, 0.0208, 0.0064, 0.0475], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,378][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.0458, 0.0035, 0.0025, 0.0074, 0.0058, 0.0018, 0.0071, 0.0236, 0.0257,
        0.0517, 0.0812, 0.0720, 0.2244, 0.3026, 0.1449], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,379][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.1046, 0.0408, 0.0543, 0.0555, 0.0232, 0.0311, 0.0253, 0.0348, 0.0407,
        0.0942, 0.0394, 0.0451, 0.1734, 0.2067, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,382][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.1856, 0.0683, 0.0592, 0.0750, 0.0641, 0.0505, 0.0361, 0.0417, 0.0456,
        0.0462, 0.0577, 0.0486, 0.0854, 0.0831, 0.0527], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,384][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.1891, 0.0882, 0.0505, 0.0725, 0.0386, 0.0369, 0.0231, 0.0368, 0.0206,
        0.0392, 0.0364, 0.0198, 0.0608, 0.0761, 0.2115], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,386][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.3156, 0.0679, 0.0640, 0.0382, 0.0263, 0.0611, 0.0326, 0.0359, 0.0412,
        0.0756, 0.0437, 0.0411, 0.0380, 0.0428, 0.0758], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,388][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.0864, 0.0634, 0.0462, 0.0080, 0.0477, 0.0565, 0.0851, 0.1121, 0.0804,
        0.0994, 0.0845, 0.0916, 0.0482, 0.0261, 0.0486, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,389][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([2.3854e-03, 2.0895e-04, 2.5717e-04, 2.8681e-03, 3.5010e-04, 3.8690e-04,
        2.0201e-04, 2.2578e-04, 2.0202e-04, 2.0501e-04, 1.7903e-03, 1.9313e-04,
        1.9476e-04, 8.5942e-04, 7.8604e-04, 9.8889e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,391][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.1792, 0.0660, 0.0502, 0.0982, 0.0569, 0.0234, 0.0318, 0.0392, 0.0474,
        0.0406, 0.0338, 0.0509, 0.0833, 0.0367, 0.0603, 0.1021],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,393][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([5.1346e-04, 6.1543e-06, 4.4379e-06, 1.8338e-05, 4.5401e-06, 1.3863e-05,
        1.5373e-05, 1.1318e-04, 1.4519e-05, 1.5111e-04, 6.9566e-04, 7.1744e-05,
        5.8046e-03, 9.5863e-03, 8.0341e-01, 1.7958e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,394][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.0219, 0.0037, 0.0035, 0.0052, 0.0030, 0.0036, 0.0018, 0.0170, 0.0050,
        0.0209, 0.0260, 0.0112, 0.2114, 0.0589, 0.1340, 0.4729],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,396][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([2.7994e-02, 6.6583e-03, 4.6749e-03, 4.2371e-03, 1.6441e-03, 4.1065e-04,
        3.0593e-04, 9.3266e-04, 1.3406e-04, 6.6179e-04, 1.5577e-03, 1.0409e-04,
        7.2350e-04, 1.4188e-03, 5.2119e-03, 9.4333e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,398][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.0419, 0.0819, 0.0902, 0.0058, 0.0463, 0.0621, 0.0695, 0.1048, 0.0721,
        0.1508, 0.0772, 0.0808, 0.0232, 0.0098, 0.0676, 0.0160],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,400][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0233, 0.0017, 0.0023, 0.0033, 0.0024, 0.0027, 0.0071, 0.0129, 0.0260,
        0.0446, 0.0345, 0.0731, 0.1240, 0.1666, 0.1638, 0.3118],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,402][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.0383, 0.0088, 0.0139, 0.0697, 0.0112, 0.0241, 0.0057, 0.0234, 0.0076,
        0.0357, 0.0135, 0.0087, 0.3306, 0.2743, 0.0189, 0.1156],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,404][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.1533, 0.0587, 0.0459, 0.0776, 0.0502, 0.0372, 0.0343, 0.0406, 0.0421,
        0.0474, 0.0489, 0.0451, 0.0847, 0.0851, 0.0574, 0.0913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,406][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.1203, 0.0452, 0.0474, 0.0590, 0.0356, 0.0298, 0.0337, 0.0324, 0.0203,
        0.0293, 0.0414, 0.0208, 0.0882, 0.0823, 0.0667, 0.2474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,408][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.2140, 0.0688, 0.0537, 0.0550, 0.0268, 0.0528, 0.0414, 0.0405, 0.0389,
        0.0713, 0.0362, 0.0388, 0.0522, 0.0343, 0.0751, 0.1002],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,410][circuit_model.py][line:1570][INFO] ##0-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.1166, 0.0495, 0.0222, 0.0082, 0.0323, 0.0856, 0.0671, 0.1789, 0.1055,
        0.0583, 0.0675, 0.1206, 0.0247, 0.0114, 0.0267, 0.0111, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,411][circuit_model.py][line:1573][INFO] ##0-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([2.7131e-02, 2.5542e-04, 2.0961e-05, 3.0705e-02, 6.9835e-04, 2.8151e-04,
        3.8641e-05, 1.1279e-04, 1.7476e-04, 2.5317e-04, 4.2775e-04, 1.7526e-04,
        4.4160e-04, 4.4301e-01, 3.9625e-04, 8.4876e-03, 4.8739e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,413][circuit_model.py][line:1576][INFO] ##0-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1437, 0.0520, 0.0372, 0.0960, 0.0583, 0.0181, 0.0453, 0.0334, 0.0294,
        0.0500, 0.0411, 0.0312, 0.1009, 0.0272, 0.0481, 0.1595, 0.0287],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,415][circuit_model.py][line:1579][INFO] ##0-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([6.2250e-03, 1.9250e-04, 6.1471e-05, 2.4989e-04, 1.8313e-04, 3.1236e-04,
        3.3465e-04, 1.4383e-03, 8.9219e-04, 4.2014e-03, 6.1448e-03, 3.8249e-03,
        3.6687e-02, 8.4951e-02, 9.9260e-02, 1.6062e-01, 5.9442e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,417][circuit_model.py][line:1582][INFO] ##0-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0520, 0.0145, 0.0068, 0.0089, 0.0056, 0.0102, 0.0049, 0.0184, 0.0075,
        0.0337, 0.0293, 0.0156, 0.1236, 0.0799, 0.1484, 0.2184, 0.2223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,418][circuit_model.py][line:1585][INFO] ##0-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1642, 0.1197, 0.0144, 0.0747, 0.0162, 0.0110, 0.0019, 0.0069, 0.0031,
        0.0051, 0.0049, 0.0026, 0.0883, 0.2206, 0.0479, 0.0179, 0.2006],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,421][circuit_model.py][line:1588][INFO] ##0-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0320, 0.0744, 0.0640, 0.0014, 0.0275, 0.0443, 0.0976, 0.1133, 0.1324,
        0.0751, 0.0966, 0.1514, 0.0070, 0.0011, 0.0749, 0.0056, 0.0013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,423][circuit_model.py][line:1591][INFO] ##0-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0192, 0.0016, 0.0020, 0.0035, 0.0031, 0.0028, 0.0037, 0.0073, 0.0123,
        0.0173, 0.0162, 0.0329, 0.0784, 0.0839, 0.0832, 0.3199, 0.3127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,425][circuit_model.py][line:1594][INFO] ##0-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0202, 0.0038, 0.0051, 0.0343, 0.0067, 0.0150, 0.0033, 0.0169, 0.0057,
        0.0245, 0.0078, 0.0067, 0.2173, 0.2539, 0.0080, 0.0637, 0.3070],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,427][circuit_model.py][line:1597][INFO] ##0-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1614, 0.0528, 0.0403, 0.0739, 0.0477, 0.0318, 0.0309, 0.0350, 0.0342,
        0.0401, 0.0329, 0.0363, 0.0730, 0.0767, 0.0530, 0.0963, 0.0834],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,429][circuit_model.py][line:1600][INFO] ##0-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2087, 0.0473, 0.0313, 0.0565, 0.0391, 0.0250, 0.0208, 0.0314, 0.0179,
        0.0298, 0.0312, 0.0178, 0.0568, 0.1346, 0.0506, 0.0647, 0.1366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,430][circuit_model.py][line:1603][INFO] ##0-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1671, 0.0535, 0.0588, 0.0308, 0.0207, 0.0561, 0.0373, 0.0406, 0.0456,
        0.0567, 0.0356, 0.0459, 0.0560, 0.0735, 0.0735, 0.0735, 0.0749],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,434][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:24,436][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  136],
        [    1],
        [ 1067],
        [  144],
        [11654],
        [ 6538],
        [21362],
        [ 3145],
        [24049],
        [ 6686],
        [ 2972],
        [27962],
        [ 2198],
        [   30],
        [  365],
        [ 1651],
        [   26]], device='cuda:0')
[2024-07-23 21:05:24,438][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[21568],
        [    1],
        [ 3570],
        [10397],
        [12034],
        [10570],
        [38752],
        [13827],
        [43043],
        [ 8065],
        [14352],
        [42949],
        [ 9159],
        [ 5251],
        [  160],
        [12895],
        [ 5167]], device='cuda:0')
[2024-07-23 21:05:24,440][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[16921],
        [15098],
        [ 9909],
        [13560],
        [19321],
        [22716],
        [16966],
        [28697],
        [25385],
        [24954],
        [31165],
        [30048],
        [33009],
        [33972],
        [24103],
        [31499],
        [33455]], device='cuda:0')
[2024-07-23 21:05:24,442][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 8100],
        [ 7778],
        [ 7004],
        [  149],
        [15781],
        [30559],
        [22496],
        [15087],
        [33347],
        [ 2150],
        [16606],
        [33502],
        [ 2182],
        [  638],
        [11861],
        [ 2527],
        [  727]], device='cuda:0')
[2024-07-23 21:05:24,443][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[37874],
        [36621],
        [35093],
        [35493],
        [33559],
        [30782],
        [30829],
        [27380],
        [27303],
        [24963],
        [25748],
        [22920],
        [23927],
        [29265],
        [31189],
        [33879],
        [35584]], device='cuda:0')
[2024-07-23 21:05:24,445][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[21932],
        [  926],
        [ 5417],
        [ 1420],
        [27670],
        [ 2455],
        [21626],
        [ 6977],
        [ 8787],
        [ 6094],
        [15614],
        [ 8745],
        [  137],
        [ 1099],
        [ 1518],
        [  891],
        [ 1528]], device='cuda:0')
[2024-07-23 21:05:24,447][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 1576],
        [  409],
        [  761],
        [  447],
        [15674],
        [  374],
        [ 3472],
        [ 3098],
        [  595],
        [24979],
        [14304],
        [ 1505],
        [ 2386],
        [ 1420],
        [ 1138],
        [ 6432],
        [ 2405]], device='cuda:0')
[2024-07-23 21:05:24,449][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[13463],
        [ 1862],
        [11580],
        [14069],
        [10664],
        [ 2977],
        [10157],
        [ 3904],
        [16552],
        [25069],
        [ 6882],
        [16725],
        [ 1846],
        [ 3780],
        [18411],
        [14118],
        [ 3528]], device='cuda:0')
[2024-07-23 21:05:24,451][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[38051],
        [29015],
        [ 3101],
        [ 1221],
        [19189],
        [14730],
        [25541],
        [16652],
        [24736],
        [19781],
        [14147],
        [22790],
        [13681],
        [13626],
        [10355],
        [13534],
        [13747]], device='cuda:0')
[2024-07-23 21:05:24,453][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 2589],
        [ 2210],
        [ 2275],
        [ 5779],
        [ 7428],
        [10100],
        [28477],
        [26888],
        [21505],
        [37745],
        [44788],
        [37378],
        [29006],
        [ 9094],
        [ 8564],
        [ 6715],
        [ 2256]], device='cuda:0')
[2024-07-23 21:05:24,455][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[16139],
        [17028],
        [19282],
        [11445],
        [22083],
        [27538],
        [35833],
        [27625],
        [32269],
        [25975],
        [38899],
        [37332],
        [10390],
        [ 1427],
        [10702],
        [ 2469],
        [  941]], device='cuda:0')
[2024-07-23 21:05:24,457][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[5305],
        [4806],
        [3596],
        [4361],
        [4316],
        [3385],
        [2629],
        [2639],
        [2315],
        [2269],
        [2661],
        [4233],
        [3703],
        [4897],
        [4396],
        [4671],
        [5908]], device='cuda:0')
[2024-07-23 21:05:24,459][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 1668],
        [ 1906],
        [ 3214],
        [ 5945],
        [ 5159],
        [12397],
        [20872],
        [23582],
        [33796],
        [18387],
        [18010],
        [39755],
        [18376],
        [ 5641],
        [16463],
        [20105],
        [ 5297]], device='cuda:0')
[2024-07-23 21:05:24,461][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 591],
        [ 521],
        [ 335],
        [ 605],
        [1408],
        [1153],
        [1947],
        [2445],
        [3758],
        [3810],
        [4052],
        [4886],
        [1953],
        [2388],
        [ 977],
        [2051],
        [2233]], device='cuda:0')
[2024-07-23 21:05:24,463][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[  168],
        [   30],
        [25653],
        [ 6874],
        [26632],
        [43763],
        [40688],
        [27436],
        [43193],
        [24488],
        [18990],
        [43132],
        [ 9143],
        [   90],
        [ 7638],
        [27616],
        [   88]], device='cuda:0')
[2024-07-23 21:05:24,465][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[27126],
        [25161],
        [22697],
        [24524],
        [25569],
        [25239],
        [21330],
        [24261],
        [20505],
        [18253],
        [16427],
        [18588],
        [16328],
        [15726],
        [20528],
        [17665],
        [16388]], device='cuda:0')
[2024-07-23 21:05:24,467][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 1218],
        [23365],
        [11695],
        [11957],
        [15884],
        [14569],
        [ 5991],
        [20359],
        [ 8974],
        [18870],
        [12529],
        [ 8831],
        [16341],
        [11800],
        [ 8349],
        [13274],
        [11498]], device='cuda:0')
[2024-07-23 21:05:24,469][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[41229],
        [40338],
        [36857],
        [40083],
        [39514],
        [35263],
        [39395],
        [33555],
        [28421],
        [32516],
        [28531],
        [24441],
        [30862],
        [34354],
        [28139],
        [35583],
        [37710]], device='cuda:0')
[2024-07-23 21:05:24,471][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[23540],
        [ 9911],
        [ 7718],
        [16974],
        [ 4677],
        [ 3664],
        [18837],
        [16239],
        [15184],
        [ 9070],
        [ 5840],
        [15659],
        [27043],
        [23446],
        [ 8680],
        [12936],
        [20400]], device='cuda:0')
[2024-07-23 21:05:24,473][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 1676],
        [ 2207],
        [ 3160],
        [ 2887],
        [ 1583],
        [ 5526],
        [ 8803],
        [ 5727],
        [36294],
        [  586],
        [ 9080],
        [37516],
        [ 2251],
        [ 1356],
        [ 1731],
        [  480],
        [  504]], device='cuda:0')
[2024-07-23 21:05:24,475][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[22772],
        [15868],
        [31054],
        [15095],
        [26609],
        [16297],
        [25890],
        [26285],
        [15892],
        [18218],
        [34872],
        [15802],
        [31165],
        [ 7148],
        [23585],
        [30644],
        [ 7776]], device='cuda:0')
[2024-07-23 21:05:24,477][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[30247],
        [23998],
        [15909],
        [16111],
        [16876],
        [17339],
        [23667],
        [20928],
        [13257],
        [15666],
        [14319],
        [10408],
        [11976],
        [14849],
        [12350],
        [10559],
        [14968]], device='cuda:0')
[2024-07-23 21:05:24,478][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[21917],
        [21734],
        [22547],
        [27457],
        [28846],
        [25264],
        [13738],
        [ 8378],
        [11562],
        [ 5349],
        [11646],
        [13066],
        [ 7559],
        [13841],
        [14836],
        [22325],
        [20603]], device='cuda:0')
[2024-07-23 21:05:24,481][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[20803],
        [20351],
        [16623],
        [15442],
        [16038],
        [ 7641],
        [13028],
        [12387],
        [12551],
        [11921],
        [ 6180],
        [ 8164],
        [12642],
        [19742],
        [10293],
        [19539],
        [24259]], device='cuda:0')
[2024-07-23 21:05:24,483][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[14651],
        [17463],
        [21621],
        [22155],
        [23091],
        [26088],
        [27437],
        [27742],
        [30726],
        [30528],
        [30970],
        [30003],
        [29865],
        [27146],
        [28821],
        [28065],
        [25715]], device='cuda:0')
[2024-07-23 21:05:24,484][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[43493],
        [37145],
        [31453],
        [41239],
        [39159],
        [34065],
        [33011],
        [32904],
        [30056],
        [37184],
        [30549],
        [24961],
        [38541],
        [41370],
        [33217],
        [38083],
        [42503]], device='cuda:0')
[2024-07-23 21:05:24,486][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[ 5146],
        [ 6379],
        [ 7439],
        [16201],
        [24971],
        [23923],
        [29533],
        [31115],
        [34923],
        [33545],
        [35528],
        [36452],
        [32739],
        [33285],
        [31969],
        [33643],
        [33572]], device='cuda:0')
[2024-07-23 21:05:24,488][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 438],
        [1084],
        [2103],
        [1015],
        [1208],
        [2366],
        [1784],
        [1721],
        [2576],
        [2134],
        [2641],
        [3146],
        [1633],
        [1286],
        [3032],
        [1466],
        [1089]], device='cuda:0')
[2024-07-23 21:05:24,490][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[43096],
        [48817],
        [ 5178],
        [15227],
        [ 5283],
        [  612],
        [ 1231],
        [ 4495],
        [  696],
        [ 6171],
        [11076],
        [  721],
        [16277],
        [48541],
        [19393],
        [ 3503],
        [48639]], device='cuda:0')
[2024-07-23 21:05:24,492][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857],
        [6857]], device='cuda:0')
[2024-07-23 21:05:24,516][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:24,517][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,518][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,518][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,519][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,520][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,521][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,522][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,522][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,523][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,524][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,524][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,525][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,526][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.0620, 0.9380], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,526][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9981, 0.0019], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,527][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9033, 0.0967], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,529][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.3626, 0.6374], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,531][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.6862, 0.3138], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,532][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.7986, 0.2014], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,535][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9730, 0.0270], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,536][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.7751, 0.2249], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,538][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9670, 0.0330], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,540][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.0027, 0.9973], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,542][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.1456, 0.8544], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,544][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.4602, 0.5398], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,546][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0044, 0.4807, 0.5149], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,548][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.9616, 0.0256, 0.0128], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,550][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.7034, 0.1007, 0.1959], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,552][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.3312, 0.4562, 0.2126], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,553][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.5011, 0.1874, 0.3115], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,556][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.3540, 0.3811, 0.2649], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,557][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.9172, 0.0122, 0.0706], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,559][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.8079, 0.1079, 0.0842], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,561][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.8335, 0.0254, 0.1411], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,563][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0043, 0.9875, 0.0082], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,564][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0817, 0.4277, 0.4905], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,567][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.2741, 0.4666, 0.2593], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,568][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ of] are: tensor([3.3820e-03, 1.5782e-01, 8.3817e-01, 6.2865e-04], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,569][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.8296, 0.0775, 0.0750, 0.0178], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,572][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.3159, 0.0782, 0.1312, 0.4747], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,573][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1575, 0.2449, 0.1132, 0.4844], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,575][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.2198, 0.1156, 0.2260, 0.4386], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,577][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0180, 0.0018, 0.0024, 0.9778], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,579][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8760, 0.0133, 0.0136, 0.0970], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,581][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.5924, 0.1422, 0.1371, 0.1283], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,583][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.7308, 0.0257, 0.1446, 0.0988], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,584][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ of] are: tensor([1.0140e-03, 9.9651e-01, 4.6687e-04, 2.0046e-03], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,586][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0622, 0.3245, 0.3827, 0.2306], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,586][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.2254, 0.3393, 0.2007, 0.2347], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,587][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ De] are: tensor([8.5032e-04, 5.0136e-02, 9.0554e-01, 2.9888e-03, 4.0484e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,588][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.6966, 0.0494, 0.0659, 0.1078, 0.0803], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,589][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.2605, 0.0648, 0.1060, 0.3309, 0.2378], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,589][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.1591, 0.2191, 0.0952, 0.4259, 0.1007], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,590][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.2382, 0.0971, 0.1670, 0.3671, 0.1307], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,592][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.3084, 0.0321, 0.0341, 0.5873, 0.0381], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,594][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.7701, 0.0201, 0.0287, 0.1439, 0.0371], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,596][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2557, 0.1218, 0.3390, 0.1713, 0.1122], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,597][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.6574, 0.0232, 0.1272, 0.0878, 0.1044], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,599][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0031, 0.7273, 0.0036, 0.0131, 0.2530], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,601][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0503, 0.2465, 0.2835, 0.1750, 0.2447], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,603][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.1421, 0.3063, 0.1648, 0.1765, 0.2103], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,604][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([5.0052e-04, 5.5920e-02, 1.3196e-01, 3.7862e-03, 7.3493e-01, 7.2899e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,606][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4758, 0.0493, 0.0458, 0.1461, 0.2751, 0.0080], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,608][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.1708, 0.0511, 0.0844, 0.2705, 0.2024, 0.2209], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,610][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.1290, 0.1647, 0.0826, 0.3746, 0.0883, 0.1608], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,612][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.2616, 0.0932, 0.1305, 0.2630, 0.1240, 0.1276], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,614][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.2435, 0.3002, 0.0540, 0.2591, 0.1298, 0.0134], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,616][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.7409, 0.0139, 0.0060, 0.1575, 0.0622, 0.0195], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,618][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.1540, 0.1553, 0.3947, 0.1741, 0.0506, 0.0712], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,619][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.7554, 0.0144, 0.0904, 0.0532, 0.0731, 0.0136], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,622][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.0043, 0.8633, 0.0443, 0.0104, 0.0761, 0.0016], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,624][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0401, 0.2020, 0.2302, 0.1436, 0.1999, 0.1842], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,625][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.0907, 0.3287, 0.1530, 0.1451, 0.1957, 0.0867], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,627][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0012, 0.0155, 0.0831, 0.0093, 0.4044, 0.4782, 0.0082],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,629][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.5660, 0.0370, 0.0433, 0.1303, 0.1230, 0.0352, 0.0652],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,631][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.1678, 0.0454, 0.0740, 0.1999, 0.1558, 0.1678, 0.1892],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,633][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.1443, 0.1611, 0.0682, 0.3484, 0.0786, 0.1602, 0.0391],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,635][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4278, 0.0751, 0.0944, 0.2345, 0.0544, 0.0901, 0.0237],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,637][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.1676, 0.0765, 0.0341, 0.1600, 0.1869, 0.3536, 0.0213],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,639][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.7773, 0.0204, 0.0213, 0.1250, 0.0286, 0.0152, 0.0122],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,641][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.6138, 0.0295, 0.0411, 0.0182, 0.0307, 0.2501, 0.0167],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,643][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.6655, 0.0156, 0.0984, 0.0643, 0.0834, 0.0174, 0.0554],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,644][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([4.0077e-03, 8.7431e-01, 5.8021e-03, 6.4547e-03, 8.4015e-02, 6.5947e-04,
        2.4750e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,646][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0360, 0.1604, 0.1888, 0.1137, 0.1585, 0.1504, 0.1922],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,649][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.0732, 0.3054, 0.1465, 0.1323, 0.2016, 0.0872, 0.0538],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,650][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([1.1779e-04, 1.0902e-02, 1.4064e-01, 4.1703e-03, 4.0224e-01, 1.2353e-01,
        2.9404e-01, 2.4369e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,652][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.2661, 0.0491, 0.0597, 0.1107, 0.2239, 0.0642, 0.1569, 0.0695],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,653][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0989, 0.0372, 0.0561, 0.1569, 0.1346, 0.1482, 0.1691, 0.1989],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,653][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.1016, 0.1468, 0.0646, 0.3357, 0.0752, 0.1464, 0.0411, 0.0886],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,654][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.2151, 0.0617, 0.0877, 0.2247, 0.0814, 0.1291, 0.0722, 0.1281],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,655][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.1193, 0.0562, 0.0581, 0.0432, 0.1366, 0.1268, 0.4555, 0.0044],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,656][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.7312, 0.0186, 0.0087, 0.1396, 0.0282, 0.0170, 0.0120, 0.0448],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,658][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1190, 0.0548, 0.0735, 0.0690, 0.0398, 0.0917, 0.4209, 0.1313],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,660][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.5915, 0.0167, 0.1107, 0.0586, 0.0891, 0.0224, 0.0731, 0.0379],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,662][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.0044, 0.8381, 0.0121, 0.0069, 0.0524, 0.0009, 0.0429, 0.0423],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,664][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0287, 0.1352, 0.1596, 0.0989, 0.1363, 0.1291, 0.1656, 0.1465],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,666][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.0803, 0.2670, 0.1303, 0.1243, 0.1680, 0.0793, 0.0395, 0.1114],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:24,667][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([9.4912e-05, 7.4980e-03, 2.9157e-02, 3.1335e-03, 1.4573e-01, 4.6982e-01,
        2.6191e-02, 3.0515e-01, 1.3230e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,669][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2196, 0.0371, 0.0698, 0.1007, 0.1963, 0.0487, 0.1292, 0.1744, 0.0243],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,671][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0947, 0.0314, 0.0515, 0.1390, 0.1134, 0.1254, 0.1437, 0.1684, 0.1326],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,673][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.1164, 0.1427, 0.0649, 0.2926, 0.0707, 0.1294, 0.0388, 0.0839, 0.0606],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,675][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.4050, 0.0705, 0.0702, 0.2489, 0.0450, 0.0921, 0.0106, 0.0551, 0.0026],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,677][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0622, 0.0940, 0.0788, 0.1388, 0.1800, 0.0411, 0.2747, 0.0786, 0.0519],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,679][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.5506, 0.0117, 0.0205, 0.1285, 0.0417, 0.0134, 0.0112, 0.0964, 0.1260],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,681][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.9195, 0.0035, 0.0046, 0.0012, 0.0109, 0.0397, 0.0102, 0.0093, 0.0012],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,683][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.5919, 0.0146, 0.0988, 0.0572, 0.0810, 0.0195, 0.0601, 0.0354, 0.0416],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,684][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([2.1182e-03, 8.4241e-01, 1.5639e-02, 1.7863e-03, 2.7739e-02, 5.4748e-04,
        3.0808e-02, 5.6881e-02, 2.2070e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,686][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0242, 0.1217, 0.1406, 0.0829, 0.1169, 0.1110, 0.1400, 0.1315, 0.1314],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,688][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0581, 0.2744, 0.1291, 0.1082, 0.1754, 0.0749, 0.0449, 0.1089, 0.0260],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:24,689][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ et] are: tensor([1.5296e-04, 1.0264e-02, 2.8705e-02, 2.0619e-03, 1.5843e-01, 6.8705e-02,
        5.6255e-01, 1.9595e-02, 1.4804e-01, 1.5006e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,692][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.3189, 0.0580, 0.1003, 0.0914, 0.1419, 0.0491, 0.0942, 0.0890, 0.0380,
        0.0192], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,694][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0746, 0.0278, 0.0422, 0.1234, 0.1036, 0.1103, 0.1292, 0.1540, 0.1208,
        0.1140], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,695][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.0974, 0.1234, 0.0558, 0.2522, 0.0649, 0.1283, 0.0374, 0.0853, 0.0605,
        0.0948], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,698][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.1623, 0.0628, 0.0881, 0.2022, 0.0707, 0.1036, 0.0538, 0.1073, 0.0471,
        0.1022], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,699][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0250, 0.0102, 0.0074, 0.0338, 0.0492, 0.0408, 0.0448, 0.1189, 0.2704,
        0.3995], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,701][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.4744, 0.0171, 0.0123, 0.1165, 0.0281, 0.0124, 0.0122, 0.0446, 0.0652,
        0.2171], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,704][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1995, 0.0991, 0.0772, 0.0832, 0.0276, 0.0384, 0.2310, 0.0385, 0.1279,
        0.0775], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,705][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.5866, 0.0161, 0.1017, 0.0596, 0.0794, 0.0164, 0.0559, 0.0306, 0.0377,
        0.0160], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,707][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ et] are: tensor([1.2613e-03, 4.7219e-01, 1.8835e-03, 5.3958e-03, 6.2351e-02, 3.0737e-04,
        9.8232e-03, 2.0838e-02, 9.3616e-03, 4.1659e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,709][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0224, 0.1055, 0.1230, 0.0752, 0.1045, 0.1002, 0.1268, 0.1160, 0.1207,
        0.1058], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,711][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.0780, 0.2378, 0.1201, 0.1250, 0.1675, 0.0737, 0.0404, 0.0921, 0.0221,
        0.0433], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:24,712][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([5.2905e-05, 4.0500e-03, 9.8949e-02, 6.9279e-04, 1.1415e-02, 1.4401e-01,
        6.4038e-02, 9.3810e-02, 2.5377e-01, 2.1618e-02, 3.0759e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,715][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.2081, 0.0591, 0.0359, 0.0868, 0.0954, 0.0504, 0.0883, 0.1478, 0.0677,
        0.1116, 0.0490], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,716][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.0734, 0.0286, 0.0430, 0.1137, 0.0931, 0.0955, 0.1134, 0.1285, 0.1007,
        0.0944, 0.1156], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,717][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.0782, 0.1156, 0.0515, 0.2524, 0.0602, 0.1160, 0.0343, 0.0681, 0.0580,
        0.0911, 0.0745], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,718][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.1095, 0.0438, 0.0736, 0.1507, 0.0699, 0.0816, 0.0746, 0.1055, 0.1293,
        0.0719, 0.0895], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,719][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0347, 0.0202, 0.0072, 0.0275, 0.0196, 0.1021, 0.5214, 0.0857, 0.0585,
        0.1017, 0.0213], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,719][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.4214, 0.0105, 0.0214, 0.1212, 0.0262, 0.0115, 0.0068, 0.0302, 0.0565,
        0.2090, 0.0854], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,720][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0070, 0.0100, 0.0188, 0.0102, 0.0107, 0.0169, 0.1532, 0.0677, 0.6686,
        0.0355, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,722][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.5147, 0.0152, 0.0992, 0.0572, 0.0907, 0.0212, 0.0639, 0.0380, 0.0430,
        0.0186, 0.0382], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,723][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([8.9802e-04, 3.8033e-01, 2.0546e-03, 9.4462e-03, 1.6742e-02, 3.7817e-04,
        1.1241e-02, 2.2964e-02, 1.4466e-02, 5.4053e-01, 9.4833e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,725][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0228, 0.0944, 0.1105, 0.0678, 0.0942, 0.0901, 0.1152, 0.1039, 0.1093,
        0.0945, 0.0973], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,727][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.0669, 0.2314, 0.1164, 0.1092, 0.1601, 0.0709, 0.0391, 0.0978, 0.0224,
        0.0407, 0.0451], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:24,728][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [orum] are: tensor([3.6416e-05, 3.1637e-03, 1.2405e-02, 1.3089e-03, 6.1204e-02, 1.9944e-01,
        1.1163e-02, 1.2912e-01, 5.6954e-03, 7.1252e-03, 5.6308e-01, 6.2636e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,730][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.1476, 0.0330, 0.0643, 0.0874, 0.1379, 0.0287, 0.0941, 0.1216, 0.0190,
        0.1154, 0.1325, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,732][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0542, 0.0222, 0.0357, 0.0885, 0.0786, 0.0875, 0.1047, 0.1193, 0.0950,
        0.0870, 0.1095, 0.1176], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,734][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0916, 0.1113, 0.0508, 0.2287, 0.0555, 0.1017, 0.0306, 0.0658, 0.0476,
        0.0876, 0.0813, 0.0474], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,736][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.3503, 0.0639, 0.0608, 0.2218, 0.0400, 0.0791, 0.0085, 0.0474, 0.0021,
        0.0555, 0.0685, 0.0022], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,738][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0161, 0.0393, 0.0196, 0.0356, 0.0608, 0.0237, 0.1522, 0.0667, 0.0466,
        0.3768, 0.1111, 0.0513], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,739][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.3304, 0.0083, 0.0150, 0.0853, 0.0288, 0.0105, 0.0073, 0.0699, 0.0862,
        0.1318, 0.0522, 0.1744], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,741][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.8757, 0.0039, 0.0064, 0.0013, 0.0132, 0.0483, 0.0185, 0.0090, 0.0023,
        0.0045, 0.0145, 0.0022], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,743][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.4945, 0.0139, 0.1004, 0.0500, 0.0879, 0.0241, 0.0610, 0.0425, 0.0440,
        0.0184, 0.0475, 0.0158], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,744][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [orum] are: tensor([9.4514e-05, 3.7685e-02, 3.1648e-04, 9.0296e-05, 1.1438e-03, 1.6940e-05,
        9.1240e-04, 1.8374e-03, 6.4401e-04, 9.5657e-01, 4.4282e-05, 6.4762e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,747][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0176, 0.0877, 0.1022, 0.0594, 0.0843, 0.0809, 0.1018, 0.0959, 0.0956,
        0.0865, 0.0865, 0.1016], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,749][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0455, 0.2503, 0.1137, 0.0920, 0.1626, 0.0679, 0.0415, 0.1046, 0.0239,
        0.0346, 0.0376, 0.0260], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:24,750][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ is] are: tensor([4.1319e-04, 2.1025e-03, 4.4160e-02, 7.5490e-04, 5.1000e-03, 1.1480e-02,
        5.0922e-02, 1.8930e-02, 3.4003e-01, 1.1951e-02, 1.3663e-01, 3.7736e-01,
        1.6147e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,752][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.5382, 0.0350, 0.0328, 0.0203, 0.0530, 0.0179, 0.0350, 0.0725, 0.0307,
        0.0447, 0.0722, 0.0294, 0.0184], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,754][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0579, 0.0263, 0.0376, 0.0797, 0.0724, 0.0724, 0.0882, 0.0929, 0.0740,
        0.0679, 0.0835, 0.0833, 0.1637], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,756][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0628, 0.0860, 0.0389, 0.1898, 0.0488, 0.0933, 0.0288, 0.0572, 0.0457,
        0.0747, 0.0731, 0.0456, 0.1553], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,758][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1096, 0.0375, 0.0666, 0.1598, 0.0413, 0.0490, 0.0526, 0.0569, 0.0667,
        0.0691, 0.0478, 0.0654, 0.1778], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,760][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0690, 0.0253, 0.0294, 0.0509, 0.0088, 0.0088, 0.0715, 0.0202, 0.1405,
        0.0989, 0.0175, 0.1851, 0.2742], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,762][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.3227, 0.0134, 0.0123, 0.0559, 0.0297, 0.0093, 0.0100, 0.0455, 0.0428,
        0.1377, 0.0467, 0.0567, 0.2174], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,764][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0164, 0.0801, 0.1051, 0.0803, 0.0090, 0.0280, 0.1108, 0.0458, 0.1201,
        0.0930, 0.0154, 0.0992, 0.1970], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,766][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.6459, 0.0071, 0.0652, 0.0371, 0.0513, 0.0130, 0.0396, 0.0218, 0.0289,
        0.0108, 0.0293, 0.0114, 0.0385], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,767][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ is] are: tensor([5.5089e-04, 1.9570e-01, 1.9437e-04, 2.7685e-04, 1.7957e-02, 1.7142e-05,
        1.5135e-03, 5.3179e-03, 1.7795e-03, 7.7474e-01, 2.0850e-04, 1.5457e-03,
        2.0336e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,769][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0161, 0.0793, 0.0925, 0.0564, 0.0799, 0.0768, 0.0977, 0.0879, 0.0924,
        0.0812, 0.0820, 0.0980, 0.0599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,771][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0683, 0.1832, 0.0980, 0.1117, 0.1427, 0.0670, 0.0390, 0.0901, 0.0220,
        0.0423, 0.0395, 0.0213, 0.0749], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:24,773][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0011, 0.0170, 0.0312, 0.0076, 0.0360, 0.0584, 0.0613, 0.0102, 0.3083,
        0.0265, 0.0928, 0.3264, 0.0085, 0.0149], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,776][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1514, 0.0325, 0.0217, 0.0924, 0.0437, 0.0336, 0.0346, 0.1038, 0.0249,
        0.0497, 0.0780, 0.0221, 0.2780, 0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,777][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0324, 0.0175, 0.0252, 0.0541, 0.0547, 0.0598, 0.0736, 0.0798, 0.0683,
        0.0622, 0.0769, 0.0824, 0.1585, 0.1547], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,780][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0622, 0.0815, 0.0385, 0.1568, 0.0425, 0.0852, 0.0253, 0.0509, 0.0428,
        0.0672, 0.0700, 0.0428, 0.1166, 0.1176], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,782][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0746, 0.0360, 0.0635, 0.1288, 0.0361, 0.0452, 0.0459, 0.0494, 0.0474,
        0.0573, 0.0383, 0.0462, 0.1398, 0.1913], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,783][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0484, 0.0169, 0.0125, 0.1695, 0.0060, 0.0116, 0.1289, 0.0192, 0.2464,
        0.0588, 0.0095, 0.1932, 0.0506, 0.0285], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,784][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.2145, 0.0079, 0.0068, 0.0344, 0.0134, 0.0064, 0.0055, 0.0209, 0.0279,
        0.0722, 0.0173, 0.0388, 0.1719, 0.3620], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,785][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0325, 0.0526, 0.0879, 0.0698, 0.0170, 0.0414, 0.1120, 0.0610, 0.0683,
        0.0633, 0.0218, 0.0584, 0.1611, 0.1531], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,786][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.4450, 0.0080, 0.0688, 0.0437, 0.0591, 0.0106, 0.0447, 0.0219, 0.0287,
        0.0112, 0.0244, 0.0085, 0.0342, 0.1912], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,787][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.3839e-04, 6.7759e-02, 1.9275e-04, 1.3412e-04, 4.7918e-03, 1.7358e-05,
        1.2631e-03, 3.5285e-03, 8.6301e-04, 8.8441e-01, 1.8354e-04, 8.4502e-04,
        4.0852e-04, 3.5361e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,788][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0156, 0.0745, 0.0891, 0.0527, 0.0741, 0.0722, 0.0931, 0.0831, 0.0870,
        0.0771, 0.0773, 0.0924, 0.0555, 0.0564], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,790][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0728, 0.1570, 0.0839, 0.0922, 0.1136, 0.0598, 0.0341, 0.0800, 0.0219,
        0.0391, 0.0368, 0.0215, 0.0592, 0.1279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:24,792][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0003, 0.0070, 0.0226, 0.0053, 0.0083, 0.0523, 0.0132, 0.0050, 0.3010,
        0.0231, 0.1100, 0.3240, 0.0039, 0.1129, 0.0110], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,794][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0695, 0.0051, 0.0070, 0.0293, 0.0194, 0.0327, 0.0190, 0.0510, 0.0201,
        0.0460, 0.0261, 0.0211, 0.4401, 0.2058, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,796][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0366, 0.0159, 0.0238, 0.0549, 0.0497, 0.0536, 0.0665, 0.0739, 0.0591,
        0.0542, 0.0679, 0.0710, 0.1406, 0.1445, 0.0879], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,798][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0531, 0.0726, 0.0361, 0.1552, 0.0399, 0.0728, 0.0257, 0.0480, 0.0419,
        0.0642, 0.0635, 0.0420, 0.1128, 0.0947, 0.0775], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,800][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0803, 0.0329, 0.0522, 0.1272, 0.0275, 0.0386, 0.0317, 0.0381, 0.0291,
        0.0499, 0.0360, 0.0289, 0.1552, 0.2040, 0.0683], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,802][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.0415, 0.0376, 0.0498, 0.0295, 0.0046, 0.0070, 0.2582, 0.1404, 0.0275,
        0.1787, 0.0197, 0.0261, 0.0329, 0.0621, 0.0845], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,804][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1798, 0.0055, 0.0057, 0.0392, 0.0104, 0.0042, 0.0066, 0.0163, 0.0219,
        0.0839, 0.0147, 0.0329, 0.2609, 0.3099, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,805][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0461, 0.0743, 0.0804, 0.0576, 0.0140, 0.0375, 0.0529, 0.0877, 0.0312,
        0.0790, 0.0156, 0.0235, 0.1928, 0.1067, 0.1007], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,808][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.4684, 0.0084, 0.0649, 0.0392, 0.0536, 0.0113, 0.0393, 0.0206, 0.0264,
        0.0106, 0.0233, 0.0088, 0.0323, 0.1903, 0.0027], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,809][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ same] are: tensor([3.1043e-05, 1.8573e-02, 4.9387e-04, 7.6467e-05, 8.0879e-04, 1.6021e-05,
        1.2224e-03, 1.4260e-03, 1.2750e-03, 9.6045e-01, 4.6970e-05, 1.1514e-03,
        1.5984e-04, 1.2121e-02, 2.1469e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,811][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0149, 0.0698, 0.0835, 0.0498, 0.0698, 0.0673, 0.0866, 0.0765, 0.0807,
        0.0715, 0.0727, 0.0866, 0.0523, 0.0523, 0.0657], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,814][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.0739, 0.1720, 0.0901, 0.0972, 0.1202, 0.0536, 0.0300, 0.0698, 0.0171,
        0.0331, 0.0318, 0.0160, 0.0487, 0.1002, 0.0464], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:24,815][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ as] are: tensor([6.0143e-04, 7.6416e-03, 5.8594e-02, 1.6956e-03, 7.2211e-03, 5.6606e-02,
        1.0296e-02, 9.8051e-03, 7.8784e-02, 7.7678e-02, 1.9243e-01, 8.3867e-02,
        2.9963e-03, 4.5853e-02, 3.6586e-01, 7.0625e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,817][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.1687, 0.0304, 0.0122, 0.0284, 0.0531, 0.0654, 0.0655, 0.0645, 0.0441,
        0.0289, 0.0570, 0.0378, 0.2076, 0.0982, 0.0309, 0.0071],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,819][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0293, 0.0187, 0.0254, 0.0456, 0.0477, 0.0488, 0.0608, 0.0631, 0.0525,
        0.0474, 0.0574, 0.0586, 0.1101, 0.1020, 0.0671, 0.1653],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,821][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0503, 0.0636, 0.0298, 0.1341, 0.0364, 0.0678, 0.0230, 0.0390, 0.0378,
        0.0563, 0.0539, 0.0378, 0.1033, 0.0852, 0.0708, 0.1111],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,823][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0512, 0.0309, 0.0491, 0.1206, 0.0311, 0.0357, 0.0315, 0.0431, 0.0326,
        0.0495, 0.0324, 0.0310, 0.1144, 0.1557, 0.0507, 0.1404],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,824][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ as] are: tensor([5.9597e-03, 8.2254e-04, 3.1353e-03, 5.1809e-03, 6.3993e-04, 3.8050e-03,
        5.7715e-02, 9.5818e-03, 8.5134e-03, 1.6893e-02, 1.1213e-02, 8.1350e-03,
        4.3125e-03, 9.2626e-03, 3.4270e-03, 8.5140e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,826][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.1330, 0.0042, 0.0031, 0.0288, 0.0116, 0.0044, 0.0029, 0.0205, 0.0221,
        0.0726, 0.0253, 0.0314, 0.2263, 0.3824, 0.0196, 0.0117],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,828][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0347, 0.0390, 0.0569, 0.0453, 0.0150, 0.0350, 0.0896, 0.0699, 0.0799,
        0.0780, 0.0173, 0.0710, 0.1386, 0.0950, 0.0857, 0.0489],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,830][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.4783, 0.0074, 0.0643, 0.0398, 0.0525, 0.0105, 0.0419, 0.0207, 0.0288,
        0.0106, 0.0249, 0.0093, 0.0341, 0.1658, 0.0023, 0.0087],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,832][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ as] are: tensor([3.5976e-04, 1.4952e-01, 2.3501e-04, 3.7960e-04, 1.7233e-02, 3.0452e-05,
        1.4717e-03, 4.7307e-03, 1.1326e-03, 6.5260e-01, 2.7781e-04, 9.1277e-04,
        3.0269e-04, 1.1099e-01, 8.8388e-03, 5.0982e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,833][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0136, 0.0659, 0.0781, 0.0470, 0.0672, 0.0636, 0.0822, 0.0712, 0.0771,
        0.0691, 0.0691, 0.0817, 0.0489, 0.0497, 0.0634, 0.0522],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,836][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.0566, 0.1357, 0.0737, 0.0840, 0.1065, 0.0539, 0.0296, 0.0707, 0.0179,
        0.0332, 0.0314, 0.0174, 0.0510, 0.1042, 0.0405, 0.0936],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:24,838][circuit_model.py][line:1532][INFO] ##1-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0010, 0.0151, 0.0274, 0.0066, 0.0302, 0.0489, 0.0541, 0.0083, 0.2833,
        0.0234, 0.0771, 0.2998, 0.0076, 0.0133, 0.0167, 0.0737, 0.0136],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,839][circuit_model.py][line:1535][INFO] ##1-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1036, 0.0259, 0.0172, 0.0755, 0.0356, 0.0368, 0.0317, 0.0852, 0.0259,
        0.0503, 0.0686, 0.0220, 0.2082, 0.0165, 0.1059, 0.0775, 0.0135],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,842][circuit_model.py][line:1538][INFO] ##1-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0211, 0.0135, 0.0184, 0.0332, 0.0370, 0.0399, 0.0504, 0.0526, 0.0467,
        0.0412, 0.0509, 0.0546, 0.1013, 0.0916, 0.0618, 0.1559, 0.1299],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,844][circuit_model.py][line:1541][INFO] ##1-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0460, 0.0595, 0.0283, 0.1155, 0.0311, 0.0623, 0.0187, 0.0373, 0.0315,
        0.0494, 0.0516, 0.0316, 0.0854, 0.0866, 0.0755, 0.1006, 0.0891],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,846][circuit_model.py][line:1544][INFO] ##1-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0482, 0.0285, 0.0457, 0.0987, 0.0282, 0.0351, 0.0281, 0.0368, 0.0260,
        0.0407, 0.0303, 0.0245, 0.1029, 0.1362, 0.0504, 0.1190, 0.1207],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,848][circuit_model.py][line:1547][INFO] ##1-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0223, 0.0085, 0.0065, 0.1058, 0.0036, 0.0088, 0.1175, 0.0138, 0.2347,
        0.0595, 0.0083, 0.2028, 0.0239, 0.0279, 0.0145, 0.1035, 0.0380],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,850][circuit_model.py][line:1550][INFO] ##1-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1756, 0.0052, 0.0039, 0.0227, 0.0087, 0.0037, 0.0034, 0.0128, 0.0162,
        0.0462, 0.0104, 0.0222, 0.1183, 0.2943, 0.0094, 0.0134, 0.2335],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,851][circuit_model.py][line:1553][INFO] ##1-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0349, 0.0263, 0.0452, 0.0350, 0.0173, 0.0439, 0.0824, 0.0761, 0.0450,
        0.0480, 0.0197, 0.0392, 0.1423, 0.1082, 0.1113, 0.0462, 0.0790],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,852][circuit_model.py][line:1556][INFO] ##1-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3156, 0.0073, 0.0617, 0.0405, 0.0527, 0.0086, 0.0418, 0.0190, 0.0261,
        0.0099, 0.0199, 0.0070, 0.0296, 0.1380, 0.0020, 0.0078, 0.2126],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,853][circuit_model.py][line:1559][INFO] ##1-th layer ##Weight##: The head10 weight for token [ the] are: tensor([4.9397e-04, 1.0930e-01, 9.1582e-05, 2.1044e-04, 9.4317e-03, 1.4855e-05,
        1.0787e-03, 3.7975e-03, 7.0510e-04, 6.4508e-01, 2.2697e-04, 6.3782e-04,
        3.5467e-04, 7.3124e-02, 9.6152e-03, 5.9989e-02, 8.5842e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,854][circuit_model.py][line:1562][INFO] ##1-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0131, 0.0627, 0.0753, 0.0444, 0.0627, 0.0609, 0.0787, 0.0702, 0.0733,
        0.0656, 0.0653, 0.0779, 0.0470, 0.0478, 0.0599, 0.0500, 0.0453],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,856][circuit_model.py][line:1565][INFO] ##1-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0600, 0.1186, 0.0648, 0.0705, 0.0847, 0.0465, 0.0259, 0.0616, 0.0172,
        0.0297, 0.0281, 0.0170, 0.0429, 0.0964, 0.0354, 0.0778, 0.1229],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:24,875][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:24,876][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,876][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,877][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,878][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,878][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,879][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,880][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,881][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,882][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,883][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,883][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,884][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:24,885][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.5497, 0.4503], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,885][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.3544, 0.6456], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,886][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.9306, 0.0694], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,887][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.3732, 0.6268], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,887][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.1128, 0.8872], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,888][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.0188, 0.9812], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,889][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.2375, 0.7625], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,889][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.3319, 0.6681], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,891][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.7264, 0.2736], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,893][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.9878, 0.0122], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,895][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.0584, 0.9416], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,897][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.2282, 0.7718], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:24,898][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.1344, 0.7951, 0.0704], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,900][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.2417, 0.4976, 0.2607], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,902][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.8056, 0.0760, 0.1183], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,904][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.3440, 0.4496, 0.2064], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,906][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0466, 0.3759, 0.5775], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,908][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0047, 0.5169, 0.4783], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,910][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.5243, 0.0044, 0.4714], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,912][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0738, 0.8612, 0.0650], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,914][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.3844, 0.1569, 0.4587], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,915][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.9522, 0.0110, 0.0368], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,918][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0494, 0.4740, 0.4766], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,919][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.1656, 0.0038, 0.8305], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:24,921][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.1862, 0.6357, 0.1724, 0.0057], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,923][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.2039, 0.4210, 0.2860, 0.0891], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,925][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.6409, 0.0772, 0.1300, 0.1520], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,927][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.1693, 0.2481, 0.1121, 0.4705], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,929][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0381, 0.2902, 0.5738, 0.0979], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,931][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0026, 0.2855, 0.5765, 0.1355], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,933][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.9553, 0.0228, 0.0049, 0.0170], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,935][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0728, 0.1173, 0.6929, 0.1170], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,936][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.3511, 0.1355, 0.3452, 0.1682], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,937][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.9114, 0.0108, 0.0188, 0.0590], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,938][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0446, 0.3122, 0.3147, 0.3285], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,939][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.2645, 0.0043, 0.0021, 0.7291], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:24,939][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0199, 0.2265, 0.6932, 0.0271, 0.0333], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,940][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.2071, 0.3761, 0.3022, 0.0879, 0.0267], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,942][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.4590, 0.0852, 0.1262, 0.1280, 0.2016], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,944][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.1723, 0.2254, 0.0958, 0.4083, 0.0982], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,945][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0331, 0.2634, 0.4237, 0.0938, 0.1859], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,947][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.0482, 0.0724, 0.8727, 0.0058, 0.0009], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,949][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.5815, 0.1357, 0.1544, 0.0678, 0.0606], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,951][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0127, 0.3321, 0.5031, 0.0450, 0.1071], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,953][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.3048, 0.1172, 0.3022, 0.1460, 0.1297], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,955][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.7409, 0.0142, 0.0368, 0.1367, 0.0714], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,957][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.0286, 0.2365, 0.2379, 0.2495, 0.2475], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,958][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([3.0468e-02, 2.3912e-03, 2.9180e-04, 4.1540e-02, 9.2531e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:24,960][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0252, 0.1214, 0.0306, 0.0304, 0.7806, 0.0118], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,962][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.2084, 0.3572, 0.3178, 0.0865, 0.0221, 0.0080], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,964][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.3749, 0.0743, 0.1039, 0.1322, 0.1662, 0.1485], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,966][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.1382, 0.1663, 0.0827, 0.3627, 0.0863, 0.1639], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,968][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0275, 0.2291, 0.3509, 0.0839, 0.1486, 0.1601], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,969][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.0304, 0.1266, 0.8270, 0.0119, 0.0012, 0.0029], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,971][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([3.6993e-01, 2.5852e-02, 1.0831e-04, 1.0808e-01, 2.1297e-01, 2.8306e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,973][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0062, 0.1766, 0.6857, 0.0528, 0.0695, 0.0093], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,974][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.2704, 0.1012, 0.2779, 0.1298, 0.1153, 0.1054], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,976][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.7328, 0.0142, 0.0368, 0.1613, 0.0396, 0.0153], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,978][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.0201, 0.1875, 0.1891, 0.1975, 0.1964, 0.2094], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,979][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([2.4691e-02, 2.1341e-03, 1.1477e-04, 1.8011e-02, 1.5395e-03, 9.5351e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:24,981][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.1836, 0.1000, 0.0534, 0.0613, 0.4923, 0.1033, 0.0060],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,983][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.2161, 0.3736, 0.2990, 0.0843, 0.0170, 0.0070, 0.0029],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,985][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.3645, 0.0677, 0.0965, 0.0951, 0.1365, 0.1242, 0.1155],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,987][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.1558, 0.1627, 0.0686, 0.3343, 0.0764, 0.1644, 0.0378],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,989][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.0211, 0.1750, 0.3283, 0.0562, 0.1270, 0.1087, 0.1836],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,990][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([2.1698e-02, 7.6109e-02, 8.9242e-01, 8.7857e-03, 5.7201e-05, 9.0547e-04,
        2.3211e-05], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,992][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.3437, 0.1289, 0.0479, 0.0430, 0.0127, 0.4037, 0.0201],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,994][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0019, 0.4181, 0.2518, 0.0269, 0.0339, 0.2032, 0.0641],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,996][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.2267, 0.0880, 0.2463, 0.1186, 0.1042, 0.0947, 0.1215],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:24,998][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.7911, 0.0072, 0.0427, 0.0952, 0.0196, 0.0218, 0.0223],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,000][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.0217, 0.1555, 0.1580, 0.1641, 0.1619, 0.1760, 0.1628],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,001][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([4.2508e-02, 1.7518e-03, 6.4136e-03, 3.7169e-02, 2.0796e-03, 3.2731e-04,
        9.0975e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,003][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.0536, 0.1191, 0.1178, 0.0663, 0.4702, 0.0500, 0.1018, 0.0212],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,004][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.2549, 0.3534, 0.2625, 0.0816, 0.0213, 0.0091, 0.0040, 0.0133],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,005][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.2738, 0.0600, 0.0746, 0.1062, 0.1423, 0.1218, 0.1105, 0.1108],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,005][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.1083, 0.1484, 0.0651, 0.3214, 0.0741, 0.1504, 0.0400, 0.0924],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,006][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0222, 0.1585, 0.2448, 0.0670, 0.0993, 0.1128, 0.1532, 0.1422],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,007][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([1.0655e-01, 7.9045e-02, 8.0579e-01, 7.8314e-03, 9.9349e-05, 5.8943e-04,
        2.7912e-05, 6.2182e-05], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,009][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.5496, 0.0729, 0.0009, 0.0503, 0.0115, 0.2618, 0.0151, 0.0379],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,011][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.0201, 0.0694, 0.4139, 0.0227, 0.0698, 0.0676, 0.2953, 0.0413],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,013][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.2203, 0.0859, 0.2335, 0.1165, 0.1014, 0.0920, 0.1136, 0.0367],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,014][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.5842, 0.0155, 0.0631, 0.1716, 0.0537, 0.0243, 0.0613, 0.0262],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,016][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.0141, 0.1243, 0.1277, 0.1337, 0.1325, 0.1423, 0.1351, 0.1902],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,018][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([1.7803e-02, 7.7889e-04, 1.9313e-04, 2.1238e-02, 6.1216e-04, 4.6991e-03,
        4.4992e-04, 9.5423e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,019][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.1409, 0.1591, 0.0374, 0.0537, 0.3156, 0.1336, 0.0195, 0.1386, 0.0016],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,021][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.2086, 0.3614, 0.3199, 0.0776, 0.0146, 0.0057, 0.0024, 0.0092, 0.0006],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,023][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.2965, 0.0412, 0.0734, 0.0876, 0.1149, 0.0949, 0.0942, 0.0830, 0.1142],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,025][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.1231, 0.1412, 0.0646, 0.2805, 0.0686, 0.1324, 0.0379, 0.0880, 0.0636],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,027][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0197, 0.1139, 0.2423, 0.0549, 0.0997, 0.0964, 0.1265, 0.1049, 0.1417],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,029][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([4.0050e-02, 2.9939e-02, 9.2545e-01, 4.4780e-03, 5.0384e-06, 7.2143e-05,
        6.7261e-07, 1.7766e-06, 1.2959e-08], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,030][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0039, 0.0015, 0.0062, 0.0053, 0.0135, 0.0723, 0.0009, 0.3529, 0.5435],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,033][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0020, 0.2854, 0.2068, 0.0093, 0.0360, 0.0711, 0.0660, 0.2920, 0.0314],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,035][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.2198, 0.0781, 0.2061, 0.1054, 0.0906, 0.0794, 0.1033, 0.0330, 0.0843],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,036][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.8080, 0.0054, 0.0250, 0.0804, 0.0147, 0.0113, 0.0175, 0.0128, 0.0249],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,039][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0150, 0.1097, 0.1103, 0.1156, 0.1148, 0.1232, 0.1142, 0.1676, 0.1298],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,040][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([1.9149e-02, 1.5952e-03, 1.7992e-03, 1.7082e-02, 3.7962e-04, 7.3201e-04,
        8.8694e-03, 6.0141e-04, 9.4979e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,042][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0474, 0.0853, 0.0236, 0.0486, 0.6104, 0.0254, 0.1408, 0.0083, 0.0085,
        0.0017], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,044][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.2343, 0.3656, 0.2740, 0.0785, 0.0171, 0.0082, 0.0036, 0.0129, 0.0010,
        0.0047], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,046][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.2200, 0.0499, 0.0730, 0.0741, 0.1091, 0.0951, 0.0988, 0.0944, 0.1250,
        0.0606], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,048][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.1040, 0.1250, 0.0559, 0.2404, 0.0632, 0.1306, 0.0363, 0.0894, 0.0628,
        0.0924], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,050][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0144, 0.1211, 0.2043, 0.0399, 0.0701, 0.0857, 0.1201, 0.1175, 0.1746,
        0.0523], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,051][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([2.2717e-02, 1.0014e-01, 8.6504e-01, 1.1472e-02, 8.6815e-05, 4.2086e-04,
        2.4904e-05, 6.1266e-05, 1.6784e-06, 3.8479e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,053][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.2855, 0.0496, 0.0076, 0.0322, 0.0158, 0.0759, 0.0070, 0.0835, 0.1991,
        0.2439], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,056][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0037, 0.1412, 0.1673, 0.0152, 0.0133, 0.0416, 0.1211, 0.2616, 0.2050,
        0.0299], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,057][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.1982, 0.0773, 0.2037, 0.1011, 0.0870, 0.0782, 0.1002, 0.0324, 0.0829,
        0.0391], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,060][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.5970, 0.0070, 0.0291, 0.1335, 0.0157, 0.0258, 0.0279, 0.0219, 0.0261,
        0.1158], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,062][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.0159, 0.0977, 0.0990, 0.1053, 0.1030, 0.1121, 0.1044, 0.1525, 0.1193,
        0.0906], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,063][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([1.2816e-02, 1.1228e-03, 2.7102e-04, 9.7523e-03, 2.6711e-03, 1.6179e-04,
        7.2337e-04, 6.5680e-05, 5.1812e-04, 9.7190e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,065][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0187, 0.0571, 0.2466, 0.0438, 0.0317, 0.1463, 0.0921, 0.1321, 0.0650,
        0.0925, 0.0741], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,067][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.2274, 0.3390, 0.2670, 0.0863, 0.0237, 0.0101, 0.0047, 0.0147, 0.0014,
        0.0060, 0.0198], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,069][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.1827, 0.0511, 0.0656, 0.0742, 0.1046, 0.0935, 0.0886, 0.0872, 0.1219,
        0.0568, 0.0736], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,070][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.0836, 0.1186, 0.0524, 0.2406, 0.0597, 0.1187, 0.0331, 0.0709, 0.0594,
        0.0896, 0.0734], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,071][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0127, 0.1011, 0.1836, 0.0429, 0.0673, 0.0735, 0.0987, 0.1027, 0.1888,
        0.0434, 0.0851], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,071][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([8.4962e-02, 1.0341e-01, 7.9116e-01, 1.5851e-02, 7.2724e-04, 2.2140e-03,
        2.1163e-04, 6.2555e-04, 2.5018e-05, 3.1116e-04, 5.0213e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,072][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.1264, 0.0046, 0.0397, 0.0301, 0.0070, 0.0207, 0.0015, 0.0064, 0.2690,
        0.0838, 0.4107], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,073][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0043, 0.1095, 0.4458, 0.0295, 0.0179, 0.0216, 0.0664, 0.0766, 0.1726,
        0.0543, 0.0015], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,075][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1729, 0.0718, 0.2043, 0.1025, 0.0877, 0.0784, 0.1003, 0.0309, 0.0795,
        0.0362, 0.0356], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,077][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.5388, 0.0076, 0.0338, 0.1409, 0.0220, 0.0229, 0.0288, 0.0147, 0.0284,
        0.1251, 0.0370], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,079][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.0153, 0.0890, 0.0913, 0.0972, 0.0953, 0.1030, 0.0975, 0.1401, 0.1103,
        0.0833, 0.0776], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,080][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([2.1042e-02, 1.0620e-03, 5.9322e-04, 7.0455e-03, 1.8923e-03, 2.1680e-04,
        3.5095e-04, 4.8858e-03, 7.5912e-04, 1.1452e-03, 9.6101e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,082][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0748, 0.1340, 0.0353, 0.0506, 0.2303, 0.1451, 0.0188, 0.1390, 0.0024,
        0.0690, 0.0972, 0.0036], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,084][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.2067, 0.3516, 0.3118, 0.0790, 0.0143, 0.0058, 0.0024, 0.0090, 0.0005,
        0.0033, 0.0146, 0.0010], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,086][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.2212, 0.0348, 0.0622, 0.0700, 0.0921, 0.0769, 0.0768, 0.0664, 0.0925,
        0.0501, 0.0657, 0.0913], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,088][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0967, 0.1099, 0.0509, 0.2184, 0.0541, 0.1038, 0.0302, 0.0692, 0.0503,
        0.0862, 0.0807, 0.0496], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,090][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0154, 0.0884, 0.1874, 0.0431, 0.0781, 0.0750, 0.0966, 0.0804, 0.1068,
        0.0452, 0.0852, 0.0983], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,091][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([3.1492e-02, 3.6984e-02, 9.2542e-01, 5.9906e-03, 8.4462e-06, 9.5668e-05,
        1.1177e-06, 2.8840e-06, 2.4620e-08, 1.2507e-06, 5.1487e-06, 2.7771e-08],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,093][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0020, 0.0008, 0.0034, 0.0028, 0.0070, 0.0358, 0.0005, 0.1803, 0.2904,
        0.0025, 0.1401, 0.3346], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,095][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0014, 0.2597, 0.1994, 0.0090, 0.0284, 0.0716, 0.0693, 0.2461, 0.0368,
        0.0270, 0.0284, 0.0229], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,097][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.1995, 0.0690, 0.1865, 0.0984, 0.0806, 0.0694, 0.0910, 0.0270, 0.0712,
        0.0327, 0.0314, 0.0436], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,099][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.7046, 0.0050, 0.0227, 0.0739, 0.0129, 0.0105, 0.0159, 0.0111, 0.0231,
        0.0732, 0.0245, 0.0228], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,101][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0135, 0.0836, 0.0851, 0.0895, 0.0883, 0.0949, 0.0880, 0.1291, 0.0994,
        0.0767, 0.0710, 0.0809], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,103][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([4.7085e-03, 9.3778e-04, 6.2919e-04, 5.0888e-03, 3.3525e-04, 4.9022e-04,
        9.7908e-03, 4.6781e-04, 7.8448e-01, 1.3240e-03, 2.2206e-04, 1.9153e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,105][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.6235, 0.0541, 0.0607, 0.0351, 0.0398, 0.0077, 0.0274, 0.0129, 0.0060,
        0.1039, 0.0159, 0.0121, 0.0009], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,107][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.2314, 0.3234, 0.2729, 0.0833, 0.0188, 0.0082, 0.0040, 0.0139, 0.0011,
        0.0053, 0.0182, 0.0019, 0.0175], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,109][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.1602, 0.0315, 0.0476, 0.0570, 0.0859, 0.0744, 0.0743, 0.0697, 0.1006,
        0.0484, 0.0669, 0.1019, 0.0817], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,111][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0659, 0.0836, 0.0383, 0.1797, 0.0472, 0.0952, 0.0285, 0.0601, 0.0489,
        0.0730, 0.0728, 0.0487, 0.1580], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,113][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0159, 0.0785, 0.1352, 0.0311, 0.0683, 0.0659, 0.0868, 0.0741, 0.1482,
        0.0430, 0.0872, 0.1393, 0.0264], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,114][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([7.7333e-03, 7.6952e-02, 9.1000e-01, 4.8495e-03, 4.2361e-05, 3.3853e-04,
        1.1957e-05, 3.0593e-05, 6.0739e-07, 1.3847e-05, 2.1053e-05, 8.0855e-07,
        2.6492e-06], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,116][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.1166, 0.0509, 0.0269, 0.0033, 0.0270, 0.0353, 0.0059, 0.1120, 0.0321,
        0.0739, 0.4373, 0.0334, 0.0455], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,118][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0019, 0.1194, 0.1223, 0.0100, 0.0133, 0.0360, 0.1032, 0.1297, 0.2010,
        0.1119, 0.0286, 0.1197, 0.0030], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,120][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1410, 0.0594, 0.1665, 0.0862, 0.0703, 0.0676, 0.0815, 0.0274, 0.0645,
        0.0341, 0.0322, 0.0420, 0.1270], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,122][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.5489, 0.0058, 0.0211, 0.1115, 0.0215, 0.0195, 0.0255, 0.0130, 0.0224,
        0.1123, 0.0475, 0.0192, 0.0318], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,124][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0134, 0.0761, 0.0785, 0.0826, 0.0810, 0.0882, 0.0818, 0.1200, 0.0923,
        0.0697, 0.0643, 0.0743, 0.0777], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,125][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([6.7565e-02, 1.2334e-03, 4.5153e-03, 2.0779e-02, 1.0554e-03, 4.7145e-04,
        1.0243e-02, 1.7756e-03, 4.3954e-03, 7.3139e-03, 1.0416e-03, 2.8631e-03,
        8.7675e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,127][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.4793, 0.1690, 0.0212, 0.1258, 0.0752, 0.0119, 0.0218, 0.0044, 0.0027,
        0.0649, 0.0049, 0.0056, 0.0069, 0.0063], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,129][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2186, 0.2981, 0.2284, 0.0863, 0.0215, 0.0115, 0.0058, 0.0167, 0.0018,
        0.0074, 0.0223, 0.0029, 0.0274, 0.0512], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,131][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1479, 0.0333, 0.0583, 0.0458, 0.0729, 0.0728, 0.0801, 0.0710, 0.0869,
        0.0470, 0.0608, 0.0880, 0.0531, 0.0821], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,133][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0661, 0.0794, 0.0378, 0.1469, 0.0408, 0.0869, 0.0247, 0.0532, 0.0460,
        0.0663, 0.0708, 0.0459, 0.1176, 0.1175], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,135][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0153, 0.0957, 0.1733, 0.0277, 0.0573, 0.0656, 0.1119, 0.0854, 0.1052,
        0.0480, 0.0723, 0.0976, 0.0170, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,136][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([1.1662e-02, 7.1751e-02, 9.1233e-01, 3.7075e-03, 4.4360e-05, 4.1167e-04,
        1.0266e-05, 3.0953e-05, 5.2462e-07, 1.4562e-05, 3.1409e-05, 6.8825e-07,
        2.3807e-06, 1.2386e-06], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,136][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0737, 0.0613, 0.0097, 0.0037, 0.0060, 0.0799, 0.0048, 0.0791, 0.1178,
        0.0591, 0.0670, 0.1251, 0.2831, 0.0298], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,137][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0044, 0.0415, 0.2273, 0.0100, 0.0069, 0.0253, 0.1117, 0.0690, 0.2585,
        0.0838, 0.0066, 0.1393, 0.0033, 0.0124], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,139][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1290, 0.0525, 0.1486, 0.0768, 0.0630, 0.0605, 0.0734, 0.0244, 0.0575,
        0.0306, 0.0289, 0.0372, 0.1161, 0.1013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,141][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.5726, 0.0071, 0.0195, 0.0477, 0.0244, 0.0108, 0.0224, 0.0161, 0.0146,
        0.0613, 0.0470, 0.0131, 0.0296, 0.1139], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,143][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0146, 0.0689, 0.0709, 0.0744, 0.0734, 0.0806, 0.0758, 0.1107, 0.0858,
        0.0646, 0.0600, 0.0695, 0.0719, 0.0790], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,144][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([2.6257e-01, 1.9140e-03, 5.3692e-03, 1.0951e-01, 1.5473e-03, 1.2677e-03,
        1.8794e-03, 5.4891e-04, 2.4396e-03, 2.5570e-03, 7.6233e-04, 2.6823e-03,
        9.0826e-03, 5.9787e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,146][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.3877, 0.1513, 0.0311, 0.0978, 0.0842, 0.0310, 0.0212, 0.0131, 0.0078,
        0.0960, 0.0183, 0.0139, 0.0101, 0.0350, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,148][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.1936, 0.3002, 0.2463, 0.0856, 0.0244, 0.0115, 0.0055, 0.0166, 0.0017,
        0.0069, 0.0223, 0.0029, 0.0202, 0.0515, 0.0108], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,150][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.1475, 0.0319, 0.0567, 0.0443, 0.0708, 0.0641, 0.0802, 0.0596, 0.0867,
        0.0445, 0.0515, 0.0862, 0.0592, 0.0632, 0.0538], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,152][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.0553, 0.0700, 0.0355, 0.1467, 0.0385, 0.0735, 0.0252, 0.0499, 0.0447,
        0.0633, 0.0635, 0.0446, 0.1132, 0.0925, 0.0836], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,154][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.0134, 0.0804, 0.1689, 0.0290, 0.0580, 0.0582, 0.1107, 0.0737, 0.1098,
        0.0429, 0.0594, 0.1024, 0.0174, 0.0230, 0.0529], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,155][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([1.2312e-02, 1.2515e-01, 8.5421e-01, 7.1139e-03, 1.3866e-04, 5.3550e-04,
        8.6295e-05, 1.5914e-04, 8.1998e-06, 8.2315e-05, 1.0422e-04, 9.0413e-06,
        1.7011e-05, 1.4616e-05, 6.3087e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,158][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0207, 0.0050, 0.0014, 0.0043, 0.0015, 0.0090, 0.0116, 0.0106, 0.0215,
        0.0194, 0.0183, 0.0211, 0.8235, 0.0142, 0.0179], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,160][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.0074, 0.1634, 0.0525, 0.0128, 0.0142, 0.0204, 0.0683, 0.1250, 0.1180,
        0.2242, 0.0146, 0.0787, 0.0200, 0.0096, 0.0711], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,162][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.1311, 0.0514, 0.1389, 0.0729, 0.0607, 0.0537, 0.0675, 0.0221, 0.0537,
        0.0272, 0.0256, 0.0345, 0.1094, 0.1010, 0.0504], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,164][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.2563, 0.0039, 0.0158, 0.0827, 0.0137, 0.0178, 0.0160, 0.0127, 0.0190,
        0.0959, 0.0332, 0.0174, 0.0390, 0.2742, 0.1024], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,166][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.0128, 0.0643, 0.0665, 0.0698, 0.0687, 0.0751, 0.0708, 0.1005, 0.0794,
        0.0603, 0.0558, 0.0645, 0.0664, 0.0722, 0.0728], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,168][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([2.3593e-02, 6.5109e-03, 1.6729e-03, 1.6036e-02, 1.6190e-04, 8.2919e-05,
        4.5582e-04, 1.6674e-04, 8.6743e-05, 7.5924e-04, 9.0222e-04, 3.0537e-05,
        5.3486e-04, 6.7862e-04, 9.4833e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,169][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([3.5380e-01, 1.2616e-01, 8.0569e-02, 1.5680e-02, 2.1872e-02, 2.5825e-02,
        9.3573e-03, 1.3332e-02, 5.5241e-03, 2.5482e-01, 4.1624e-02, 1.2642e-02,
        3.3906e-03, 2.1825e-02, 1.3545e-02, 4.2206e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,171][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.1999, 0.3057, 0.2314, 0.0816, 0.0210, 0.0101, 0.0048, 0.0141, 0.0015,
        0.0059, 0.0201, 0.0025, 0.0229, 0.0501, 0.0100, 0.0185],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,173][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.1277, 0.0317, 0.0454, 0.0427, 0.0690, 0.0635, 0.0681, 0.0600, 0.0870,
        0.0431, 0.0540, 0.0881, 0.0487, 0.0659, 0.0438, 0.0613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,175][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.0533, 0.0614, 0.0289, 0.1272, 0.0351, 0.0683, 0.0224, 0.0399, 0.0400,
        0.0549, 0.0531, 0.0399, 0.1043, 0.0834, 0.0761, 0.1117],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,177][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.0141, 0.0767, 0.1107, 0.0264, 0.0617, 0.0626, 0.1043, 0.0757, 0.1301,
        0.0508, 0.0615, 0.1210, 0.0160, 0.0241, 0.0352, 0.0291],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,179][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([2.1910e-02, 8.2570e-02, 8.8931e-01, 5.3224e-03, 7.3488e-05, 5.7047e-04,
        2.7664e-05, 5.0196e-05, 2.3142e-06, 2.9592e-05, 5.2153e-05, 3.0512e-06,
        4.4867e-06, 4.7690e-06, 1.8569e-05, 4.8074e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,180][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([6.2336e-03, 1.7707e-03, 8.5003e-05, 7.0683e-04, 1.9262e-03, 1.8193e-02,
        7.2285e-05, 4.3092e-02, 9.2485e-03, 7.1512e-03, 1.6067e-01, 9.5525e-03,
        2.5978e-01, 5.8888e-03, 4.7541e-01, 2.1375e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,182][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0132, 0.1411, 0.0755, 0.0170, 0.0138, 0.0139, 0.0593, 0.0539, 0.2041,
        0.1843, 0.0129, 0.1301, 0.0062, 0.0063, 0.0383, 0.0301],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,184][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.1308, 0.0473, 0.1198, 0.0690, 0.0554, 0.0507, 0.0629, 0.0212, 0.0501,
        0.0262, 0.0244, 0.0328, 0.0945, 0.0831, 0.0443, 0.0877],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,186][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.5192, 0.0034, 0.0131, 0.0704, 0.0135, 0.0132, 0.0118, 0.0091, 0.0107,
        0.0573, 0.0226, 0.0090, 0.0108, 0.1102, 0.0763, 0.0493],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,188][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.0122, 0.0610, 0.0624, 0.0655, 0.0644, 0.0703, 0.0649, 0.0963, 0.0741,
        0.0555, 0.0514, 0.0597, 0.0618, 0.0680, 0.0676, 0.0650],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,190][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([1.0965e-01, 2.8941e-03, 7.9725e-03, 7.4657e-02, 1.5984e-03, 6.7453e-04,
        1.3515e-02, 4.9147e-03, 8.5194e-04, 1.9343e-03, 8.5865e-04, 5.2605e-04,
        1.0526e-02, 4.7477e-03, 1.7359e-03, 7.6295e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,192][circuit_model.py][line:1570][INFO] ##1-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3069, 0.2291, 0.0370, 0.1313, 0.0812, 0.0214, 0.0352, 0.0065, 0.0073,
        0.0785, 0.0085, 0.0138, 0.0098, 0.0089, 0.0022, 0.0038, 0.0184],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,193][circuit_model.py][line:1573][INFO] ##1-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2092, 0.2707, 0.2037, 0.0839, 0.0221, 0.0123, 0.0061, 0.0167, 0.0019,
        0.0077, 0.0222, 0.0030, 0.0279, 0.0470, 0.0112, 0.0206, 0.0339],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,196][circuit_model.py][line:1576][INFO] ##1-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.1115, 0.0281, 0.0502, 0.0379, 0.0607, 0.0608, 0.0671, 0.0591, 0.0722,
        0.0397, 0.0516, 0.0735, 0.0425, 0.0674, 0.0567, 0.0588, 0.0622],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,198][circuit_model.py][line:1579][INFO] ##1-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0491, 0.0577, 0.0276, 0.1085, 0.0299, 0.0632, 0.0181, 0.0386, 0.0335,
        0.0485, 0.0517, 0.0335, 0.0862, 0.0860, 0.0829, 0.1010, 0.0840],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,199][circuit_model.py][line:1582][INFO] ##1-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0140, 0.0843, 0.1538, 0.0244, 0.0497, 0.0576, 0.0966, 0.0748, 0.0926,
        0.0423, 0.0649, 0.0862, 0.0151, 0.0249, 0.0679, 0.0295, 0.0213],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,200][circuit_model.py][line:1585][INFO] ##1-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([1.4138e-02, 6.2076e-02, 9.2052e-01, 2.7660e-03, 3.7611e-05, 3.7790e-04,
        7.5475e-06, 2.1539e-05, 4.0663e-07, 1.0332e-05, 2.8152e-05, 5.3319e-07,
        1.0979e-06, 7.4251e-07, 4.3612e-06, 9.4593e-06, 1.0599e-06],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,201][circuit_model.py][line:1588][INFO] ##1-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0721, 0.0526, 0.0071, 0.0033, 0.0050, 0.0637, 0.0040, 0.0652, 0.1018,
        0.0488, 0.0532, 0.1079, 0.2600, 0.0286, 0.0924, 0.0053, 0.0290],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,202][circuit_model.py][line:1591][INFO] ##1-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0046, 0.0425, 0.1708, 0.0089, 0.0059, 0.0229, 0.0958, 0.0637, 0.2276,
        0.0820, 0.0064, 0.1238, 0.0034, 0.0120, 0.0742, 0.0381, 0.0175],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,203][circuit_model.py][line:1594][INFO] ##1-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1091, 0.0423, 0.1155, 0.0637, 0.0499, 0.0473, 0.0565, 0.0184, 0.0435,
        0.0230, 0.0212, 0.0272, 0.0845, 0.0734, 0.0390, 0.0757, 0.1098],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,206][circuit_model.py][line:1597][INFO] ##1-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.4432, 0.0056, 0.0170, 0.0422, 0.0184, 0.0098, 0.0184, 0.0140, 0.0124,
        0.0514, 0.0369, 0.0110, 0.0257, 0.0976, 0.0796, 0.0321, 0.0847],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,207][circuit_model.py][line:1600][INFO] ##1-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0126, 0.0560, 0.0575, 0.0602, 0.0593, 0.0653, 0.0609, 0.0896, 0.0692,
        0.0520, 0.0483, 0.0559, 0.0576, 0.0635, 0.0634, 0.0610, 0.0677],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,209][circuit_model.py][line:1603][INFO] ##1-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1517, 0.0013, 0.0030, 0.0701, 0.0011, 0.0010, 0.0012, 0.0004, 0.0017,
        0.0018, 0.0005, 0.0020, 0.0051, 0.3895, 0.0007, 0.0093, 0.3596],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,212][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:25,215][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  441],
        [  931],
        [10604],
        [13796],
        [29895],
        [34760],
        [18545],
        [12330],
        [21683],
        [30314],
        [14451],
        [23565],
        [23760],
        [ 2717],
        [ 8249],
        [17815],
        [ 2914]], device='cuda:0')
[2024-07-23 21:05:25,217][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  168],
        [    5],
        [ 3739],
        [ 1064],
        [14262],
        [11826],
        [16451],
        [ 2741],
        [21672],
        [ 7222],
        [  676],
        [18986],
        [ 5520],
        [   19],
        [  621],
        [ 3139],
        [   17]], device='cuda:0')
[2024-07-23 21:05:25,219][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 4822],
        [  713],
        [ 2813],
        [ 8960],
        [11152],
        [ 3563],
        [ 5579],
        [ 4892],
        [ 8585],
        [ 7455],
        [ 5768],
        [ 4135],
        [11210],
        [ 9792],
        [10355],
        [18112],
        [ 6979]], device='cuda:0')
[2024-07-23 21:05:25,221][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[18415],
        [18348],
        [17857],
        [17809],
        [19616],
        [17931],
        [18800],
        [15595],
        [14847],
        [16560],
        [12506],
        [13430],
        [15298],
        [17055],
        [12685],
        [11565],
        [11509]], device='cuda:0')
[2024-07-23 21:05:25,224][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[11062],
        [12303],
        [15208],
        [12592],
        [13737],
        [11575],
        [12726],
        [12000],
        [12226],
        [11682],
        [10935],
        [10694],
        [ 8959],
        [ 6718],
        [ 6589],
        [ 6220],
        [ 5232]], device='cuda:0')
[2024-07-23 21:05:25,225][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 7866],
        [15367],
        [17491],
        [15414],
        [15361],
        [14818],
        [14803],
        [15953],
        [16834],
        [16707],
        [15852],
        [16300],
        [15486],
        [15361],
        [14207],
        [14081],
        [13761]], device='cuda:0')
[2024-07-23 21:05:25,228][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 3820],
        [ 3979],
        [ 8072],
        [13691],
        [14901],
        [13417],
        [11513],
        [17142],
        [11815],
        [20170],
        [26502],
        [12244],
        [18367],
        [11017],
        [ 7735],
        [ 9434],
        [ 7894]], device='cuda:0')
[2024-07-23 21:05:25,230][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[26231],
        [24009],
        [21374],
        [26754],
        [31695],
        [23574],
        [12682],
        [ 3513],
        [ 6974],
        [33843],
        [ 3721],
        [21176],
        [20812],
        [24377],
        [10593],
        [28967],
        [26241]], device='cuda:0')
[2024-07-23 21:05:25,232][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[12936],
        [11681],
        [11857],
        [12284],
        [11600],
        [11965],
        [11722],
        [11765],
        [11349],
        [11680],
        [12086],
        [11430],
        [12838],
        [11292],
        [12008],
        [11316],
        [10340]], device='cuda:0')
[2024-07-23 21:05:25,234][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 4048],
        [ 1947],
        [ 4494],
        [10193],
        [29744],
        [27426],
        [ 4761],
        [13695],
        [ 3988],
        [18796],
        [33120],
        [ 4271],
        [19649],
        [14452],
        [ 9513],
        [16769],
        [15258]], device='cuda:0')
[2024-07-23 21:05:25,236][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[2846],
        [2958],
        [3124],
        [3225],
        [3267],
        [3143],
        [3223],
        [3349],
        [3259],
        [3267],
        [3412],
        [3430],
        [3238],
        [3828],
        [3800],
        [3763],
        [4485]], device='cuda:0')
[2024-07-23 21:05:25,238][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[28217],
        [14788],
        [14844],
        [14748],
        [12174],
        [14192],
        [14307],
        [13890],
        [12999],
        [20696],
        [22621],
        [26582],
        [26359],
        [27164],
        [26618],
        [25820],
        [27368]], device='cuda:0')
[2024-07-23 21:05:25,241][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[16978],
        [36489],
        [30952],
        [32285],
        [34106],
        [33815],
        [33389],
        [33135],
        [32546],
        [31936],
        [32363],
        [31955],
        [32001],
        [31848],
        [32029],
        [32157],
        [31975]], device='cuda:0')
[2024-07-23 21:05:25,243][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[27312],
        [29314],
        [28778],
        [28219],
        [28363],
        [28051],
        [27927],
        [27836],
        [27781],
        [27522],
        [27567],
        [27564],
        [27361],
        [28012],
        [27964],
        [27963],
        [28494]], device='cuda:0')
[2024-07-23 21:05:25,245][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[  345],
        [  562],
        [ 3827],
        [  874],
        [ 9988],
        [38805],
        [ 1351],
        [ 5618],
        [ 1877],
        [32532],
        [ 2603],
        [ 1971],
        [ 1037],
        [  340],
        [  636],
        [ 1429],
        [  389]], device='cuda:0')
[2024-07-23 21:05:25,247][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 3041],
        [ 8118],
        [19237],
        [19390],
        [17125],
        [ 9396],
        [ 9940],
        [ 7825],
        [14760],
        [ 7748],
        [11093],
        [11020],
        [ 2370],
        [ 5738],
        [ 5478],
        [ 3695],
        [ 8012]], device='cuda:0')
[2024-07-23 21:05:25,249][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[22599],
        [18590],
        [14014],
        [13257],
        [12821],
        [12595],
        [12903],
        [13549],
        [12534],
        [13173],
        [12812],
        [12413],
        [12625],
        [12049],
        [11538],
        [11817],
        [11717]], device='cuda:0')
[2024-07-23 21:05:25,252][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[459],
        [426],
        [415],
        [437],
        [470],
        [558],
        [530],
        [584],
        [582],
        [582],
        [595],
        [587],
        [590],
        [566],
        [534],
        [593],
        [566]], device='cuda:0')
[2024-07-23 21:05:25,253][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[6500],
        [3027],
        [2778],
        [2556],
        [2450],
        [2246],
        [2225],
        [1925],
        [1787],
        [1694],
        [1662],
        [1589],
        [1622],
        [1607],
        [1740],
        [1911],
        [1923]], device='cuda:0')
[2024-07-23 21:05:25,256][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 920],
        [ 909],
        [ 557],
        [ 454],
        [ 736],
        [ 873],
        [1007],
        [1112],
        [ 876],
        [ 881],
        [ 808],
        [ 795],
        [ 658],
        [ 775],
        [ 646],
        [ 657],
        [ 640]], device='cuda:0')
[2024-07-23 21:05:25,258][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[2872],
        [ 668],
        [2568],
        [2847],
        [5334],
        [4986],
        [5428],
        [4891],
        [5681],
        [5243],
        [4773],
        [5670],
        [5544],
        [5566],
        [5193],
        [5429],
        [5632]], device='cuda:0')
[2024-07-23 21:05:25,260][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 7210],
        [  300],
        [ 4497],
        [ 6956],
        [ 3033],
        [12382],
        [18102],
        [15590],
        [14407],
        [ 6364],
        [ 8073],
        [13792],
        [ 8469],
        [ 7966],
        [ 6586],
        [ 2765],
        [ 6133]], device='cuda:0')
[2024-07-23 21:05:25,262][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 9286],
        [27365],
        [31603],
        [20031],
        [24839],
        [24564],
        [21909],
        [19265],
        [22429],
        [11633],
        [21373],
        [18577],
        [11058],
        [12099],
        [10756],
        [10736],
        [ 8387]], device='cuda:0')
[2024-07-23 21:05:25,264][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[459],
        [466],
        [484],
        [533],
        [547],
        [560],
        [591],
        [593],
        [623],
        [624],
        [646],
        [649],
        [711],
        [768],
        [787],
        [799],
        [832]], device='cuda:0')
[2024-07-23 21:05:25,266][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[608],
        [611],
        [606],
        [636],
        [694],
        [692],
        [665],
        [729],
        [663],
        [722],
        [732],
        [691],
        [715],
        [685],
        [727],
        [698],
        [691]], device='cuda:0')
[2024-07-23 21:05:25,267][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[653],
        [680],
        [820],
        [755],
        [722],
        [721],
        [682],
        [727],
        [706],
        [690],
        [685],
        [678],
        [667],
        [664],
        [657],
        [652],
        [650]], device='cuda:0')
[2024-07-23 21:05:25,269][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[1214],
        [ 716],
        [2007],
        [1912],
        [ 155],
        [1964],
        [ 581],
        [2112],
        [ 586],
        [1646],
        [2769],
        [ 609],
        [2870],
        [1439],
        [2779],
        [1700],
        [1857]], device='cuda:0')
[2024-07-23 21:05:25,270][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[27472],
        [30058],
        [21189],
        [24498],
        [27479],
        [16183],
        [21856],
        [15664],
        [15643],
        [27054],
        [20019],
        [17131],
        [25181],
        [30940],
        [20194],
        [34342],
        [30807]], device='cuda:0')
[2024-07-23 21:05:25,272][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 7900],
        [36926],
        [15005],
        [32951],
        [14911],
        [10576],
        [23485],
        [22925],
        [26159],
        [ 4362],
        [30710],
        [26036],
        [19060],
        [26699],
        [20992],
        [19178],
        [23728]], device='cuda:0')
[2024-07-23 21:05:25,274][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174],
        [7174]], device='cuda:0')
[2024-07-23 21:05:25,298][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:25,300][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,302][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,303][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,305][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,305][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,306][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,307][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,307][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,308][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,309][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,309][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,310][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,311][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.6179, 0.3821], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,311][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.9476, 0.0524], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,312][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.0172, 0.9828], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,313][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.3168, 0.6832], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,313][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9966, 0.0034], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,314][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.4630, 0.5370], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,315][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.4477, 0.5523], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,317][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.5943, 0.4057], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,318][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9643, 0.0357], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,320][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.6729, 0.3271], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,322][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.1254, 0.8746], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,324][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.8462, 0.1538], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,325][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.4457, 0.2916, 0.2627], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,328][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.8050, 0.0606, 0.1344], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,329][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ language] are: tensor([3.7418e-05, 3.7714e-01, 6.2282e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,331][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.1881, 0.7105, 0.1014], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,333][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.1533, 0.8292, 0.0175], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,334][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.2985, 0.3571, 0.3443], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,336][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.2178, 0.2777, 0.5045], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,338][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.4217, 0.2956, 0.2827], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,340][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9765, 0.0199, 0.0036], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,342][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.5162, 0.2667, 0.2171], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,344][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0996, 0.5414, 0.3590], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,346][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.5743, 0.1497, 0.2760], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,347][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.2906, 0.2346, 0.2270, 0.2478], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,349][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0126, 0.2168, 0.6298, 0.1408], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,351][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ of] are: tensor([1.4445e-05, 1.3316e-02, 5.1601e-01, 4.7066e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,352][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.1793, 0.4815, 0.1701, 0.1690], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,354][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0877, 0.1410, 0.7485, 0.0228], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,355][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.2176, 0.2727, 0.2713, 0.2384], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,355][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2216, 0.2305, 0.3490, 0.1989], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,356][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3166, 0.2308, 0.2220, 0.2305], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,357][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.6124, 0.1685, 0.1204, 0.0986], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,358][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.3136, 0.2289, 0.2226, 0.2349], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,360][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0628, 0.3178, 0.2038, 0.4157], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,361][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.5434, 0.0613, 0.1776, 0.2178], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,363][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.2639, 0.1867, 0.1707, 0.2041, 0.1746], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,365][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.0575, 0.1663, 0.4096, 0.2731, 0.0936], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,366][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ De] are: tensor([6.1089e-07, 1.1863e-03, 1.6926e-02, 7.9167e-01, 1.9022e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,368][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.0923, 0.2919, 0.1160, 0.1919, 0.3079], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,369][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.3722, 0.0772, 0.0069, 0.4860, 0.0576], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,371][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.1760, 0.2299, 0.2293, 0.1935, 0.1712], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,373][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.1254, 0.1898, 0.3193, 0.1106, 0.2549], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,375][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.2713, 0.1931, 0.1840, 0.1931, 0.1586], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,377][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.8691, 0.0414, 0.0202, 0.0486, 0.0206], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,379][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.3173, 0.1797, 0.1546, 0.1959, 0.1525], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,381][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0745, 0.2198, 0.1389, 0.2620, 0.3048], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,383][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.3252, 0.0796, 0.2088, 0.0869, 0.2996], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,385][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.2289, 0.1511, 0.1396, 0.1819, 0.1540, 0.1445], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,386][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0163, 0.1061, 0.2100, 0.1955, 0.0625, 0.4096], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,388][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([1.2879e-08, 1.3497e-04, 4.2734e-04, 3.0224e-02, 9.3793e-01, 3.1284e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,390][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.0437, 0.1876, 0.0502, 0.1295, 0.5189, 0.0701], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,392][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.3864, 0.1094, 0.0117, 0.0665, 0.4232, 0.0027], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,394][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.1392, 0.1958, 0.1938, 0.1638, 0.1492, 0.1582], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,396][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.1115, 0.1737, 0.2316, 0.1205, 0.2669, 0.0958], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,397][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.2368, 0.1660, 0.1583, 0.1664, 0.1352, 0.1373], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,399][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.8948, 0.0089, 0.0096, 0.0511, 0.0174, 0.0183], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,401][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.2877, 0.1518, 0.1211, 0.1819, 0.1449, 0.1126], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,403][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0880, 0.1618, 0.1115, 0.1937, 0.2351, 0.2099], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,405][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.3032, 0.0913, 0.1856, 0.0881, 0.2189, 0.1129], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,407][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.1958, 0.1341, 0.1219, 0.1535, 0.1325, 0.1312, 0.1311],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,409][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.2259, 0.0740, 0.0908, 0.2464, 0.0658, 0.1798, 0.1174],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,410][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([6.8177e-08, 4.7377e-05, 4.3794e-04, 2.8612e-02, 2.4937e-01, 3.8181e-01,
        3.3973e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,412][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.0552, 0.1933, 0.0578, 0.1234, 0.3336, 0.0767, 0.1600],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,414][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([7.5774e-01, 1.7251e-02, 6.2932e-03, 1.3764e-01, 6.5661e-02, 1.5097e-02,
        3.1745e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,415][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.1279, 0.1643, 0.1604, 0.1385, 0.1215, 0.1277, 0.1597],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,418][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0754, 0.1166, 0.2085, 0.0836, 0.2034, 0.0844, 0.2280],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,419][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.2008, 0.1465, 0.1398, 0.1453, 0.1203, 0.1222, 0.1251],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,421][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.8009, 0.0166, 0.0059, 0.0313, 0.0215, 0.0617, 0.0621],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,422][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.2612, 0.1341, 0.1107, 0.1599, 0.1248, 0.1080, 0.1013],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,422][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0258, 0.0906, 0.0694, 0.1662, 0.2359, 0.1759, 0.2362],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,423][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2522, 0.0866, 0.2022, 0.0489, 0.1991, 0.0865, 0.1245],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,424][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.1966, 0.1122, 0.1025, 0.1467, 0.1206, 0.1071, 0.1143, 0.1000],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,425][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0112, 0.0583, 0.1049, 0.1104, 0.0382, 0.1511, 0.1363, 0.3896],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,426][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([8.0315e-09, 3.1574e-06, 5.3279e-05, 1.9582e-03, 6.8355e-02, 1.4639e-02,
        8.2743e-01, 8.7560e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,428][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.0403, 0.1412, 0.0453, 0.1095, 0.2808, 0.0730, 0.2766, 0.0333],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,430][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.4550, 0.0008, 0.0047, 0.2025, 0.2562, 0.0681, 0.0101, 0.0025],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,431][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.1058, 0.1486, 0.1455, 0.1224, 0.1094, 0.1171, 0.1550, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,433][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0689, 0.1163, 0.1619, 0.0818, 0.1713, 0.0814, 0.2561, 0.0623],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,435][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.1838, 0.1310, 0.1245, 0.1302, 0.1069, 0.1087, 0.1112, 0.1038],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,437][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.9537, 0.0028, 0.0014, 0.0223, 0.0042, 0.0056, 0.0088, 0.0013],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,439][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.2838, 0.1136, 0.0885, 0.1544, 0.1118, 0.0852, 0.0840, 0.0787],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,441][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0260, 0.0779, 0.0539, 0.1473, 0.1710, 0.1589, 0.1975, 0.1674],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,443][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.2436, 0.0867, 0.1617, 0.0839, 0.1307, 0.0814, 0.1297, 0.0824],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,445][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.1602, 0.1058, 0.0971, 0.1261, 0.1078, 0.0991, 0.1029, 0.0939, 0.1070],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,447][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0687, 0.0469, 0.1371, 0.1064, 0.0328, 0.0972, 0.0960, 0.3366, 0.0783],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,448][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([2.9123e-10, 1.3909e-07, 3.8168e-06, 3.9728e-04, 4.1723e-03, 9.4121e-03,
        7.5784e-02, 3.8718e-01, 5.2305e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,450][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0370, 0.1526, 0.0375, 0.0943, 0.2832, 0.0652, 0.1586, 0.0670, 0.1047],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,452][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.3356, 0.0271, 0.1125, 0.0630, 0.0944, 0.0034, 0.0389, 0.3225, 0.0026],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,454][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.1006, 0.1307, 0.1278, 0.1102, 0.0973, 0.1015, 0.1284, 0.0897, 0.1139],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,456][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0470, 0.0753, 0.1246, 0.0595, 0.1420, 0.0624, 0.1714, 0.0494, 0.2684],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,458][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1544, 0.1190, 0.1130, 0.1162, 0.0984, 0.0998, 0.1021, 0.0963, 0.1009],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,460][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8326, 0.0129, 0.0070, 0.0449, 0.0122, 0.0358, 0.0307, 0.0182, 0.0057],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,462][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2444, 0.1087, 0.0879, 0.1436, 0.1071, 0.0833, 0.0803, 0.0775, 0.0672],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,464][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0201, 0.0552, 0.0421, 0.1170, 0.1289, 0.1360, 0.1557, 0.1786, 0.1663],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,466][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2439, 0.0777, 0.1667, 0.0576, 0.1506, 0.0768, 0.0832, 0.0991, 0.0443],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,468][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.1629, 0.0950, 0.0840, 0.1202, 0.0989, 0.0908, 0.0959, 0.0853, 0.0988,
        0.0683], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,470][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0132, 0.0553, 0.0740, 0.0896, 0.0344, 0.1378, 0.0961, 0.3058, 0.0362,
        0.1576], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,471][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ et] are: tensor([7.8496e-09, 2.1405e-07, 6.8829e-06, 6.7189e-05, 1.3501e-04, 1.8628e-03,
        4.4345e-02, 1.5308e-01, 7.8235e-01, 1.8150e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,473][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.0244, 0.1211, 0.0460, 0.0552, 0.1900, 0.0560, 0.1988, 0.0688, 0.1868,
        0.0528], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,475][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.1275, 0.0205, 0.0027, 0.1677, 0.0151, 0.0808, 0.4657, 0.0893, 0.0118,
        0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,477][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0823, 0.1165, 0.1169, 0.0988, 0.0898, 0.0965, 0.1249, 0.0816, 0.1081,
        0.0847], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,479][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0471, 0.0687, 0.1074, 0.0479, 0.1168, 0.0453, 0.1447, 0.0398, 0.2362,
        0.1462], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,481][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1474, 0.1089, 0.1030, 0.1069, 0.0897, 0.0913, 0.0930, 0.0884, 0.0906,
        0.0808], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,483][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.8225, 0.0202, 0.0098, 0.0339, 0.0146, 0.0332, 0.0342, 0.0149, 0.0134,
        0.0032], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,485][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.2677, 0.1067, 0.0779, 0.1467, 0.1029, 0.0739, 0.0707, 0.0691, 0.0578,
        0.0266], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,487][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0087, 0.0521, 0.0357, 0.0923, 0.1250, 0.1185, 0.1381, 0.1349, 0.1655,
        0.1293], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,489][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.1685, 0.0579, 0.1035, 0.0647, 0.1692, 0.0936, 0.0935, 0.1095, 0.0673,
        0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,490][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.1421, 0.0910, 0.0806, 0.1088, 0.0899, 0.0846, 0.0890, 0.0804, 0.0926,
        0.0670, 0.0739], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,490][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0168, 0.0544, 0.0758, 0.1176, 0.0327, 0.1169, 0.0840, 0.2808, 0.0397,
        0.1145, 0.0669], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,491][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([2.2705e-10, 2.6841e-08, 8.8846e-07, 1.2858e-05, 7.1413e-05, 1.1918e-04,
        4.0142e-03, 8.6297e-03, 1.5217e-01, 1.3517e-01, 6.9980e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,492][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.0273, 0.1656, 0.0334, 0.0822, 0.1456, 0.0474, 0.1990, 0.0592, 0.1349,
        0.0925, 0.0129], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,493][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.2122, 0.1419, 0.0207, 0.0947, 0.0113, 0.0720, 0.0576, 0.2218, 0.1265,
        0.0301, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,495][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0802, 0.1120, 0.1098, 0.0911, 0.0809, 0.0872, 0.1194, 0.0719, 0.1009,
        0.0776, 0.0690], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,497][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0536, 0.0743, 0.0996, 0.0481, 0.1182, 0.0547, 0.1458, 0.0423, 0.2109,
        0.1248, 0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,499][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.1358, 0.1003, 0.0951, 0.0989, 0.0830, 0.0843, 0.0860, 0.0815, 0.0840,
        0.0748, 0.0764], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,500][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.8221, 0.0118, 0.0120, 0.0533, 0.0099, 0.0253, 0.0340, 0.0129, 0.0101,
        0.0061, 0.0024], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,502][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2209, 0.1048, 0.0819, 0.1365, 0.1029, 0.0756, 0.0737, 0.0716, 0.0612,
        0.0326, 0.0383], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,504][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0046, 0.0501, 0.0317, 0.0807, 0.1034, 0.0852, 0.1353, 0.1103, 0.1523,
        0.1676, 0.0789], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,506][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.1711, 0.0854, 0.1189, 0.0632, 0.1263, 0.0780, 0.0769, 0.0717, 0.0653,
        0.0769, 0.0663], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,508][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.1272, 0.0835, 0.0758, 0.0989, 0.0839, 0.0773, 0.0807, 0.0735, 0.0843,
        0.0621, 0.0691, 0.0835], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,510][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0320, 0.0430, 0.1064, 0.0773, 0.0275, 0.0872, 0.0770, 0.2507, 0.0641,
        0.0824, 0.0621, 0.0900], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,511][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [orum] are: tensor([4.1960e-12, 2.8231e-10, 1.3597e-08, 1.1776e-06, 5.4198e-06, 1.0877e-05,
        1.9088e-04, 5.1567e-04, 1.5814e-03, 1.3634e-02, 6.9959e-01, 2.8448e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,514][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0303, 0.1264, 0.0304, 0.0726, 0.2248, 0.0525, 0.1174, 0.0510, 0.0783,
        0.1172, 0.0411, 0.0579], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,515][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.1617, 0.0138, 0.0530, 0.0302, 0.0505, 0.0017, 0.0201, 0.1699, 0.0013,
        0.4132, 0.0833, 0.0013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,517][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0798, 0.1049, 0.1014, 0.0863, 0.0752, 0.0775, 0.1029, 0.0667, 0.0889,
        0.0702, 0.0647, 0.0817], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,519][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0337, 0.0535, 0.0869, 0.0421, 0.1018, 0.0453, 0.1188, 0.0338, 0.1844,
        0.1051, 0.0265, 0.1682], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,521][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1208, 0.0929, 0.0881, 0.0906, 0.0770, 0.0781, 0.0798, 0.0756, 0.0784,
        0.0700, 0.0717, 0.0770], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,523][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8682, 0.0101, 0.0055, 0.0415, 0.0075, 0.0229, 0.0213, 0.0107, 0.0038,
        0.0040, 0.0018, 0.0028], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,525][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2202, 0.0959, 0.0759, 0.1285, 0.0950, 0.0723, 0.0693, 0.0672, 0.0573,
        0.0308, 0.0387, 0.0490], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,527][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0058, 0.0357, 0.0262, 0.0786, 0.0988, 0.0867, 0.1200, 0.1395, 0.1578,
        0.1341, 0.0652, 0.0515], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,529][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1895, 0.0677, 0.1310, 0.0492, 0.1120, 0.0604, 0.0654, 0.0821, 0.0371,
        0.0885, 0.0759, 0.0412], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,531][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.1223, 0.0753, 0.0680, 0.0915, 0.0767, 0.0725, 0.0768, 0.0688, 0.0788,
        0.0573, 0.0626, 0.0778, 0.0716], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,533][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0363, 0.0664, 0.0816, 0.1411, 0.0432, 0.0895, 0.0853, 0.1735, 0.0404,
        0.0982, 0.0478, 0.0610, 0.0358], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,534][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ is] are: tensor([5.2332e-10, 1.0124e-09, 7.3063e-08, 9.5059e-07, 2.0677e-06, 1.2855e-06,
        1.1703e-04, 2.5208e-04, 3.9530e-03, 2.7556e-03, 1.0258e-01, 5.7223e-01,
        3.1811e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,536][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0283, 0.0898, 0.0402, 0.0555, 0.1539, 0.0536, 0.1344, 0.0728, 0.1396,
        0.0565, 0.0496, 0.1193, 0.0066], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,538][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0988, 0.0293, 0.0167, 0.0168, 0.0019, 0.0575, 0.0015, 0.0162, 0.2804,
        0.0746, 0.1296, 0.2688, 0.0079], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,540][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0601, 0.0855, 0.0873, 0.0756, 0.0738, 0.0828, 0.1022, 0.0678, 0.0868,
        0.0733, 0.0659, 0.0806, 0.0583], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,542][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0577, 0.0463, 0.0691, 0.0465, 0.0798, 0.0368, 0.1140, 0.0323, 0.1657,
        0.1270, 0.0266, 0.1595, 0.0386], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,544][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.1171, 0.0865, 0.0821, 0.0849, 0.0705, 0.0719, 0.0734, 0.0693, 0.0723,
        0.0640, 0.0652, 0.0708, 0.0718], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,546][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.7731, 0.0153, 0.0077, 0.0383, 0.0124, 0.0150, 0.0215, 0.0075, 0.0089,
        0.0032, 0.0034, 0.0081, 0.0856], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,548][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.2104, 0.0890, 0.0709, 0.1199, 0.0884, 0.0698, 0.0668, 0.0649, 0.0584,
        0.0302, 0.0361, 0.0497, 0.0454], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,550][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0031, 0.0405, 0.0228, 0.0813, 0.0809, 0.0728, 0.1089, 0.0931, 0.1207,
        0.1096, 0.0531, 0.0417, 0.1714], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,552][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2302, 0.0301, 0.0556, 0.0463, 0.1440, 0.0420, 0.0657, 0.0774, 0.0420,
        0.1071, 0.0846, 0.0495, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,554][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.1105, 0.0694, 0.0637, 0.0847, 0.0724, 0.0679, 0.0716, 0.0646, 0.0732,
        0.0554, 0.0594, 0.0726, 0.0689, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,556][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0094, 0.0597, 0.0984, 0.0610, 0.0339, 0.0832, 0.1039, 0.1621, 0.0708,
        0.0938, 0.0616, 0.0999, 0.0254, 0.0368], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,557][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([4.4711e-11, 1.7031e-10, 4.1833e-09, 4.0199e-07, 3.8352e-07, 4.9397e-07,
        2.2734e-05, 6.0401e-05, 4.5168e-04, 3.0951e-04, 2.0307e-02, 5.0929e-02,
        8.1920e-01, 1.0872e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,558][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0183, 0.0902, 0.0404, 0.0454, 0.1512, 0.0504, 0.1547, 0.0625, 0.1149,
        0.0625, 0.0626, 0.0995, 0.0101, 0.0373], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,558][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0197, 0.0225, 0.0010, 0.0159, 0.0049, 0.3920, 0.0052, 0.0220, 0.2466,
        0.0079, 0.0047, 0.2326, 0.0213, 0.0036], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,559][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0569, 0.0818, 0.0842, 0.0723, 0.0703, 0.0793, 0.0970, 0.0639, 0.0818,
        0.0687, 0.0633, 0.0758, 0.0551, 0.0497], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,560][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0439, 0.0568, 0.0641, 0.0489, 0.0917, 0.0449, 0.1099, 0.0310, 0.1517,
        0.1067, 0.0286, 0.1502, 0.0439, 0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,563][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1060, 0.0804, 0.0767, 0.0788, 0.0665, 0.0676, 0.0688, 0.0653, 0.0678,
        0.0606, 0.0615, 0.0665, 0.0674, 0.0661], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,565][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.4971, 0.0155, 0.0147, 0.0373, 0.0225, 0.0400, 0.0366, 0.0224, 0.0266,
        0.0117, 0.0086, 0.0244, 0.1174, 0.1254], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,566][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1946, 0.0842, 0.0687, 0.1114, 0.0820, 0.0674, 0.0635, 0.0630, 0.0576,
        0.0306, 0.0353, 0.0494, 0.0458, 0.0467], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,568][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0013, 0.0359, 0.0166, 0.0553, 0.0605, 0.0564, 0.0923, 0.0893, 0.1046,
        0.1051, 0.0420, 0.0352, 0.1507, 0.1548], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,570][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1884, 0.0331, 0.0660, 0.1015, 0.1418, 0.0420, 0.0588, 0.0619, 0.0320,
        0.1029, 0.0741, 0.0367, 0.0331, 0.0278], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,572][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.1114, 0.0642, 0.0585, 0.0835, 0.0691, 0.0621, 0.0678, 0.0589, 0.0688,
        0.0497, 0.0540, 0.0679, 0.0651, 0.0622, 0.0567], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,574][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0103, 0.0616, 0.0950, 0.0878, 0.0274, 0.0762, 0.0989, 0.1925, 0.0476,
        0.0927, 0.0491, 0.0694, 0.0224, 0.0465, 0.0226], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,576][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ same] are: tensor([6.0480e-11, 3.5006e-11, 1.2159e-09, 1.3640e-07, 3.8111e-08, 2.6910e-07,
        1.9591e-06, 1.4098e-05, 1.5966e-04, 2.8734e-04, 1.8321e-03, 2.3474e-02,
        4.3771e-01, 1.7852e-01, 3.5801e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,577][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0188, 0.0620, 0.0257, 0.0444, 0.1782, 0.0384, 0.1054, 0.0417, 0.1219,
        0.0649, 0.0465, 0.1128, 0.0097, 0.0635, 0.0660], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,579][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ same] are: tensor([4.1083e-01, 5.5541e-03, 1.3141e-03, 6.4860e-02, 2.6787e-03, 8.3649e-03,
        6.4717e-04, 2.3691e-03, 2.7635e-02, 3.3709e-02, 2.2960e-04, 2.7081e-02,
        6.4850e-02, 3.4129e-01, 8.5882e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,581][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.0457, 0.0770, 0.0798, 0.0667, 0.0674, 0.0769, 0.0990, 0.0602, 0.0803,
        0.0658, 0.0590, 0.0731, 0.0503, 0.0447, 0.0542], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,583][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0285, 0.0423, 0.0713, 0.0321, 0.0859, 0.0254, 0.1169, 0.0333, 0.1642,
        0.1000, 0.0241, 0.1640, 0.0366, 0.0189, 0.0565], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,585][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.1031, 0.0764, 0.0723, 0.0749, 0.0629, 0.0639, 0.0645, 0.0620, 0.0624,
        0.0563, 0.0574, 0.0614, 0.0642, 0.0621, 0.0563], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,586][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ same] are: tensor([8.2997e-01, 3.1203e-03, 2.1114e-03, 2.0477e-02, 3.4957e-03, 7.4815e-03,
        7.2012e-03, 3.5804e-03, 2.2926e-03, 8.4534e-04, 4.5890e-04, 1.6204e-03,
        4.3643e-02, 7.2850e-02, 8.4905e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,588][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.2246, 0.0814, 0.0616, 0.1190, 0.0826, 0.0603, 0.0587, 0.0557, 0.0491,
        0.0230, 0.0289, 0.0410, 0.0384, 0.0397, 0.0360], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,590][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0016, 0.0419, 0.0201, 0.0638, 0.0606, 0.0410, 0.0812, 0.0649, 0.1024,
        0.0848, 0.0444, 0.0366, 0.1680, 0.1780, 0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,592][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.1231, 0.0550, 0.1303, 0.0367, 0.1384, 0.0555, 0.0858, 0.0739, 0.0387,
        0.0751, 0.0807, 0.0452, 0.0152, 0.0197, 0.0267], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,594][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0948, 0.0624, 0.0562, 0.0727, 0.0625, 0.0610, 0.0628, 0.0580, 0.0659,
        0.0496, 0.0534, 0.0657, 0.0618, 0.0584, 0.0562, 0.0586],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,596][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0016, 0.0660, 0.1128, 0.0378, 0.0240, 0.0932, 0.1184, 0.1721, 0.0641,
        0.0901, 0.0629, 0.0978, 0.0132, 0.0171, 0.0165, 0.0123],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,597][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ as] are: tensor([3.3985e-12, 2.8313e-13, 3.7891e-12, 4.0108e-10, 2.3165e-10, 1.6522e-10,
        6.5865e-09, 2.6179e-08, 2.5531e-07, 3.6657e-07, 1.1104e-05, 4.4602e-05,
        2.1828e-03, 6.5510e-03, 3.2076e-02, 9.5913e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,599][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0236, 0.0747, 0.0210, 0.0448, 0.1713, 0.0291, 0.1170, 0.0400, 0.1017,
        0.0631, 0.0349, 0.0829, 0.0147, 0.0502, 0.1126, 0.0184],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,600][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ as] are: tensor([7.5411e-02, 1.4879e-04, 1.4973e-03, 6.6096e-02, 1.4794e-02, 6.6844e-03,
        2.1166e-03, 3.7349e-03, 1.0771e-01, 3.7627e-03, 3.1708e-03, 9.8738e-02,
        3.2197e-02, 6.2133e-02, 5.1954e-01, 2.2712e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,602][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.0489, 0.0752, 0.0763, 0.0648, 0.0635, 0.0710, 0.0904, 0.0573, 0.0758,
        0.0621, 0.0563, 0.0696, 0.0491, 0.0446, 0.0493, 0.0457],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,604][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0494, 0.0390, 0.0637, 0.0322, 0.0833, 0.0368, 0.1080, 0.0307, 0.1432,
        0.1076, 0.0228, 0.1375, 0.0423, 0.0246, 0.0584, 0.0205],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,606][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0942, 0.0718, 0.0682, 0.0702, 0.0593, 0.0605, 0.0612, 0.0586, 0.0600,
        0.0541, 0.0551, 0.0591, 0.0606, 0.0591, 0.0535, 0.0543],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,609][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.3024, 0.0150, 0.0165, 0.0318, 0.0166, 0.0379, 0.0506, 0.0292, 0.0252,
        0.0141, 0.0098, 0.0267, 0.1395, 0.1490, 0.0216, 0.1141],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,611][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.1748, 0.0798, 0.0631, 0.1050, 0.0780, 0.0620, 0.0583, 0.0592, 0.0518,
        0.0272, 0.0319, 0.0442, 0.0422, 0.0435, 0.0422, 0.0369],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,612][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0007, 0.0251, 0.0123, 0.0424, 0.0414, 0.0343, 0.0545, 0.0426, 0.0719,
        0.0633, 0.0272, 0.0243, 0.1129, 0.1074, 0.0068, 0.3329],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,615][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.1901, 0.0515, 0.0841, 0.0530, 0.1436, 0.0407, 0.0825, 0.0646, 0.0310,
        0.0796, 0.0531, 0.0359, 0.0171, 0.0198, 0.0286, 0.0248],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,617][circuit_model.py][line:1532][INFO] ##2-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0960, 0.0574, 0.0526, 0.0726, 0.0614, 0.0563, 0.0601, 0.0534, 0.0615,
        0.0454, 0.0489, 0.0610, 0.0584, 0.0556, 0.0522, 0.0549, 0.0523],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,619][circuit_model.py][line:1535][INFO] ##2-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0055, 0.0660, 0.1013, 0.0462, 0.0295, 0.0788, 0.1155, 0.1344, 0.0771,
        0.0781, 0.0550, 0.1018, 0.0192, 0.0268, 0.0267, 0.0195, 0.0186],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,620][circuit_model.py][line:1538][INFO] ##2-th layer ##Weight##: The head3 weight for token [ the] are: tensor([7.2613e-14, 1.7738e-14, 2.6833e-13, 3.5848e-11, 3.0128e-11, 3.3455e-11,
        1.3547e-09, 4.4218e-09, 3.6559e-08, 3.0135e-08, 1.7558e-06, 4.6116e-06,
        9.9586e-05, 1.0436e-05, 1.3004e-03, 9.3850e-01, 6.0084e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,622][circuit_model.py][line:1541][INFO] ##2-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0144, 0.0714, 0.0336, 0.0346, 0.1206, 0.0411, 0.1244, 0.0501, 0.0910,
        0.0520, 0.0526, 0.0795, 0.0086, 0.0298, 0.1195, 0.0554, 0.0213],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,624][circuit_model.py][line:1544][INFO] ##2-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0181, 0.0196, 0.0009, 0.0151, 0.0040, 0.3331, 0.0045, 0.0187, 0.2392,
        0.0070, 0.0041, 0.2255, 0.0203, 0.0033, 0.0009, 0.0824, 0.0031],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,625][circuit_model.py][line:1547][INFO] ##2-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0447, 0.0695, 0.0728, 0.0617, 0.0616, 0.0712, 0.0871, 0.0563, 0.0718,
        0.0606, 0.0564, 0.0664, 0.0469, 0.0412, 0.0491, 0.0446, 0.0381],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,626][circuit_model.py][line:1550][INFO] ##2-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0397, 0.0502, 0.0559, 0.0437, 0.0832, 0.0395, 0.0990, 0.0280, 0.1326,
        0.0967, 0.0262, 0.1309, 0.0418, 0.0248, 0.0571, 0.0304, 0.0202],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,627][circuit_model.py][line:1553][INFO] ##2-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0896, 0.0678, 0.0647, 0.0665, 0.0563, 0.0573, 0.0581, 0.0554, 0.0570,
        0.0512, 0.0519, 0.0560, 0.0571, 0.0558, 0.0504, 0.0515, 0.0534],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,628][circuit_model.py][line:1556][INFO] ##2-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3699, 0.0090, 0.0099, 0.0318, 0.0151, 0.0239, 0.0237, 0.0122, 0.0188,
        0.0072, 0.0059, 0.0166, 0.0766, 0.0916, 0.0105, 0.1110, 0.1663],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,630][circuit_model.py][line:1559][INFO] ##2-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1981, 0.0769, 0.0593, 0.1068, 0.0759, 0.0580, 0.0549, 0.0541, 0.0479,
        0.0231, 0.0279, 0.0402, 0.0376, 0.0387, 0.0367, 0.0320, 0.0321],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,632][circuit_model.py][line:1562][INFO] ##2-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0005, 0.0188, 0.0082, 0.0280, 0.0287, 0.0284, 0.0410, 0.0424, 0.0506,
        0.0506, 0.0198, 0.0161, 0.0816, 0.0822, 0.0049, 0.2761, 0.2220],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,634][circuit_model.py][line:1565][INFO] ##2-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1682, 0.0298, 0.0529, 0.1022, 0.1246, 0.0365, 0.0489, 0.0511, 0.0276,
        0.0870, 0.0592, 0.0317, 0.0306, 0.0239, 0.0335, 0.0615, 0.0308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,671][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:25,672][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,674][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,674][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,675][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,676][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,676][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,677][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,678][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,678][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,679][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,680][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,680][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:25,681][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.9019, 0.0981], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,682][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.0024, 0.9976], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,682][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.0011, 0.9989], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,683][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.6238, 0.3762], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,685][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.9170, 0.0830], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,687][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.7472, 0.2528], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,689][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.4347, 0.5653], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,691][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.5147, 0.4853], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,692][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([1.2368e-04, 9.9988e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,694][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.1641, 0.8359], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,696][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.1648, 0.8352], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,698][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.0843, 0.9157], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:25,699][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.5192, 0.4538, 0.0271], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,701][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([3.2163e-04, 4.4548e-02, 9.5513e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,702][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([6.7564e-06, 4.3489e-01, 5.6511e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,704][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.3765, 0.2296, 0.3939], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,706][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.4281, 0.4472, 0.1248], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,708][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0966, 0.8775, 0.0259], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,710][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.2119, 0.2979, 0.4902], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,711][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.3764, 0.3285, 0.2951], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,712][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([3.4183e-05, 2.3344e-01, 7.6652e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,712][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0063, 0.9508, 0.0429], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,713][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0797, 0.4035, 0.5168], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,714][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0604, 0.4440, 0.4956], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:25,714][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.1366, 0.2004, 0.5958, 0.0672], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,717][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0027, 0.0325, 0.9231, 0.0417], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,718][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([3.4834e-06, 2.2540e-02, 6.3297e-01, 3.4448e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,720][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.2367, 0.1773, 0.3287, 0.2573], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,721][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.3119, 0.1914, 0.3117, 0.1849], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,723][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0183, 0.2947, 0.6826, 0.0044], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,725][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.1212, 0.1704, 0.2766, 0.4318], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,726][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.2693, 0.2600, 0.2252, 0.2454], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,728][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([5.3913e-05, 1.0376e-01, 3.4303e-01, 5.5315e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,729][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([1.2666e-03, 8.0225e-02, 9.1766e-01, 8.4843e-04], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,731][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0740, 0.3172, 0.4067, 0.2021], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,732][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0422, 0.2565, 0.2800, 0.4213], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:25,734][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.1904, 0.0681, 0.1464, 0.2586, 0.3365], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,736][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0017, 0.2616, 0.6005, 0.0281, 0.1081], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,737][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([3.6106e-07, 2.7588e-03, 2.9706e-02, 6.9361e-01, 2.7393e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,739][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.1553, 0.1231, 0.2489, 0.2203, 0.2524], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,741][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.3219, 0.1358, 0.0465, 0.3843, 0.1115], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,743][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.0066, 0.2214, 0.6566, 0.1029, 0.0125], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,745][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.0616, 0.1126, 0.2152, 0.3503, 0.2603], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,747][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.2212, 0.2109, 0.1866, 0.1968, 0.1845], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,748][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([2.1521e-05, 3.3751e-02, 1.5643e-01, 3.6207e-01, 4.4772e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,750][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.0047, 0.1938, 0.7639, 0.0175, 0.0202], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,752][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.0476, 0.2560, 0.3240, 0.1362, 0.2362], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,754][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0226, 0.1722, 0.2024, 0.2060, 0.3968], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:25,755][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.1009, 0.1276, 0.2132, 0.1184, 0.4117, 0.0282], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,758][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.0012, 0.3730, 0.3858, 0.0207, 0.0352, 0.1840], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,759][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([1.0456e-08, 2.6284e-04, 7.3391e-04, 2.9337e-02, 7.4749e-01, 2.2218e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,761][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.0957, 0.0676, 0.1389, 0.1501, 0.1799, 0.3677], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,763][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.3189, 0.1410, 0.0579, 0.1611, 0.2756, 0.0455], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,765][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.0138, 0.0815, 0.7786, 0.0805, 0.0426, 0.0030], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,766][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.0466, 0.0829, 0.1535, 0.2379, 0.1970, 0.2821], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,768][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.1922, 0.1838, 0.1600, 0.1691, 0.1534, 0.1414], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,770][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([1.7606e-05, 1.1790e-02, 4.5722e-02, 1.3244e-01, 2.1410e-01, 5.9593e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,772][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.0067, 0.2767, 0.5199, 0.0203, 0.1722, 0.0042], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,774][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.0339, 0.1968, 0.2657, 0.1080, 0.1862, 0.2095], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,776][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.0143, 0.1267, 0.1186, 0.1290, 0.2033, 0.4081], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:25,777][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.1266, 0.0945, 0.0806, 0.1500, 0.2496, 0.0965, 0.2022],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,778][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([3.6731e-04, 1.1759e-01, 6.3631e-02, 6.2032e-03, 1.1664e-01, 1.0778e-01,
        5.8779e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,779][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([7.6472e-09, 2.7943e-05, 2.1400e-04, 7.1918e-03, 6.6727e-02, 6.0354e-01,
        3.2230e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,780][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.0620, 0.0496, 0.1067, 0.0978, 0.1272, 0.3114, 0.2454],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,781][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.4007, 0.0635, 0.0530, 0.2578, 0.1128, 0.0872, 0.0250],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,781][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.0186, 0.2746, 0.3041, 0.1575, 0.1143, 0.1129, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,782][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.0249, 0.0561, 0.1059, 0.1652, 0.1761, 0.2789, 0.1930],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,784][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.1598, 0.1461, 0.1349, 0.1519, 0.1509, 0.1344, 0.1221],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,786][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([5.5362e-06, 1.9291e-03, 8.3483e-03, 2.9302e-02, 3.7042e-02, 1.2326e-01,
        8.0011e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,787][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.0130, 0.4565, 0.3356, 0.0338, 0.0866, 0.0620, 0.0125],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,789][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.0206, 0.1533, 0.1890, 0.0730, 0.1280, 0.1365, 0.2997],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,791][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.0150, 0.0751, 0.0797, 0.1084, 0.1418, 0.2565, 0.3234],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:25,793][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.1145, 0.0397, 0.0585, 0.1103, 0.3383, 0.1227, 0.1979, 0.0182],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,794][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0062, 0.0486, 0.0537, 0.0100, 0.1262, 0.1025, 0.5491, 0.1036],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,796][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([1.3571e-09, 1.7185e-06, 2.3675e-05, 4.9060e-04, 1.2276e-02, 2.0023e-02,
        7.6733e-01, 1.9985e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,798][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.0485, 0.0393, 0.0792, 0.0838, 0.1066, 0.2192, 0.1940, 0.2294],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,800][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.2699, 0.0169, 0.0303, 0.2364, 0.1817, 0.1424, 0.0663, 0.0561],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,801][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([6.5356e-03, 9.2208e-02, 6.7889e-01, 7.0416e-02, 3.1777e-02, 8.7302e-03,
        1.1098e-01, 4.6558e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,803][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.0226, 0.0504, 0.0826, 0.1265, 0.1398, 0.2112, 0.1423, 0.2246],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,805][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.1440, 0.1363, 0.1200, 0.1382, 0.1263, 0.1154, 0.1091, 0.1107],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,806][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([2.5782e-06, 5.9654e-04, 2.0747e-03, 1.0163e-02, 1.4055e-02, 4.8419e-02,
        3.3661e-01, 5.8808e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,809][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.0097, 0.1089, 0.4337, 0.0149, 0.0822, 0.0595, 0.2874, 0.0037],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,810][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.0204, 0.1264, 0.1530, 0.0722, 0.1146, 0.1215, 0.2499, 0.1420],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,812][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.0090, 0.0615, 0.0584, 0.0722, 0.0983, 0.2210, 0.2519, 0.2277],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:25,814][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.1494, 0.0753, 0.0798, 0.0982, 0.2375, 0.0269, 0.2847, 0.0198, 0.0285],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,816][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0006, 0.0486, 0.2157, 0.0074, 0.1129, 0.0943, 0.2520, 0.2128, 0.0558],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,817][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([3.9828e-11, 7.2371e-08, 1.5736e-06, 8.0383e-05, 8.2552e-04, 1.1199e-02,
        4.9009e-02, 7.0243e-01, 2.3646e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,819][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0317, 0.0261, 0.0562, 0.0577, 0.0747, 0.1716, 0.1363, 0.1763, 0.2693],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,821][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.1725, 0.0422, 0.0804, 0.1269, 0.0701, 0.0253, 0.0830, 0.3367, 0.0631],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,823][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0299, 0.2415, 0.2438, 0.1321, 0.0870, 0.0868, 0.1187, 0.0192, 0.0410],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,826][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0171, 0.0378, 0.0636, 0.1113, 0.1109, 0.1736, 0.1222, 0.1951, 0.1684],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,827][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.1296, 0.1174, 0.1062, 0.1179, 0.1152, 0.1031, 0.0932, 0.0994, 0.1178],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,829][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([5.5040e-07, 1.0497e-04, 4.3352e-04, 2.4666e-03, 3.4269e-03, 1.1999e-02,
        7.4487e-02, 2.0956e-01, 6.9752e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,831][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0130, 0.2950, 0.3366, 0.0149, 0.1185, 0.0423, 0.0873, 0.0867, 0.0058],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,833][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0154, 0.1059, 0.1360, 0.0537, 0.0902, 0.1011, 0.1934, 0.1156, 0.1886],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,835][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0106, 0.0436, 0.0460, 0.0615, 0.0717, 0.1370, 0.1597, 0.1807, 0.2892],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:25,837][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0471, 0.0188, 0.0250, 0.0731, 0.1344, 0.0883, 0.2350, 0.0592, 0.2872,
        0.0318], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,839][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0013, 0.0422, 0.3548, 0.0033, 0.0281, 0.0295, 0.1992, 0.0436, 0.0906,
        0.2075], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,840][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([1.8590e-09, 1.4539e-07, 3.6267e-06, 1.7241e-05, 4.8757e-05, 2.7505e-03,
        4.2236e-02, 3.6127e-01, 5.6486e-01, 2.8813e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,842][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.0261, 0.0221, 0.0440, 0.0476, 0.0604, 0.1331, 0.1049, 0.1355, 0.2036,
        0.2227], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,844][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0964, 0.0306, 0.0138, 0.1458, 0.0315, 0.0971, 0.2247, 0.1993, 0.0860,
        0.0749], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,846][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.0078, 0.0489, 0.3081, 0.0674, 0.0608, 0.0517, 0.1961, 0.0198, 0.2139,
        0.0255], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,847][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0118, 0.0357, 0.0563, 0.0900, 0.0884, 0.1418, 0.0976, 0.1743, 0.1547,
        0.1494], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,847][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.1173, 0.1130, 0.0980, 0.1072, 0.1011, 0.0928, 0.0803, 0.0884, 0.1066,
        0.0953], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,848][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([7.1126e-07, 6.6607e-05, 2.2370e-04, 1.0134e-03, 1.1710e-03, 4.2121e-03,
        2.8692e-02, 5.8886e-02, 2.6627e-01, 6.3947e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,849][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.0019, 0.1692, 0.1959, 0.0070, 0.0579, 0.0513, 0.1150, 0.0602, 0.3320,
        0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,850][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.0156, 0.0874, 0.1127, 0.0477, 0.0865, 0.0899, 0.1878, 0.1072, 0.1693,
        0.0959], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,852][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0106, 0.0336, 0.0354, 0.0494, 0.0787, 0.1339, 0.1237, 0.1623, 0.2306,
        0.1418], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:25,854][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0500, 0.0580, 0.0187, 0.0466, 0.0534, 0.0801, 0.2229, 0.0623, 0.2518,
        0.1424, 0.0139], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,855][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0052, 0.0118, 0.2489, 0.0336, 0.0126, 0.0225, 0.0804, 0.0303, 0.3194,
        0.0644, 0.1709], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,857][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([5.2312e-11, 4.9809e-09, 1.2099e-07, 8.8632e-07, 5.8852e-06, 6.3355e-05,
        1.0337e-03, 6.6083e-03, 2.8716e-02, 6.4496e-02, 8.9908e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,859][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.0170, 0.0142, 0.0321, 0.0345, 0.0436, 0.1048, 0.0834, 0.1138, 0.1849,
        0.2166, 0.1550], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,861][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.1053, 0.0679, 0.0355, 0.0961, 0.0264, 0.0979, 0.0580, 0.2544, 0.1500,
        0.0869, 0.0216], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,862][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.0050, 0.1219, 0.2818, 0.0475, 0.0292, 0.0195, 0.1856, 0.0106, 0.2523,
        0.0425, 0.0041], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,865][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.0122, 0.0331, 0.0549, 0.0877, 0.0901, 0.1433, 0.0764, 0.1561, 0.1168,
        0.1200, 0.1094], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,867][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.1015, 0.0975, 0.0898, 0.0968, 0.0931, 0.0861, 0.0818, 0.0851, 0.1055,
        0.0962, 0.0668], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,868][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([1.4674e-07, 1.6978e-05, 4.7820e-05, 2.8284e-04, 4.0185e-04, 1.0655e-03,
        7.3815e-03, 1.8157e-02, 6.6414e-02, 2.6088e-01, 6.4536e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,870][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.0031, 0.0807, 0.4856, 0.0089, 0.0800, 0.0510, 0.0622, 0.0503, 0.1032,
        0.0698, 0.0052], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,872][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.0120, 0.0823, 0.1048, 0.0443, 0.0767, 0.0804, 0.1706, 0.0981, 0.1609,
        0.0927, 0.0771], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,874][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0083, 0.0323, 0.0286, 0.0332, 0.0434, 0.0838, 0.0945, 0.0897, 0.1879,
        0.1128, 0.2854], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:25,876][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.1007, 0.0498, 0.0567, 0.0661, 0.1616, 0.0188, 0.1890, 0.0129, 0.0200,
        0.2576, 0.0441, 0.0227], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,878][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0007, 0.0294, 0.0750, 0.0049, 0.0590, 0.0294, 0.1081, 0.1059, 0.0282,
        0.2301, 0.2734, 0.0557], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,879][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([4.8844e-13, 4.7025e-11, 1.4364e-09, 6.4124e-08, 4.1595e-07, 4.6809e-06,
        3.2039e-05, 3.2971e-04, 2.0187e-04, 5.2199e-03, 8.0228e-01, 1.9193e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,881][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0082, 0.0077, 0.0191, 0.0230, 0.0289, 0.0723, 0.0464, 0.0742, 0.1124,
        0.1634, 0.1055, 0.3391], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,883][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.1011, 0.0224, 0.0652, 0.0783, 0.0549, 0.0143, 0.0416, 0.2310, 0.0279,
        0.2792, 0.0633, 0.0208], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,885][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0174, 0.1423, 0.1449, 0.0762, 0.0511, 0.0517, 0.0690, 0.0115, 0.0230,
        0.3365, 0.0538, 0.0225], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,887][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0085, 0.0249, 0.0453, 0.0788, 0.0734, 0.1183, 0.0732, 0.1326, 0.1018,
        0.1110, 0.1179, 0.1142], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,889][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0963, 0.0856, 0.0801, 0.0888, 0.0900, 0.0805, 0.0741, 0.0768, 0.0926,
        0.0862, 0.0612, 0.0879], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,890][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([3.2003e-08, 1.7895e-06, 6.9177e-06, 5.6718e-05, 7.6197e-05, 2.5370e-04,
        1.6912e-03, 4.9913e-03, 1.5820e-02, 9.5930e-02, 2.6478e-01, 6.1639e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,893][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0090, 0.2038, 0.2347, 0.0101, 0.0840, 0.0301, 0.0618, 0.0611, 0.0040,
        0.2592, 0.0379, 0.0044], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,894][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0109, 0.0794, 0.1042, 0.0397, 0.0661, 0.0757, 0.1422, 0.0843, 0.1409,
        0.0802, 0.0602, 0.1162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,896][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0084, 0.0232, 0.0227, 0.0315, 0.0335, 0.0607, 0.0678, 0.0779, 0.1179,
        0.1015, 0.1947, 0.2603], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:25,898][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0299, 0.0234, 0.0199, 0.0429, 0.1338, 0.0152, 0.1886, 0.0165, 0.1925,
        0.0862, 0.0483, 0.1988, 0.0040], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,900][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([1.0580e-04, 5.9877e-03, 4.4600e-02, 5.5980e-04, 1.1401e-02, 5.3824e-02,
        2.5290e-02, 1.6641e-02, 4.1546e-02, 2.6445e-01, 4.5162e-01, 8.0792e-02,
        3.1815e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,901][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([9.9165e-12, 1.2134e-10, 5.5815e-09, 3.8777e-08, 1.4076e-07, 4.8989e-07,
        2.3998e-05, 1.3534e-04, 6.5244e-04, 1.2032e-03, 1.4758e-01, 6.9150e-01,
        1.5890e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,903][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0169, 0.0142, 0.0303, 0.0236, 0.0316, 0.0762, 0.0595, 0.0791, 0.1090,
        0.1242, 0.1001, 0.2630, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,905][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0719, 0.0281, 0.0259, 0.0577, 0.0100, 0.0627, 0.0212, 0.0835, 0.2204,
        0.1458, 0.0355, 0.1795, 0.0579], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,907][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0047, 0.0732, 0.1091, 0.0345, 0.0241, 0.0526, 0.4603, 0.0182, 0.0625,
        0.0566, 0.0403, 0.0634, 0.0005], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,909][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0080, 0.0203, 0.0349, 0.0550, 0.0601, 0.1099, 0.0695, 0.1299, 0.0953,
        0.0886, 0.1121, 0.1140, 0.1024], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,911][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0919, 0.0882, 0.0750, 0.0861, 0.0840, 0.0741, 0.0658, 0.0700, 0.0825,
        0.0769, 0.0557, 0.0773, 0.0726], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,912][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([6.1177e-07, 1.6873e-05, 4.3418e-05, 1.4455e-04, 1.3472e-04, 3.5775e-04,
        1.7158e-03, 2.8821e-03, 1.5379e-02, 5.8280e-02, 1.5902e-01, 2.8014e-01,
        4.8189e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,913][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([9.4149e-04, 8.0713e-02, 2.2413e-01, 1.2395e-02, 3.9261e-02, 6.4160e-02,
        5.2614e-02, 3.8006e-02, 1.6085e-01, 8.2210e-02, 7.4093e-02, 1.7042e-01,
        2.1085e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,914][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0146, 0.0719, 0.0849, 0.0432, 0.0770, 0.0675, 0.1311, 0.0861, 0.1217,
        0.0794, 0.0731, 0.1046, 0.0448], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,915][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0093, 0.0256, 0.0197, 0.0305, 0.0449, 0.0657, 0.0507, 0.0693, 0.0884,
        0.0929, 0.1765, 0.1814, 0.1451], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:25,916][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0166, 0.0184, 0.0184, 0.0537, 0.0312, 0.0338, 0.1443, 0.0245, 0.2851,
        0.0266, 0.0422, 0.2884, 0.0059, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,917][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([4.4753e-05, 9.5067e-03, 8.9649e-03, 2.0951e-03, 1.1326e-02, 3.6878e-02,
        2.3414e-02, 3.9134e-02, 1.2414e-01, 2.2881e-01, 2.2111e-01, 2.8462e-01,
        8.8282e-03, 1.1186e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,918][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([3.4218e-12, 4.9309e-11, 8.9025e-10, 4.6372e-08, 8.0909e-08, 3.4014e-07,
        9.0433e-06, 5.4272e-05, 1.3531e-04, 2.6609e-04, 4.3683e-02, 1.1427e-01,
        6.2256e-01, 2.1902e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,920][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0178, 0.0126, 0.0265, 0.0222, 0.0280, 0.0632, 0.0593, 0.0695, 0.1057,
        0.1132, 0.0950, 0.2666, 0.0740, 0.0463], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,922][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0460, 0.0252, 0.0102, 0.0489, 0.0204, 0.1487, 0.0311, 0.0849, 0.1910,
        0.0723, 0.0173, 0.1839, 0.0641, 0.0560], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,924][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0010, 0.0772, 0.2045, 0.0067, 0.0395, 0.0832, 0.1331, 0.0077, 0.1396,
        0.0619, 0.0872, 0.1367, 0.0206, 0.0010], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,926][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0070, 0.0175, 0.0333, 0.0539, 0.0453, 0.0917, 0.0762, 0.1169, 0.1048,
        0.0827, 0.1010, 0.1104, 0.0943, 0.0649], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,928][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0820, 0.0790, 0.0710, 0.0786, 0.0757, 0.0686, 0.0618, 0.0668, 0.0800,
        0.0701, 0.0534, 0.0758, 0.0667, 0.0705], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,929][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([1.9325e-07, 7.0170e-06, 1.6111e-05, 6.8488e-05, 7.1111e-05, 2.0037e-04,
        9.2573e-04, 1.6406e-03, 6.8073e-03, 2.5485e-02, 7.5861e-02, 1.2124e-01,
        4.2205e-01, 3.4563e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,932][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0007, 0.0666, 0.2551, 0.0035, 0.0520, 0.0383, 0.1657, 0.0518, 0.0715,
        0.1206, 0.0942, 0.0774, 0.0012, 0.0014], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,934][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0140, 0.0685, 0.0825, 0.0433, 0.0687, 0.0683, 0.1272, 0.0797, 0.1182,
        0.0745, 0.0669, 0.1023, 0.0428, 0.0430], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,935][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0094, 0.0254, 0.0195, 0.0264, 0.0346, 0.0531, 0.0524, 0.0570, 0.0866,
        0.0740, 0.1489, 0.1714, 0.1325, 0.1089], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:25,938][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.0195, 0.0436, 0.0554, 0.0278, 0.0203, 0.0261, 0.1360, 0.0135, 0.2678,
        0.0443, 0.0150, 0.2744, 0.0110, 0.0252, 0.0200], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,939][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0026, 0.0076, 0.0042, 0.0200, 0.0101, 0.0422, 0.0854, 0.0185, 0.1634,
        0.2169, 0.0113, 0.2776, 0.0112, 0.0860, 0.0430], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,941][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([2.7340e-12, 8.6632e-12, 1.7125e-10, 1.3978e-08, 4.9455e-09, 1.0751e-07,
        4.9556e-07, 7.8098e-06, 2.6782e-05, 1.5544e-04, 2.1893e-03, 2.3448e-02,
        2.0013e-01, 2.1193e-01, 5.6211e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,943][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.0171, 0.0104, 0.0226, 0.0210, 0.0254, 0.0557, 0.0558, 0.0612, 0.1061,
        0.1042, 0.0807, 0.2769, 0.0759, 0.0482, 0.0388], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,945][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.0862, 0.0169, 0.0128, 0.0807, 0.0132, 0.0378, 0.0185, 0.0451, 0.1431,
        0.1073, 0.0028, 0.1195, 0.0913, 0.1963, 0.0285], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,947][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.0090, 0.2226, 0.0716, 0.0133, 0.0688, 0.0381, 0.1212, 0.0134, 0.1449,
        0.0716, 0.0176, 0.1504, 0.0119, 0.0448, 0.0009], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,949][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0094, 0.0195, 0.0321, 0.0482, 0.0476, 0.0779, 0.0684, 0.1011, 0.1002,
        0.0756, 0.0929, 0.1015, 0.0947, 0.0770, 0.0538], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,951][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.0772, 0.0738, 0.0639, 0.0760, 0.0713, 0.0636, 0.0586, 0.0624, 0.0765,
        0.0691, 0.0495, 0.0724, 0.0666, 0.0686, 0.0505], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,953][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([6.6269e-08, 1.4386e-06, 3.8281e-06, 2.0090e-05, 1.8017e-05, 5.5968e-05,
        2.4098e-04, 4.6478e-04, 1.9328e-03, 7.3257e-03, 2.0295e-02, 3.5241e-02,
        1.4583e-01, 1.1563e-01, 6.7294e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,955][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.0046, 0.0612, 0.2305, 0.0103, 0.0518, 0.0145, 0.1259, 0.0092, 0.1086,
        0.1858, 0.0492, 0.1145, 0.0016, 0.0274, 0.0048], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,957][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.0108, 0.0665, 0.0759, 0.0361, 0.0590, 0.0613, 0.1244, 0.0705, 0.1154,
        0.0680, 0.0604, 0.1005, 0.0409, 0.0368, 0.0734], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,959][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.0080, 0.0230, 0.0178, 0.0204, 0.0260, 0.0451, 0.0460, 0.0452, 0.0812,
        0.0481, 0.1398, 0.1591, 0.1080, 0.0872, 0.1452], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:25,961][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.0107, 0.0173, 0.0162, 0.0154, 0.0084, 0.0225, 0.0785, 0.0183, 0.2849,
        0.1230, 0.0058, 0.2952, 0.0164, 0.0415, 0.0270, 0.0190],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,963][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0004, 0.0129, 0.0075, 0.0031, 0.0018, 0.0337, 0.0408, 0.0386, 0.1970,
        0.1398, 0.0108, 0.3879, 0.0023, 0.0095, 0.0930, 0.0209],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,964][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([1.4437e-13, 1.5143e-13, 1.8418e-12, 7.1151e-11, 7.7948e-11, 1.7934e-10,
        5.5885e-09, 4.1068e-08, 1.6006e-07, 4.5107e-07, 3.7712e-05, 1.7198e-04,
        2.0612e-03, 1.8822e-02, 1.2030e-01, 8.5861e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,966][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.0157, 0.0109, 0.0256, 0.0187, 0.0234, 0.0583, 0.0521, 0.0597, 0.1025,
        0.1032, 0.0828, 0.2712, 0.0599, 0.0399, 0.0396, 0.0366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,968][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.0488, 0.0049, 0.0095, 0.0731, 0.0247, 0.0273, 0.0316, 0.0515, 0.1740,
        0.0592, 0.0121, 0.1501, 0.1050, 0.1247, 0.0779, 0.0256],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,971][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.0209, 0.1837, 0.0321, 0.0498, 0.0612, 0.0388, 0.0915, 0.0139, 0.1483,
        0.0932, 0.0226, 0.1475, 0.0659, 0.0227, 0.0063, 0.0016],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,973][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.0073, 0.0176, 0.0326, 0.0516, 0.0408, 0.0709, 0.0614, 0.0914, 0.0871,
        0.0744, 0.0862, 0.0954, 0.0835, 0.0687, 0.0526, 0.0785],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,974][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0755, 0.0720, 0.0636, 0.0688, 0.0665, 0.0605, 0.0546, 0.0585, 0.0712,
        0.0641, 0.0466, 0.0669, 0.0598, 0.0616, 0.0474, 0.0625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,976][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([1.3010e-07, 9.1996e-07, 2.4847e-06, 9.0908e-06, 7.0185e-06, 1.7721e-05,
        1.0195e-04, 1.3142e-04, 6.5671e-04, 2.8541e-03, 7.1112e-03, 9.7834e-03,
        5.2269e-02, 4.6548e-02, 2.9278e-01, 5.8772e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,978][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.0036, 0.1200, 0.3063, 0.0076, 0.0526, 0.0331, 0.0345, 0.0386, 0.1282,
        0.0594, 0.0379, 0.1348, 0.0016, 0.0211, 0.0203, 0.0003],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,979][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.0118, 0.0628, 0.0718, 0.0352, 0.0587, 0.0622, 0.1153, 0.0664, 0.1087,
        0.0686, 0.0583, 0.0929, 0.0406, 0.0362, 0.0694, 0.0411],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,980][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.0081, 0.0186, 0.0132, 0.0156, 0.0214, 0.0339, 0.0351, 0.0354, 0.0583,
        0.0434, 0.0932, 0.1151, 0.0919, 0.0733, 0.1635, 0.1798],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:25,981][circuit_model.py][line:1570][INFO] ##2-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0152, 0.0158, 0.0154, 0.0487, 0.0276, 0.0308, 0.1230, 0.0216, 0.2582,
        0.0224, 0.0366, 0.2615, 0.0051, 0.0099, 0.0334, 0.0638, 0.0110],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,982][circuit_model.py][line:1573][INFO] ##2-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([2.1221e-05, 8.1340e-03, 8.1577e-03, 1.1935e-03, 8.1691e-03, 3.8717e-02,
        2.4835e-02, 3.7416e-02, 1.0438e-01, 2.2749e-01, 1.8489e-01, 2.4990e-01,
        4.9615e-03, 7.1569e-04, 8.8634e-02, 1.1634e-02, 7.5028e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,983][circuit_model.py][line:1576][INFO] ##2-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([9.4359e-15, 1.3130e-14, 1.8530e-13, 9.0972e-12, 1.6602e-11, 4.5611e-11,
        1.3007e-09, 7.8089e-09, 2.5282e-08, 4.2710e-08, 6.4923e-06, 2.1551e-05,
        1.1371e-04, 3.7864e-05, 5.5389e-03, 8.4382e-01, 1.5046e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,985][circuit_model.py][line:1579][INFO] ##2-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0151, 0.0108, 0.0240, 0.0190, 0.0234, 0.0557, 0.0508, 0.0613, 0.0972,
        0.1011, 0.0818, 0.2506, 0.0576, 0.0367, 0.0381, 0.0365, 0.0405],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,987][circuit_model.py][line:1582][INFO] ##2-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0399, 0.0231, 0.0083, 0.0382, 0.0179, 0.1308, 0.0270, 0.0736, 0.1691,
        0.0585, 0.0160, 0.1681, 0.0606, 0.0463, 0.0111, 0.0770, 0.0345],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,988][circuit_model.py][line:1585][INFO] ##2-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0009, 0.0646, 0.1840, 0.0060, 0.0341, 0.0706, 0.1159, 0.0065, 0.1206,
        0.0538, 0.0776, 0.1181, 0.0184, 0.0009, 0.0248, 0.1026, 0.0008],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,990][circuit_model.py][line:1588][INFO] ##2-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0051, 0.0149, 0.0296, 0.0493, 0.0354, 0.0758, 0.0625, 0.1008, 0.0921,
        0.0743, 0.0804, 0.0957, 0.0729, 0.0583, 0.0481, 0.0618, 0.0430],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,992][circuit_model.py][line:1591][INFO] ##2-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0682, 0.0660, 0.0595, 0.0652, 0.0630, 0.0576, 0.0518, 0.0562, 0.0674,
        0.0589, 0.0448, 0.0640, 0.0557, 0.0591, 0.0463, 0.0600, 0.0563],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,994][circuit_model.py][line:1594][INFO] ##2-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([6.2568e-08, 5.4362e-07, 1.0146e-06, 4.8267e-06, 3.6836e-06, 1.1137e-05,
        5.7419e-05, 7.4320e-05, 3.0304e-04, 1.1324e-03, 3.0736e-03, 4.0451e-03,
        2.4735e-02, 1.9547e-02, 1.5033e-01, 4.0961e-01, 3.8707e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,995][circuit_model.py][line:1597][INFO] ##2-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0007, 0.0650, 0.2521, 0.0034, 0.0498, 0.0358, 0.1554, 0.0489, 0.0681,
        0.1166, 0.0901, 0.0739, 0.0011, 0.0013, 0.0265, 0.0100, 0.0012],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:25,997][circuit_model.py][line:1600][INFO] ##2-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0125, 0.0580, 0.0713, 0.0372, 0.0589, 0.0595, 0.1096, 0.0692, 0.1006,
        0.0643, 0.0591, 0.0885, 0.0369, 0.0378, 0.0651, 0.0414, 0.0300],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,000][circuit_model.py][line:1603][INFO] ##2-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0077, 0.0169, 0.0124, 0.0132, 0.0177, 0.0296, 0.0317, 0.0306, 0.0524,
        0.0375, 0.0821, 0.1010, 0.0727, 0.0529, 0.1435, 0.1451, 0.1531],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,003][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:26,005][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  446],
        [  270],
        [ 3195],
        [ 2733],
        [13563],
        [ 5822],
        [ 3891],
        [ 2592],
        [ 4046],
        [ 7585],
        [ 2282],
        [ 5348],
        [ 7694],
        [  146],
        [ 1116],
        [ 3049],
        [  147]], device='cuda:0')
[2024-07-23 21:05:26,007][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  415],
        [  645],
        [ 9582],
        [12688],
        [28874],
        [34563],
        [18525],
        [13950],
        [23177],
        [28562],
        [16752],
        [23442],
        [23706],
        [ 2092],
        [ 8457],
        [16253],
        [ 2286]], device='cuda:0')
[2024-07-23 21:05:26,009][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[5482],
        [5558],
        [5786],
        [5244],
        [5058],
        [4918],
        [4852],
        [4901],
        [4894],
        [4918],
        [4900],
        [4881],
        [4873],
        [4824],
        [4844],
        [4914],
        [4913]], device='cuda:0')
[2024-07-23 21:05:26,011][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[5553],
        [5996],
        [5097],
        [4790],
        [4911],
        [6213],
        [4662],
        [2126],
        [2201],
        [1634],
        [1827],
        [1915],
        [2254],
        [2219],
        [2267],
        [2093],
        [2485]], device='cuda:0')
[2024-07-23 21:05:26,013][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[22531],
        [30820],
        [15842],
        [16639],
        [21015],
        [ 5948],
        [17696],
        [26479],
        [18949],
        [12892],
        [32490],
        [32307],
        [13039],
        [11378],
        [13687],
        [36792],
        [37165]], device='cuda:0')
[2024-07-23 21:05:26,015][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[15040],
        [23484],
        [26030],
        [27208],
        [24975],
        [22518],
        [25929],
        [27675],
        [26705],
        [27441],
        [27426],
        [25839],
        [26988],
        [26799],
        [24579],
        [24005],
        [24304]], device='cuda:0')
[2024-07-23 21:05:26,017][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 4384],
        [ 4314],
        [   54],
        [ 5554],
        [ 1491],
        [  922],
        [ 2819],
        [ 1655],
        [  667],
        [ 1083],
        [  101],
        [  604],
        [ 1768],
        [  718],
        [ 3113],
        [24905],
        [  859]], device='cuda:0')
[2024-07-23 21:05:26,019][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[34293],
        [34019],
        [33546],
        [32908],
        [32706],
        [31990],
        [32048],
        [31864],
        [32128],
        [32177],
        [32236],
        [32584],
        [32351],
        [32456],
        [32492],
        [32673],
        [32760]], device='cuda:0')
[2024-07-23 21:05:26,021][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[18116],
        [10487],
        [13205],
        [12894],
        [11607],
        [11581],
        [ 9943],
        [ 9736],
        [10292],
        [ 9961],
        [ 9951],
        [10034],
        [10055],
        [10096],
        [10065],
        [10197],
        [10240]], device='cuda:0')
[2024-07-23 21:05:26,023][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[21836],
        [22260],
        [21779],
        [21807],
        [21736],
        [21605],
        [21497],
        [21437],
        [21682],
        [21620],
        [21660],
        [21750],
        [21761],
        [21820],
        [21696],
        [21668],
        [21679]], device='cuda:0')
[2024-07-23 21:05:26,025][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[20219],
        [19987],
        [20102],
        [19660],
        [20234],
        [20375],
        [21880],
        [20567],
        [21573],
        [21641],
        [21837],
        [21315],
        [20936],
        [21409],
        [20407],
        [20710],
        [20137]], device='cuda:0')
[2024-07-23 21:05:26,027][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[1989],
        [1674],
        [1573],
        [1499],
        [1532],
        [1469],
        [1516],
        [1503],
        [1456],
        [1452],
        [1386],
        [1382],
        [1380],
        [1368],
        [1374],
        [1357],
        [1368]], device='cuda:0')
[2024-07-23 21:05:26,029][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[1841],
        [ 156],
        [  72],
        [ 109],
        [ 130],
        [ 122],
        [ 124],
        [ 143],
        [ 110],
        [  86],
        [  97],
        [  89],
        [  94],
        [  80],
        [  84],
        [  77],
        [  72]], device='cuda:0')
[2024-07-23 21:05:26,031][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[2270],
        [3031],
        [2823],
        [1959],
        [1134],
        [1105],
        [1304],
        [1951],
        [2320],
        [2910],
        [3508],
        [3938],
        [3396],
        [3072],
        [4232],
        [3190],
        [3213]], device='cuda:0')
[2024-07-23 21:05:26,033][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[  590],
        [ 6607],
        [ 8636],
        [15616],
        [21173],
        [ 5476],
        [18161],
        [20719],
        [12128],
        [17335],
        [13327],
        [13957],
        [ 7806],
        [ 9783],
        [ 1898],
        [10025],
        [ 6210]], device='cuda:0')
[2024-07-23 21:05:26,035][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[  585],
        [  635],
        [ 5706],
        [18738],
        [  693],
        [ 4995],
        [ 5169],
        [ 2012],
        [ 7135],
        [10207],
        [11117],
        [ 3613],
        [10478],
        [11172],
        [12386],
        [ 9335],
        [12354]], device='cuda:0')
[2024-07-23 21:05:26,037][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 394],
        [ 110],
        [ 213],
        [ 210],
        [ 126],
        [ 151],
        [ 173],
        [ 315],
        [ 351],
        [ 427],
        [ 459],
        [1003],
        [2453],
        [ 709],
        [ 506],
        [ 547],
        [ 920]], device='cuda:0')
[2024-07-23 21:05:26,038][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 346],
        [ 675],
        [ 633],
        [ 964],
        [4173],
        [4395],
        [ 582],
        [ 370],
        [ 398],
        [ 841],
        [1075],
        [1268],
        [2710],
        [3009],
        [ 461],
        [ 225],
        [ 210]], device='cuda:0')
[2024-07-23 21:05:26,041][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[1320],
        [1383],
        [1278],
        [1474],
        [1463],
        [1419],
        [1443],
        [1359],
        [1382],
        [1471],
        [1380],
        [1308],
        [1396],
        [1417],
        [1426],
        [1429],
        [1454]], device='cuda:0')
[2024-07-23 21:05:26,042][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 625],
        [ 713],
        [ 672],
        [ 506],
        [ 717],
        [ 892],
        [ 791],
        [ 949],
        [1357],
        [1522],
        [1267],
        [1195],
        [ 598],
        [ 759],
        [ 422],
        [ 327],
        [ 684]], device='cuda:0')
[2024-07-23 21:05:26,044][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[4299],
        [4504],
        [ 590],
        [3374],
        [3466],
        [2633],
        [2652],
        [2715],
        [2866],
        [2552],
        [3560],
        [7855],
        [2176],
        [2052],
        [2910],
        [2918],
        [3446]], device='cuda:0')
[2024-07-23 21:05:26,046][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[13279],
        [16014],
        [17894],
        [18296],
        [18967],
        [19935],
        [19043],
        [18344],
        [15826],
        [13329],
        [13110],
        [10247],
        [10181],
        [ 9942],
        [ 9983],
        [10541],
        [10673]], device='cuda:0')
[2024-07-23 21:05:26,048][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[24545],
        [25824],
        [23295],
        [24318],
        [23684],
        [24187],
        [23344],
        [23189],
        [22275],
        [22514],
        [21844],
        [21171],
        [21193],
        [21095],
        [21300],
        [21635],
        [21599]], device='cuda:0')
[2024-07-23 21:05:26,050][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[1952],
        [3445],
        [1993],
        [2347],
        [2795],
        [3556],
        [3182],
        [3399],
        [3075],
        [4791],
        [3784],
        [2843],
        [3424],
        [3519],
        [3209],
        [3884],
        [3872]], device='cuda:0')
[2024-07-23 21:05:26,052][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[216],
        [234],
        [161],
        [523],
        [294],
        [188],
        [138],
        [135],
        [149],
        [205],
        [246],
        [213],
        [287],
        [184],
        [216],
        [257],
        [173]], device='cuda:0')
[2024-07-23 21:05:26,053][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 730],
        [1098],
        [1097],
        [1001],
        [ 799],
        [ 671],
        [ 683],
        [ 615],
        [ 592],
        [ 592],
        [ 575],
        [ 550],
        [ 573],
        [ 570],
        [ 610],
        [ 621],
        [ 617]], device='cuda:0')
[2024-07-23 21:05:26,055][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[9631],
        [8387],
        [4288],
        [2910],
        [3313],
        [3349],
        [3212],
        [3078],
        [2825],
        [3010],
        [2358],
        [2236],
        [2317],
        [2185],
        [1520],
        [1186],
        [1209]], device='cuda:0')
[2024-07-23 21:05:26,057][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[3290],
        [1790],
        [3515],
        [2080],
        [2599],
        [2367],
        [2298],
        [2975],
        [2032],
        [1166],
        [1410],
        [1845],
        [1243],
        [1357],
        [1786],
        [1646],
        [1543]], device='cuda:0')
[2024-07-23 21:05:26,059][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 1625],
        [11174],
        [15415],
        [ 5134],
        [ 2927],
        [ 4136],
        [  829],
        [  240],
        [  657],
        [  998],
        [ 2142],
        [  569],
        [ 2070],
        [  801],
        [ 1912],
        [  943],
        [  833]], device='cuda:0')
[2024-07-23 21:05:26,061][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896],
        [9896]], device='cuda:0')
[2024-07-23 21:05:26,110][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:26,112][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,113][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,115][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,116][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,117][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,117][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,118][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,119][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,119][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,120][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,121][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,121][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,122][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.1682, 0.8318], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,123][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.6814, 0.3186], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,124][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.6714, 0.3286], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,125][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.0153, 0.9847], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,126][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.4931, 0.5069], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,127][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.8713, 0.1287], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,127][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.9010, 0.0990], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,128][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.7850, 0.2150], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,129][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.8129, 0.1871], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,129][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.5811, 0.4189], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,130][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.3980, 0.6020], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,131][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.7587, 0.2413], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,132][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0683, 0.0366, 0.8950], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,134][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.4006, 0.3011, 0.2983], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,136][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1112, 0.6885, 0.2003], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,137][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ language] are: tensor([6.3453e-04, 9.7415e-01, 2.5213e-02], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,139][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.2823, 0.4484, 0.2693], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,141][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.2575, 0.3542, 0.3883], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,143][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.7554, 0.1444, 0.1002], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,145][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.6621, 0.2152, 0.1226], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,146][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.2626, 0.7192, 0.0182], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,147][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.1967, 0.6891, 0.1142], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,148][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.2647, 0.4051, 0.3302], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,148][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.3291, 0.6163, 0.0545], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,149][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.1336, 0.0649, 0.1121, 0.6894], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,150][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.2551, 0.2853, 0.3550, 0.1045], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,152][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1494, 0.5893, 0.2418, 0.0194], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,154][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0012, 0.7497, 0.2375, 0.0116], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,155][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.1930, 0.2395, 0.3264, 0.2411], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,157][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.1201, 0.1390, 0.7103, 0.0307], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,159][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.6905, 0.0744, 0.0861, 0.1490], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,161][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3442, 0.1521, 0.1208, 0.3829], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,162][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.2515, 0.5462, 0.1440, 0.0582], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,165][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.3620, 0.4864, 0.0679, 0.0837], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,167][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.2041, 0.3200, 0.2498, 0.2261], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,168][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1509, 0.7269, 0.1052, 0.0170], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,170][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0933, 0.0353, 0.0726, 0.6277, 0.1711], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,172][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.1028, 0.2114, 0.4463, 0.0934, 0.1461], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,174][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0826, 0.4881, 0.2441, 0.1467, 0.0385], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,176][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.0014, 0.7595, 0.1715, 0.0561, 0.0114], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,178][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.1841, 0.2662, 0.2469, 0.1791, 0.1237], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,179][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.1879, 0.3142, 0.4025, 0.0521, 0.0433], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,182][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.4144, 0.0856, 0.0991, 0.3321, 0.0687], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,184][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.1724, 0.0801, 0.0560, 0.2685, 0.4229], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,185][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.0937, 0.5876, 0.1181, 0.1517, 0.0490], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,187][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.3140, 0.1673, 0.1232, 0.2131, 0.1824], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,189][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.1686, 0.2633, 0.2041, 0.1875, 0.1764], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,191][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.3122, 0.4518, 0.1648, 0.0604, 0.0108], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,193][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0899, 0.0552, 0.0302, 0.4388, 0.0322, 0.3537], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,194][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.1355, 0.1771, 0.2308, 0.1161, 0.2907, 0.0498], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,196][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.1619, 0.2074, 0.1019, 0.1121, 0.2854, 0.1313], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,198][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.0026, 0.5943, 0.1043, 0.0521, 0.1687, 0.0780], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,200][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.1399, 0.2580, 0.2037, 0.1900, 0.1195, 0.0889], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,202][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.0759, 0.3467, 0.3345, 0.0331, 0.1781, 0.0317], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,204][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.3085, 0.0781, 0.0811, 0.3453, 0.0947, 0.0924], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,206][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.1205, 0.0591, 0.0397, 0.2492, 0.4177, 0.1138], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,207][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.1114, 0.1966, 0.2210, 0.1421, 0.2905, 0.0385], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,210][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.1645, 0.0639, 0.0520, 0.0764, 0.2969, 0.3463], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,211][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.1400, 0.2242, 0.1769, 0.1629, 0.1492, 0.1469], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,213][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.3042, 0.3279, 0.0972, 0.0840, 0.0859, 0.1007], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,214][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.1207, 0.0543, 0.1422, 0.5702, 0.0548, 0.0207, 0.0371],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,215][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0807, 0.2089, 0.2014, 0.0928, 0.1429, 0.0940, 0.1794],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,216][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.1520, 0.1393, 0.0279, 0.0964, 0.0680, 0.0946, 0.4219],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,217][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.0166, 0.5479, 0.0553, 0.0293, 0.0306, 0.0630, 0.2573],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,217][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.1002, 0.1407, 0.1870, 0.1559, 0.0899, 0.2320, 0.0944],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,218][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.1153, 0.2230, 0.1505, 0.1257, 0.2574, 0.0265, 0.1017],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,220][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.5583, 0.0472, 0.0363, 0.1945, 0.0568, 0.0586, 0.0483],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,222][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0747, 0.0491, 0.0325, 0.1990, 0.4498, 0.1644, 0.0303],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,224][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.2594, 0.1944, 0.0262, 0.1081, 0.0267, 0.0427, 0.3426],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,226][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.2214, 0.2289, 0.0630, 0.0913, 0.1468, 0.0494, 0.1991],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,228][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.1200, 0.1797, 0.1455, 0.1351, 0.1273, 0.1242, 0.1683],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,230][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2140, 0.2384, 0.0636, 0.0451, 0.0133, 0.1598, 0.2656],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,232][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.1178, 0.0651, 0.0427, 0.5215, 0.0308, 0.0338, 0.0043, 0.1841],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,234][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0707, 0.1638, 0.2708, 0.0646, 0.1550, 0.0601, 0.1412, 0.0737],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,235][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0991, 0.0222, 0.0103, 0.0210, 0.0138, 0.0182, 0.6401, 0.1754],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,237][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.0013, 0.0134, 0.0019, 0.0028, 0.0033, 0.0034, 0.9219, 0.0519],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,239][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0919, 0.1695, 0.1721, 0.1536, 0.0985, 0.1619, 0.0916, 0.0609],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,241][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.1391, 0.1680, 0.1484, 0.2169, 0.0958, 0.0928, 0.1130, 0.0260],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,243][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.4547, 0.0475, 0.0422, 0.1677, 0.0463, 0.0466, 0.0622, 0.1329],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,245][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0752, 0.0440, 0.0310, 0.1782, 0.3431, 0.1144, 0.0276, 0.1864],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,247][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2041, 0.0727, 0.0529, 0.0346, 0.0235, 0.0139, 0.5025, 0.0959],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,249][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.2164, 0.0278, 0.0192, 0.0413, 0.0979, 0.1002, 0.2756, 0.2218],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,251][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.1022, 0.1571, 0.1258, 0.1162, 0.1067, 0.1071, 0.1475, 0.1374],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,253][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.4223, 0.0707, 0.0366, 0.0338, 0.0232, 0.0868, 0.1279, 0.1986],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,255][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.1109, 0.0356, 0.0724, 0.5340, 0.0372, 0.0383, 0.0129, 0.0221, 0.1366],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,257][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0480, 0.1221, 0.1284, 0.0427, 0.0885, 0.0579, 0.1456, 0.0726, 0.2942],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,259][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0369, 0.0212, 0.0020, 0.0067, 0.0141, 0.0083, 0.1048, 0.6760, 0.1300],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,260][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([1.7722e-04, 3.4749e-03, 4.4166e-04, 3.1144e-04, 7.7506e-04, 6.3378e-03,
        1.2449e-01, 8.4271e-01, 2.1285e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,262][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0818, 0.0920, 0.1654, 0.1106, 0.0569, 0.1858, 0.0986, 0.0979, 0.1110],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,264][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0982, 0.1164, 0.1489, 0.1683, 0.2238, 0.0395, 0.1170, 0.0760, 0.0120],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,265][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.2332, 0.0436, 0.0296, 0.2119, 0.0730, 0.0686, 0.0468, 0.2514, 0.0418],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,268][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0808, 0.0369, 0.0213, 0.1728, 0.3464, 0.0990, 0.0248, 0.1969, 0.0211],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,269][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.1501, 0.0618, 0.0073, 0.0258, 0.0096, 0.0082, 0.3330, 0.3183, 0.0860],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,271][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.1237, 0.0153, 0.0082, 0.0144, 0.0278, 0.0258, 0.2393, 0.2136, 0.3319],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,273][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0925, 0.1377, 0.1107, 0.1027, 0.0958, 0.0940, 0.1291, 0.1177, 0.1198],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,275][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1504, 0.0526, 0.0187, 0.0203, 0.0048, 0.0559, 0.2468, 0.2907, 0.1596],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,277][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0887, 0.0394, 0.0545, 0.4417, 0.0439, 0.0211, 0.0062, 0.0063, 0.0302,
        0.2679], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,279][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0478, 0.0912, 0.1339, 0.0353, 0.0712, 0.0432, 0.0908, 0.0698, 0.2112,
        0.2056], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,281][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0065, 0.0029, 0.0023, 0.0006, 0.0018, 0.0048, 0.0692, 0.5774, 0.2429,
        0.0916], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,282][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ et] are: tensor([4.2830e-04, 9.2781e-03, 8.9101e-04, 1.0954e-03, 5.9488e-04, 2.5192e-03,
        1.6828e-01, 2.1415e-01, 5.8031e-01, 2.2446e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,284][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0740, 0.1080, 0.1400, 0.0836, 0.0700, 0.0971, 0.1090, 0.0762, 0.1718,
        0.0702], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,286][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.1495, 0.1032, 0.1302, 0.0373, 0.0740, 0.0543, 0.1534, 0.0164, 0.1956,
        0.0861], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,288][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.1847, 0.0324, 0.0291, 0.1817, 0.0544, 0.0625, 0.0442, 0.3149, 0.0550,
        0.0410], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,289][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0467, 0.0349, 0.0205, 0.1441, 0.3018, 0.0982, 0.0192, 0.2083, 0.0219,
        0.1044], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,289][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.0705, 0.0212, 0.0076, 0.0123, 0.0186, 0.0078, 0.2413, 0.2269, 0.3553,
        0.0384], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,290][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.1328, 0.0038, 0.0023, 0.0100, 0.0170, 0.0777, 0.2367, 0.2799, 0.1901,
        0.0498], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,291][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0820, 0.1222, 0.0990, 0.0915, 0.0848, 0.0842, 0.1145, 0.1066, 0.1070,
        0.1083], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,292][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2661, 0.0161, 0.0034, 0.0085, 0.0031, 0.0342, 0.0300, 0.4556, 0.0952,
        0.0877], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,294][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.1132, 0.0368, 0.1263, 0.5054, 0.0273, 0.0173, 0.0074, 0.0248, 0.0309,
        0.0336, 0.0769], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,296][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0233, 0.0484, 0.1248, 0.0307, 0.0462, 0.0292, 0.0642, 0.0442, 0.2201,
        0.3515, 0.0173], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,298][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.0758, 0.0014, 0.0008, 0.0009, 0.0007, 0.0008, 0.0269, 0.0738, 0.1323,
        0.3566, 0.3300], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,299][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([1.5639e-03, 6.6471e-04, 2.4156e-04, 1.9403e-04, 6.4588e-05, 6.4915e-04,
        1.3001e-01, 6.2785e-02, 1.4952e-01, 9.2446e-02, 5.6185e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,301][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0626, 0.0874, 0.1277, 0.0802, 0.0608, 0.1331, 0.0836, 0.0726, 0.1628,
        0.0728, 0.0563], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,303][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0864, 0.0132, 0.5326, 0.0190, 0.0829, 0.0614, 0.0433, 0.0199, 0.1027,
        0.0224, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,304][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.3049, 0.0452, 0.0441, 0.1307, 0.0342, 0.0378, 0.0478, 0.1787, 0.0523,
        0.0333, 0.0910], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,307][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0191, 0.0278, 0.0214, 0.1410, 0.2720, 0.0878, 0.0164, 0.1916, 0.0216,
        0.1456, 0.0558], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,309][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.1554, 0.0097, 0.0029, 0.0052, 0.0038, 0.0056, 0.1087, 0.1063, 0.1097,
        0.0581, 0.4345], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,310][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2381, 0.0045, 0.0021, 0.0119, 0.0109, 0.0140, 0.0526, 0.0982, 0.1003,
        0.1332, 0.3342], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,312][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0743, 0.1132, 0.0898, 0.0844, 0.0774, 0.0768, 0.1044, 0.0965, 0.0963,
        0.0979, 0.0890], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,314][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.2950, 0.0194, 0.0051, 0.0069, 0.0014, 0.0100, 0.0431, 0.0941, 0.0803,
        0.2029, 0.2419], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,316][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0870, 0.0318, 0.0572, 0.4009, 0.0269, 0.0318, 0.0116, 0.0185, 0.1330,
        0.0399, 0.0036, 0.1579], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,318][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0248, 0.0605, 0.0642, 0.0201, 0.0410, 0.0299, 0.0644, 0.0357, 0.1333,
        0.3632, 0.0238, 0.1392], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,319][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [orum] are: tensor([4.9721e-03, 7.1830e-04, 7.1107e-05, 2.3055e-04, 3.1100e-04, 1.3546e-04,
        4.4074e-03, 2.8525e-02, 7.8992e-03, 5.0738e-02, 7.6471e-01, 1.3728e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,321][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [orum] are: tensor([9.4075e-05, 2.6211e-04, 3.4477e-05, 2.0456e-05, 2.3507e-05, 1.8646e-04,
        1.1448e-02, 4.8614e-02, 3.4147e-03, 8.3332e-02, 6.5394e-01, 1.9863e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,323][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0638, 0.0707, 0.1266, 0.0866, 0.0466, 0.1434, 0.0770, 0.0717, 0.0879,
        0.0881, 0.0535, 0.0841], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,325][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0726, 0.0878, 0.1242, 0.1335, 0.1605, 0.0339, 0.0810, 0.0590, 0.0093,
        0.1492, 0.0814, 0.0078], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,326][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.1279, 0.0275, 0.0137, 0.1442, 0.0509, 0.0442, 0.0277, 0.2343, 0.0226,
        0.0199, 0.2286, 0.0585], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,329][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0144, 0.0224, 0.0125, 0.1385, 0.3106, 0.0717, 0.0148, 0.1757, 0.0142,
        0.1246, 0.0954, 0.0053], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,331][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0601, 0.0076, 0.0007, 0.0025, 0.0006, 0.0006, 0.0491, 0.0317, 0.0109,
        0.0423, 0.6404, 0.1533], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,332][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0527, 0.0021, 0.0013, 0.0022, 0.0024, 0.0023, 0.0455, 0.0308, 0.0884,
        0.0715, 0.3142, 0.3868], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,335][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0671, 0.1044, 0.0820, 0.0772, 0.0714, 0.0704, 0.0974, 0.0892, 0.0895,
        0.0903, 0.0824, 0.0788], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,337][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0868, 0.0095, 0.0032, 0.0034, 0.0006, 0.0062, 0.0454, 0.0595, 0.0327,
        0.0899, 0.5011, 0.1617], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,338][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0764, 0.0252, 0.0267, 0.2057, 0.0298, 0.0235, 0.0033, 0.0116, 0.0209,
        0.0567, 0.0056, 0.0176, 0.4969], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,340][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0513, 0.0357, 0.1137, 0.0191, 0.0696, 0.0353, 0.0874, 0.0455, 0.1077,
        0.2413, 0.0297, 0.1188, 0.0448], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,342][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ is] are: tensor([9.5849e-03, 1.1913e-03, 7.7981e-04, 5.7361e-04, 5.1435e-04, 3.5965e-04,
        7.5980e-03, 3.0779e-02, 2.7741e-02, 4.0139e-02, 5.6956e-01, 3.0876e-01,
        2.4240e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,343][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ is] are: tensor([2.0341e-04, 7.6650e-04, 1.4271e-04, 9.2519e-05, 3.2649e-05, 8.8328e-05,
        2.0394e-03, 1.1900e-02, 2.1541e-02, 6.0820e-03, 3.9089e-01, 5.6318e-01,
        3.0406e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,345][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0601, 0.0625, 0.0752, 0.0584, 0.0426, 0.0742, 0.0644, 0.0572, 0.1244,
        0.0855, 0.0538, 0.1180, 0.1237], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,347][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0870, 0.0531, 0.1197, 0.0374, 0.1988, 0.0435, 0.1400, 0.0590, 0.0468,
        0.1106, 0.0521, 0.0446, 0.0073], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,349][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.1638, 0.0314, 0.0349, 0.0696, 0.0285, 0.0403, 0.0405, 0.1781, 0.0344,
        0.0341, 0.1295, 0.0917, 0.1232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,351][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0124, 0.0245, 0.0182, 0.1076, 0.2595, 0.0972, 0.0151, 0.1876, 0.0202,
        0.1315, 0.1008, 0.0093, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,353][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0722, 0.0095, 0.0039, 0.0051, 0.0013, 0.0022, 0.0259, 0.0813, 0.0221,
        0.0882, 0.4862, 0.1563, 0.0459], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,355][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.1780, 0.0072, 0.0056, 0.0061, 0.0026, 0.0073, 0.0695, 0.0456, 0.0717,
        0.0592, 0.3125, 0.1888, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,356][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0634, 0.0950, 0.0768, 0.0712, 0.0653, 0.0643, 0.0885, 0.0819, 0.0821,
        0.0834, 0.0751, 0.0717, 0.0812], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,357][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.1635, 0.0272, 0.0039, 0.0028, 0.0009, 0.0056, 0.0132, 0.0396, 0.0516,
        0.0409, 0.5219, 0.1181, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,358][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0632, 0.0156, 0.0261, 0.1600, 0.0135, 0.0129, 0.0034, 0.0099, 0.0203,
        0.0335, 0.0029, 0.0184, 0.1202, 0.5001], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,358][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0195, 0.0475, 0.1486, 0.0170, 0.0354, 0.0267, 0.0540, 0.0384, 0.1181,
        0.1935, 0.0341, 0.1315, 0.0649, 0.0708], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,359][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([3.2166e-02, 9.6990e-04, 5.7085e-04, 7.9140e-04, 2.6176e-04, 7.0284e-04,
        8.2205e-03, 4.4421e-02, 2.9135e-02, 4.4575e-02, 4.4315e-01, 2.2698e-01,
        4.3882e-02, 1.2418e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,361][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([4.0795e-04, 6.3204e-04, 9.9201e-05, 2.4913e-05, 2.5267e-05, 6.3218e-05,
        6.9647e-03, 1.0431e-02, 3.1462e-02, 6.5792e-03, 3.2806e-01, 5.5552e-01,
        1.0257e-02, 4.9471e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,363][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0578, 0.0598, 0.0810, 0.0686, 0.0419, 0.0732, 0.0564, 0.0413, 0.0975,
        0.0813, 0.0511, 0.0957, 0.0979, 0.0965], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,365][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0303, 0.0241, 0.2414, 0.0365, 0.1097, 0.0150, 0.1535, 0.0370, 0.0534,
        0.2202, 0.0155, 0.0495, 0.0103, 0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,366][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1850, 0.0259, 0.0359, 0.0645, 0.0225, 0.0350, 0.0389, 0.1384, 0.0362,
        0.0326, 0.0833, 0.0889, 0.1258, 0.0872], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,369][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0184, 0.0284, 0.0210, 0.1217, 0.2684, 0.0912, 0.0169, 0.1746, 0.0213,
        0.1196, 0.0795, 0.0081, 0.0161, 0.0148], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,370][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1118, 0.0050, 0.0014, 0.0023, 0.0007, 0.0016, 0.0121, 0.0285, 0.0159,
        0.0211, 0.4141, 0.1095, 0.1350, 0.1411], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,372][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1119, 0.0216, 0.0038, 0.0070, 0.0060, 0.0091, 0.0535, 0.0881, 0.0378,
        0.0943, 0.2562, 0.1093, 0.1147, 0.0867], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,374][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0593, 0.0887, 0.0716, 0.0646, 0.0593, 0.0586, 0.0828, 0.0752, 0.0768,
        0.0768, 0.0692, 0.0666, 0.0754, 0.0750], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,376][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1864, 0.0196, 0.0023, 0.0029, 0.0009, 0.0082, 0.0108, 0.0829, 0.0286,
        0.0518, 0.4752, 0.0665, 0.0184, 0.0456], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,378][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0369, 0.0153, 0.0265, 0.1642, 0.0076, 0.0085, 0.0023, 0.0035, 0.0097,
        0.0316, 0.0016, 0.0071, 0.1309, 0.4153, 0.1389], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,380][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0177, 0.0339, 0.0338, 0.0106, 0.0221, 0.0151, 0.0403, 0.0176, 0.1664,
        0.1837, 0.0093, 0.1850, 0.0745, 0.1447, 0.0452], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,381][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ same] are: tensor([1.9059e-02, 2.8439e-04, 4.4484e-04, 6.5211e-04, 5.5866e-05, 8.7923e-05,
        2.0098e-03, 8.9281e-03, 1.3152e-02, 8.6656e-03, 1.0456e-01, 7.5614e-02,
        1.7615e-02, 3.4472e-01, 4.0415e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,383][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ same] are: tensor([7.3312e-04, 5.4303e-04, 1.8207e-05, 1.4510e-05, 6.6430e-06, 3.2852e-06,
        5.7150e-04, 2.0682e-03, 5.9110e-03, 1.9393e-03, 8.9210e-02, 8.8910e-02,
        4.0378e-03, 4.2529e-02, 7.6350e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,385][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0541, 0.0678, 0.0959, 0.0605, 0.0406, 0.0485, 0.0556, 0.0484, 0.0943,
        0.0706, 0.0444, 0.0926, 0.0863, 0.0790, 0.0616], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,387][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.1743, 0.0491, 0.0253, 0.0800, 0.1400, 0.0265, 0.0271, 0.0210, 0.0270,
        0.2048, 0.0580, 0.0277, 0.0211, 0.0725, 0.0456], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,389][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.1991, 0.0314, 0.0308, 0.0953, 0.0223, 0.0274, 0.0305, 0.0903, 0.0298,
        0.0245, 0.0629, 0.0637, 0.1322, 0.1000, 0.0596], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,391][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0382, 0.0314, 0.0205, 0.1294, 0.2498, 0.0801, 0.0184, 0.1640, 0.0206,
        0.1051, 0.0756, 0.0083, 0.0192, 0.0204, 0.0190], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,392][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ same] are: tensor([2.1763e-01, 2.5580e-03, 1.4710e-04, 1.4593e-03, 6.1736e-04, 4.7025e-04,
        1.3053e-02, 3.1991e-02, 1.0349e-02, 8.1120e-03, 2.5235e-01, 4.9420e-02,
        5.6258e-02, 4.8396e-02, 3.0719e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,394][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.1538, 0.0104, 0.0030, 0.0032, 0.0018, 0.0023, 0.0395, 0.0946, 0.0649,
        0.0255, 0.1039, 0.1524, 0.1230, 0.0970, 0.1246], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,397][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0553, 0.0812, 0.0669, 0.0614, 0.0548, 0.0543, 0.0746, 0.0691, 0.0693,
        0.0702, 0.0632, 0.0605, 0.0684, 0.0685, 0.0823], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,399][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.2594, 0.0097, 0.0004, 0.0012, 0.0008, 0.0005, 0.0346, 0.0147, 0.0139,
        0.0043, 0.1094, 0.0232, 0.0149, 0.0781, 0.4347], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,400][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0196, 0.0061, 0.0231, 0.0788, 0.0057, 0.0075, 0.0028, 0.0072, 0.0094,
        0.0122, 0.0029, 0.0082, 0.0596, 0.1860, 0.0342, 0.5367],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,403][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0179, 0.0329, 0.0280, 0.0117, 0.0212, 0.0222, 0.0256, 0.0255, 0.1275,
        0.1767, 0.0170, 0.1445, 0.1324, 0.0953, 0.0501, 0.0715],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,404][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ as] are: tensor([1.0070e-02, 1.2035e-04, 8.9632e-05, 1.4572e-04, 6.0686e-05, 1.0661e-04,
        6.1232e-04, 1.0152e-02, 1.9695e-03, 1.0662e-02, 9.4817e-02, 1.5012e-02,
        8.0923e-03, 1.5719e-01, 5.8253e-01, 1.0837e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,405][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ as] are: tensor([2.7719e-06, 8.6275e-07, 3.0007e-07, 1.7544e-07, 1.0838e-07, 7.0153e-08,
        1.5032e-05, 1.0534e-05, 7.4943e-05, 5.2769e-06, 7.7762e-04, 1.0988e-03,
        4.8848e-05, 1.0828e-03, 9.9364e-01, 3.2391e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,407][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0510, 0.0569, 0.0660, 0.0474, 0.0268, 0.0746, 0.0493, 0.0385, 0.0923,
        0.0778, 0.0314, 0.0895, 0.0903, 0.0761, 0.0604, 0.0716],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,409][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.1016, 0.0536, 0.0683, 0.0752, 0.0675, 0.0261, 0.0676, 0.0233, 0.0582,
        0.1606, 0.0343, 0.0661, 0.0184, 0.0581, 0.0373, 0.0838],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,411][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.2065, 0.0278, 0.0347, 0.0427, 0.0181, 0.0265, 0.0317, 0.0998, 0.0257,
        0.0281, 0.0727, 0.0587, 0.0980, 0.0864, 0.0717, 0.0708],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,414][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0386, 0.0354, 0.0264, 0.1086, 0.2460, 0.0921, 0.0168, 0.1612, 0.0202,
        0.0969, 0.0702, 0.0073, 0.0125, 0.0147, 0.0233, 0.0297],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,415][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ as] are: tensor([3.0944e-02, 7.3958e-04, 7.0275e-05, 5.3998e-04, 1.3971e-04, 2.2151e-04,
        1.2282e-03, 4.5469e-03, 2.2709e-03, 2.4427e-03, 2.2081e-02, 1.0366e-02,
        1.2029e-02, 3.0196e-02, 6.9306e-01, 1.8913e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,416][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ as] are: tensor([2.4735e-02, 6.8793e-04, 9.4380e-04, 3.8549e-04, 4.0491e-04, 2.9065e-04,
        1.6559e-03, 5.2384e-03, 5.7813e-03, 3.0234e-03, 2.5580e-02, 1.9106e-02,
        1.2597e-02, 2.0071e-02, 8.6909e-01, 1.0405e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,418][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0532, 0.0766, 0.0618, 0.0562, 0.0512, 0.0499, 0.0701, 0.0631, 0.0645,
        0.0648, 0.0585, 0.0566, 0.0640, 0.0645, 0.0771, 0.0680],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,420][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ as] are: tensor([6.4407e-02, 2.2421e-03, 2.6612e-04, 4.6183e-04, 1.6177e-04, 8.1383e-04,
        1.4797e-03, 1.6822e-02, 7.0919e-03, 5.3821e-03, 9.6327e-02, 1.8760e-02,
        4.4572e-03, 3.0450e-02, 7.1084e-01, 4.0034e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,422][circuit_model.py][line:1532][INFO] ##3-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0311, 0.0069, 0.0112, 0.0732, 0.0063, 0.0057, 0.0015, 0.0045, 0.0095,
        0.0176, 0.0014, 0.0087, 0.0589, 0.2520, 0.0209, 0.1285, 0.3621],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,423][circuit_model.py][line:1535][INFO] ##3-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0171, 0.0394, 0.1087, 0.0135, 0.0294, 0.0208, 0.0425, 0.0339, 0.0936,
        0.1531, 0.0292, 0.1040, 0.0577, 0.0585, 0.0546, 0.0842, 0.0598],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,424][circuit_model.py][line:1538][INFO] ##3-th layer ##Weight##: The head3 weight for token [ the] are: tensor([3.6021e-02, 7.6022e-05, 5.0306e-05, 8.7046e-05, 2.8644e-05, 6.0373e-05,
        6.7465e-04, 3.0500e-03, 2.4205e-03, 2.5450e-03, 2.5644e-02, 1.0449e-02,
        2.8697e-03, 8.5502e-03, 1.4121e-01, 3.6286e-01, 4.0341e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,425][circuit_model.py][line:1541][INFO] ##3-th layer ##Weight##: The head4 weight for token [ the] are: tensor([2.8993e-04, 4.9666e-05, 5.2101e-06, 2.2694e-06, 2.0497e-06, 4.4039e-06,
        4.1599e-04, 5.1237e-04, 1.6735e-03, 2.9389e-04, 1.6892e-02, 1.5817e-02,
        4.6596e-04, 2.9689e-03, 5.4972e-01, 2.0101e-01, 2.0988e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,426][circuit_model.py][line:1544][INFO] ##3-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0460, 0.0479, 0.0650, 0.0543, 0.0338, 0.0573, 0.0441, 0.0323, 0.0777,
        0.0653, 0.0394, 0.0761, 0.0790, 0.0780, 0.0586, 0.0696, 0.0757],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,428][circuit_model.py][line:1547][INFO] ##3-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0270, 0.0229, 0.2212, 0.0309, 0.0999, 0.0139, 0.1521, 0.0364, 0.0492,
        0.2089, 0.0138, 0.0455, 0.0084, 0.0034, 0.0225, 0.0407, 0.0035],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,430][circuit_model.py][line:1550][INFO] ##3-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.1526, 0.0220, 0.0319, 0.0398, 0.0158, 0.0273, 0.0323, 0.1186, 0.0271,
        0.0234, 0.0618, 0.0558, 0.0810, 0.0617, 0.0654, 0.0694, 0.1142],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,432][circuit_model.py][line:1553][INFO] ##3-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0165, 0.0316, 0.0242, 0.1131, 0.2523, 0.0863, 0.0153, 0.1598, 0.0209,
        0.0963, 0.0585, 0.0063, 0.0108, 0.0120, 0.0213, 0.0289, 0.0462],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,433][circuit_model.py][line:1556][INFO] ##3-th layer ##Weight##: The head9 weight for token [ the] are: tensor([4.1074e-02, 2.2004e-04, 5.4757e-05, 1.3655e-04, 4.7325e-05, 8.9117e-05,
        5.8790e-04, 1.3431e-03, 6.7074e-04, 6.7101e-04, 1.4168e-02, 2.4578e-03,
        5.0328e-03, 5.5471e-03, 3.4397e-01, 4.0566e-01, 1.7827e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,436][circuit_model.py][line:1559][INFO] ##3-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1742, 0.0054, 0.0011, 0.0026, 0.0029, 0.0035, 0.0165, 0.0296, 0.0132,
        0.0223, 0.0759, 0.0298, 0.0320, 0.0274, 0.1910, 0.1105, 0.2621],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,438][circuit_model.py][line:1562][INFO] ##3-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0497, 0.0715, 0.0577, 0.0525, 0.0480, 0.0471, 0.0657, 0.0593, 0.0608,
        0.0610, 0.0552, 0.0531, 0.0600, 0.0607, 0.0727, 0.0643, 0.0608],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,439][circuit_model.py][line:1565][INFO] ##3-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.6929e-01, 2.6904e-03, 3.0024e-04, 6.0969e-04, 1.7147e-04, 1.3495e-03,
        1.6257e-03, 1.2100e-02, 5.6201e-03, 6.4707e-03, 6.7228e-02, 8.9944e-03,
        2.3465e-03, 7.5611e-03, 5.0747e-01, 1.2804e-01, 7.8137e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,490][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:26,492][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,494][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,495][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,496][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,497][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,497][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,498][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,499][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,499][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,500][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,501][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,501][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,502][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.0082, 0.9918], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,503][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.9222, 0.0778], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,505][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.6714, 0.3286], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,506][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.0153, 0.9847], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,508][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.9664, 0.0336], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,509][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.9178, 0.0822], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,509][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.5518, 0.4482], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,510][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.0600, 0.9400], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,511][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.7582, 0.2418], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,512][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.5811, 0.4189], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,512][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.3081, 0.6919], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,514][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.7587, 0.2413], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,516][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0378, 0.1202, 0.8420], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,518][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.6252, 0.2343, 0.1405], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,519][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.1112, 0.6885, 0.2003], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,521][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([6.3453e-04, 9.7415e-01, 2.5213e-02], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,522][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.8103, 0.1881, 0.0016], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,524][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.6422, 0.2264, 0.1314], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,526][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.2337, 0.5566, 0.2097], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,528][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0068, 0.9545, 0.0387], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,530][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.2132, 0.7677, 0.0192], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,531][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.1967, 0.6891, 0.1142], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,533][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.1593, 0.4694, 0.3713], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,535][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.3291, 0.6163, 0.0545], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,537][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0275, 0.6671, 0.0713, 0.2341], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,539][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.4868, 0.2138, 0.1771, 0.1223], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,541][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.1494, 0.5893, 0.2418, 0.0194], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,542][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0012, 0.7497, 0.2375, 0.0116], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,545][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.8139, 0.0673, 0.0886, 0.0302], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,546][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.6768, 0.2012, 0.0678, 0.0542], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,548][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.2052, 0.4277, 0.3198, 0.0473], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,550][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0036, 0.5474, 0.4299, 0.0191], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,552][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.2314, 0.5567, 0.1471, 0.0648], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,553][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.3620, 0.4864, 0.0679, 0.0837], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,556][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0666, 0.2093, 0.1936, 0.5305], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,557][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.1509, 0.7269, 0.1052, 0.0170], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,559][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0429, 0.0922, 0.0783, 0.0857, 0.7010], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,561][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.2210, 0.3314, 0.2536, 0.1303, 0.0638], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,563][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.0826, 0.4881, 0.2441, 0.1467, 0.0385], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,565][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.0014, 0.7595, 0.1715, 0.0561, 0.0114], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,567][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.6755, 0.1796, 0.0406, 0.0222, 0.0822], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,569][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.4523, 0.2588, 0.1326, 0.0961, 0.0603], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,570][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.1472, 0.3478, 0.3071, 0.1010, 0.0969], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,572][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0026, 0.7414, 0.2057, 0.0476, 0.0028], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,574][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.1024, 0.5557, 0.1184, 0.1537, 0.0698], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,576][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.3140, 0.1673, 0.1232, 0.2131, 0.1824], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,577][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.0482, 0.1443, 0.1177, 0.2204, 0.4695], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,577][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.3122, 0.4518, 0.1648, 0.0604, 0.0108], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:26,578][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0447, 0.2966, 0.0728, 0.1721, 0.0277, 0.3862], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,579][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.1452, 0.1620, 0.1702, 0.1791, 0.1133, 0.2303], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,580][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.1619, 0.2074, 0.1019, 0.1121, 0.2854, 0.1313], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,582][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.0026, 0.5943, 0.1043, 0.0521, 0.1687, 0.0780], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,584][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.3835, 0.3571, 0.0251, 0.0442, 0.1888, 0.0013], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,585][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.5286, 0.1309, 0.0801, 0.0738, 0.1126, 0.0740], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,587][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.2287, 0.1573, 0.1378, 0.1045, 0.1925, 0.1792], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,589][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0152, 0.7068, 0.1816, 0.0572, 0.0252, 0.0140], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,591][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.1383, 0.1395, 0.1706, 0.1108, 0.3526, 0.0883], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,592][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.1645, 0.0639, 0.0520, 0.0764, 0.2969, 0.3463], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,595][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.0244, 0.0997, 0.0809, 0.1617, 0.3405, 0.2928], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,596][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.3042, 0.3279, 0.0972, 0.0840, 0.0859, 0.1007], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:26,598][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.0169, 0.1485, 0.1435, 0.2465, 0.0387, 0.0107, 0.3951],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,600][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.3752, 0.1841, 0.1327, 0.0594, 0.0192, 0.0297, 0.1997],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,602][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.1520, 0.1393, 0.0279, 0.0964, 0.0680, 0.0946, 0.4219],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,604][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.0166, 0.5479, 0.0553, 0.0293, 0.0306, 0.0630, 0.2573],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,606][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.2530, 0.0398, 0.0385, 0.0380, 0.0939, 0.5359, 0.0010],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,608][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.4113, 0.1650, 0.0991, 0.0694, 0.0816, 0.1446, 0.0289],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,609][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.4144, 0.1149, 0.0548, 0.0477, 0.0498, 0.0734, 0.2450],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,612][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0185, 0.8510, 0.0484, 0.0209, 0.0037, 0.0150, 0.0425],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,613][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.2597, 0.1343, 0.0199, 0.0815, 0.0297, 0.0721, 0.4027],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,615][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.2214, 0.2289, 0.0630, 0.0913, 0.1468, 0.0494, 0.1991],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,617][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.0307, 0.0821, 0.0677, 0.1265, 0.2689, 0.2188, 0.2054],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,619][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.2140, 0.2384, 0.0636, 0.0451, 0.0133, 0.1598, 0.2656],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:26,621][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.0973, 0.3473, 0.1331, 0.2467, 0.0232, 0.0383, 0.0311, 0.0829],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,623][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.2002, 0.1207, 0.0583, 0.0298, 0.0223, 0.0449, 0.1816, 0.3422],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,625][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.0991, 0.0222, 0.0103, 0.0210, 0.0138, 0.0182, 0.6401, 0.1754],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,626][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.0013, 0.0134, 0.0019, 0.0028, 0.0033, 0.0034, 0.9219, 0.0519],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,628][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.3793, 0.2535, 0.0393, 0.0483, 0.1951, 0.0776, 0.0047, 0.0022],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,630][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.6971, 0.1115, 0.0370, 0.0567, 0.0309, 0.0250, 0.0204, 0.0214],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,632][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.2545, 0.0148, 0.0229, 0.0117, 0.0180, 0.0327, 0.4546, 0.1908],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,634][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.0293, 0.0855, 0.0199, 0.0112, 0.0030, 0.0324, 0.7692, 0.0495],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,636][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.2457, 0.0366, 0.0301, 0.0212, 0.0214, 0.0215, 0.4806, 0.1430],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,637][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.2164, 0.0278, 0.0192, 0.0413, 0.0979, 0.1002, 0.2756, 0.2218],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,640][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.0177, 0.0577, 0.0514, 0.0890, 0.2014, 0.1605, 0.1249, 0.2975],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,641][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.4223, 0.0707, 0.0366, 0.0338, 0.0232, 0.0868, 0.1279, 0.1986],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:26,643][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0241, 0.1695, 0.0782, 0.3693, 0.0209, 0.0238, 0.0290, 0.0095, 0.2758],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,644][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.1807, 0.0350, 0.0238, 0.0183, 0.0167, 0.0204, 0.2083, 0.2658, 0.2308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,645][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0369, 0.0212, 0.0020, 0.0067, 0.0141, 0.0083, 0.1048, 0.6760, 0.1300],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,646][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([1.7722e-04, 3.4749e-03, 4.4166e-04, 3.1144e-04, 7.7506e-04, 6.3378e-03,
        1.2449e-01, 8.4271e-01, 2.1285e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,646][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([1.2957e-01, 9.9195e-03, 9.6801e-02, 1.2698e-02, 2.7406e-02, 6.1839e-01,
        9.7694e-03, 9.4911e-02, 5.3844e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,647][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.4847, 0.0816, 0.0556, 0.0424, 0.0501, 0.0682, 0.0216, 0.1631, 0.0328],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,650][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.1072, 0.0110, 0.0060, 0.0081, 0.0186, 0.0342, 0.0825, 0.5055, 0.2270],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,651][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0035, 0.0555, 0.0052, 0.0044, 0.0021, 0.0121, 0.3364, 0.3598, 0.2211],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,653][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.1527, 0.0267, 0.0037, 0.0135, 0.0076, 0.0111, 0.2784, 0.3951, 0.1112],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,655][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.1237, 0.0153, 0.0082, 0.0144, 0.0278, 0.0258, 0.2393, 0.2136, 0.3319],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,657][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0189, 0.0541, 0.0468, 0.0895, 0.1822, 0.1338, 0.1118, 0.2774, 0.0855],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,659][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.1504, 0.0526, 0.0187, 0.0203, 0.0048, 0.0559, 0.2468, 0.2907, 0.1596],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:26,661][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0422, 0.3838, 0.0540, 0.2407, 0.0622, 0.0097, 0.0619, 0.0020, 0.0653,
        0.0783], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,663][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.2537, 0.0325, 0.0175, 0.0201, 0.0084, 0.0162, 0.1642, 0.2397, 0.2088,
        0.0390], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,665][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.0065, 0.0029, 0.0023, 0.0006, 0.0018, 0.0048, 0.0692, 0.5774, 0.2429,
        0.0916], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,666][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([4.2830e-04, 9.2781e-03, 8.9101e-04, 1.0954e-03, 5.9488e-04, 2.5192e-03,
        1.6828e-01, 2.1415e-01, 5.8031e-01, 2.2446e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,668][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.3160, 0.0840, 0.0819, 0.0139, 0.1613, 0.0315, 0.0944, 0.0602, 0.1491,
        0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,670][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.5576, 0.1212, 0.0384, 0.0794, 0.0396, 0.0508, 0.0246, 0.0400, 0.0382,
        0.0102], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,672][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0663, 0.0083, 0.0054, 0.0045, 0.0077, 0.0152, 0.0483, 0.4218, 0.3979,
        0.0246], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,674][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0012, 0.0086, 0.0017, 0.0010, 0.0006, 0.0017, 0.1017, 0.3012, 0.5688,
        0.0135], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,676][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.0538, 0.0084, 0.0034, 0.0060, 0.0132, 0.0092, 0.1831, 0.2614, 0.4194,
        0.0420], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,678][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.1328, 0.0038, 0.0023, 0.0100, 0.0170, 0.0777, 0.2367, 0.2799, 0.1901,
        0.0498], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,680][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.0104, 0.0447, 0.0364, 0.0723, 0.1512, 0.1067, 0.1053, 0.2378, 0.0763,
        0.1590], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,682][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.2661, 0.0161, 0.0034, 0.0085, 0.0031, 0.0342, 0.0300, 0.4556, 0.0952,
        0.0877], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:26,684][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0845, 0.1340, 0.1857, 0.2548, 0.0542, 0.0112, 0.0416, 0.0040, 0.0899,
        0.0049, 0.1352], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,686][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.2708, 0.0393, 0.0142, 0.0123, 0.0050, 0.0106, 0.1505, 0.2134, 0.1167,
        0.1070, 0.0602], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,688][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.0758, 0.0014, 0.0008, 0.0009, 0.0007, 0.0008, 0.0269, 0.0738, 0.1323,
        0.3566, 0.3300], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,689][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([1.5639e-03, 6.6471e-04, 2.4156e-04, 1.9403e-04, 6.4588e-05, 6.4915e-04,
        1.3001e-01, 6.2785e-02, 1.4952e-01, 9.2446e-02, 5.6185e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,691][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.3276, 0.0429, 0.0646, 0.0145, 0.1288, 0.2411, 0.0221, 0.0647, 0.0719,
        0.0148, 0.0069], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,693][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.8066, 0.0352, 0.0251, 0.0365, 0.0172, 0.0106, 0.0107, 0.0271, 0.0137,
        0.0109, 0.0066], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,695][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.2939, 0.0084, 0.0107, 0.0033, 0.0031, 0.0042, 0.0762, 0.1212, 0.2194,
        0.0433, 0.2162], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,696][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([2.1653e-02, 8.6591e-03, 1.8998e-03, 1.0053e-03, 1.0333e-04, 1.4962e-03,
        1.1984e-01, 7.3897e-02, 4.3423e-01, 1.5743e-01, 1.7978e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,698][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1104, 0.0029, 0.0010, 0.0020, 0.0021, 0.0050, 0.0649, 0.1065, 0.1152,
        0.0533, 0.5367], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,701][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.2381, 0.0045, 0.0021, 0.0119, 0.0109, 0.0140, 0.0526, 0.0982, 0.1003,
        0.1332, 0.3342], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,703][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.0111, 0.0421, 0.0323, 0.0433, 0.1153, 0.1042, 0.0733, 0.2234, 0.0829,
        0.1256, 0.1466], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,704][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.2950, 0.0194, 0.0051, 0.0069, 0.0014, 0.0100, 0.0431, 0.0941, 0.0803,
        0.2029, 0.2419], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:26,707][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0352, 0.1593, 0.0709, 0.3976, 0.0203, 0.0237, 0.0370, 0.0136, 0.1748,
        0.0107, 0.0147, 0.0423], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,708][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.2529, 0.0164, 0.0100, 0.0059, 0.0043, 0.0056, 0.1014, 0.0635, 0.0797,
        0.0735, 0.0510, 0.3359], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,710][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([4.9721e-03, 7.1830e-04, 7.1107e-05, 2.3055e-04, 3.1100e-04, 1.3546e-04,
        4.4074e-03, 2.8525e-02, 7.8992e-03, 5.0738e-02, 7.6471e-01, 1.3728e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,710][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([9.4075e-05, 2.6211e-04, 3.4477e-05, 2.0456e-05, 2.3507e-05, 1.8646e-04,
        1.1448e-02, 4.8614e-02, 3.4147e-03, 8.3332e-02, 6.5394e-01, 1.9863e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,711][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([1.2181e-01, 1.0558e-02, 8.8191e-02, 1.1393e-02, 2.5538e-02, 5.8253e-01,
        9.8954e-03, 1.0209e-01, 4.6730e-04, 3.8189e-02, 8.9208e-03, 4.2007e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,712][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.6328, 0.0548, 0.0369, 0.0349, 0.0314, 0.0358, 0.0125, 0.0892, 0.0203,
        0.0119, 0.0267, 0.0128], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,713][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([3.5549e-02, 8.3008e-04, 3.9077e-04, 5.4549e-04, 9.1351e-04, 2.0282e-03,
        7.7673e-03, 5.6977e-02, 2.9339e-02, 1.2970e-02, 6.2262e-01, 2.3007e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,714][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([5.0757e-04, 1.6562e-03, 1.6652e-04, 1.0679e-04, 2.3463e-05, 2.7841e-04,
        1.6089e-02, 1.6703e-02, 1.2021e-02, 1.2528e-02, 4.4410e-01, 4.9582e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,716][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([4.2968e-02, 1.7818e-03, 2.0189e-04, 7.9363e-04, 3.2874e-04, 5.6310e-04,
        2.5375e-02, 2.8325e-02, 1.0219e-02, 3.3682e-02, 6.9592e-01, 1.5985e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,718][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0527, 0.0021, 0.0013, 0.0022, 0.0024, 0.0023, 0.0455, 0.0308, 0.0884,
        0.0715, 0.3142, 0.3868], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,719][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0125, 0.0369, 0.0344, 0.0501, 0.1154, 0.0871, 0.0722, 0.2068, 0.0749,
        0.1245, 0.1197, 0.0656], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,721][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0868, 0.0095, 0.0032, 0.0034, 0.0006, 0.0062, 0.0454, 0.0595, 0.0327,
        0.0899, 0.5011, 0.1617], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:26,723][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0293, 0.3903, 0.0457, 0.2899, 0.0386, 0.0107, 0.0310, 0.0032, 0.0795,
        0.0125, 0.0050, 0.0195, 0.0446], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,725][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.3214, 0.0084, 0.0106, 0.0056, 0.0047, 0.0028, 0.0613, 0.1692, 0.0449,
        0.0932, 0.0876, 0.1497, 0.0407], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,726][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([9.5849e-03, 1.1913e-03, 7.7981e-04, 5.7361e-04, 5.1435e-04, 3.5965e-04,
        7.5980e-03, 3.0779e-02, 2.7741e-02, 4.0139e-02, 5.6956e-01, 3.0876e-01,
        2.4240e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,728][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([2.0341e-04, 7.6650e-04, 1.4271e-04, 9.2519e-05, 3.2649e-05, 8.8328e-05,
        2.0394e-03, 1.1900e-02, 2.1541e-02, 6.0820e-03, 3.9089e-01, 5.6318e-01,
        3.0406e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,730][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.1834, 0.0062, 0.0033, 0.0018, 0.0285, 0.0132, 0.0045, 0.0165, 0.0467,
        0.0479, 0.0104, 0.0478, 0.5898], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,732][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.7367, 0.0521, 0.0173, 0.0369, 0.0197, 0.0191, 0.0103, 0.0288, 0.0195,
        0.0068, 0.0225, 0.0105, 0.0197], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,734][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0419, 0.0048, 0.0033, 0.0010, 0.0024, 0.0021, 0.0269, 0.0489, 0.0589,
        0.0306, 0.2517, 0.5046, 0.0230], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,736][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([7.0427e-04, 9.0361e-04, 3.0252e-04, 8.8937e-05, 2.9807e-05, 1.5046e-04,
        5.6804e-03, 8.2663e-03, 2.7663e-02, 6.5202e-03, 2.0227e-01, 7.4165e-01,
        5.7705e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,737][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0425, 0.0038, 0.0016, 0.0023, 0.0008, 0.0020, 0.0172, 0.0752, 0.0222,
        0.0778, 0.5492, 0.1779, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,740][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.1780, 0.0072, 0.0056, 0.0061, 0.0026, 0.0073, 0.0695, 0.0456, 0.0717,
        0.0592, 0.3125, 0.1888, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,742][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0029, 0.0174, 0.0181, 0.0314, 0.0976, 0.0750, 0.0763, 0.1934, 0.0635,
        0.1255, 0.2015, 0.0886, 0.0089], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,743][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.1635, 0.0272, 0.0039, 0.0028, 0.0009, 0.0056, 0.0132, 0.0396, 0.0516,
        0.0409, 0.5219, 0.1181, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:26,746][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0297, 0.2927, 0.0415, 0.4162, 0.0365, 0.0122, 0.0278, 0.0048, 0.0803,
        0.0093, 0.0040, 0.0208, 0.0131, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,748][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.3221, 0.0085, 0.0055, 0.0045, 0.0023, 0.0022, 0.0271, 0.0569, 0.0384,
        0.0424, 0.0500, 0.0888, 0.0586, 0.2927], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,749][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([3.2166e-02, 9.6990e-04, 5.7085e-04, 7.9140e-04, 2.6176e-04, 7.0284e-04,
        8.2205e-03, 4.4421e-02, 2.9135e-02, 4.4575e-02, 4.4315e-01, 2.2698e-01,
        4.3882e-02, 1.2418e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,750][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([4.0795e-04, 6.3204e-04, 9.9201e-05, 2.4913e-05, 2.5267e-05, 6.3218e-05,
        6.9647e-03, 1.0431e-02, 3.1462e-02, 6.5792e-03, 3.2806e-01, 5.5552e-01,
        1.0257e-02, 4.9471e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,752][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.3288, 0.0142, 0.0330, 0.0125, 0.0403, 0.0223, 0.0060, 0.0046, 0.0260,
        0.0690, 0.0142, 0.0263, 0.1857, 0.2170], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,754][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.7827, 0.0329, 0.0158, 0.0255, 0.0117, 0.0093, 0.0067, 0.0182, 0.0114,
        0.0046, 0.0129, 0.0055, 0.0183, 0.0444], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,757][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.1626, 0.0029, 0.0042, 0.0012, 0.0013, 0.0013, 0.0229, 0.0369, 0.0585,
        0.0288, 0.2259, 0.3255, 0.0588, 0.0692], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,758][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([5.5343e-03, 1.8003e-03, 5.7211e-04, 1.4900e-04, 3.4223e-05, 8.6409e-05,
        9.0133e-03, 5.4645e-03, 4.0596e-02, 1.1943e-02, 1.0725e-01, 6.1914e-01,
        3.6348e-02, 1.6207e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,759][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([8.3603e-02, 1.9423e-03, 5.5605e-04, 1.0317e-03, 4.3415e-04, 1.3664e-03,
        7.8313e-03, 2.9622e-02, 1.6510e-02, 2.0798e-02, 5.1009e-01, 1.2582e-01,
        8.5752e-02, 1.1464e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,761][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1119, 0.0216, 0.0038, 0.0070, 0.0060, 0.0091, 0.0535, 0.0881, 0.0378,
        0.0943, 0.2562, 0.1093, 0.1147, 0.0867], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,763][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0058, 0.0248, 0.0287, 0.0365, 0.0972, 0.0885, 0.0570, 0.2100, 0.0709,
        0.1057, 0.1501, 0.0972, 0.0090, 0.0186], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,765][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.1864, 0.0196, 0.0023, 0.0029, 0.0009, 0.0082, 0.0108, 0.0829, 0.0286,
        0.0518, 0.4752, 0.0665, 0.0184, 0.0456], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:26,768][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.0337, 0.2668, 0.0792, 0.2410, 0.0642, 0.0238, 0.0371, 0.0101, 0.0951,
        0.0223, 0.0201, 0.0316, 0.0250, 0.0115, 0.0386], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,769][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([5.3400e-01, 3.8861e-03, 1.2348e-03, 1.9024e-03, 4.6807e-04, 3.6058e-04,
        5.0506e-03, 1.6448e-02, 1.1143e-02, 8.3844e-03, 6.4972e-03, 2.3363e-02,
        2.7865e-02, 2.0946e-01, 1.4994e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,770][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([1.9059e-02, 2.8439e-04, 4.4484e-04, 6.5211e-04, 5.5866e-05, 8.7923e-05,
        2.0098e-03, 8.9281e-03, 1.3152e-02, 8.6656e-03, 1.0456e-01, 7.5614e-02,
        1.7615e-02, 3.4472e-01, 4.0415e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,771][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([7.3312e-04, 5.4303e-04, 1.8207e-05, 1.4510e-05, 6.6430e-06, 3.2852e-06,
        5.7150e-04, 2.0682e-03, 5.9110e-03, 1.9393e-03, 8.9210e-02, 8.8910e-02,
        4.0378e-03, 4.2529e-02, 7.6350e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,774][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.2754, 0.0469, 0.1260, 0.0108, 0.0783, 0.0032, 0.0151, 0.0497, 0.0405,
        0.0575, 0.0334, 0.0409, 0.0934, 0.0596, 0.0693], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,776][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.7627, 0.0262, 0.0093, 0.0232, 0.0193, 0.0052, 0.0054, 0.0150, 0.0120,
        0.0045, 0.0150, 0.0052, 0.0204, 0.0475, 0.0292], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,777][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([8.1540e-02, 2.0486e-03, 9.9660e-04, 4.3373e-04, 2.3533e-04, 1.2861e-04,
        8.3645e-03, 4.4152e-03, 1.1396e-02, 4.7659e-03, 2.0746e-02, 4.4444e-02,
        3.0908e-02, 5.8680e-02, 7.3090e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,778][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([3.5906e-03, 6.3185e-04, 1.3266e-04, 5.9289e-05, 1.2987e-05, 6.1633e-06,
        9.6225e-04, 4.3679e-04, 4.5291e-03, 1.5852e-03, 1.6598e-02, 6.7358e-02,
        1.4536e-02, 7.8095e-02, 8.1147e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,779][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([1.3082e-01, 1.0314e-03, 5.1107e-05, 5.5742e-04, 3.2213e-04, 3.3866e-04,
        7.4138e-03, 3.1356e-02, 9.8462e-03, 7.4025e-03, 3.1174e-01, 5.1225e-02,
        3.3951e-02, 4.0924e-02, 3.7303e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,779][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.1538, 0.0104, 0.0030, 0.0032, 0.0018, 0.0023, 0.0395, 0.0946, 0.0649,
        0.0255, 0.1039, 0.1524, 0.1230, 0.0970, 0.1246], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,780][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.0078, 0.0395, 0.0329, 0.0475, 0.1036, 0.0807, 0.0758, 0.1727, 0.0687,
        0.1160, 0.1287, 0.0824, 0.0087, 0.0091, 0.0258], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,782][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.2594, 0.0097, 0.0004, 0.0012, 0.0008, 0.0005, 0.0346, 0.0147, 0.0139,
        0.0043, 0.1094, 0.0232, 0.0149, 0.0781, 0.4347], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:26,784][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.0813, 0.2227, 0.0997, 0.3422, 0.0134, 0.0121, 0.0656, 0.0078, 0.0806,
        0.0068, 0.0039, 0.0199, 0.0159, 0.0102, 0.0061, 0.0118],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,785][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.4014, 0.0015, 0.0007, 0.0020, 0.0008, 0.0007, 0.0083, 0.0163, 0.0133,
        0.0134, 0.0102, 0.0304, 0.0307, 0.1522, 0.1794, 0.1388],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,787][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([1.0070e-02, 1.2035e-04, 8.9632e-05, 1.4572e-04, 6.0686e-05, 1.0661e-04,
        6.1232e-04, 1.0152e-02, 1.9695e-03, 1.0662e-02, 9.4817e-02, 1.5012e-02,
        8.0923e-03, 1.5719e-01, 5.8253e-01, 1.0837e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,788][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([2.7719e-06, 8.6275e-07, 3.0007e-07, 1.7544e-07, 1.0838e-07, 7.0153e-08,
        1.5032e-05, 1.0534e-05, 7.4943e-05, 5.2769e-06, 7.7762e-04, 1.0988e-03,
        4.8848e-05, 1.0828e-03, 9.9364e-01, 3.2391e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,790][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.2364, 0.0188, 0.0062, 0.0029, 0.0075, 0.0450, 0.0070, 0.0072, 0.0398,
        0.1344, 0.0018, 0.0396, 0.2563, 0.0645, 0.0744, 0.0581],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,792][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.8166, 0.0158, 0.0052, 0.0149, 0.0068, 0.0057, 0.0024, 0.0119, 0.0049,
        0.0023, 0.0054, 0.0021, 0.0131, 0.0400, 0.0285, 0.0243],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,793][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([8.4814e-02, 9.1251e-04, 5.5515e-04, 3.2957e-04, 4.2862e-04, 1.6834e-04,
        6.1863e-03, 6.1446e-03, 1.0494e-02, 7.2153e-03, 5.6372e-02, 6.1059e-02,
        2.0159e-02, 6.1714e-02, 6.1657e-01, 6.6879e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,795][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([9.9270e-05, 3.2161e-05, 6.9701e-06, 2.7340e-06, 4.5403e-07, 1.6118e-06,
        9.0521e-05, 7.3008e-05, 3.4042e-04, 1.3046e-04, 3.4131e-03, 5.5216e-03,
        8.7143e-04, 1.0133e-02, 9.5488e-01, 2.4404e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,796][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([2.1347e-02, 2.8058e-04, 2.6726e-05, 2.2426e-04, 7.5692e-05, 1.6616e-04,
        7.2222e-04, 4.1788e-03, 2.2255e-03, 2.2630e-03, 2.7022e-02, 1.1712e-02,
        7.3654e-03, 2.5919e-02, 7.4535e-01, 1.5112e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,798][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([2.4735e-02, 6.8793e-04, 9.4380e-04, 3.8549e-04, 4.0491e-04, 2.9065e-04,
        1.6559e-03, 5.2384e-03, 5.7813e-03, 3.0234e-03, 2.5580e-02, 1.9106e-02,
        1.2597e-02, 2.0071e-02, 8.6909e-01, 1.0405e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,799][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.0083, 0.0356, 0.0332, 0.0392, 0.1023, 0.0791, 0.0678, 0.1721, 0.0707,
        0.0975, 0.1354, 0.0895, 0.0084, 0.0129, 0.0233, 0.0248],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,801][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([6.4407e-02, 2.2421e-03, 2.6612e-04, 4.6183e-04, 1.6177e-04, 8.1383e-04,
        1.4797e-03, 1.6822e-02, 7.0919e-03, 5.3821e-03, 9.6327e-02, 1.8760e-02,
        4.4572e-03, 3.0450e-02, 7.1084e-01, 4.0034e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:26,803][circuit_model.py][line:1570][INFO] ##3-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0352, 0.2680, 0.0403, 0.3889, 0.0349, 0.0152, 0.0335, 0.0059, 0.0994,
        0.0119, 0.0056, 0.0245, 0.0138, 0.0128, 0.0025, 0.0030, 0.0046],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,804][circuit_model.py][line:1573][INFO] ##3-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2501, 0.0015, 0.0008, 0.0011, 0.0006, 0.0006, 0.0048, 0.0076, 0.0069,
        0.0056, 0.0072, 0.0109, 0.0129, 0.0548, 0.1183, 0.1743, 0.3421],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,806][circuit_model.py][line:1576][INFO] ##3-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([3.6021e-02, 7.6022e-05, 5.0306e-05, 8.7046e-05, 2.8644e-05, 6.0373e-05,
        6.7465e-04, 3.0500e-03, 2.4205e-03, 2.5450e-03, 2.5644e-02, 1.0449e-02,
        2.8697e-03, 8.5502e-03, 1.4121e-01, 3.6286e-01, 4.0341e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,807][circuit_model.py][line:1579][INFO] ##3-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([2.8993e-04, 4.9666e-05, 5.2101e-06, 2.2694e-06, 2.0497e-06, 4.4039e-06,
        4.1599e-04, 5.1237e-04, 1.6735e-03, 2.9389e-04, 1.6892e-02, 1.5817e-02,
        4.6596e-04, 2.9689e-03, 5.4972e-01, 2.0101e-01, 2.0988e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,809][circuit_model.py][line:1582][INFO] ##3-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2449, 0.0096, 0.0240, 0.0087, 0.0271, 0.0156, 0.0039, 0.0030, 0.0179,
        0.0480, 0.0097, 0.0180, 0.1240, 0.1544, 0.0726, 0.0604, 0.1580],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,812][circuit_model.py][line:1585][INFO] ##3-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.8504, 0.0119, 0.0063, 0.0099, 0.0052, 0.0047, 0.0021, 0.0079, 0.0037,
        0.0015, 0.0045, 0.0015, 0.0080, 0.0166, 0.0171, 0.0177, 0.0310],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,813][circuit_model.py][line:1588][INFO] ##3-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([2.1382e-01, 7.1904e-04, 7.6444e-04, 3.2625e-04, 3.0505e-04, 2.7155e-04,
        4.3842e-03, 7.1209e-03, 1.2571e-02, 4.5978e-03, 3.9772e-02, 4.3606e-02,
        1.0977e-02, 1.2344e-02, 3.7798e-01, 8.7436e-02, 1.8301e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,814][circuit_model.py][line:1591][INFO] ##3-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([7.7437e-04, 3.5037e-05, 7.4028e-06, 3.6106e-06, 7.1282e-07, 1.4902e-06,
        1.5833e-04, 8.4903e-05, 5.6626e-04, 1.3850e-04, 1.4956e-03, 4.5416e-03,
        5.1966e-04, 2.7665e-03, 7.4134e-01, 6.0234e-02, 1.8733e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,816][circuit_model.py][line:1594][INFO] ##3-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([4.1119e-02, 9.2562e-05, 2.5171e-05, 7.3526e-05, 3.5054e-05, 9.9262e-05,
        4.7340e-04, 1.7751e-03, 9.2184e-04, 8.2945e-04, 2.2319e-02, 3.7975e-03,
        3.7833e-03, 5.1825e-03, 3.6250e-01, 3.7249e-01, 1.8448e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,818][circuit_model.py][line:1597][INFO] ##3-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.1742, 0.0054, 0.0011, 0.0026, 0.0029, 0.0035, 0.0165, 0.0296, 0.0132,
        0.0223, 0.0759, 0.0298, 0.0320, 0.0274, 0.1910, 0.1105, 0.2621],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,820][circuit_model.py][line:1600][INFO] ##3-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0097, 0.0312, 0.0315, 0.0401, 0.0954, 0.0836, 0.0519, 0.1734, 0.0684,
        0.0839, 0.1179, 0.0849, 0.0065, 0.0153, 0.0289, 0.0292, 0.0481],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,821][circuit_model.py][line:1603][INFO] ##3-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([1.6929e-01, 2.6904e-03, 3.0024e-04, 6.0969e-04, 1.7147e-04, 1.3495e-03,
        1.6257e-03, 1.2100e-02, 5.6201e-03, 6.4707e-03, 6.7228e-02, 8.9944e-03,
        2.3465e-03, 7.5611e-03, 5.0747e-01, 1.2804e-01, 7.8137e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:26,825][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:26,827][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 397],
        [ 869],
        [2048],
        [ 935],
        [5122],
        [2578],
        [ 204],
        [ 391],
        [ 560],
        [2991],
        [ 355],
        [ 250],
        [2143],
        [  20],
        [ 294],
        [ 416],
        [  28]], device='cuda:0')
[2024-07-23 21:05:26,829][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[ 422],
        [ 752],
        [3772],
        [1893],
        [9655],
        [4766],
        [ 920],
        [ 756],
        [1358],
        [3957],
        [ 417],
        [1151],
        [3505],
        [  45],
        [ 743],
        [ 719],
        [  40]], device='cuda:0')
[2024-07-23 21:05:26,831][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[1409],
        [1219],
        [1516],
        [1226],
        [1279],
        [1479],
        [1231],
        [1204],
        [1275],
        [1425],
        [1323],
        [1343],
        [2466],
        [1903],
        [1844],
        [1567],
        [1674]], device='cuda:0')
[2024-07-23 21:05:26,833][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[30655],
        [30606],
        [32666],
        [33506],
        [32134],
        [29933],
        [29599],
        [30855],
        [33326],
        [33342],
        [33186],
        [32806],
        [32955],
        [34302],
        [33979],
        [34123],
        [33680]], device='cuda:0')
[2024-07-23 21:05:26,835][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 9978],
        [14096],
        [16451],
        [15415],
        [12009],
        [ 4654],
        [ 4934],
        [10235],
        [27357],
        [22586],
        [ 9958],
        [15262],
        [13170],
        [12081],
        [ 7124],
        [ 9738],
        [29110]], device='cuda:0')
[2024-07-23 21:05:26,837][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 2400],
        [30121],
        [29836],
        [25564],
        [25985],
        [18337],
        [20247],
        [ 9269],
        [30547],
        [16512],
        [14252],
        [14005],
        [10368],
        [ 9696],
        [ 2741],
        [ 2491],
        [ 2405]], device='cuda:0')
[2024-07-23 21:05:26,839][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[12856],
        [16453],
        [17551],
        [18153],
        [19624],
        [19260],
        [17288],
        [17661],
        [14752],
        [14820],
        [14891],
        [14514],
        [11056],
        [12262],
        [11784],
        [11724],
        [12445]], device='cuda:0')
[2024-07-23 21:05:26,840][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[14086],
        [ 5526],
        [  331],
        [ 1970],
        [  468],
        [  352],
        [ 1145],
        [ 1984],
        [ 3785],
        [ 6152],
        [ 6176],
        [ 6074],
        [ 9344],
        [ 7205],
        [11862],
        [11648],
        [ 7400]], device='cuda:0')
[2024-07-23 21:05:26,843][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[8560],
        [6637],
        [5570],
        [5614],
        [5022],
        [4719],
        [5249],
        [5192],
        [4920],
        [4904],
        [4973],
        [5060],
        [5021],
        [4948],
        [4790],
        [4607],
        [4592]], device='cuda:0')
[2024-07-23 21:05:26,844][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[14321],
        [12437],
        [10629],
        [ 8986],
        [ 7971],
        [ 7880],
        [ 7804],
        [ 8258],
        [ 8245],
        [ 8301],
        [ 8229],
        [ 8109],
        [ 8174],
        [ 8243],
        [ 8318],
        [ 8309],
        [ 8567]], device='cuda:0')
[2024-07-23 21:05:26,846][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[14154],
        [ 7139],
        [ 1331],
        [ 1514],
        [ 1792],
        [ 5918],
        [ 7227],
        [ 9335],
        [ 9031],
        [ 6739],
        [ 6589],
        [ 5504],
        [ 5110],
        [ 7226],
        [ 8998],
        [12020],
        [14206]], device='cuda:0')
[2024-07-23 21:05:26,848][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[24105],
        [39471],
        [36548],
        [37573],
        [34689],
        [43449],
        [32363],
        [35970],
        [21093],
        [30555],
        [28625],
        [14603],
        [19402],
        [27147],
        [34519],
        [46698],
        [36123]], device='cuda:0')
[2024-07-23 21:05:26,849][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[2091],
        [2932],
        [3568],
        [3991],
        [4211],
        [4551],
        [4526],
        [4842],
        [4921],
        [4973],
        [5071],
        [5082],
        [5061],
        [5031],
        [5017],
        [4926],
        [4892]], device='cuda:0')
[2024-07-23 21:05:26,851][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[18415],
        [33344],
        [43675],
        [44409],
        [40457],
        [37028],
        [21307],
        [16264],
        [ 9524],
        [14139],
        [22906],
        [23373],
        [26471],
        [27946],
        [35523],
        [36031],
        [35758]], device='cuda:0')
[2024-07-23 21:05:26,853][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 1420],
        [12990],
        [23340],
        [24969],
        [ 7493],
        [10009],
        [ 4123],
        [ 6653],
        [ 2322],
        [ 8453],
        [ 4704],
        [ 1481],
        [ 1560],
        [ 2877],
        [ 3124],
        [  916],
        [ 5761]], device='cuda:0')
[2024-07-23 21:05:26,855][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[5132],
        [3497],
        [3632],
        [4646],
        [8866],
        [3630],
        [8717],
        [5705],
        [9026],
        [6534],
        [8472],
        [9812],
        [7102],
        [9036],
        [7468],
        [8715],
        [8971]], device='cuda:0')
[2024-07-23 21:05:26,857][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 504],
        [ 419],
        [1321],
        [2528],
        [4216],
        [4836],
        [1633],
        [3962],
        [2894],
        [3201],
        [3844],
        [2729],
        [4284],
        [3474],
        [2275],
        [3601],
        [3039]], device='cuda:0')
[2024-07-23 21:05:26,859][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[2635],
        [1065],
        [ 663],
        [ 716],
        [ 962],
        [ 996],
        [ 408],
        [ 283],
        [1246],
        [1503],
        [2370],
        [3883],
        [3567],
        [2387],
        [1561],
        [1625],
        [1979]], device='cuda:0')
[2024-07-23 21:05:26,861][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[3717],
        [4472],
        [4597],
        [5215],
        [4797],
        [3678],
        [2485],
        [ 766],
        [2667],
        [3082],
        [6984],
        [8356],
        [8206],
        [7688],
        [1319],
        [ 817],
        [1503]], device='cuda:0')
[2024-07-23 21:05:26,863][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 2982],
        [ 3048],
        [ 3900],
        [ 2819],
        [ 4298],
        [ 7412],
        [19951],
        [ 7983],
        [21292],
        [ 5753],
        [13850],
        [21163],
        [ 2321],
        [ 3715],
        [ 4841],
        [ 5949],
        [ 5874]], device='cuda:0')
[2024-07-23 21:05:26,864][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[2652],
        [ 437],
        [ 325],
        [ 222],
        [ 239],
        [ 450],
        [ 458],
        [ 303],
        [ 304],
        [ 307],
        [ 623],
        [ 411],
        [ 493],
        [ 654],
        [ 695],
        [ 781],
        [ 892]], device='cuda:0')
[2024-07-23 21:05:26,867][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 1392],
        [ 1536],
        [ 1904],
        [ 2348],
        [ 2397],
        [ 2749],
        [ 3319],
        [ 8406],
        [ 8508],
        [ 7957],
        [ 6317],
        [ 9293],
        [ 3855],
        [ 3340],
        [15910],
        [12956],
        [ 6706]], device='cuda:0')
[2024-07-23 21:05:26,869][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 815],
        [ 275],
        [ 302],
        [ 722],
        [ 464],
        [ 509],
        [ 352],
        [3290],
        [3871],
        [4579],
        [3188],
        [2491],
        [2394],
        [1380],
        [1181],
        [1597],
        [ 981]], device='cuda:0')
[2024-07-23 21:05:26,870][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 4552],
        [ 5277],
        [10272],
        [ 8660],
        [ 8701],
        [ 6166],
        [ 3547],
        [ 2327],
        [ 2855],
        [ 3561],
        [ 3199],
        [ 2461],
        [ 2780],
        [ 2166],
        [ 1641],
        [ 1168],
        [  875]], device='cuda:0')
[2024-07-23 21:05:26,872][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[3819],
        [3961],
        [4535],
        [3983],
        [2048],
        [ 774],
        [3201],
        [3189],
        [3562],
        [3343],
        [1841],
        [1625],
        [1978],
        [1880],
        [1854],
        [1069],
        [1471]], device='cuda:0')
[2024-07-23 21:05:26,874][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[5032],
        [2681],
        [2251],
        [2491],
        [2249],
        [2135],
        [2081],
        [2071],
        [2083],
        [1928],
        [1765],
        [1840],
        [1751],
        [1797],
        [1825],
        [1794],
        [1779]], device='cuda:0')
[2024-07-23 21:05:26,876][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[2209],
        [1889],
        [ 806],
        [ 789],
        [1145],
        [1088],
        [1437],
        [1525],
        [2219],
        [2185],
        [2921],
        [1839],
        [1499],
        [1507],
        [2860],
        [4534],
        [3448]], device='cuda:0')
[2024-07-23 21:05:26,878][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[14795],
        [17214],
        [17640],
        [16062],
        [15182],
        [14155],
        [15238],
        [14285],
        [ 8950],
        [ 8290],
        [ 7111],
        [ 8065],
        [10366],
        [11341],
        [14278],
        [14090],
        [15160]], device='cuda:0')
[2024-07-23 21:05:26,880][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[ 8244],
        [14671],
        [13544],
        [13058],
        [18903],
        [19992],
        [15670],
        [12662],
        [14949],
        [12872],
        [11075],
        [15079],
        [16401],
        [17591],
        [17799],
        [19541],
        [14702]], device='cuda:0')
[2024-07-23 21:05:26,882][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303],
        [25303]], device='cuda:0')
[2024-07-23 21:05:26,936][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:26,937][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,938][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,938][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,939][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,940][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,940][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,941][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,942][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,942][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,943][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,944][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,945][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:26,947][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.1440, 0.8560], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,948][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.5219, 0.4781], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,950][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.0931, 0.9069], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,951][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.1902, 0.8098], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,953][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9769, 0.0231], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,954][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.1836, 0.8164], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,956][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.4808, 0.5192], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,958][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.0805, 0.9195], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,960][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.5918, 0.4082], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,961][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.5262, 0.4738], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,964][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.1437, 0.8563], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,965][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ original] are: tensor([1.0474e-09, 1.0000e+00], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:26,966][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0312, 0.6757, 0.2931], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,969][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.1292, 0.8626, 0.0081], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,970][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0439, 0.5202, 0.4359], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,972][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0329, 0.7652, 0.2019], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,974][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.9596, 0.0198, 0.0206], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,976][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0976, 0.4394, 0.4630], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,978][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.3708, 0.5010, 0.1282], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,980][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0221, 0.2787, 0.6991], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,982][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.5178, 0.2588, 0.2234], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,983][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.4612, 0.1543, 0.3845], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,985][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.2239, 0.3169, 0.4593], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,987][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ language] are: tensor([1.2033e-13, 9.9998e-01, 1.9370e-05], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:26,988][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0107, 0.6598, 0.2788, 0.0507], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,990][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.2386, 0.2818, 0.2585, 0.2211], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,992][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0296, 0.3787, 0.3156, 0.2762], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,994][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0198, 0.6663, 0.2521, 0.0618], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,995][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.9697, 0.0120, 0.0120, 0.0064], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,996][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0468, 0.4597, 0.3248, 0.1687], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,997][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.2010, 0.3216, 0.2893, 0.1881], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,998][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0246, 0.2434, 0.5931, 0.1389], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,998][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.4435, 0.1622, 0.2546, 0.1397], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:26,999][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.2441, 0.1235, 0.4585, 0.1740], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,000][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.2306, 0.2654, 0.2755, 0.2285], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,002][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ of] are: tensor([1.3644e-13, 9.5450e-01, 4.5366e-02, 1.3184e-04], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,003][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0111, 0.2464, 0.3926, 0.2899, 0.0601], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,005][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.3248, 0.1733, 0.4023, 0.0818, 0.0178], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,007][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0202, 0.2918, 0.2429, 0.2127, 0.2323], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,009][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.0373, 0.4142, 0.3160, 0.1650, 0.0675], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,010][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.9465, 0.0158, 0.0164, 0.0091, 0.0121], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,012][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.0501, 0.1852, 0.3001, 0.3960, 0.0685], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,014][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.1490, 0.2931, 0.2191, 0.1673, 0.1715], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,016][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.0119, 0.1823, 0.4475, 0.0873, 0.2710], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,018][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.4116, 0.1361, 0.1417, 0.1636, 0.1470], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,020][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0810, 0.0852, 0.5155, 0.1773, 0.1410], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,022][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.1613, 0.2131, 0.2118, 0.1247, 0.2891], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,023][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ De] are: tensor([6.9097e-13, 9.1084e-01, 1.3428e-02, 7.5728e-02, 8.4971e-07],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,025][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0319, 0.3293, 0.2432, 0.1518, 0.1933, 0.0505], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,027][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0287, 0.1510, 0.0125, 0.6273, 0.1705, 0.0099], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,028][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0175, 0.2309, 0.1954, 0.1716, 0.1872, 0.1974], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,030][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.0957, 0.4322, 0.1739, 0.1104, 0.1221, 0.0657], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,032][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.9155, 0.0188, 0.0198, 0.0110, 0.0152, 0.0196], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,034][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.0375, 0.2221, 0.1701, 0.2588, 0.1110, 0.2004], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,036][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.1228, 0.2017, 0.1578, 0.1641, 0.2298, 0.1238], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,038][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0086, 0.1245, 0.3252, 0.0607, 0.1961, 0.2849], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,040][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2757, 0.1049, 0.2883, 0.1444, 0.1357, 0.0510], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,042][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.1445, 0.0454, 0.2842, 0.2113, 0.1380, 0.1765], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,044][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.1802, 0.3134, 0.1987, 0.0823, 0.1409, 0.0845], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,045][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([6.9175e-14, 9.8869e-01, 5.1006e-05, 9.3097e-03, 1.9007e-03, 4.7027e-05],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,047][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0387, 0.2354, 0.0771, 0.1616, 0.0443, 0.0854, 0.3575],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,049][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0280, 0.0654, 0.0071, 0.7220, 0.1556, 0.0142, 0.0077],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,051][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0145, 0.1907, 0.1608, 0.1414, 0.1540, 0.1624, 0.1763],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,052][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.0308, 0.3503, 0.1011, 0.0833, 0.0564, 0.0399, 0.3382],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,055][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.8981, 0.0181, 0.0188, 0.0103, 0.0144, 0.0181, 0.0222],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,056][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.0581, 0.1864, 0.1089, 0.1824, 0.0477, 0.0844, 0.3321],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,058][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.1097, 0.1910, 0.1465, 0.1214, 0.1486, 0.1717, 0.1112],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,060][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0072, 0.1082, 0.2669, 0.0505, 0.1610, 0.2305, 0.1757],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,062][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.2570, 0.2061, 0.1083, 0.1046, 0.1196, 0.0635, 0.1408],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,064][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1663, 0.0477, 0.2026, 0.1382, 0.1413, 0.1832, 0.1206],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,065][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0537, 0.3052, 0.2213, 0.1560, 0.1259, 0.0845, 0.0535],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,065][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([1.9276e-11, 1.5272e-01, 1.2874e-06, 1.0100e-04, 1.4443e-03, 8.3754e-01,
        8.1979e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,066][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.0660, 0.0576, 0.0685, 0.0545, 0.0299, 0.0342, 0.3590, 0.3302],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,067][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0402, 0.0664, 0.0082, 0.6021, 0.2093, 0.0285, 0.0183, 0.0271],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,068][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0125, 0.1601, 0.1372, 0.1208, 0.1302, 0.1370, 0.1473, 0.1549],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,070][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.1472, 0.1154, 0.0421, 0.0340, 0.0348, 0.0185, 0.2041, 0.4039],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,072][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.8524, 0.0217, 0.0226, 0.0120, 0.0171, 0.0215, 0.0251, 0.0276],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,074][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.0661, 0.0742, 0.1145, 0.1107, 0.0228, 0.0745, 0.2899, 0.2474],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,076][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0824, 0.1694, 0.1303, 0.1222, 0.1268, 0.1487, 0.1186, 0.1017],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,078][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0060, 0.0787, 0.2140, 0.0374, 0.1255, 0.1749, 0.1314, 0.2321],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,079][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.2810, 0.1282, 0.1098, 0.0716, 0.1025, 0.1103, 0.1747, 0.0219],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,082][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.0754, 0.0204, 0.2481, 0.1913, 0.1384, 0.1081, 0.1016, 0.1167],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,083][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0717, 0.2712, 0.1826, 0.0915, 0.1138, 0.0826, 0.0879, 0.0986],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,085][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([1.4512e-11, 2.2123e-03, 4.9750e-06, 1.5727e-04, 2.9864e-05, 2.2384e-04,
        9.6822e-01, 2.9153e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,087][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0042, 0.0462, 0.0123, 0.0120, 0.0126, 0.0126, 0.1739, 0.2975, 0.4285],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,089][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0049, 0.0088, 0.0025, 0.9083, 0.0556, 0.0052, 0.0015, 0.0056, 0.0076],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,090][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0115, 0.1388, 0.1189, 0.1045, 0.1127, 0.1190, 0.1276, 0.1333, 0.1337],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,092][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0167, 0.0901, 0.0171, 0.0190, 0.0273, 0.0163, 0.1556, 0.3282, 0.3297],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,095][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.8206, 0.0209, 0.0220, 0.0123, 0.0167, 0.0214, 0.0244, 0.0268, 0.0349],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,096][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0255, 0.0484, 0.0321, 0.0586, 0.0123, 0.0379, 0.1776, 0.4288, 0.1786],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,099][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0866, 0.1621, 0.0964, 0.1018, 0.1162, 0.1402, 0.1036, 0.1219, 0.0711],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,101][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0056, 0.0728, 0.1865, 0.0332, 0.1150, 0.1675, 0.1225, 0.2133, 0.0836],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,102][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.3200, 0.0864, 0.1273, 0.0866, 0.0568, 0.0622, 0.1145, 0.0165, 0.1297],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,104][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.3568, 0.0584, 0.0721, 0.0862, 0.0531, 0.0704, 0.0772, 0.1578, 0.0681],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,106][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0323, 0.3068, 0.2028, 0.0805, 0.0915, 0.0753, 0.0483, 0.0536, 0.1089],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,107][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [orum] are: tensor([2.8764e-13, 1.1394e-05, 1.3018e-10, 1.4792e-08, 6.9391e-07, 8.9895e-05,
        3.7903e-04, 9.9952e-01, 7.2234e-07], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,110][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0051, 0.0126, 0.0137, 0.0094, 0.0094, 0.0102, 0.1482, 0.1470, 0.5761,
        0.0683], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,112][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.1102, 0.0556, 0.0503, 0.3509, 0.0517, 0.0427, 0.0112, 0.1383, 0.1333,
        0.0558], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,113][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0111, 0.1214, 0.1053, 0.0931, 0.0992, 0.1042, 0.1113, 0.1156, 0.1157,
        0.1231], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,116][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.0475, 0.1033, 0.0406, 0.0319, 0.0126, 0.0186, 0.0557, 0.2227, 0.3958,
        0.0713], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,117][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.7420, 0.0239, 0.0245, 0.0140, 0.0192, 0.0233, 0.0279, 0.0282, 0.0382,
        0.0587], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,119][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0395, 0.0527, 0.0439, 0.1012, 0.0134, 0.0302, 0.1179, 0.3989, 0.0927,
        0.1096], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,122][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0752, 0.1315, 0.1050, 0.0830, 0.0996, 0.1418, 0.0910, 0.1168, 0.0803,
        0.0758], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,124][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0021, 0.0365, 0.0942, 0.0176, 0.0569, 0.0818, 0.0591, 0.1056, 0.0446,
        0.5018], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,125][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.2402, 0.0604, 0.0487, 0.0757, 0.0724, 0.0552, 0.1026, 0.0228, 0.2455,
        0.0764], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,128][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.6661, 0.0159, 0.0524, 0.0728, 0.0197, 0.0215, 0.0374, 0.0459, 0.0294,
        0.0390], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,129][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0283, 0.2064, 0.1346, 0.0367, 0.0552, 0.0549, 0.0471, 0.0653, 0.2309,
        0.1406], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,131][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ et] are: tensor([1.3857e-12, 2.7870e-04, 5.8348e-07, 4.1883e-07, 2.0623e-08, 4.6799e-06,
        1.7971e-01, 7.9830e-01, 2.0275e-02, 1.4259e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,131][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0104, 0.0190, 0.0112, 0.0187, 0.0055, 0.0028, 0.0956, 0.0881, 0.3670,
        0.0847, 0.2971], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,132][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0223, 0.0997, 0.0204, 0.4697, 0.0511, 0.0236, 0.0407, 0.0262, 0.0905,
        0.1490, 0.0069], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,133][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.0081, 0.1096, 0.0938, 0.0819, 0.0882, 0.0933, 0.1005, 0.1052, 0.1051,
        0.1129, 0.1014], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,134][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.0215, 0.0503, 0.0190, 0.0190, 0.0077, 0.0059, 0.0546, 0.1968, 0.2393,
        0.1124, 0.2733], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,135][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.7352, 0.0213, 0.0224, 0.0122, 0.0171, 0.0216, 0.0245, 0.0261, 0.0332,
        0.0505, 0.0358], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,137][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0297, 0.0307, 0.0360, 0.0614, 0.0082, 0.0132, 0.1572, 0.1706, 0.1080,
        0.2016, 0.1834], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,139][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0722, 0.1293, 0.0823, 0.0718, 0.0771, 0.1278, 0.0914, 0.1103, 0.0729,
        0.1212, 0.0437], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,140][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0014, 0.0266, 0.0716, 0.0123, 0.0407, 0.0604, 0.0416, 0.0761, 0.0298,
        0.3673, 0.2722], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,142][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.1693, 0.0336, 0.0362, 0.0710, 0.1015, 0.0831, 0.1228, 0.0220, 0.2033,
        0.1078, 0.0494], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,144][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.2981, 0.0672, 0.0999, 0.1291, 0.0610, 0.0461, 0.0564, 0.0717, 0.0367,
        0.0624, 0.0715], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,146][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0440, 0.1589, 0.1125, 0.0318, 0.0571, 0.0455, 0.0625, 0.0809, 0.2000,
        0.1265, 0.0804], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,147][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([3.6798e-13, 4.9323e-05, 6.8324e-08, 7.2991e-07, 1.2109e-08, 3.5023e-07,
        1.1992e-03, 6.5623e-02, 2.6081e-03, 8.0900e-01, 1.2152e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,149][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0012, 0.0081, 0.0021, 0.0019, 0.0016, 0.0012, 0.0274, 0.0665, 0.0887,
        0.0467, 0.3542, 0.4003], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,151][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0024, 0.0075, 0.0013, 0.8889, 0.0438, 0.0038, 0.0010, 0.0051, 0.0061,
        0.0332, 0.0025, 0.0042], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,153][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0079, 0.0989, 0.0848, 0.0741, 0.0800, 0.0846, 0.0907, 0.0948, 0.0947,
        0.1018, 0.0916, 0.0961], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,155][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0077, 0.0363, 0.0066, 0.0070, 0.0061, 0.0041, 0.0531, 0.1352, 0.1820,
        0.0853, 0.1641, 0.3124], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,157][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.7681, 0.0166, 0.0177, 0.0093, 0.0129, 0.0170, 0.0192, 0.0211, 0.0265,
        0.0431, 0.0292, 0.0192], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,159][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0177, 0.0182, 0.0121, 0.0260, 0.0039, 0.0100, 0.0771, 0.2209, 0.0649,
        0.1035, 0.2622, 0.1834], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,161][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0712, 0.1310, 0.0761, 0.0755, 0.0912, 0.1086, 0.0737, 0.0956, 0.0524,
        0.1095, 0.0696, 0.0457], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,163][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0018, 0.0256, 0.0648, 0.0117, 0.0400, 0.0583, 0.0418, 0.0741, 0.0282,
        0.3471, 0.2699, 0.0366], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,164][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2722, 0.0628, 0.0995, 0.0681, 0.0427, 0.0472, 0.0805, 0.0126, 0.1006,
        0.0549, 0.0590, 0.0999], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,167][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.2750, 0.0406, 0.0599, 0.1150, 0.0465, 0.0656, 0.0602, 0.1058, 0.0454,
        0.0899, 0.0786, 0.0175], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,168][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0257, 0.2720, 0.1752, 0.0618, 0.0629, 0.0425, 0.0375, 0.0422, 0.0811,
        0.0678, 0.0460, 0.0852], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,170][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [orum] are: tensor([2.4659e-16, 7.7441e-10, 3.9054e-14, 3.2966e-13, 2.5800e-11, 5.2620e-09,
        2.1037e-07, 6.0780e-04, 5.0089e-10, 2.8096e-05, 9.9936e-01, 2.7507e-07],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,172][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0040, 0.0225, 0.0148, 0.0086, 0.0066, 0.0030, 0.0203, 0.0587, 0.1725,
        0.0407, 0.2649, 0.3680, 0.0153], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,174][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0245, 0.0115, 0.0048, 0.6508, 0.0370, 0.0113, 0.0025, 0.0163, 0.0086,
        0.2089, 0.0083, 0.0063, 0.0092], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,176][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0076, 0.0900, 0.0771, 0.0677, 0.0730, 0.0770, 0.0826, 0.0865, 0.0866,
        0.0926, 0.0838, 0.0879, 0.0876], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,178][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0051, 0.0422, 0.0168, 0.0155, 0.0036, 0.0028, 0.0433, 0.1292, 0.2684,
        0.0371, 0.1065, 0.3090, 0.0206], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,180][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.7688, 0.0155, 0.0158, 0.0083, 0.0115, 0.0145, 0.0178, 0.0185, 0.0244,
        0.0412, 0.0260, 0.0177, 0.0200], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,181][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0365, 0.0620, 0.0368, 0.0559, 0.0172, 0.0161, 0.1020, 0.0913, 0.1278,
        0.0647, 0.1939, 0.1554, 0.0404], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,184][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0708, 0.1030, 0.0656, 0.0654, 0.0873, 0.1063, 0.0778, 0.0887, 0.0665,
        0.0983, 0.0631, 0.0606, 0.0467], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,186][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0017, 0.0253, 0.0621, 0.0134, 0.0388, 0.0535, 0.0406, 0.0674, 0.0289,
        0.2934, 0.2268, 0.0368, 0.1111], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,187][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.1459, 0.0252, 0.0533, 0.0474, 0.0456, 0.0391, 0.0798, 0.0198, 0.1829,
        0.0847, 0.0504, 0.1818, 0.0439], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,189][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.2245, 0.0305, 0.0802, 0.1174, 0.0551, 0.0492, 0.0701, 0.0709, 0.0429,
        0.0614, 0.0913, 0.0343, 0.0721], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,191][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0675, 0.1642, 0.1010, 0.0237, 0.0479, 0.0394, 0.0349, 0.0598, 0.1367,
        0.0669, 0.0638, 0.1736, 0.0207], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,192][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ is] are: tensor([1.6558e-15, 8.2947e-07, 9.8417e-09, 1.6762e-09, 9.1700e-11, 4.4772e-08,
        1.3917e-04, 1.7066e-03, 1.7923e-04, 1.4774e-03, 6.4936e-01, 3.4704e-01,
        9.8217e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,195][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0056, 0.0148, 0.0085, 0.0085, 0.0058, 0.0028, 0.0239, 0.0639, 0.1455,
        0.0302, 0.2451, 0.2627, 0.0422, 0.1404], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,197][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0131, 0.0173, 0.0031, 0.2159, 0.0381, 0.0053, 0.0100, 0.0156, 0.0290,
        0.0785, 0.0053, 0.0169, 0.4871, 0.0648], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,198][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0072, 0.0824, 0.0711, 0.0625, 0.0673, 0.0710, 0.0760, 0.0794, 0.0793,
        0.0847, 0.0769, 0.0804, 0.0804, 0.0815], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,199][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0128, 0.0376, 0.0155, 0.0105, 0.0054, 0.0045, 0.0407, 0.1346, 0.1667,
        0.0912, 0.1329, 0.2035, 0.0518, 0.0923], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,200][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.7873, 0.0137, 0.0134, 0.0070, 0.0098, 0.0126, 0.0151, 0.0162, 0.0205,
        0.0356, 0.0222, 0.0148, 0.0166, 0.0152], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,201][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0490, 0.0271, 0.0153, 0.0231, 0.0085, 0.0093, 0.0481, 0.1218, 0.0900,
        0.0749, 0.1205, 0.0989, 0.0876, 0.2259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,202][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0618, 0.0884, 0.0747, 0.0579, 0.0702, 0.0866, 0.0686, 0.0819, 0.0699,
        0.0803, 0.0639, 0.0668, 0.0603, 0.0688], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,205][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0016, 0.0234, 0.0565, 0.0129, 0.0347, 0.0478, 0.0367, 0.0596, 0.0265,
        0.2581, 0.1955, 0.0334, 0.1009, 0.1125], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,206][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1286, 0.0410, 0.0347, 0.0568, 0.0529, 0.0423, 0.0549, 0.0188, 0.1817,
        0.0887, 0.0516, 0.1859, 0.0411, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,208][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.2103, 0.0341, 0.1162, 0.0975, 0.0634, 0.0551, 0.0544, 0.0561, 0.0311,
        0.0613, 0.0805, 0.0243, 0.0610, 0.0548], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,210][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0330, 0.1946, 0.1444, 0.0301, 0.0647, 0.0378, 0.0355, 0.0498, 0.1272,
        0.0552, 0.0575, 0.1432, 0.0135, 0.0133], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,211][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([1.7576e-12, 8.2534e-06, 1.1366e-08, 1.7946e-07, 3.0390e-10, 1.9749e-08,
        8.7354e-05, 1.0488e-03, 4.9980e-05, 9.3596e-04, 6.3366e-02, 6.2344e-03,
        3.9227e-02, 8.8904e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,213][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0597, 0.0137, 0.0100, 0.0161, 0.0017, 0.0008, 0.0080, 0.0347, 0.0579,
        0.0144, 0.0781, 0.0557, 0.0321, 0.2445, 0.3726], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,215][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0025, 0.0113, 0.0035, 0.3508, 0.1230, 0.0075, 0.0041, 0.0208, 0.0089,
        0.1112, 0.0114, 0.0055, 0.0774, 0.2596, 0.0024], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,218][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0056, 0.0766, 0.0649, 0.0566, 0.0615, 0.0652, 0.0703, 0.0741, 0.0742,
        0.0801, 0.0714, 0.0752, 0.0751, 0.0763, 0.0728], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,219][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0172, 0.0345, 0.0098, 0.0057, 0.0024, 0.0019, 0.0203, 0.0389, 0.0669,
        0.0465, 0.0420, 0.0468, 0.0197, 0.0338, 0.6138], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,222][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.7330, 0.0164, 0.0163, 0.0090, 0.0119, 0.0158, 0.0182, 0.0191, 0.0242,
        0.0399, 0.0259, 0.0175, 0.0191, 0.0175, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,223][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.0229, 0.0207, 0.0116, 0.0163, 0.0020, 0.0046, 0.0402, 0.0504, 0.0402,
        0.0165, 0.0291, 0.0337, 0.0395, 0.1416, 0.5308], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,225][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0592, 0.0999, 0.0657, 0.0590, 0.0575, 0.0857, 0.0763, 0.0748, 0.0621,
        0.0836, 0.0490, 0.0588, 0.0489, 0.0744, 0.0450], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,228][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0012, 0.0200, 0.0500, 0.0098, 0.0315, 0.0421, 0.0313, 0.0552, 0.0220,
        0.2436, 0.1794, 0.0278, 0.0873, 0.0925, 0.1064], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,230][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.1715, 0.0365, 0.0492, 0.0494, 0.0307, 0.0300, 0.0515, 0.0113, 0.1561,
        0.1155, 0.0475, 0.1599, 0.0355, 0.0197, 0.0357], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,231][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.2358, 0.0172, 0.1356, 0.1056, 0.0354, 0.0313, 0.0409, 0.0290, 0.0192,
        0.0446, 0.1581, 0.0128, 0.0522, 0.0602, 0.0222], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,234][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0324, 0.1171, 0.1447, 0.0573, 0.0666, 0.0632, 0.0354, 0.0519, 0.1000,
        0.0889, 0.0602, 0.0992, 0.0251, 0.0278, 0.0300], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,235][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ same] are: tensor([1.5056e-11, 4.0462e-07, 6.7428e-11, 6.7356e-11, 1.6630e-12, 5.4238e-11,
        1.4576e-07, 8.7370e-07, 1.0226e-08, 3.2650e-07, 3.7343e-04, 2.8617e-07,
        1.1469e-05, 9.9001e-01, 9.6008e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,236][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ as] are: tensor([2.9533e-03, 4.8867e-03, 1.4165e-03, 3.2207e-03, 4.0000e-03, 3.9903e-04,
        6.4965e-03, 2.1216e-02, 3.3745e-02, 1.0715e-02, 6.2372e-02, 5.7808e-02,
        1.4519e-02, 8.7106e-02, 6.2541e-01, 6.3735e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,238][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0150, 0.0683, 0.0044, 0.2755, 0.0940, 0.0127, 0.0033, 0.0165, 0.0082,
        0.0772, 0.0120, 0.0058, 0.2481, 0.1505, 0.0077, 0.0008],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,240][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0059, 0.0715, 0.0601, 0.0524, 0.0569, 0.0606, 0.0654, 0.0691, 0.0693,
        0.0747, 0.0667, 0.0703, 0.0703, 0.0711, 0.0677, 0.0680],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,242][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0034, 0.0084, 0.0035, 0.0024, 0.0017, 0.0013, 0.0092, 0.0278, 0.0375,
        0.0171, 0.0339, 0.0449, 0.0163, 0.0527, 0.6441, 0.0956],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,245][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.8188, 0.0107, 0.0100, 0.0052, 0.0072, 0.0100, 0.0115, 0.0125, 0.0156,
        0.0289, 0.0177, 0.0113, 0.0123, 0.0116, 0.0098, 0.0069],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,247][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.0042, 0.0027, 0.0018, 0.0037, 0.0022, 0.0015, 0.0068, 0.0179, 0.0096,
        0.0125, 0.0295, 0.0135, 0.0163, 0.0683, 0.6045, 0.2050],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,248][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0507, 0.1020, 0.0684, 0.0590, 0.0535, 0.0634, 0.0665, 0.0607, 0.0570,
        0.0880, 0.0502, 0.0571, 0.0503, 0.0781, 0.0582, 0.0371],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,251][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0011, 0.0189, 0.0445, 0.0096, 0.0283, 0.0376, 0.0277, 0.0477, 0.0189,
        0.2041, 0.1559, 0.0240, 0.0776, 0.0845, 0.0897, 0.1301],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,252][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.1605, 0.0420, 0.0343, 0.0700, 0.0507, 0.0262, 0.0378, 0.0173, 0.1022,
        0.0769, 0.0740, 0.1170, 0.0414, 0.0269, 0.0521, 0.0706],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,255][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.2842, 0.0234, 0.1058, 0.0803, 0.0518, 0.0573, 0.0542, 0.0391, 0.0157,
        0.0490, 0.0934, 0.0118, 0.0389, 0.0476, 0.0242, 0.0231],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,257][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0367, 0.1936, 0.1622, 0.0470, 0.0901, 0.0649, 0.0399, 0.0425, 0.0674,
        0.0616, 0.0532, 0.0708, 0.0134, 0.0152, 0.0349, 0.0067],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,258][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ as] are: tensor([1.5641e-17, 2.8362e-10, 7.4203e-14, 1.1965e-14, 1.3138e-14, 1.1424e-12,
        3.9590e-10, 2.4260e-08, 4.1448e-11, 3.7139e-09, 1.6279e-05, 3.9629e-09,
        6.2975e-10, 6.7463e-06, 9.9878e-01, 1.1939e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,260][circuit_model.py][line:1532][INFO] ##4-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0095, 0.0045, 0.0020, 0.0028, 0.0027, 0.0010, 0.0063, 0.0176, 0.0275,
        0.0052, 0.0624, 0.0272, 0.0069, 0.0298, 0.1486, 0.1508, 0.4951],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,262][circuit_model.py][line:1535][INFO] ##4-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0154, 0.0184, 0.0037, 0.1812, 0.0421, 0.0053, 0.0092, 0.0133, 0.0241,
        0.0865, 0.0053, 0.0146, 0.4245, 0.0526, 0.0213, 0.0261, 0.0564],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,264][circuit_model.py][line:1538][INFO] ##4-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0052, 0.0665, 0.0562, 0.0493, 0.0535, 0.0569, 0.0613, 0.0647, 0.0647,
        0.0697, 0.0624, 0.0657, 0.0660, 0.0668, 0.0637, 0.0641, 0.0633],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,266][circuit_model.py][line:1541][INFO] ##4-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0131, 0.0097, 0.0033, 0.0029, 0.0014, 0.0012, 0.0093, 0.0283, 0.0295,
        0.0173, 0.0268, 0.0234, 0.0118, 0.0220, 0.5037, 0.1522, 0.1442],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,266][circuit_model.py][line:1544][INFO] ##4-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.7937, 0.0120, 0.0112, 0.0059, 0.0081, 0.0108, 0.0129, 0.0133, 0.0171,
        0.0309, 0.0184, 0.0123, 0.0132, 0.0125, 0.0104, 0.0075, 0.0097],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,267][circuit_model.py][line:1547][INFO] ##4-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0279, 0.0036, 0.0020, 0.0045, 0.0015, 0.0015, 0.0055, 0.0154, 0.0095,
        0.0092, 0.0162, 0.0064, 0.0104, 0.0304, 0.3762, 0.2934, 0.1864],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,268][circuit_model.py][line:1550][INFO] ##4-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0462, 0.0717, 0.0649, 0.0466, 0.0554, 0.0709, 0.0576, 0.0687, 0.0578,
        0.0761, 0.0563, 0.0559, 0.0491, 0.0598, 0.0578, 0.0488, 0.0564],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,269][circuit_model.py][line:1553][INFO] ##4-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0011, 0.0181, 0.0415, 0.0095, 0.0260, 0.0346, 0.0260, 0.0445, 0.0189,
        0.1806, 0.1347, 0.0235, 0.0715, 0.0798, 0.0819, 0.1165, 0.0913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,272][circuit_model.py][line:1556][INFO] ##4-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.1309, 0.0395, 0.0330, 0.0537, 0.0472, 0.0400, 0.0480, 0.0175, 0.1527,
        0.0768, 0.0494, 0.1566, 0.0370, 0.0196, 0.0331, 0.0471, 0.0178],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,274][circuit_model.py][line:1559][INFO] ##4-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1419, 0.0316, 0.1277, 0.0715, 0.0616, 0.0546, 0.0510, 0.0399, 0.0245,
        0.0702, 0.1038, 0.0150, 0.0553, 0.0489, 0.0230, 0.0395, 0.0397],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,275][circuit_model.py][line:1562][INFO] ##4-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0287, 0.1680, 0.1804, 0.0418, 0.0961, 0.0500, 0.0336, 0.0367, 0.1080,
        0.0427, 0.0496, 0.0974, 0.0117, 0.0136, 0.0291, 0.0079, 0.0046],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,277][circuit_model.py][line:1565][INFO] ##4-th layer ##Weight##: The head12 weight for token [ the] are: tensor([3.7257e-15, 7.3084e-11, 2.1860e-14, 2.0064e-12, 8.9346e-15, 2.0438e-13,
        2.8834e-10, 3.9913e-09, 4.9866e-11, 3.8259e-09, 2.5309e-07, 5.4744e-10,
        3.9562e-08, 5.2627e-06, 1.3915e-02, 1.1367e-01, 8.7241e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,331][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:27,333][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,333][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,334][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,335][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,335][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,336][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,338][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,339][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,340][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,342][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,343][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,344][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,345][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.1440, 0.8560], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,346][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.1127, 0.8873], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,346][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.8029, 0.1971], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,347][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.0320, 0.9680], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,348][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.1033, 0.8967], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,348][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.6445, 0.3555], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,349][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.1762, 0.8238], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,350][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.0069, 0.9931], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,351][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.7486, 0.2514], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,352][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.7320, 0.2680], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,353][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.6034, 0.3966], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,354][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.9896, 0.0104], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,356][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0312, 0.6757, 0.2931], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,357][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0190, 0.9764, 0.0046], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,360][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.7047, 0.2096, 0.0857], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,361][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0141, 0.4989, 0.4870], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,363][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0611, 0.5805, 0.3584], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,365][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.6655, 0.3170, 0.0175], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,367][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0869, 0.6249, 0.2881], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,369][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0026, 0.5705, 0.4269], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,371][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.6179, 0.3169, 0.0652], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,373][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.4559, 0.3859, 0.1582], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,374][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.5068, 0.3177, 0.1756], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,376][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([9.9999e-01, 8.8927e-06, 1.4832e-06], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,378][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0107, 0.6598, 0.2788, 0.0507], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,380][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0548, 0.2885, 0.1400, 0.5167], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,382][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.7025, 0.1714, 0.0551, 0.0711], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,383][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0103, 0.3948, 0.3971, 0.1979], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,385][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0412, 0.3626, 0.2090, 0.3871], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,387][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.4066, 0.3820, 0.1554, 0.0560], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,389][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0476, 0.3860, 0.2717, 0.2947], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,391][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0012, 0.3858, 0.2880, 0.3250], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,393][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.5619, 0.1704, 0.1328, 0.1349], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,395][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.5208, 0.3334, 0.0965, 0.0494], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,396][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.3925, 0.2442, 0.1330, 0.2303], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,398][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([9.9105e-01, 5.5885e-03, 3.3644e-03, 1.3601e-07], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,400][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0111, 0.2464, 0.3926, 0.2899, 0.0601], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,402][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0791, 0.2440, 0.2396, 0.3587, 0.0785], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,403][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.5640, 0.1959, 0.0823, 0.0969, 0.0609], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,405][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.0083, 0.2888, 0.2808, 0.1685, 0.2537], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,407][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0283, 0.2571, 0.1578, 0.2839, 0.2729], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,409][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.1008, 0.0868, 0.0815, 0.0653, 0.6657], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,410][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.0363, 0.3029, 0.2048, 0.2253, 0.2307], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,410][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0011, 0.3323, 0.2471, 0.2611, 0.1584], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,411][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.3768, 0.1256, 0.0882, 0.2807, 0.1287], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,412][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.4665, 0.2132, 0.1842, 0.0920, 0.0441], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,413][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.3279, 0.2231, 0.1122, 0.2054, 0.1313], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,414][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([8.0381e-01, 1.6200e-01, 3.4116e-02, 4.6583e-06, 7.1496e-05],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,416][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0319, 0.3293, 0.2432, 0.1518, 0.1933, 0.0505], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,418][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.0019, 0.1113, 0.0048, 0.5806, 0.2970, 0.0044], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,419][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.5456, 0.1692, 0.0735, 0.0931, 0.0641, 0.0546], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,422][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.0063, 0.2347, 0.2326, 0.1400, 0.2192, 0.1671], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,424][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0218, 0.1859, 0.1288, 0.2187, 0.2081, 0.2367], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,425][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.1151, 0.0781, 0.0311, 0.0747, 0.6246, 0.0766], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,428][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.0236, 0.2329, 0.1558, 0.1890, 0.2232, 0.1755], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,429][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0009, 0.3023, 0.2116, 0.2220, 0.1400, 0.1231], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,431][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.4193, 0.0821, 0.1592, 0.1259, 0.1697, 0.0438], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,433][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.3563, 0.2581, 0.1380, 0.0649, 0.0897, 0.0930], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,435][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.2752, 0.1917, 0.0955, 0.1744, 0.1114, 0.1518], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,436][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([9.2127e-01, 7.4365e-02, 4.2400e-03, 1.2848e-06, 1.0734e-05, 1.1382e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,438][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.0387, 0.2354, 0.0771, 0.1616, 0.0443, 0.0854, 0.3575],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,440][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.0033, 0.0503, 0.0027, 0.6572, 0.2790, 0.0056, 0.0019],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,442][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.4214, 0.1905, 0.0621, 0.0979, 0.0507, 0.0569, 0.1204],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,444][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.0056, 0.1976, 0.1935, 0.1102, 0.1712, 0.1477, 0.1742],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,446][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.0175, 0.1586, 0.1107, 0.1875, 0.1823, 0.1938, 0.1498],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,448][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.1061, 0.0899, 0.0543, 0.0731, 0.5024, 0.1302, 0.0441],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,450][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.0207, 0.1912, 0.1369, 0.1553, 0.1792, 0.1901, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,452][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0016, 0.2472, 0.1767, 0.1815, 0.1217, 0.1069, 0.1644],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,453][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.3235, 0.2089, 0.0428, 0.1378, 0.1194, 0.0791, 0.0885],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,456][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.3866, 0.1845, 0.0501, 0.0977, 0.0633, 0.1486, 0.0693],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,457][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.2780, 0.1744, 0.0791, 0.1454, 0.0925, 0.1279, 0.1027],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,459][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([9.8732e-01, 1.1564e-02, 1.0668e-03, 8.0412e-08, 4.3716e-06, 1.4597e-05,
        2.9638e-05], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,461][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.0660, 0.0576, 0.0685, 0.0545, 0.0299, 0.0342, 0.3590, 0.3302],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,463][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0034, 0.0531, 0.0031, 0.5659, 0.3523, 0.0105, 0.0035, 0.0082],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,464][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.5727, 0.1110, 0.0382, 0.0624, 0.0357, 0.0295, 0.0864, 0.0641],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,467][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.0054, 0.1591, 0.1490, 0.1012, 0.1472, 0.1098, 0.1389, 0.1895],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,469][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0152, 0.1459, 0.0981, 0.1720, 0.1652, 0.1694, 0.1341, 0.1001],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,470][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.0951, 0.0631, 0.0548, 0.0989, 0.3758, 0.1779, 0.1093, 0.0252],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,472][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.0193, 0.1722, 0.1196, 0.1416, 0.1517, 0.1694, 0.1157, 0.1105],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,474][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.0013, 0.2242, 0.1511, 0.1449, 0.1079, 0.1031, 0.1456, 0.1219],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,476][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.1600, 0.1184, 0.0487, 0.1552, 0.1610, 0.0731, 0.2112, 0.0724],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,477][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.3044, 0.1078, 0.0752, 0.0816, 0.0427, 0.0537, 0.0970, 0.2377],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,477][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.2420, 0.1617, 0.0722, 0.1321, 0.0886, 0.1179, 0.0919, 0.0936],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,478][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([9.0525e-01, 8.3387e-02, 8.9031e-03, 4.6049e-06, 1.1412e-04, 1.1753e-03,
        8.6028e-04, 3.0831e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,479][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0042, 0.0462, 0.0123, 0.0120, 0.0126, 0.0126, 0.1739, 0.2975, 0.4285],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,480][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([9.4805e-04, 1.1813e-02, 1.6090e-03, 8.2868e-01, 1.4709e-01, 3.9423e-03,
        7.6250e-04, 3.3367e-03, 1.8214e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,482][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.4189, 0.1775, 0.0512, 0.0721, 0.0343, 0.0414, 0.0915, 0.0626, 0.0506],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,484][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0042, 0.1351, 0.1336, 0.0847, 0.1297, 0.1043, 0.1270, 0.1680, 0.1133],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,486][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0125, 0.1209, 0.0885, 0.1430, 0.1376, 0.1455, 0.1172, 0.0898, 0.1451],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,488][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.1035, 0.0654, 0.0277, 0.0979, 0.4791, 0.1326, 0.0432, 0.0369, 0.0136],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,490][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0170, 0.1551, 0.1017, 0.1242, 0.1367, 0.1529, 0.1020, 0.1157, 0.0948],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,492][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0012, 0.2014, 0.1386, 0.1336, 0.0981, 0.0935, 0.1350, 0.1111, 0.0876],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,494][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.4229, 0.0745, 0.0441, 0.2001, 0.0846, 0.0485, 0.0612, 0.0231, 0.0410],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,496][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.3195, 0.0662, 0.0261, 0.0301, 0.0248, 0.0578, 0.0361, 0.3126, 0.1269],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,497][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.2399, 0.1517, 0.0670, 0.1190, 0.0762, 0.0980, 0.0873, 0.0803, 0.0805],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,499][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([8.1892e-01, 1.6836e-01, 1.1949e-02, 1.1997e-06, 5.4580e-05, 1.5822e-04,
        4.3556e-04, 4.0106e-05, 8.1960e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,501][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0051, 0.0126, 0.0137, 0.0094, 0.0094, 0.0102, 0.1482, 0.1470, 0.5761,
        0.0683], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,503][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0141, 0.0316, 0.0135, 0.6550, 0.1514, 0.0128, 0.0023, 0.0366, 0.0129,
        0.0697], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,505][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.4620, 0.1377, 0.0294, 0.0668, 0.0270, 0.0281, 0.0710, 0.0559, 0.0479,
        0.0743], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,507][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.0041, 0.1138, 0.1018, 0.0691, 0.0964, 0.0704, 0.0903, 0.1284, 0.0869,
        0.2388], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,509][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0126, 0.1066, 0.0762, 0.1235, 0.1186, 0.1241, 0.1007, 0.0776, 0.1219,
        0.1382], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,511][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.1460, 0.0498, 0.0292, 0.0597, 0.3617, 0.1001, 0.0708, 0.0482, 0.0261,
        0.1084], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,513][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0188, 0.1362, 0.0976, 0.1071, 0.1184, 0.1352, 0.0893, 0.1088, 0.0897,
        0.0990], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,515][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0013, 0.1714, 0.1205, 0.1142, 0.0849, 0.0777, 0.1121, 0.0924, 0.0716,
        0.1537], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,517][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.1250, 0.0428, 0.0356, 0.1237, 0.0543, 0.0445, 0.1477, 0.0931, 0.2959,
        0.0372], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,519][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.2056, 0.0500, 0.0109, 0.0182, 0.0247, 0.0304, 0.0482, 0.3695, 0.1722,
        0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,521][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.1980, 0.1339, 0.0637, 0.1096, 0.0718, 0.0936, 0.0825, 0.0738, 0.0741,
        0.0989], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,522][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([5.1383e-01, 3.4167e-01, 1.4036e-01, 3.2286e-06, 2.0846e-04, 1.0876e-03,
        1.8595e-03, 4.9273e-04, 4.7234e-04, 1.3653e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,524][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0104, 0.0190, 0.0112, 0.0187, 0.0055, 0.0028, 0.0956, 0.0881, 0.3670,
        0.0847, 0.2971], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,525][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0019, 0.0625, 0.0057, 0.5287, 0.1646, 0.0099, 0.0071, 0.0100, 0.0072,
        0.1986, 0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,528][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.4532, 0.1246, 0.0280, 0.0700, 0.0330, 0.0143, 0.0512, 0.0612, 0.0436,
        0.0945, 0.0266], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,530][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.0023, 0.0900, 0.0812, 0.0480, 0.0746, 0.0561, 0.0704, 0.1078, 0.0666,
        0.2161, 0.1869], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,531][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0109, 0.0966, 0.0663, 0.1092, 0.1057, 0.1118, 0.0927, 0.0700, 0.1151,
        0.1243, 0.0974], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,534][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.0621, 0.0658, 0.1082, 0.0940, 0.2993, 0.1041, 0.0552, 0.0230, 0.0245,
        0.1379, 0.0257], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,536][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.0118, 0.1226, 0.0847, 0.0929, 0.1037, 0.1271, 0.0892, 0.0983, 0.0850,
        0.1147, 0.0702], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,537][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0008, 0.1510, 0.1095, 0.1099, 0.0740, 0.0641, 0.0951, 0.0698, 0.0523,
        0.1212, 0.1522], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,539][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.2459, 0.0337, 0.0178, 0.1447, 0.0934, 0.0847, 0.0896, 0.0935, 0.0600,
        0.1124, 0.0243], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,541][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.1728, 0.0185, 0.0152, 0.0255, 0.0107, 0.0202, 0.0468, 0.1059, 0.2212,
        0.1917, 0.1714], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,543][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.1818, 0.1293, 0.0591, 0.0996, 0.0661, 0.0836, 0.0769, 0.0711, 0.0734,
        0.0921, 0.0669], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,544][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([4.2134e-01, 3.2364e-01, 2.4218e-01, 3.5347e-06, 1.3323e-04, 5.6982e-04,
        1.5062e-03, 1.9152e-04, 3.1093e-04, 7.1743e-06, 1.0120e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,545][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0012, 0.0081, 0.0021, 0.0019, 0.0016, 0.0012, 0.0274, 0.0665, 0.0887,
        0.0467, 0.3542, 0.4003], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,545][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([4.7050e-04, 9.6160e-03, 9.9368e-04, 7.9975e-01, 1.1492e-01, 3.0951e-03,
        5.3471e-04, 2.9530e-03, 1.5421e-03, 6.3193e-02, 2.1153e-03, 8.2013e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,546][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.4341, 0.1325, 0.0334, 0.0622, 0.0251, 0.0234, 0.0690, 0.0512, 0.0417,
        0.0783, 0.0272, 0.0219], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,548][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0022, 0.0756, 0.0732, 0.0451, 0.0700, 0.0563, 0.0689, 0.0945, 0.0623,
        0.1962, 0.1715, 0.0842], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,550][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0086, 0.0860, 0.0628, 0.0999, 0.0974, 0.1038, 0.0848, 0.0650, 0.1083,
        0.1154, 0.0943, 0.0738], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,552][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0904, 0.0525, 0.0203, 0.0845, 0.4186, 0.1084, 0.0317, 0.0313, 0.0118,
        0.0949, 0.0482, 0.0073], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,553][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0132, 0.1120, 0.0783, 0.0905, 0.1022, 0.1146, 0.0763, 0.0867, 0.0709,
        0.1014, 0.0838, 0.0700], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,556][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0007, 0.1342, 0.0972, 0.0948, 0.0656, 0.0590, 0.0865, 0.0695, 0.0521,
        0.1197, 0.1409, 0.0797], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,558][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.4752, 0.0584, 0.0425, 0.1703, 0.0638, 0.0305, 0.0359, 0.0176, 0.0285,
        0.0274, 0.0156, 0.0342], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,559][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.1959, 0.0269, 0.0126, 0.0137, 0.0087, 0.0254, 0.0206, 0.2467, 0.0795,
        0.0663, 0.1367, 0.1672], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,562][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.1828, 0.1215, 0.0553, 0.0924, 0.0598, 0.0758, 0.0701, 0.0636, 0.0676,
        0.0797, 0.0621, 0.0693], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,563][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([8.5952e-01, 1.0855e-01, 2.9506e-02, 2.7859e-07, 3.1971e-05, 2.9233e-04,
        3.7588e-04, 2.3251e-05, 7.5444e-05, 6.4240e-07, 1.4199e-03, 1.9784e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:27,565][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0040, 0.0225, 0.0148, 0.0086, 0.0066, 0.0030, 0.0203, 0.0587, 0.1725,
        0.0407, 0.2649, 0.3680, 0.0153], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,567][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0057, 0.0182, 0.0048, 0.5982, 0.1057, 0.0064, 0.0013, 0.0080, 0.0023,
        0.2098, 0.0090, 0.0011, 0.0295], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,569][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.5119, 0.0922, 0.0200, 0.0355, 0.0188, 0.0170, 0.0425, 0.0432, 0.0399,
        0.0689, 0.0342, 0.0224, 0.0535], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,571][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0020, 0.0682, 0.0651, 0.0394, 0.0567, 0.0459, 0.0601, 0.0840, 0.0548,
        0.1693, 0.1463, 0.0720, 0.1362], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,573][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0098, 0.0885, 0.0594, 0.0995, 0.0927, 0.0939, 0.0774, 0.0590, 0.0941,
        0.1039, 0.0781, 0.0587, 0.0850], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,575][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0747, 0.0354, 0.0500, 0.0387, 0.3992, 0.0757, 0.0772, 0.0282, 0.0231,
        0.0902, 0.0715, 0.0137, 0.0225], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,576][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0147, 0.1096, 0.0733, 0.0826, 0.0948, 0.1074, 0.0725, 0.0793, 0.0709,
        0.0929, 0.0729, 0.0707, 0.0584], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,579][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0006, 0.1159, 0.0835, 0.0845, 0.0571, 0.0491, 0.0750, 0.0562, 0.0427,
        0.1008, 0.1194, 0.0659, 0.1494], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,580][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.2847, 0.0332, 0.0453, 0.1036, 0.0657, 0.0272, 0.0581, 0.0499, 0.0502,
        0.0666, 0.0738, 0.0341, 0.1077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,582][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.2665, 0.0556, 0.0157, 0.0271, 0.0081, 0.0208, 0.0181, 0.1576, 0.0923,
        0.0608, 0.1293, 0.1049, 0.0431], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,584][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.1587, 0.1086, 0.0521, 0.0888, 0.0580, 0.0757, 0.0664, 0.0626, 0.0618,
        0.0773, 0.0578, 0.0644, 0.0678], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,586][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([2.2308e-01, 2.3656e-01, 4.8355e-01, 5.1595e-06, 2.1498e-04, 2.4276e-03,
        2.2944e-03, 8.8452e-04, 7.8124e-04, 4.0641e-05, 4.5404e-02, 4.6824e-03,
        6.2238e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:27,587][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0056, 0.0148, 0.0085, 0.0085, 0.0058, 0.0028, 0.0239, 0.0639, 0.1455,
        0.0302, 0.2451, 0.2627, 0.0422, 0.1404], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,590][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0020, 0.0245, 0.0030, 0.2776, 0.1451, 0.0039, 0.0023, 0.0080, 0.0033,
        0.1368, 0.0080, 0.0014, 0.3374, 0.0466], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,592][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.5034, 0.0696, 0.0193, 0.0305, 0.0177, 0.0139, 0.0474, 0.0348, 0.0401,
        0.0699, 0.0341, 0.0227, 0.0493, 0.0473], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,594][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0017, 0.0587, 0.0536, 0.0310, 0.0475, 0.0389, 0.0488, 0.0732, 0.0458,
        0.1466, 0.1227, 0.0599, 0.1160, 0.1556], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,596][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0081, 0.0854, 0.0541, 0.0947, 0.0891, 0.0907, 0.0727, 0.0550, 0.0865,
        0.0956, 0.0699, 0.0519, 0.0791, 0.0671], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,598][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0465, 0.0435, 0.0303, 0.0231, 0.2375, 0.0364, 0.0460, 0.0353, 0.0220,
        0.1326, 0.0308, 0.0133, 0.0818, 0.2209], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,600][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0115, 0.1033, 0.0741, 0.0768, 0.0857, 0.0932, 0.0656, 0.0739, 0.0681,
        0.0825, 0.0702, 0.0690, 0.0594, 0.0668], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,602][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0005, 0.1056, 0.0782, 0.0792, 0.0491, 0.0409, 0.0654, 0.0470, 0.0349,
        0.0824, 0.1023, 0.0556, 0.1275, 0.1314], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,604][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.4644, 0.0450, 0.0460, 0.0707, 0.0434, 0.0229, 0.0404, 0.0217, 0.0386,
        0.0255, 0.0439, 0.0211, 0.0626, 0.0538], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,606][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2918, 0.0323, 0.0142, 0.0193, 0.0100, 0.0154, 0.0194, 0.1047, 0.0751,
        0.0444, 0.0732, 0.0785, 0.0588, 0.1629], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,608][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1466, 0.1027, 0.0478, 0.0814, 0.0537, 0.0681, 0.0620, 0.0572, 0.0590,
        0.0763, 0.0560, 0.0620, 0.0650, 0.0622], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,609][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([7.0189e-02, 6.0654e-01, 3.0490e-01, 9.8349e-06, 1.6359e-04, 5.2555e-04,
        1.7729e-03, 6.2918e-04, 5.0947e-04, 3.0750e-05, 1.3841e-02, 8.1598e-04,
        5.2586e-05, 2.7493e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:27,610][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.0597, 0.0137, 0.0100, 0.0161, 0.0017, 0.0008, 0.0080, 0.0347, 0.0579,
        0.0144, 0.0781, 0.0557, 0.0321, 0.2445, 0.3726], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,611][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0007, 0.0196, 0.0023, 0.3574, 0.2597, 0.0054, 0.0013, 0.0090, 0.0018,
        0.1582, 0.0078, 0.0010, 0.0872, 0.0876, 0.0013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,612][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.5373, 0.0650, 0.0215, 0.0319, 0.0215, 0.0109, 0.0349, 0.0283, 0.0371,
        0.0492, 0.0275, 0.0178, 0.0333, 0.0394, 0.0445], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,613][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.0015, 0.0478, 0.0429, 0.0288, 0.0418, 0.0310, 0.0399, 0.0597, 0.0355,
        0.1216, 0.0994, 0.0467, 0.1072, 0.1431, 0.1531], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,614][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.0079, 0.0756, 0.0496, 0.0840, 0.0819, 0.0860, 0.0676, 0.0506, 0.0806,
        0.0911, 0.0637, 0.0487, 0.0702, 0.0591, 0.0836], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,616][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.0372, 0.0344, 0.0214, 0.0499, 0.1048, 0.0511, 0.0309, 0.0263, 0.0184,
        0.1306, 0.0322, 0.0132, 0.1704, 0.2004, 0.0786], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,618][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0116, 0.0982, 0.0673, 0.0739, 0.0785, 0.0899, 0.0658, 0.0697, 0.0619,
        0.0772, 0.0618, 0.0623, 0.0527, 0.0637, 0.0656], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,620][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.0007, 0.0863, 0.0660, 0.0652, 0.0444, 0.0395, 0.0553, 0.0437, 0.0317,
        0.0725, 0.0914, 0.0495, 0.1087, 0.1144, 0.1307], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,622][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.5341, 0.0456, 0.0242, 0.0573, 0.0291, 0.0113, 0.0361, 0.0128, 0.0326,
        0.0476, 0.0396, 0.0209, 0.0400, 0.0512, 0.0174], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,624][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.3593, 0.0432, 0.0115, 0.0137, 0.0066, 0.0081, 0.0143, 0.0792, 0.0276,
        0.0239, 0.0559, 0.0253, 0.0401, 0.1382, 0.1530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,626][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.1396, 0.0921, 0.0453, 0.0768, 0.0497, 0.0635, 0.0596, 0.0499, 0.0539,
        0.0716, 0.0534, 0.0595, 0.0617, 0.0597, 0.0638], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,627][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([5.9133e-01, 2.5601e-01, 1.3191e-01, 1.2231e-05, 5.3369e-04, 1.2785e-03,
        4.2647e-03, 2.6286e-04, 5.2575e-04, 2.0846e-05, 1.1630e-02, 2.0391e-03,
        6.6839e-05, 1.0067e-04, 7.4222e-06], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:27,629][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([2.9533e-03, 4.8867e-03, 1.4165e-03, 3.2207e-03, 4.0000e-03, 3.9903e-04,
        6.4965e-03, 2.1216e-02, 3.3745e-02, 1.0715e-02, 6.2372e-02, 5.7808e-02,
        1.4519e-02, 8.7106e-02, 6.2541e-01, 6.3735e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,631][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0059, 0.0645, 0.0049, 0.2806, 0.2647, 0.0111, 0.0016, 0.0106, 0.0025,
        0.0916, 0.0161, 0.0013, 0.1822, 0.0591, 0.0029, 0.0004],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,633][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.4534, 0.0512, 0.0136, 0.0227, 0.0134, 0.0150, 0.0346, 0.0291, 0.0283,
        0.0426, 0.0261, 0.0155, 0.0358, 0.0381, 0.0614, 0.1190],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,635][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.0012, 0.0409, 0.0356, 0.0218, 0.0336, 0.0255, 0.0331, 0.0496, 0.0305,
        0.1000, 0.0804, 0.0392, 0.0808, 0.1118, 0.1179, 0.1979],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,637][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.0076, 0.0752, 0.0470, 0.0830, 0.0807, 0.0810, 0.0650, 0.0473, 0.0753,
        0.0822, 0.0566, 0.0429, 0.0665, 0.0558, 0.0792, 0.0547],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,639][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.0430, 0.0493, 0.0649, 0.0340, 0.1298, 0.0285, 0.0367, 0.0235, 0.0192,
        0.1553, 0.0430, 0.0129, 0.0796, 0.1454, 0.1074, 0.0275],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,641][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.0113, 0.0981, 0.0652, 0.0710, 0.0720, 0.0772, 0.0581, 0.0625, 0.0577,
        0.0727, 0.0577, 0.0594, 0.0510, 0.0624, 0.0680, 0.0559],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,643][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0006, 0.0773, 0.0606, 0.0639, 0.0408, 0.0350, 0.0518, 0.0367, 0.0288,
        0.0639, 0.0824, 0.0440, 0.0921, 0.1033, 0.1077, 0.1111],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,646][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.6399, 0.0259, 0.0164, 0.0416, 0.0242, 0.0149, 0.0126, 0.0159, 0.0130,
        0.0085, 0.0241, 0.0057, 0.0283, 0.0327, 0.0189, 0.0774],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,647][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.0987, 0.0114, 0.0074, 0.0108, 0.0064, 0.0045, 0.0102, 0.0482, 0.0399,
        0.0184, 0.0663, 0.0432, 0.0418, 0.0977, 0.3339, 0.1611],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,649][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.1275, 0.0902, 0.0414, 0.0711, 0.0468, 0.0570, 0.0528, 0.0486, 0.0501,
        0.0657, 0.0493, 0.0556, 0.0575, 0.0565, 0.0579, 0.0718],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,650][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([9.5715e-01, 9.9554e-03, 2.8260e-02, 1.6482e-06, 1.3445e-04, 2.3416e-03,
        6.4599e-04, 9.1690e-05, 8.3867e-05, 1.1500e-05, 8.8105e-04, 4.3248e-04,
        3.1137e-06, 5.1544e-06, 1.6851e-06, 2.5059e-07], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:27,652][circuit_model.py][line:1570][INFO] ##4-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0095, 0.0045, 0.0020, 0.0028, 0.0027, 0.0010, 0.0063, 0.0176, 0.0275,
        0.0052, 0.0624, 0.0272, 0.0069, 0.0298, 0.1486, 0.1508, 0.4951],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,654][circuit_model.py][line:1573][INFO] ##4-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0027, 0.0245, 0.0038, 0.2380, 0.1139, 0.0043, 0.0027, 0.0076, 0.0039,
        0.1291, 0.0080, 0.0018, 0.3779, 0.0390, 0.0056, 0.0040, 0.0331],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,657][circuit_model.py][line:1576][INFO] ##4-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4296, 0.0448, 0.0133, 0.0198, 0.0131, 0.0117, 0.0364, 0.0239, 0.0297,
        0.0434, 0.0265, 0.0172, 0.0349, 0.0352, 0.0574, 0.1153, 0.0476],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,659][circuit_model.py][line:1579][INFO] ##4-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0011, 0.0344, 0.0293, 0.0192, 0.0278, 0.0208, 0.0275, 0.0425, 0.0255,
        0.0811, 0.0661, 0.0326, 0.0702, 0.0919, 0.0997, 0.1749, 0.1554],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,660][circuit_model.py][line:1582][INFO] ##4-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0067, 0.0722, 0.0454, 0.0809, 0.0800, 0.0808, 0.0619, 0.0448, 0.0705,
        0.0774, 0.0549, 0.0404, 0.0631, 0.0541, 0.0732, 0.0498, 0.0440],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,663][circuit_model.py][line:1585][INFO] ##4-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0350, 0.0307, 0.0269, 0.0185, 0.1532, 0.0254, 0.0352, 0.0282, 0.0156,
        0.1047, 0.0226, 0.0096, 0.0698, 0.1347, 0.0918, 0.0383, 0.1597],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,665][circuit_model.py][line:1588][INFO] ##4-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0092, 0.0848, 0.0629, 0.0629, 0.0707, 0.0756, 0.0551, 0.0607, 0.0566,
        0.0693, 0.0578, 0.0571, 0.0485, 0.0543, 0.0620, 0.0557, 0.0567],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,667][circuit_model.py][line:1591][INFO] ##4-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0005, 0.0705, 0.0545, 0.0573, 0.0348, 0.0292, 0.0448, 0.0305, 0.0231,
        0.0530, 0.0708, 0.0370, 0.0824, 0.0897, 0.0940, 0.0970, 0.1308],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,669][circuit_model.py][line:1594][INFO] ##4-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.6456, 0.0232, 0.0258, 0.0351, 0.0260, 0.0135, 0.0168, 0.0103, 0.0178,
        0.0111, 0.0203, 0.0075, 0.0233, 0.0221, 0.0100, 0.0572, 0.0345],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,671][circuit_model.py][line:1597][INFO] ##4-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2992, 0.0126, 0.0059, 0.0076, 0.0046, 0.0064, 0.0076, 0.0376, 0.0244,
        0.0132, 0.0291, 0.0185, 0.0227, 0.0605, 0.0874, 0.1427, 0.2199],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,673][circuit_model.py][line:1600][INFO] ##4-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.1196, 0.0836, 0.0390, 0.0673, 0.0446, 0.0554, 0.0503, 0.0475, 0.0490,
        0.0634, 0.0464, 0.0514, 0.0532, 0.0511, 0.0523, 0.0658, 0.0600],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,674][circuit_model.py][line:1603][INFO] ##4-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([8.0577e-02, 6.6114e-01, 2.4496e-01, 3.0304e-05, 3.3073e-04, 6.2439e-04,
        4.6729e-03, 4.3769e-04, 7.6666e-04, 5.5868e-05, 5.2233e-03, 9.5944e-04,
        1.1476e-04, 9.3646e-05, 7.1717e-06, 2.4722e-06, 1.7307e-06],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:27,678][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:27,679][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 424],
        [5499],
        [1329],
        [1327],
        [1969],
        [2608],
        [1208],
        [ 490],
        [3947],
        [ 499],
        [ 464],
        [1023],
        [ 237],
        [  11],
        [ 179],
        [ 179],
        [  21]], device='cuda:0')
[2024-07-23 21:05:27,681][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  416],
        [ 8844],
        [ 9726],
        [ 4058],
        [10588],
        [11399],
        [  458],
        [ 1236],
        [ 1134],
        [ 4753],
        [  615],
        [  540],
        [ 6457],
        [  134],
        [ 1896],
        [  623],
        [  266]], device='cuda:0')
[2024-07-23 21:05:27,682][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 3231],
        [43872],
        [41823],
        [42063],
        [35993],
        [40828],
        [40065],
        [36341],
        [30802],
        [23787],
        [26773],
        [24542],
        [24547],
        [25268],
        [28166],
        [31438],
        [29160]], device='cuda:0')
[2024-07-23 21:05:27,684][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[  67],
        [ 202],
        [ 395],
        [ 863],
        [ 634],
        [4601],
        [6481],
        [4513],
        [9818],
        [1950],
        [4188],
        [9933],
        [8171],
        [5323],
        [5889],
        [4313],
        [4706]], device='cuda:0')
[2024-07-23 21:05:27,686][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[ 4650],
        [ 7789],
        [ 8708],
        [ 8524],
        [ 8229],
        [ 8273],
        [ 8679],
        [ 8615],
        [ 8603],
        [ 8478],
        [ 8769],
        [ 9108],
        [ 9269],
        [ 9375],
        [ 9585],
        [ 9957],
        [10158]], device='cuda:0')
[2024-07-23 21:05:27,688][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[33575],
        [40854],
        [41057],
        [41067],
        [41907],
        [42561],
        [41101],
        [33245],
        [31815],
        [33679],
        [32785],
        [36382],
        [35389],
        [35933],
        [ 4932],
        [ 4365],
        [ 7193]], device='cuda:0')
[2024-07-23 21:05:27,690][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[19026],
        [18975],
        [19297],
        [19254],
        [19605],
        [20046],
        [20431],
        [20895],
        [20883],
        [21409],
        [22032],
        [21728],
        [21681],
        [21644],
        [22222],
        [21422],
        [21730]], device='cuda:0')
[2024-07-23 21:05:27,692][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[43817],
        [25861],
        [22720],
        [22304],
        [19287],
        [19133],
        [22334],
        [32039],
        [36726],
        [39769],
        [38571],
        [35606],
        [29186],
        [33561],
        [41783],
        [38434],
        [34630]], device='cuda:0')
[2024-07-23 21:05:27,694][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[22373],
        [14223],
        [13747],
        [12247],
        [10807],
        [ 9118],
        [ 9657],
        [ 9864],
        [10160],
        [10636],
        [11012],
        [11048],
        [11386],
        [12159],
        [12859],
        [13753],
        [14043]], device='cuda:0')
[2024-07-23 21:05:27,695][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[2036],
        [1661],
        [1854],
        [1786],
        [1722],
        [1871],
        [1919],
        [2010],
        [2027],
        [2218],
        [2329],
        [2337],
        [2310],
        [2276],
        [2215],
        [2193],
        [2162]], device='cuda:0')
[2024-07-23 21:05:27,697][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 4783],
        [46418],
        [36859],
        [30212],
        [28124],
        [25264],
        [32725],
        [25434],
        [17349],
        [13423],
        [13411],
        [14001],
        [ 8987],
        [ 9920],
        [10669],
        [13199],
        [11046]], device='cuda:0')
[2024-07-23 21:05:27,699][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[47],
        [11],
        [32],
        [48],
        [75],
        [73],
        [71],
        [64],
        [34],
        [13],
        [19],
        [24],
        [21],
        [22],
        [20],
        [23],
        [26]], device='cuda:0')
[2024-07-23 21:05:27,701][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[34891],
        [14875],
        [13086],
        [ 9673],
        [ 8026],
        [ 9050],
        [ 8069],
        [ 8210],
        [ 8767],
        [ 8184],
        [ 7704],
        [ 8629],
        [ 8107],
        [ 8214],
        [ 6928],
        [ 7648],
        [ 7777]], device='cuda:0')
[2024-07-23 21:05:27,703][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 6496],
        [23312],
        [23312],
        [22257],
        [22896],
        [23283],
        [22502],
        [10750],
        [22416],
        [20618],
        [13366],
        [11811],
        [11502],
        [30939],
        [32242],
        [17294],
        [32380]], device='cuda:0')
[2024-07-23 21:05:27,705][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 885],
        [1552],
        [ 119],
        [1143],
        [ 926],
        [2536],
        [4288],
        [1273],
        [8998],
        [  59],
        [1681],
        [1679],
        [ 133],
        [  33],
        [  97],
        [ 589],
        [  25]], device='cuda:0')
[2024-07-23 21:05:27,707][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 238],
        [3263],
        [4435],
        [4256],
        [3235],
        [2538],
        [ 632],
        [ 632],
        [ 760],
        [1049],
        [1105],
        [1282],
        [1238],
        [1301],
        [ 155],
        [  45],
        [1088]], device='cuda:0')
[2024-07-23 21:05:27,709][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[2366],
        [ 592],
        [ 581],
        [6489],
        [7644],
        [8280],
        [9066],
        [8906],
        [9203],
        [9525],
        [8392],
        [9252],
        [8749],
        [6348],
        [8224],
        [7202],
        [6199]], device='cuda:0')
[2024-07-23 21:05:27,711][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 969],
        [2163],
        [2821],
        [3086],
        [3603],
        [3146],
        [2310],
        [2183],
        [2204],
        [2198],
        [2239],
        [2091],
        [2166],
        [2003],
        [1868],
        [1591],
        [1472]], device='cuda:0')
[2024-07-23 21:05:27,713][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[1637],
        [2003],
        [1465],
        [1544],
        [1519],
        [1533],
        [1721],
        [1816],
        [1895],
        [1896],
        [2068],
        [2122],
        [2107],
        [2181],
        [2352],
        [2560],
        [2589]], device='cuda:0')
[2024-07-23 21:05:27,715][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[  83],
        [ 232],
        [ 319],
        [ 460],
        [ 553],
        [ 615],
        [ 643],
        [ 643],
        [ 694],
        [ 729],
        [ 783],
        [ 834],
        [ 851],
        [ 876],
        [ 947],
        [ 995],
        [1014]], device='cuda:0')
[2024-07-23 21:05:27,717][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 3231],
        [ 2994],
        [ 3152],
        [ 1462],
        [11828],
        [12200],
        [10587],
        [ 8873],
        [10257],
        [11360],
        [ 9519],
        [11626],
        [10832],
        [ 5069],
        [ 2429],
        [ 4389],
        [ 2946]], device='cuda:0')
[2024-07-23 21:05:27,719][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[5312],
        [6622],
        [6205],
        [5310],
        [5392],
        [5739],
        [5626],
        [5675],
        [5618],
        [5494],
        [5396],
        [5398],
        [5375],
        [5343],
        [5468],
        [5274],
        [5302]], device='cuda:0')
[2024-07-23 21:05:27,721][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[12714],
        [  687],
        [  744],
        [  824],
        [  878],
        [  889],
        [  877],
        [  836],
        [  808],
        [  752],
        [  742],
        [  725],
        [  699],
        [  719],
        [  729],
        [  744],
        [  768]], device='cuda:0')
[2024-07-23 21:05:27,723][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 5380],
        [ 8241],
        [ 7295],
        [ 7791],
        [10898],
        [ 7837],
        [ 9789],
        [ 7807],
        [11957],
        [ 7300],
        [ 6194],
        [11173],
        [ 7787],
        [10535],
        [10751],
        [12375],
        [11508]], device='cuda:0')
[2024-07-23 21:05:27,725][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 8958],
        [ 8583],
        [13648],
        [13286],
        [22537],
        [23229],
        [20069],
        [21359],
        [22603],
        [25005],
        [29781],
        [24722],
        [25541],
        [23929],
        [20730],
        [17578],
        [18530]], device='cuda:0')
[2024-07-23 21:05:27,727][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[2716],
        [3468],
        [3875],
        [3942],
        [3992],
        [3773],
        [3679],
        [3747],
        [3688],
        [3689],
        [3655],
        [3661],
        [3590],
        [3621],
        [3550],
        [3449],
        [3441]], device='cuda:0')
[2024-07-23 21:05:27,729][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[2558],
        [2506],
        [2558],
        [2515],
        [1534],
        [2110],
        [2490],
        [2005],
        [1614],
        [ 544],
        [ 511],
        [1799],
        [ 722],
        [ 901],
        [ 728],
        [2350],
        [ 840]], device='cuda:0')
[2024-07-23 21:05:27,731][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[34156],
        [27515],
        [26161],
        [24470],
        [20703],
        [21102],
        [22426],
        [23811],
        [22232],
        [22385],
        [22574],
        [20584],
        [21805],
        [22684],
        [25732],
        [25418],
        [24265]], device='cuda:0')
[2024-07-23 21:05:27,732][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[17385],
        [40419],
        [40027],
        [34659],
        [29174],
        [24006],
        [30055],
        [31643],
        [28537],
        [35812],
        [31236],
        [28589],
        [31121],
        [38912],
        [32911],
        [31244],
        [39960]], device='cuda:0')
[2024-07-23 21:05:27,735][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992],
        [6992]], device='cuda:0')
[2024-07-23 21:05:27,802][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:27,804][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,805][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,807][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,808][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,808][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,809][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,810][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,810][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,811][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,812][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,812][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,813][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:27,814][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.0038, 0.9962], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,814][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.0659, 0.9341], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,816][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.5783, 0.4217], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,818][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9356, 0.0644], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,820][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9608, 0.0392], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,822][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.5668, 0.4332], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,824][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.0584, 0.9416], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,825][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.1537, 0.8463], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,828][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.3874, 0.6126], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,829][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.1385, 0.8615], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,831][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.9487, 0.0513], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,832][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9172, 0.0828], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:27,832][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0014, 0.5923, 0.4062], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,833][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0053, 0.7618, 0.2329], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,834][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.5251, 0.3875, 0.0874], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,834][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7655, 0.1249, 0.1096], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,835][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.8972, 0.0731, 0.0297], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,837][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.4472, 0.2731, 0.2797], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,839][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0232, 0.4677, 0.5091], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,841][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0745, 0.4624, 0.4631], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,842][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0519, 0.8191, 0.1290], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,844][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0689, 0.4816, 0.4494], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,846][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.6145, 0.0232, 0.3622], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,848][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.9507, 0.0142, 0.0351], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:27,850][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0010, 0.4113, 0.2760, 0.3117], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,852][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0013, 0.1405, 0.7567, 0.1015], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,853][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.5045, 0.3249, 0.0764, 0.0942], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,856][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.7267, 0.0823, 0.1020, 0.0891], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,857][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.5759, 0.1743, 0.2449, 0.0048], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,859][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.3196, 0.2551, 0.2515, 0.1738], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,861][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0112, 0.3355, 0.4032, 0.2501], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,863][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0409, 0.3573, 0.3490, 0.2528], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,864][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0297, 0.4077, 0.4969, 0.0657], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,867][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0480, 0.3498, 0.3271, 0.2750], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,869][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.7913, 0.0579, 0.0715, 0.0794], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,870][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.9058, 0.0186, 0.0336, 0.0420], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:27,872][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0008, 0.3170, 0.2163, 0.2441, 0.2218], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,874][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.0037, 0.0354, 0.4825, 0.2018, 0.2766], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,876][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.2790, 0.2711, 0.0854, 0.1243, 0.2402], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,878][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.4222, 0.1080, 0.1381, 0.1641, 0.1676], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,880][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.4483, 0.2951, 0.2232, 0.0215, 0.0119], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,881][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.2549, 0.2377, 0.2251, 0.1555, 0.1268], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,884][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.0103, 0.2480, 0.2649, 0.2057, 0.2711], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,885][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.0415, 0.2524, 0.2643, 0.1948, 0.2471], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,887][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.0161, 0.2088, 0.4616, 0.2321, 0.0813], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,889][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0372, 0.2783, 0.2599, 0.2173, 0.2073], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,891][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.7438, 0.0543, 0.0533, 0.0350, 0.1136], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,892][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.9190, 0.0175, 0.0282, 0.0216, 0.0137], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:27,895][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0006, 0.2567, 0.1741, 0.1940, 0.1805, 0.1941], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,896][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0023, 0.0623, 0.1387, 0.0979, 0.6520, 0.0468], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,898][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.2561, 0.2328, 0.0752, 0.1017, 0.2008, 0.1334], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,899][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.2669, 0.1028, 0.1286, 0.1626, 0.2317, 0.1073], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,900][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.4554, 0.1984, 0.0889, 0.1390, 0.0388, 0.0794], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,900][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.1914, 0.2257, 0.1905, 0.1380, 0.1280, 0.1264], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,901][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0095, 0.2001, 0.2098, 0.1681, 0.2334, 0.1792], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,902][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0393, 0.2065, 0.2023, 0.1593, 0.2084, 0.1842], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,904][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.0394, 0.2713, 0.2384, 0.1633, 0.1775, 0.1101], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,906][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.0282, 0.2314, 0.2181, 0.1803, 0.1739, 0.1681], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,907][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.7101, 0.0640, 0.0351, 0.0320, 0.0807, 0.0780], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,909][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.9451, 0.0179, 0.0118, 0.0152, 0.0059, 0.0041], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:27,911][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0005, 0.2147, 0.1460, 0.1609, 0.1489, 0.1612, 0.1678],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,913][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0005, 0.0512, 0.0341, 0.0659, 0.4151, 0.2767, 0.1565],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,914][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.1976, 0.1721, 0.0522, 0.0829, 0.1551, 0.1099, 0.2303],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,917][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.3296, 0.0962, 0.0811, 0.1751, 0.1714, 0.0941, 0.0525],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,919][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.4100, 0.1509, 0.0640, 0.1551, 0.0285, 0.1721, 0.0195],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,920][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.1705, 0.1696, 0.1659, 0.1263, 0.1211, 0.1153, 0.1313],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,922][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0087, 0.1634, 0.1606, 0.1375, 0.1851, 0.1556, 0.1889],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,924][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0326, 0.1814, 0.1625, 0.1360, 0.1692, 0.1583, 0.1599],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,926][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.0129, 0.1939, 0.1539, 0.2026, 0.1015, 0.2748, 0.0603],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,928][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.0299, 0.1807, 0.1704, 0.1470, 0.1431, 0.1392, 0.1897],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,930][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.7632, 0.0258, 0.0509, 0.0312, 0.0971, 0.0145, 0.0173],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,931][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.9555, 0.0126, 0.0093, 0.0099, 0.0066, 0.0014, 0.0047],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:27,934][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.0004, 0.1814, 0.1223, 0.1363, 0.1283, 0.1370, 0.1408, 0.1535],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,935][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0006, 0.0058, 0.2382, 0.1912, 0.3788, 0.0767, 0.0739, 0.0348],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,937][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.1512, 0.1400, 0.0489, 0.0748, 0.1280, 0.0923, 0.1900, 0.1747],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,939][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.2164, 0.0602, 0.1041, 0.1410, 0.1923, 0.1296, 0.0847, 0.0716],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,941][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.3607, 0.2154, 0.0701, 0.1277, 0.0344, 0.1296, 0.0332, 0.0288],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,943][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.1305, 0.1651, 0.1526, 0.1157, 0.1147, 0.0962, 0.1319, 0.0934],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,945][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0087, 0.1332, 0.1312, 0.1214, 0.1591, 0.1252, 0.1628, 0.1584],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,947][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0332, 0.1424, 0.1345, 0.1190, 0.1431, 0.1275, 0.1238, 0.1765],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,948][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.0078, 0.0969, 0.2888, 0.1885, 0.1313, 0.1805, 0.0770, 0.0291],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,951][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.0247, 0.1493, 0.1433, 0.1241, 0.1209, 0.1167, 0.1593, 0.1618],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,952][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.6927, 0.0457, 0.0477, 0.0363, 0.0741, 0.0351, 0.0210, 0.0474],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,954][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.9555, 0.0139, 0.0038, 0.0101, 0.0042, 0.0024, 0.0042, 0.0058],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:27,956][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0004, 0.1498, 0.1038, 0.1145, 0.1073, 0.1159, 0.1189, 0.1280, 0.1613],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,958][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0011, 0.0285, 0.0432, 0.0914, 0.2970, 0.1381, 0.1147, 0.2042, 0.0819],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,960][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.1367, 0.1258, 0.0379, 0.0602, 0.1063, 0.0839, 0.1543, 0.1587, 0.1363],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,962][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.1844, 0.0960, 0.0852, 0.1512, 0.1637, 0.0841, 0.0480, 0.0942, 0.0932],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,964][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.2943, 0.0905, 0.1263, 0.1998, 0.0571, 0.0846, 0.0507, 0.0313, 0.0654],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,965][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.1293, 0.1312, 0.1278, 0.0990, 0.0952, 0.0913, 0.1159, 0.0977, 0.1127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,966][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0056, 0.1126, 0.1165, 0.1022, 0.1394, 0.1192, 0.1459, 0.1454, 0.1132],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,967][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0216, 0.1193, 0.1186, 0.1040, 0.1272, 0.1153, 0.1209, 0.1610, 0.1120],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,968][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0195, 0.1768, 0.1380, 0.1864, 0.0837, 0.1682, 0.0795, 0.0989, 0.0489],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,969][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0207, 0.1303, 0.1231, 0.1056, 0.1029, 0.0999, 0.1388, 0.1434, 0.1352],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,970][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.7131, 0.0239, 0.0599, 0.0403, 0.0849, 0.0115, 0.0190, 0.0180, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,972][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.9075, 0.0126, 0.0095, 0.0189, 0.0091, 0.0017, 0.0061, 0.0063, 0.0284],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:27,974][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0005, 0.1289, 0.0898, 0.0993, 0.0938, 0.0999, 0.1026, 0.1118, 0.1396,
        0.1337], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,975][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0014, 0.0717, 0.1322, 0.1336, 0.1520, 0.1979, 0.0752, 0.1534, 0.0650,
        0.0174], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,977][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.1210, 0.0912, 0.0356, 0.0489, 0.0863, 0.0673, 0.1413, 0.1286, 0.1183,
        0.1615], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,979][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.3599, 0.0574, 0.0693, 0.0842, 0.1227, 0.0660, 0.0399, 0.0752, 0.0956,
        0.0298], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,981][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.2792, 0.1507, 0.0984, 0.0605, 0.0332, 0.1413, 0.0419, 0.1082, 0.0421,
        0.0446], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,983][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0831, 0.1201, 0.1211, 0.0900, 0.0832, 0.0942, 0.1094, 0.1016, 0.1117,
        0.0856], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,985][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0061, 0.0873, 0.0909, 0.0817, 0.0986, 0.0816, 0.1050, 0.1120, 0.0899,
        0.2468], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,987][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0291, 0.1134, 0.0937, 0.0873, 0.1015, 0.0918, 0.0907, 0.1456, 0.0939,
        0.1530], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,989][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.0133, 0.1932, 0.1220, 0.0825, 0.0730, 0.2448, 0.0715, 0.1063, 0.0702,
        0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,991][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0192, 0.1132, 0.1087, 0.0932, 0.0910, 0.0884, 0.1210, 0.1242, 0.1185,
        0.1226], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,993][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.5932, 0.0519, 0.0718, 0.0318, 0.1111, 0.0313, 0.0233, 0.0214, 0.0377,
        0.0265], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,995][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.9250, 0.0086, 0.0049, 0.0157, 0.0034, 0.0032, 0.0052, 0.0036, 0.0072,
        0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:27,997][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0003, 0.1150, 0.0777, 0.0862, 0.0809, 0.0882, 0.0909, 0.0997, 0.1253,
        0.1202, 0.1155], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:27,999][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0022, 0.0771, 0.1564, 0.1054, 0.2022, 0.1253, 0.0817, 0.1397, 0.0876,
        0.0186, 0.0039], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,001][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.1033, 0.0763, 0.0263, 0.0443, 0.0756, 0.0520, 0.1221, 0.1173, 0.1182,
        0.1612, 0.1034], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,003][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.1390, 0.0597, 0.0853, 0.1246, 0.1501, 0.0892, 0.0694, 0.0674, 0.1108,
        0.0419, 0.0626], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,004][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.3823, 0.0521, 0.1357, 0.0486, 0.0233, 0.0949, 0.0342, 0.0459, 0.0854,
        0.0595, 0.0380], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,006][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0953, 0.1207, 0.1110, 0.0893, 0.0811, 0.0819, 0.0998, 0.0838, 0.0972,
        0.0772, 0.0627], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,008][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0048, 0.0769, 0.0723, 0.0645, 0.0868, 0.0683, 0.0853, 0.0965, 0.0719,
        0.2085, 0.1643], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,010][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0230, 0.0924, 0.0845, 0.0729, 0.0859, 0.0820, 0.0856, 0.1272, 0.0811,
        0.1402, 0.1252], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,012][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.0086, 0.1306, 0.1805, 0.1034, 0.0739, 0.1256, 0.0523, 0.0679, 0.1358,
        0.0935, 0.0279], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,014][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.0157, 0.1054, 0.0984, 0.0833, 0.0812, 0.0788, 0.1124, 0.1145, 0.1084,
        0.1130, 0.0888], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,016][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.7642, 0.0167, 0.0516, 0.0116, 0.0718, 0.0155, 0.0062, 0.0197, 0.0179,
        0.0082, 0.0166], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,018][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.9222, 0.0040, 0.0069, 0.0112, 0.0082, 0.0022, 0.0031, 0.0031, 0.0091,
        0.0095, 0.0204], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,020][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0004, 0.1032, 0.0710, 0.0781, 0.0726, 0.0796, 0.0816, 0.0875, 0.1100,
        0.1040, 0.0993, 0.1128], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,021][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0010, 0.0241, 0.0359, 0.0683, 0.2565, 0.1018, 0.1012, 0.1792, 0.0628,
        0.0447, 0.0844, 0.0402], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,023][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0924, 0.0747, 0.0233, 0.0409, 0.0738, 0.0535, 0.1000, 0.1036, 0.0918,
        0.1498, 0.1061, 0.0901], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,025][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.1057, 0.0866, 0.0734, 0.1325, 0.1355, 0.0641, 0.0386, 0.0777, 0.0736,
        0.0533, 0.0683, 0.0907], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,027][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.2353, 0.0847, 0.1035, 0.1479, 0.0371, 0.0644, 0.0312, 0.0238, 0.0442,
        0.1247, 0.0540, 0.0492], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,030][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0992, 0.1039, 0.1023, 0.0784, 0.0748, 0.0736, 0.0895, 0.0760, 0.0864,
        0.0703, 0.0629, 0.0828], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,031][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0033, 0.0649, 0.0660, 0.0568, 0.0790, 0.0663, 0.0818, 0.0850, 0.0644,
        0.1936, 0.1500, 0.0889], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,033][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0149, 0.0806, 0.0768, 0.0675, 0.0835, 0.0758, 0.0805, 0.1100, 0.0743,
        0.1348, 0.1138, 0.0876], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,033][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0185, 0.1638, 0.1029, 0.1673, 0.0603, 0.1147, 0.0591, 0.0761, 0.0330,
        0.0917, 0.0817, 0.0308], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,034][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0140, 0.0973, 0.0908, 0.0766, 0.0743, 0.0718, 0.1028, 0.1055, 0.0992,
        0.1042, 0.0832, 0.0803], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,035][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.7032, 0.0189, 0.0452, 0.0374, 0.0692, 0.0102, 0.0187, 0.0163, 0.0273,
        0.0101, 0.0073, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,036][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [orum] are: tensor([9.2648e-01, 3.0630e-03, 3.7355e-03, 8.3339e-03, 3.1076e-03, 4.3629e-04,
        2.0698e-03, 1.9514e-03, 1.2308e-02, 1.1623e-02, 6.7875e-03, 2.0103e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,038][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0003, 0.0921, 0.0628, 0.0697, 0.0661, 0.0720, 0.0737, 0.0809, 0.1010,
        0.0962, 0.0912, 0.1032, 0.0908], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,040][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0009, 0.0368, 0.1025, 0.0823, 0.3250, 0.0803, 0.0496, 0.1033, 0.0414,
        0.0426, 0.0327, 0.0327, 0.0699], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,042][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0999, 0.0701, 0.0223, 0.0411, 0.0684, 0.0450, 0.1020, 0.0931, 0.0897,
        0.1345, 0.0855, 0.0837, 0.0647], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,043][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.2341, 0.0469, 0.0592, 0.0748, 0.0981, 0.0760, 0.0366, 0.0508, 0.0686,
        0.0349, 0.0584, 0.0907, 0.0709], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,045][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1830, 0.0998, 0.0731, 0.0611, 0.0223, 0.0883, 0.0288, 0.1103, 0.0103,
        0.0563, 0.2525, 0.0114, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,047][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0532, 0.1000, 0.0974, 0.0738, 0.0669, 0.0718, 0.0854, 0.0771, 0.0910,
        0.0657, 0.0655, 0.0886, 0.0637], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,049][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0041, 0.0579, 0.0593, 0.0538, 0.0686, 0.0542, 0.0710, 0.0738, 0.0563,
        0.1605, 0.1260, 0.0751, 0.1393], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,051][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0157, 0.0778, 0.0718, 0.0595, 0.0734, 0.0695, 0.0664, 0.1005, 0.0675,
        0.1177, 0.1004, 0.0758, 0.1041], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,053][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0268, 0.1070, 0.1293, 0.0877, 0.0605, 0.1349, 0.0640, 0.0850, 0.0492,
        0.0738, 0.0999, 0.0502, 0.0316], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,055][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0137, 0.0888, 0.0834, 0.0705, 0.0690, 0.0664, 0.0959, 0.0981, 0.0932,
        0.0974, 0.0771, 0.0753, 0.0712], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,057][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.4605, 0.0525, 0.0520, 0.0428, 0.1488, 0.0268, 0.0280, 0.0389, 0.0406,
        0.0213, 0.0169, 0.0476, 0.0233], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,058][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ is] are: tensor([9.1638e-01, 8.6245e-03, 6.4583e-03, 1.0159e-02, 3.4436e-03, 8.4675e-04,
        4.9392e-03, 1.8755e-03, 8.4148e-03, 1.0267e-02, 3.2140e-03, 7.9872e-03,
        1.7385e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,060][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0003, 0.0845, 0.0576, 0.0640, 0.0600, 0.0661, 0.0671, 0.0735, 0.0917,
        0.0876, 0.0828, 0.0937, 0.0832, 0.0877], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,062][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0002, 0.0260, 0.1247, 0.0923, 0.1862, 0.1382, 0.0402, 0.1334, 0.0682,
        0.0153, 0.0224, 0.0460, 0.0954, 0.0114], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,064][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0929, 0.0653, 0.0197, 0.0353, 0.0616, 0.0435, 0.0931, 0.0925, 0.0856,
        0.1276, 0.0756, 0.0754, 0.0625, 0.0694], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,066][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2588, 0.0479, 0.0654, 0.0585, 0.0755, 0.0702, 0.0388, 0.0471, 0.0558,
        0.0326, 0.0562, 0.0716, 0.0726, 0.0490], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,068][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.4233, 0.0821, 0.0708, 0.0076, 0.0116, 0.0771, 0.0230, 0.0507, 0.0157,
        0.0116, 0.1953, 0.0167, 0.0105, 0.0040], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,070][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0584, 0.0971, 0.0938, 0.0713, 0.0622, 0.0663, 0.0780, 0.0699, 0.0830,
        0.0592, 0.0601, 0.0785, 0.0623, 0.0599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,072][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0035, 0.0503, 0.0492, 0.0423, 0.0562, 0.0457, 0.0579, 0.0632, 0.0485,
        0.1348, 0.1059, 0.0639, 0.1189, 0.1597], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,074][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0148, 0.0696, 0.0615, 0.0527, 0.0663, 0.0644, 0.0598, 0.0933, 0.0590,
        0.1064, 0.0951, 0.0664, 0.0916, 0.0990], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,076][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0114, 0.0858, 0.1117, 0.0548, 0.0804, 0.1229, 0.0482, 0.0798, 0.0678,
        0.0479, 0.1082, 0.0636, 0.0993, 0.0183], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,078][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0134, 0.0821, 0.0776, 0.0659, 0.0646, 0.0626, 0.0878, 0.0895, 0.0856,
        0.0890, 0.0726, 0.0703, 0.0667, 0.0722], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,080][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4811, 0.0537, 0.0573, 0.0351, 0.1358, 0.0308, 0.0251, 0.0307, 0.0353,
        0.0275, 0.0133, 0.0403, 0.0133, 0.0208], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,081][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.4220e-01, 4.9965e-03, 4.4347e-03, 5.3084e-03, 2.1901e-03, 6.8292e-04,
        3.5922e-03, 1.7797e-03, 5.0511e-03, 7.6411e-03, 2.2283e-03, 5.1381e-03,
        4.1432e-03, 1.0612e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,083][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0002, 0.0780, 0.0529, 0.0600, 0.0562, 0.0616, 0.0623, 0.0676, 0.0850,
        0.0821, 0.0762, 0.0874, 0.0777, 0.0821, 0.0706], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,085][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0007, 0.0214, 0.0458, 0.0930, 0.3866, 0.0484, 0.0378, 0.0507, 0.0631,
        0.0300, 0.0263, 0.0468, 0.0805, 0.0457, 0.0232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,087][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0981, 0.0562, 0.0188, 0.0336, 0.0552, 0.0402, 0.0862, 0.0809, 0.0750,
        0.1074, 0.0688, 0.0655, 0.0661, 0.0737, 0.0744], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,089][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.2131, 0.0469, 0.0489, 0.0667, 0.0809, 0.0563, 0.0327, 0.0373, 0.0590,
        0.0309, 0.0405, 0.0773, 0.0921, 0.0611, 0.0563], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,091][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.1546, 0.0393, 0.1350, 0.0584, 0.0145, 0.0728, 0.0413, 0.0413, 0.0555,
        0.0609, 0.1172, 0.0577, 0.0243, 0.1242, 0.0033], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,093][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.0320, 0.0901, 0.0923, 0.0687, 0.0615, 0.0665, 0.0824, 0.0673, 0.0880,
        0.0587, 0.0537, 0.0824, 0.0592, 0.0564, 0.0408], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,095][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0029, 0.0412, 0.0396, 0.0391, 0.0499, 0.0386, 0.0480, 0.0538, 0.0402,
        0.1146, 0.0927, 0.0536, 0.1080, 0.1545, 0.1233], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,097][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0142, 0.0628, 0.0555, 0.0500, 0.0590, 0.0540, 0.0489, 0.0830, 0.0516,
        0.0929, 0.0806, 0.0593, 0.0866, 0.0877, 0.1138], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,099][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.0149, 0.0613, 0.0877, 0.0577, 0.0708, 0.1621, 0.0565, 0.0672, 0.0701,
        0.0600, 0.0980, 0.0671, 0.0571, 0.0439, 0.0257], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,100][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.0128, 0.0765, 0.0729, 0.0619, 0.0602, 0.0581, 0.0817, 0.0836, 0.0786,
        0.0815, 0.0668, 0.0650, 0.0620, 0.0671, 0.0712], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,101][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.5587, 0.0267, 0.0446, 0.0414, 0.0894, 0.0170, 0.0213, 0.0209, 0.0364,
        0.0229, 0.0100, 0.0421, 0.0189, 0.0237, 0.0261], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,102][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ same] are: tensor([9.8119e-01, 1.4822e-03, 7.8380e-04, 1.2637e-03, 7.5028e-04, 2.1621e-04,
        5.3767e-04, 1.3564e-03, 1.9769e-03, 1.6279e-03, 7.5203e-04, 2.5167e-03,
        8.5668e-04, 1.2946e-03, 3.3919e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,103][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0002, 0.0734, 0.0502, 0.0560, 0.0516, 0.0567, 0.0581, 0.0636, 0.0801,
        0.0775, 0.0708, 0.0834, 0.0742, 0.0776, 0.0665, 0.0600],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,104][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0007, 0.0339, 0.1886, 0.0455, 0.2115, 0.0527, 0.0459, 0.1743, 0.0232,
        0.0087, 0.0235, 0.0166, 0.0611, 0.0616, 0.0366, 0.0156],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,106][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0906, 0.0569, 0.0155, 0.0265, 0.0483, 0.0351, 0.0758, 0.0830, 0.0729,
        0.1102, 0.0668, 0.0649, 0.0573, 0.0673, 0.0814, 0.0476],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,108][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.2339, 0.0361, 0.0439, 0.0464, 0.0693, 0.0484, 0.0381, 0.0261, 0.0670,
        0.0255, 0.0355, 0.0876, 0.0778, 0.0530, 0.0742, 0.0373],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,110][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.3175, 0.1386, 0.0593, 0.0190, 0.0097, 0.0727, 0.0323, 0.0563, 0.0265,
        0.0208, 0.0668, 0.0293, 0.0133, 0.1053, 0.0307, 0.0018],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,111][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.0539, 0.0863, 0.0872, 0.0623, 0.0579, 0.0607, 0.0729, 0.0633, 0.0781,
        0.0536, 0.0519, 0.0762, 0.0554, 0.0538, 0.0437, 0.0429],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,113][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0032, 0.0364, 0.0337, 0.0352, 0.0455, 0.0337, 0.0442, 0.0478, 0.0352,
        0.0959, 0.0784, 0.0458, 0.0863, 0.1263, 0.1005, 0.1521],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,115][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0128, 0.0558, 0.0486, 0.0428, 0.0529, 0.0456, 0.0474, 0.0709, 0.0446,
        0.0795, 0.0749, 0.0501, 0.0750, 0.0814, 0.1063, 0.1113],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,117][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0073, 0.0801, 0.1150, 0.0398, 0.0601, 0.1006, 0.0539, 0.0589, 0.0597,
        0.0651, 0.0795, 0.0518, 0.0798, 0.0751, 0.0552, 0.0182],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,119][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.0109, 0.0724, 0.0683, 0.0577, 0.0562, 0.0541, 0.0775, 0.0801, 0.0753,
        0.0780, 0.0630, 0.0613, 0.0584, 0.0632, 0.0680, 0.0557],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,121][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.5591, 0.0437, 0.0300, 0.0298, 0.1007, 0.0246, 0.0242, 0.0212, 0.0349,
        0.0162, 0.0115, 0.0399, 0.0151, 0.0187, 0.0208, 0.0097],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,122][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ as] are: tensor([9.6214e-01, 3.8094e-03, 2.5809e-03, 4.3950e-03, 1.2383e-03, 2.8448e-04,
        8.2292e-04, 1.1038e-03, 2.7842e-03, 4.3592e-03, 1.1716e-03, 2.3473e-03,
        2.6653e-03, 3.1357e-03, 1.7488e-03, 5.4106e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,125][circuit_model.py][line:1532][INFO] ##5-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0002, 0.0694, 0.0468, 0.0522, 0.0486, 0.0539, 0.0545, 0.0595, 0.0742,
        0.0715, 0.0669, 0.0767, 0.0688, 0.0723, 0.0622, 0.0555, 0.0666],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,127][circuit_model.py][line:1535][INFO] ##5-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0002, 0.0275, 0.1224, 0.0919, 0.1678, 0.1319, 0.0377, 0.1252, 0.0592,
        0.0150, 0.0216, 0.0390, 0.0788, 0.0118, 0.0318, 0.0286, 0.0096],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,129][circuit_model.py][line:1538][INFO] ##5-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0881, 0.0541, 0.0145, 0.0261, 0.0481, 0.0340, 0.0756, 0.0774, 0.0698,
        0.1105, 0.0604, 0.0612, 0.0537, 0.0600, 0.0713, 0.0457, 0.0496],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,131][circuit_model.py][line:1541][INFO] ##5-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2094, 0.0381, 0.0546, 0.0461, 0.0613, 0.0563, 0.0344, 0.0387, 0.0473,
        0.0272, 0.0444, 0.0579, 0.0578, 0.0393, 0.0875, 0.0626, 0.0370],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,133][circuit_model.py][line:1544][INFO] ##5-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.4294, 0.0657, 0.0762, 0.0072, 0.0098, 0.0645, 0.0249, 0.0460, 0.0134,
        0.0108, 0.1846, 0.0130, 0.0103, 0.0038, 0.0292, 0.0068, 0.0045],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,135][circuit_model.py][line:1547][INFO] ##5-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0474, 0.0832, 0.0816, 0.0619, 0.0535, 0.0572, 0.0684, 0.0602, 0.0744,
        0.0524, 0.0499, 0.0695, 0.0542, 0.0509, 0.0464, 0.0440, 0.0450],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,137][circuit_model.py][line:1550][INFO] ##5-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0030, 0.0321, 0.0300, 0.0287, 0.0367, 0.0287, 0.0364, 0.0412, 0.0314,
        0.0820, 0.0658, 0.0405, 0.0768, 0.1023, 0.0892, 0.1321, 0.1430],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,139][circuit_model.py][line:1553][INFO] ##5-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0133, 0.0517, 0.0432, 0.0393, 0.0469, 0.0461, 0.0426, 0.0710, 0.0435,
        0.0725, 0.0664, 0.0483, 0.0668, 0.0709, 0.0902, 0.0998, 0.0874],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,141][circuit_model.py][line:1556][INFO] ##5-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0130, 0.0778, 0.1000, 0.0519, 0.0759, 0.1048, 0.0460, 0.0737, 0.0634,
        0.0457, 0.0998, 0.0589, 0.0888, 0.0161, 0.0392, 0.0322, 0.0127],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,143][circuit_model.py][line:1559][INFO] ##5-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0109, 0.0684, 0.0647, 0.0548, 0.0534, 0.0515, 0.0731, 0.0750, 0.0710,
        0.0738, 0.0600, 0.0582, 0.0552, 0.0597, 0.0633, 0.0520, 0.0550],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,145][circuit_model.py][line:1562][INFO] ##5-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4429, 0.0476, 0.0516, 0.0363, 0.1290, 0.0263, 0.0253, 0.0300, 0.0332,
        0.0251, 0.0133, 0.0379, 0.0138, 0.0218, 0.0261, 0.0117, 0.0282],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,146][circuit_model.py][line:1565][INFO] ##5-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.6729e-01, 2.4484e-03, 2.1345e-03, 3.1563e-03, 9.8658e-04, 2.7988e-04,
        1.2906e-03, 7.0437e-04, 2.3309e-03, 3.2181e-03, 8.2118e-04, 2.0001e-03,
        1.8389e-03, 4.2923e-03, 7.3384e-04, 1.6948e-03, 4.7802e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,217][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:28,219][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,220][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,221][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,223][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,223][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,224][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,225][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,225][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,226][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,227][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,227][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,228][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,229][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.9588, 0.0412], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,229][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.8409, 0.1591], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,230][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.2248, 0.7752], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,231][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.4918, 0.5082], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,233][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.2343, 0.7657], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,234][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.5403, 0.4597], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,236][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.0021, 0.9979], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,238][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.2881, 0.7119], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,240][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.5219, 0.4781], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,241][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.9965, 0.0035], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,244][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.7619, 0.2381], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,246][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.5228, 0.4772], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,247][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7686, 0.0541, 0.1773], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,249][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.4616, 0.0080, 0.5304], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,251][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.1276, 0.6401, 0.2323], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,253][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0819, 0.8091, 0.1090], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,254][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0475, 0.7352, 0.2173], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,254][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.3336, 0.1118, 0.5545], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,255][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([4.5102e-04, 9.2497e-01, 7.4578e-02], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,256][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0518, 0.6321, 0.3161], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,256][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.3904, 0.1400, 0.4697], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,258][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.9555, 0.0304, 0.0141], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,260][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.1655, 0.0122, 0.8223], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,262][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.8488, 0.0276, 0.1236], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,263][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.7379, 0.0448, 0.0881, 0.1292], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,266][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0277, 0.0097, 0.1699, 0.7927], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,267][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.2436, 0.4251, 0.3137, 0.0177], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,269][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0758, 0.7231, 0.1213, 0.0799], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,271][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0113, 0.5375, 0.3042, 0.1471], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,273][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.2293, 0.1991, 0.3819, 0.1897], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,274][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([4.2923e-05, 4.9001e-01, 2.7562e-01, 2.3432e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,275][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0184, 0.5013, 0.1100, 0.3703], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,277][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.3953, 0.2611, 0.2748, 0.0688], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,279][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.8136, 0.0535, 0.0772, 0.0556], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,281][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.4520, 0.0863, 0.3605, 0.1011], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,283][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.6180, 0.1025, 0.1405, 0.1390], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,285][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.4786, 0.0291, 0.1684, 0.2475, 0.0764], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,286][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.2170, 0.0591, 0.4174, 0.2354, 0.0710], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,289][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.2530, 0.4063, 0.1876, 0.0906, 0.0624], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,290][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.1632, 0.4937, 0.1535, 0.1499, 0.0396], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,292][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0204, 0.4119, 0.2434, 0.2755, 0.0489], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,294][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.2614, 0.1987, 0.2630, 0.2583, 0.0185], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,295][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([6.6421e-05, 1.2660e-01, 5.5604e-02, 7.5680e-01, 6.0937e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,297][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0727, 0.2143, 0.3174, 0.3500, 0.0456], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,299][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.2293, 0.3084, 0.2715, 0.1562, 0.0345], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,301][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.6375, 0.0549, 0.1421, 0.1268, 0.0387], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,303][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.2222, 0.0497, 0.4935, 0.1272, 0.1075], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,305][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.6473, 0.0942, 0.1474, 0.0606, 0.0505], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,307][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.2033, 0.0257, 0.0982, 0.2258, 0.2146, 0.2324], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,308][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.4754, 0.0314, 0.0355, 0.0219, 0.0149, 0.4210], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,311][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.1368, 0.3376, 0.2235, 0.0713, 0.1379, 0.0928], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,312][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.1435, 0.4537, 0.0927, 0.1477, 0.0647, 0.0978], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,314][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0361, 0.2408, 0.1637, 0.3209, 0.0908, 0.1476], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,316][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.3787, 0.4358, 0.0618, 0.0623, 0.0395, 0.0219], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,318][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([4.1789e-05, 1.7066e-01, 1.2953e-02, 1.9257e-01, 5.9971e-01, 2.4064e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,319][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0817, 0.2633, 0.2124, 0.1332, 0.1206, 0.1888], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,321][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.5209, 0.2515, 0.0622, 0.0612, 0.0291, 0.0751], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,323][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.4820, 0.0272, 0.1382, 0.1217, 0.2216, 0.0093], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,325][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.3898, 0.0668, 0.2457, 0.1664, 0.0518, 0.0794], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,327][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.7562, 0.1246, 0.0453, 0.0407, 0.0207, 0.0125], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,327][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.5130, 0.0261, 0.1012, 0.0881, 0.0647, 0.0939, 0.1129],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,328][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.1412, 0.0059, 0.0816, 0.1453, 0.3773, 0.1190, 0.1298],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,329][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.3042, 0.1705, 0.0656, 0.0573, 0.0806, 0.0596, 0.2622],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,330][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.1133, 0.2783, 0.0405, 0.0446, 0.0333, 0.0790, 0.4109],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,331][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.0333, 0.1471, 0.0329, 0.0654, 0.0277, 0.0668, 0.6268],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,333][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.0763, 0.0371, 0.0607, 0.3548, 0.3809, 0.0834, 0.0068],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,334][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.0004, 0.1721, 0.0017, 0.1320, 0.2680, 0.2373, 0.1884],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,336][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0921, 0.2366, 0.0446, 0.0538, 0.0263, 0.2686, 0.2780],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,338][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.4346, 0.0852, 0.0766, 0.1522, 0.0931, 0.0514, 0.1071],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,340][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.6952, 0.0600, 0.0422, 0.0759, 0.0805, 0.0325, 0.0138],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,341][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.8117, 0.0753, 0.0474, 0.0359, 0.0104, 0.0072, 0.0122],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,343][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.8507, 0.0706, 0.0273, 0.0231, 0.0148, 0.0022, 0.0113],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,345][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.1597, 0.0138, 0.0324, 0.1506, 0.2477, 0.0868, 0.0685, 0.2406],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,347][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.2867, 0.0045, 0.0095, 0.0054, 0.0419, 0.0535, 0.5630, 0.0355],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,349][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.1727, 0.0921, 0.0437, 0.0417, 0.0621, 0.0357, 0.3032, 0.2488],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,351][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.1152, 0.0983, 0.0293, 0.0447, 0.0246, 0.0269, 0.4859, 0.1751],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,353][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0410, 0.0526, 0.0348, 0.0653, 0.0218, 0.0585, 0.4574, 0.2685],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,354][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.2024, 0.1208, 0.0571, 0.0848, 0.4160, 0.0143, 0.1002, 0.0045],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,356][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([3.1947e-04, 8.5250e-02, 5.0217e-03, 1.3186e-01, 3.9347e-01, 3.2256e-02,
        3.0213e-01, 4.9689e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,357][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.2418, 0.2038, 0.0623, 0.0698, 0.0347, 0.0870, 0.1241, 0.1765],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,359][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.4743, 0.0741, 0.0404, 0.0558, 0.0686, 0.0200, 0.1796, 0.0872],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,362][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.4941, 0.0426, 0.1199, 0.0906, 0.1053, 0.0214, 0.1193, 0.0068],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,364][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.5556, 0.1616, 0.0511, 0.0816, 0.0235, 0.0350, 0.0318, 0.0599],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,365][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.8275, 0.0946, 0.0097, 0.0259, 0.0131, 0.0050, 0.0128, 0.0114],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,368][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.5017, 0.0131, 0.1004, 0.0808, 0.0619, 0.0907, 0.0734, 0.0463, 0.0317],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,369][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.6228, 0.0056, 0.0457, 0.0356, 0.1298, 0.0708, 0.0271, 0.0530, 0.0095],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,371][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0809, 0.0564, 0.0130, 0.0154, 0.0238, 0.0426, 0.1204, 0.4981, 0.1496],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,373][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0399, 0.0643, 0.0123, 0.0145, 0.0137, 0.0437, 0.1886, 0.3509, 0.2720],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,375][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0124, 0.0158, 0.0068, 0.0225, 0.0060, 0.0174, 0.2609, 0.2635, 0.3947],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,377][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.1295, 0.0804, 0.0628, 0.2556, 0.2905, 0.0506, 0.0451, 0.0751, 0.0104],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,378][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([1.0082e-04, 7.2531e-02, 6.8223e-04, 4.5253e-02, 6.9117e-02, 1.6479e-01,
        2.4694e-01, 3.7542e-01, 2.5168e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,380][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0140, 0.0147, 0.0136, 0.0237, 0.0064, 0.0359, 0.2049, 0.3420, 0.3448],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,382][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.4012, 0.0265, 0.0336, 0.0535, 0.0363, 0.0245, 0.0437, 0.3154, 0.0655],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,384][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.5909, 0.0481, 0.0459, 0.0789, 0.1372, 0.0175, 0.0371, 0.0388, 0.0056],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,386][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.6937, 0.0289, 0.0969, 0.0556, 0.0159, 0.0095, 0.0102, 0.0207, 0.0687],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,388][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.6753, 0.1055, 0.0329, 0.0630, 0.0316, 0.0032, 0.0213, 0.0130, 0.0541],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,390][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.3649, 0.0045, 0.0510, 0.0493, 0.0446, 0.0430, 0.0556, 0.1943, 0.0495,
        0.1434], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,392][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0033, 0.0007, 0.0086, 0.0070, 0.0075, 0.0761, 0.0462, 0.5701, 0.0097,
        0.2708], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,394][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.0891, 0.0228, 0.0149, 0.0101, 0.0193, 0.0218, 0.2359, 0.2873, 0.1840,
        0.1150], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,394][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.0300, 0.0658, 0.0080, 0.0200, 0.0114, 0.0294, 0.1223, 0.2911, 0.3570,
        0.0650], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,395][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0131, 0.0210, 0.0134, 0.0327, 0.0101, 0.0307, 0.2649, 0.3320, 0.2153,
        0.0667], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,396][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.0044, 0.0171, 0.0332, 0.0809, 0.0256, 0.1290, 0.0604, 0.6032, 0.0287,
        0.0177], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,397][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([6.8031e-05, 2.3724e-02, 5.5488e-03, 1.9607e-01, 6.4458e-02, 3.1817e-02,
        1.4267e-01, 1.4150e-01, 2.2269e-02, 3.7189e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,398][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0589, 0.0411, 0.0063, 0.0288, 0.0145, 0.1001, 0.0610, 0.3210, 0.1999,
        0.1684], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,400][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.0418, 0.0180, 0.0216, 0.0382, 0.0123, 0.0334, 0.0538, 0.6409, 0.0939,
        0.0461], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,402][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.5827, 0.0266, 0.0787, 0.0780, 0.0384, 0.0392, 0.0577, 0.0447, 0.0500,
        0.0041], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,404][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.2602, 0.1096, 0.1619, 0.1862, 0.0476, 0.0157, 0.0108, 0.0143, 0.0295,
        0.1643], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,406][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.7333, 0.0660, 0.0197, 0.0607, 0.0096, 0.0063, 0.0166, 0.0061, 0.0112,
        0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,408][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0794, 0.0027, 0.0191, 0.0599, 0.0418, 0.0204, 0.0506, 0.1193, 0.0451,
        0.2190, 0.3429], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,409][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([1.6502e-02, 4.8482e-04, 1.8953e-03, 1.7669e-02, 2.0363e-02, 2.8207e-02,
        1.3752e-01, 1.4055e-01, 3.5813e-02, 5.4229e-01, 5.8709e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,411][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.0553, 0.0472, 0.0248, 0.0286, 0.0145, 0.0126, 0.1312, 0.1565, 0.1656,
        0.0870, 0.2767], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,413][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.0507, 0.0576, 0.0139, 0.0229, 0.0175, 0.0206, 0.1391, 0.1712, 0.2149,
        0.1193, 0.1723], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,415][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0294, 0.0206, 0.0155, 0.0342, 0.0076, 0.0110, 0.2152, 0.1762, 0.2129,
        0.0994, 0.1782], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,417][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.0600, 0.0853, 0.0262, 0.1660, 0.0947, 0.0632, 0.1334, 0.1797, 0.0568,
        0.1170, 0.0178], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,418][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([5.3202e-04, 2.7143e-02, 2.0143e-03, 5.0247e-02, 5.9236e-02, 9.1642e-03,
        6.1436e-02, 8.0038e-02, 2.7962e-02, 6.2588e-01, 5.6353e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,420][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.2001, 0.0168, 0.0160, 0.0565, 0.0034, 0.0261, 0.1067, 0.0699, 0.1256,
        0.0858, 0.2931], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,422][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1005, 0.0232, 0.0131, 0.0276, 0.0180, 0.0208, 0.1279, 0.1986, 0.1435,
        0.1587, 0.1681], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,424][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.2842, 0.0208, 0.0469, 0.0844, 0.2055, 0.0436, 0.0914, 0.1117, 0.0519,
        0.0486, 0.0110], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,426][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.2382, 0.0332, 0.2395, 0.0922, 0.0570, 0.0132, 0.0207, 0.0340, 0.0614,
        0.0359, 0.1747], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,428][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.7571, 0.0227, 0.0239, 0.0308, 0.0288, 0.0058, 0.0077, 0.0061, 0.0146,
        0.0253, 0.0771], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,430][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.5587, 0.0112, 0.0992, 0.0640, 0.0435, 0.0623, 0.0586, 0.0244, 0.0258,
        0.0247, 0.0165, 0.0111], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,432][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.6875, 0.0020, 0.0352, 0.0161, 0.0658, 0.1000, 0.0121, 0.0158, 0.0031,
        0.0369, 0.0161, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,435][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0314, 0.0109, 0.0023, 0.0038, 0.0054, 0.0069, 0.0284, 0.1611, 0.0344,
        0.1635, 0.4591, 0.0928], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,436][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0360, 0.0311, 0.0057, 0.0073, 0.0066, 0.0195, 0.1002, 0.2007, 0.1525,
        0.0723, 0.1336, 0.2346], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,438][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0113, 0.0045, 0.0014, 0.0052, 0.0013, 0.0035, 0.0636, 0.0870, 0.1065,
        0.0448, 0.1540, 0.5168], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,440][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0997, 0.0937, 0.0846, 0.1603, 0.2524, 0.0774, 0.0380, 0.0773, 0.0102,
        0.0396, 0.0378, 0.0291], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,442][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([5.4165e-05, 6.8244e-03, 7.8326e-05, 6.7135e-03, 6.1074e-03, 1.2933e-02,
        2.9401e-02, 9.2798e-02, 3.2412e-03, 6.3171e-01, 1.5502e-01, 5.5122e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,443][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0088, 0.0039, 0.0028, 0.0064, 0.0015, 0.0116, 0.1001, 0.2161, 0.1423,
        0.0491, 0.1035, 0.3538], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,446][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.3160, 0.0163, 0.0304, 0.0283, 0.0237, 0.0121, 0.0210, 0.1172, 0.0296,
        0.1038, 0.2068, 0.0949], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,448][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.4709, 0.0523, 0.0456, 0.0757, 0.1471, 0.0124, 0.0351, 0.0398, 0.0049,
        0.0422, 0.0721, 0.0019], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,449][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.6232, 0.0313, 0.0646, 0.0319, 0.0169, 0.0042, 0.0053, 0.0163, 0.0447,
        0.0268, 0.0887, 0.0462], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,452][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.7926, 0.0279, 0.0130, 0.0270, 0.0098, 0.0008, 0.0066, 0.0035, 0.0218,
        0.0305, 0.0176, 0.0488], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,453][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.3775, 0.0061, 0.0167, 0.0193, 0.0432, 0.0539, 0.0211, 0.1419, 0.0538,
        0.0970, 0.0697, 0.0396, 0.0600], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,455][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0877, 0.0017, 0.0338, 0.0473, 0.1841, 0.1537, 0.0093, 0.1872, 0.0048,
        0.1396, 0.0447, 0.0083, 0.0977], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,457][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0528, 0.0350, 0.0076, 0.0207, 0.0089, 0.0051, 0.0715, 0.0651, 0.0977,
        0.0587, 0.2645, 0.2555, 0.0571], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,459][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0441, 0.0278, 0.0043, 0.0052, 0.0040, 0.0100, 0.0999, 0.0771, 0.2561,
        0.0477, 0.0754, 0.3170, 0.0314], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,461][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0129, 0.0102, 0.0082, 0.0113, 0.0042, 0.0079, 0.0636, 0.1417, 0.0684,
        0.0373, 0.1716, 0.2580, 0.2047], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,462][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0595, 0.0543, 0.0417, 0.1457, 0.1243, 0.0507, 0.0243, 0.1928, 0.0365,
        0.0212, 0.0770, 0.0469, 0.1252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,462][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([7.1346e-05, 1.2335e-02, 1.4867e-03, 2.8525e-02, 3.4633e-02, 1.0952e-02,
        5.8929e-02, 7.4477e-02, 2.4833e-02, 3.0947e-01, 1.3922e-01, 1.3187e-01,
        1.7320e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,463][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0228, 0.0353, 0.0121, 0.0133, 0.0039, 0.0259, 0.0218, 0.0394, 0.3066,
        0.0597, 0.1442, 0.2579, 0.0571], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,465][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1689, 0.0242, 0.0177, 0.0242, 0.0262, 0.0143, 0.0162, 0.1704, 0.0457,
        0.0259, 0.2183, 0.2057, 0.0424], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,467][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.5832, 0.0133, 0.0338, 0.0307, 0.0454, 0.0075, 0.0410, 0.0386, 0.0391,
        0.0190, 0.0498, 0.0171, 0.0814], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,469][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.1874, 0.0521, 0.0884, 0.0406, 0.0446, 0.0091, 0.0267, 0.0265, 0.0533,
        0.0314, 0.1077, 0.0548, 0.2773], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,470][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.6308, 0.0932, 0.0312, 0.0387, 0.0103, 0.0018, 0.0193, 0.0039, 0.0166,
        0.0242, 0.0085, 0.0141, 0.1074], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,473][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.3562, 0.0072, 0.0244, 0.0361, 0.0240, 0.0505, 0.0226, 0.0624, 0.0291,
        0.0845, 0.0580, 0.0160, 0.1843, 0.0446], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,474][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0413, 0.0033, 0.0247, 0.0392, 0.0504, 0.1853, 0.0440, 0.1195, 0.0088,
        0.1194, 0.0540, 0.0257, 0.1646, 0.1198], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,476][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.2101, 0.0327, 0.0072, 0.0136, 0.0048, 0.0062, 0.0410, 0.0730, 0.0712,
        0.0593, 0.1298, 0.1549, 0.0808, 0.1156], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,478][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1181, 0.0215, 0.0062, 0.0078, 0.0041, 0.0077, 0.0691, 0.0583, 0.1364,
        0.0564, 0.0891, 0.1578, 0.0490, 0.2185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,480][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0316, 0.0054, 0.0043, 0.0097, 0.0023, 0.0047, 0.0379, 0.0837, 0.0259,
        0.0269, 0.1451, 0.0814, 0.3004, 0.2406], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,482][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0259, 0.0291, 0.0162, 0.0447, 0.0294, 0.0218, 0.0136, 0.0698, 0.0335,
        0.0148, 0.0659, 0.0451, 0.4159, 0.1743], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,484][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([2.1237e-05, 2.5850e-03, 4.8285e-04, 5.3150e-03, 4.8444e-03, 1.0562e-03,
        6.3259e-03, 8.8380e-03, 8.6454e-03, 5.1891e-02, 1.6500e-02, 4.3555e-02,
        1.4332e-01, 7.0662e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,486][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0640, 0.0314, 0.0093, 0.0163, 0.0043, 0.0401, 0.0241, 0.0828, 0.1227,
        0.0686, 0.2324, 0.1032, 0.0733, 0.1275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,488][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.3396, 0.0335, 0.0155, 0.0130, 0.0179, 0.0137, 0.0238, 0.1032, 0.0365,
        0.0236, 0.1674, 0.0778, 0.0928, 0.0416], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,490][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.6887, 0.0078, 0.0220, 0.0182, 0.0301, 0.0069, 0.0234, 0.0208, 0.0192,
        0.0095, 0.0473, 0.0072, 0.0537, 0.0453], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,492][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.3837, 0.0415, 0.0336, 0.0269, 0.0202, 0.0074, 0.0122, 0.0243, 0.0467,
        0.0343, 0.0700, 0.0432, 0.2105, 0.0456], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,493][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.7668, 0.0591, 0.0203, 0.0180, 0.0065, 0.0016, 0.0131, 0.0033, 0.0099,
        0.0168, 0.0063, 0.0101, 0.0181, 0.0501], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,496][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.1748, 0.0036, 0.0224, 0.0632, 0.0524, 0.0712, 0.0277, 0.0718, 0.0119,
        0.0385, 0.0267, 0.0063, 0.1504, 0.1776, 0.1015], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,498][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0065, 0.0005, 0.0077, 0.0781, 0.0816, 0.2562, 0.0420, 0.0748, 0.0311,
        0.0624, 0.0514, 0.0590, 0.1064, 0.1212, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,499][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.1514, 0.0160, 0.0052, 0.0124, 0.0037, 0.0028, 0.0333, 0.0438, 0.0164,
        0.0105, 0.0652, 0.0225, 0.1025, 0.3328, 0.1816], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,502][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.0771, 0.0191, 0.0045, 0.0038, 0.0013, 0.0046, 0.0387, 0.0320, 0.0879,
        0.0184, 0.0732, 0.1116, 0.0322, 0.1631, 0.3325], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,503][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([4.1564e-02, 1.7759e-03, 1.6221e-03, 2.1926e-03, 3.7200e-04, 5.7050e-04,
        5.6188e-03, 1.7444e-02, 5.7917e-03, 3.0358e-03, 3.8014e-02, 1.4571e-02,
        5.1661e-02, 7.2730e-02, 7.4304e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,505][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.0074, 0.0143, 0.0380, 0.1439, 0.0906, 0.0569, 0.0357, 0.1145, 0.0353,
        0.0125, 0.0240, 0.0456, 0.1455, 0.2310, 0.0048], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,506][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([1.3286e-05, 7.3084e-04, 2.5666e-05, 3.2017e-03, 1.5871e-03, 5.5628e-04,
        1.3930e-03, 2.6226e-03, 1.8254e-03, 1.4492e-02, 1.0724e-02, 8.6752e-03,
        3.1057e-02, 4.8139e-01, 4.4170e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,508][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.1449, 0.0406, 0.0071, 0.0249, 0.0064, 0.0087, 0.0063, 0.0151, 0.0178,
        0.0152, 0.1055, 0.0173, 0.0443, 0.1073, 0.4386], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,510][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.1149, 0.0137, 0.0233, 0.0236, 0.0169, 0.0113, 0.0192, 0.0634, 0.0271,
        0.0312, 0.2074, 0.0739, 0.0878, 0.1970, 0.0891], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,513][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.6240, 0.0108, 0.0255, 0.0263, 0.0428, 0.0095, 0.0322, 0.0321, 0.0244,
        0.0064, 0.0281, 0.0154, 0.0584, 0.0557, 0.0086], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,514][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.6807, 0.0464, 0.0311, 0.0119, 0.0107, 0.0053, 0.0086, 0.0173, 0.0137,
        0.0078, 0.0268, 0.0166, 0.0587, 0.0276, 0.0368], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,516][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.8383, 0.0331, 0.0063, 0.0072, 0.0043, 0.0009, 0.0031, 0.0055, 0.0074,
        0.0060, 0.0044, 0.0104, 0.0062, 0.0090, 0.0581], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,518][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.3697, 0.0042, 0.0475, 0.0398, 0.0495, 0.0401, 0.0337, 0.0616, 0.0476,
        0.0227, 0.0194, 0.0178, 0.0709, 0.0355, 0.0529, 0.0873],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,520][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0262, 0.0010, 0.0283, 0.0439, 0.3110, 0.1852, 0.0293, 0.0817, 0.0131,
        0.0481, 0.0210, 0.0276, 0.0341, 0.0576, 0.0156, 0.0763],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,522][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.0536, 0.0107, 0.0022, 0.0035, 0.0015, 0.0015, 0.0089, 0.0193, 0.0144,
        0.0088, 0.0476, 0.0316, 0.0208, 0.1184, 0.5903, 0.0670],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,525][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.1413, 0.0128, 0.0027, 0.0042, 0.0025, 0.0033, 0.0338, 0.0224, 0.0855,
        0.0325, 0.0500, 0.0674, 0.0278, 0.1298, 0.3006, 0.0833],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,526][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([2.0190e-03, 2.2116e-04, 1.4468e-04, 5.1008e-04, 2.1022e-04, 2.9496e-04,
        1.1099e-03, 4.7788e-03, 1.2401e-03, 1.3286e-03, 8.9513e-03, 3.4989e-03,
        1.2146e-02, 3.1993e-02, 8.6657e-01, 6.4978e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,528][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.0827, 0.0263, 0.0470, 0.0783, 0.1474, 0.0493, 0.0133, 0.0708, 0.0188,
        0.0091, 0.0263, 0.0311, 0.0746, 0.1272, 0.0595, 0.1383],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,529][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([2.3730e-06, 3.6101e-04, 2.6666e-05, 1.0630e-03, 2.2394e-03, 5.7463e-04,
        9.0214e-04, 2.2342e-03, 4.4356e-04, 7.3000e-03, 6.5298e-03, 1.6744e-03,
        8.5039e-03, 2.2793e-01, 6.3859e-01, 1.0162e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,529][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([3.3630e-03, 8.9458e-04, 5.1180e-04, 1.0744e-03, 3.0965e-04, 7.3397e-04,
        1.1829e-03, 1.6197e-03, 3.9667e-03, 2.7183e-03, 2.6959e-02, 3.6532e-03,
        3.3716e-03, 2.0951e-02, 8.5889e-01, 6.9797e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,530][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.1760, 0.0093, 0.0071, 0.0129, 0.0224, 0.0087, 0.0126, 0.0267, 0.0240,
        0.0264, 0.0872, 0.0793, 0.0534, 0.1134, 0.1725, 0.1680],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,531][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.8140, 0.0064, 0.0122, 0.0128, 0.0120, 0.0038, 0.0085, 0.0095, 0.0085,
        0.0045, 0.0205, 0.0037, 0.0151, 0.0265, 0.0132, 0.0289],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,533][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.7959, 0.0100, 0.0138, 0.0046, 0.0052, 0.0021, 0.0031, 0.0068, 0.0095,
        0.0049, 0.0277, 0.0097, 0.0326, 0.0123, 0.0247, 0.0372],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,534][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([8.0115e-01, 5.1287e-02, 1.2504e-02, 1.8177e-02, 4.5468e-03, 7.6739e-04,
        3.1201e-03, 2.9525e-03, 8.3058e-03, 1.3949e-02, 4.0446e-03, 6.9823e-03,
        1.3316e-02, 1.3920e-02, 1.1769e-02, 3.3202e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:28,536][circuit_model.py][line:1570][INFO] ##5-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.4392, 0.0045, 0.0157, 0.0169, 0.0128, 0.0318, 0.0130, 0.0371, 0.0170,
        0.0351, 0.0231, 0.0064, 0.0790, 0.0223, 0.0735, 0.1393, 0.0333],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,538][circuit_model.py][line:1573][INFO] ##5-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0279, 0.0014, 0.0088, 0.0147, 0.0222, 0.0604, 0.0250, 0.0643, 0.0063,
        0.0832, 0.0220, 0.0172, 0.0859, 0.0543, 0.2292, 0.1807, 0.0964],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,540][circuit_model.py][line:1576][INFO] ##5-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3254, 0.0072, 0.0017, 0.0027, 0.0009, 0.0011, 0.0052, 0.0107, 0.0101,
        0.0110, 0.0277, 0.0137, 0.0142, 0.0259, 0.2021, 0.1716, 0.1686],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,542][circuit_model.py][line:1579][INFO] ##5-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2293, 0.0049, 0.0017, 0.0021, 0.0015, 0.0027, 0.0122, 0.0140, 0.0265,
        0.0143, 0.0279, 0.0196, 0.0110, 0.0541, 0.2866, 0.0909, 0.2007],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,543][circuit_model.py][line:1582][INFO] ##5-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([1.0376e-02, 2.0639e-04, 1.0449e-04, 4.6612e-04, 1.5323e-04, 2.5926e-04,
        1.0606e-03, 3.0840e-03, 7.8115e-04, 1.1199e-03, 7.8911e-03, 1.2797e-03,
        1.1082e-02, 1.0543e-02, 6.9061e-01, 1.4305e-01, 1.1793e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,545][circuit_model.py][line:1585][INFO] ##5-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0353, 0.0105, 0.0058, 0.0163, 0.0090, 0.0061, 0.0049, 0.0222, 0.0111,
        0.0080, 0.0216, 0.0120, 0.1033, 0.0540, 0.1575, 0.3600, 0.1622],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,546][circuit_model.py][line:1588][INFO] ##5-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([4.7827e-06, 1.4758e-04, 2.2154e-05, 5.0096e-04, 4.0688e-04, 9.8498e-05,
        3.3716e-04, 5.5729e-04, 2.9622e-04, 3.6361e-03, 1.1016e-03, 9.9376e-04,
        9.2740e-03, 4.9108e-02, 5.1218e-01, 1.1837e-01, 3.0296e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,548][circuit_model.py][line:1591][INFO] ##5-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0534, 0.0063, 0.0015, 0.0035, 0.0009, 0.0093, 0.0040, 0.0143, 0.0155,
        0.0084, 0.0536, 0.0097, 0.0143, 0.0266, 0.2982, 0.3346, 0.1460],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,550][circuit_model.py][line:1594][INFO] ##5-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.3889, 0.0089, 0.0045, 0.0036, 0.0055, 0.0036, 0.0055, 0.0228, 0.0096,
        0.0078, 0.0411, 0.0132, 0.0194, 0.0111, 0.1475, 0.2529, 0.0540],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,553][circuit_model.py][line:1597][INFO] ##5-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.8390, 0.0030, 0.0095, 0.0069, 0.0094, 0.0026, 0.0065, 0.0080, 0.0062,
        0.0032, 0.0176, 0.0018, 0.0128, 0.0122, 0.0095, 0.0314, 0.0205],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,554][circuit_model.py][line:1600][INFO] ##5-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.7051, 0.0269, 0.0148, 0.0092, 0.0090, 0.0041, 0.0035, 0.0100, 0.0124,
        0.0148, 0.0337, 0.0076, 0.0600, 0.0145, 0.0167, 0.0318, 0.0259],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,556][circuit_model.py][line:1603][INFO] ##5-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([8.6384e-01, 3.0058e-02, 1.0294e-02, 1.1173e-02, 3.0386e-03, 6.5216e-04,
        4.2861e-03, 1.4021e-03, 4.8238e-03, 7.2906e-03, 2.3531e-03, 3.9371e-03,
        7.6574e-03, 1.8329e-02, 3.9424e-03, 6.2613e-03, 2.0656e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:28,559][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:28,561][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  474],
        [14499],
        [10359],
        [ 6694],
        [ 9569],
        [17258],
        [ 8000],
        [ 6583],
        [15759],
        [ 2964],
        [ 3000],
        [ 8133],
        [ 2493],
        [  341],
        [ 2500],
        [  932],
        [  302]], device='cuda:0')
[2024-07-23 21:05:28,563][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  453],
        [22363],
        [13477],
        [ 8485],
        [11260],
        [21498],
        [11515],
        [ 7711],
        [20967],
        [ 3295],
        [ 2470],
        [ 9587],
        [ 2521],
        [  462],
        [ 2447],
        [ 1197],
        [  437]], device='cuda:0')
[2024-07-23 21:05:28,566][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[29687],
        [17651],
        [15733],
        [15148],
        [15319],
        [15368],
        [15539],
        [15592],
        [15910],
        [16465],
        [16731],
        [16849],
        [17012],
        [17322],
        [17276],
        [17085],
        [17264]], device='cuda:0')
[2024-07-23 21:05:28,568][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[3334],
        [  50],
        [  92],
        [1155],
        [1132],
        [ 811],
        [ 765],
        [1042],
        [ 981],
        [ 851],
        [ 860],
        [1156],
        [1125],
        [1268],
        [1238],
        [1109],
        [1227]], device='cuda:0')
[2024-07-23 21:05:28,570][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[15132],
        [22386],
        [21910],
        [20536],
        [14940],
        [14722],
        [15068],
        [14592],
        [14985],
        [14413],
        [13933],
        [14148],
        [14217],
        [14144],
        [13644],
        [13576],
        [13408]], device='cuda:0')
[2024-07-23 21:05:28,572][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 6306],
        [ 5166],
        [ 7227],
        [10854],
        [14687],
        [16301],
        [17259],
        [19774],
        [19568],
        [19184],
        [21974],
        [21619],
        [21732],
        [21380],
        [21059],
        [21483],
        [21361]], device='cuda:0')
[2024-07-23 21:05:28,574][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[5798],
        [3183],
        [2302],
        [4217],
        [1578],
        [ 935],
        [1562],
        [1143],
        [2909],
        [3745],
        [9439],
        [5570],
        [6941],
        [7918],
        [7112],
        [2741],
        [7723]], device='cuda:0')
[2024-07-23 21:05:28,576][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 7203],
        [20760],
        [19899],
        [21065],
        [21347],
        [23028],
        [23544],
        [24244],
        [24554],
        [25039],
        [24480],
        [24651],
        [24996],
        [25161],
        [25325],
        [25018],
        [25232]], device='cuda:0')
[2024-07-23 21:05:28,578][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[19168],
        [29640],
        [24731],
        [27320],
        [31822],
        [32063],
        [32418],
        [33632],
        [33441],
        [34053],
        [33982],
        [33715],
        [34364],
        [35563],
        [34502],
        [34063],
        [34787]], device='cuda:0')
[2024-07-23 21:05:28,580][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 711],
        [9527],
        [2666],
        [1525],
        [1028],
        [ 855],
        [ 701],
        [ 598],
        [ 611],
        [ 695],
        [ 768],
        [ 772],
        [ 797],
        [ 764],
        [ 887],
        [ 780],
        [ 769]], device='cuda:0')
[2024-07-23 21:05:28,582][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[13585],
        [18554],
        [18654],
        [18119],
        [18403],
        [17192],
        [17344],
        [17179],
        [17684],
        [16627],
        [16341],
        [16961],
        [15629],
        [15095],
        [15165],
        [16073],
        [15426]], device='cuda:0')
[2024-07-23 21:05:28,584][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 9421],
        [10582],
        [10692],
        [10206],
        [ 9466],
        [ 9322],
        [ 9523],
        [ 9302],
        [ 9035],
        [ 8572],
        [ 8411],
        [ 8417],
        [ 8288],
        [ 8103],
        [ 8106],
        [ 8171],
        [ 8088]], device='cuda:0')
[2024-07-23 21:05:28,586][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[43907],
        [38500],
        [43919],
        [41792],
        [42499],
        [42546],
        [44540],
        [43898],
        [44926],
        [43544],
        [45214],
        [45171],
        [43582],
        [43505],
        [44462],
        [43814],
        [43536]], device='cuda:0')
[2024-07-23 21:05:28,588][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[20854],
        [ 3724],
        [16603],
        [14757],
        [16086],
        [15307],
        [16938],
        [16037],
        [16697],
        [16285],
        [18385],
        [18710],
        [17155],
        [18354],
        [20393],
        [19383],
        [19543]], device='cuda:0')
[2024-07-23 21:05:28,590][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[23264],
        [19816],
        [21515],
        [20668],
        [19465],
        [21693],
        [19920],
        [20507],
        [19233],
        [15678],
        [16740],
        [18431],
        [16931],
        [13888],
        [17601],
        [19431],
        [13200]], device='cuda:0')
[2024-07-23 21:05:28,592][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[4845],
        [4463],
        [1813],
        [1336],
        [ 521],
        [ 596],
        [ 545],
        [1079],
        [ 648],
        [ 475],
        [ 463],
        [ 628],
        [ 704],
        [ 557],
        [ 574],
        [ 750],
        [1227]], device='cuda:0')
[2024-07-23 21:05:28,595][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 7151],
        [ 8199],
        [16327],
        [21128],
        [17302],
        [13135],
        [14308],
        [18353],
        [16809],
        [15874],
        [12589],
        [15600],
        [15264],
        [17774],
        [17145],
        [14316],
        [20545]], device='cuda:0')
[2024-07-23 21:05:28,597][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[ 4740],
        [ 4052],
        [ 3654],
        [ 4140],
        [ 6092],
        [ 7770],
        [14040],
        [14104],
        [11500],
        [11856],
        [ 9143],
        [ 5976],
        [ 5884],
        [ 5852],
        [ 4976],
        [ 3661],
        [ 2741]], device='cuda:0')
[2024-07-23 21:05:28,599][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[11976],
        [ 3126],
        [ 1516],
        [ 1764],
        [ 3001],
        [ 3787],
        [ 2276],
        [ 2487],
        [ 2773],
        [ 2694],
        [ 3886],
        [ 2985],
        [ 2988],
        [ 4483],
        [ 2945],
        [ 3626],
        [ 4475]], device='cuda:0')
[2024-07-23 21:05:28,600][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[516],
        [126],
        [307],
        [634],
        [711],
        [661],
        [ 23],
        [ 63],
        [286],
        [229],
        [209],
        [897],
        [462],
        [525],
        [137],
        [124],
        [167]], device='cuda:0')
[2024-07-23 21:05:28,601][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[16768],
        [ 8507],
        [15331],
        [16304],
        [17177],
        [10585],
        [19265],
        [15861],
        [19061],
        [22019],
        [21257],
        [18876],
        [23308],
        [24421],
        [22165],
        [20488],
        [18354]], device='cuda:0')
[2024-07-23 21:05:28,604][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[12719],
        [17861],
        [19583],
        [23281],
        [ 5632],
        [14335],
        [ 8160],
        [ 7689],
        [ 7008],
        [ 8508],
        [10584],
        [10360],
        [ 7469],
        [ 6895],
        [17535],
        [20391],
        [18910]], device='cuda:0')
[2024-07-23 21:05:28,606][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[9318],
        [ 702],
        [2827],
        [ 509],
        [ 646],
        [1921],
        [1735],
        [1053],
        [1314],
        [1203],
        [ 387],
        [ 627],
        [ 358],
        [ 302],
        [ 376],
        [ 164],
        [ 426]], device='cuda:0')
[2024-07-23 21:05:28,608][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[ 3180],
        [ 5429],
        [11337],
        [11306],
        [14548],
        [ 7619],
        [11734],
        [ 8803],
        [ 7680],
        [ 6904],
        [11123],
        [10743],
        [ 9704],
        [ 9281],
        [ 7596],
        [ 6071],
        [ 4499]], device='cuda:0')
[2024-07-23 21:05:28,609][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[1742],
        [1729],
        [1637],
        [1801],
        [2176],
        [2156],
        [1728],
        [2676],
        [2010],
        [2753],
        [3055],
        [2227],
        [3307],
        [2930],
        [3543],
        [2073],
        [2000]], device='cuda:0')
[2024-07-23 21:05:28,611][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[6157],
        [ 145],
        [2682],
        [2981],
        [3530],
        [4169],
        [2258],
        [2193],
        [4543],
        [5204],
        [8401],
        [7001],
        [9620],
        [7992],
        [4790],
        [6861],
        [6192]], device='cuda:0')
[2024-07-23 21:05:28,614][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[  243],
        [ 6122],
        [ 1565],
        [20523],
        [15987],
        [13107],
        [ 3646],
        [ 8021],
        [13862],
        [12750],
        [ 3267],
        [ 3881],
        [15661],
        [ 7123],
        [  436],
        [ 2854],
        [  737]], device='cuda:0')
[2024-07-23 21:05:28,615][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[20913],
        [28693],
        [23753],
        [18623],
        [16920],
        [13530],
        [19691],
        [16059],
        [13127],
        [12356],
        [12922],
        [14060],
        [11408],
        [14324],
        [17280],
        [19451],
        [19179]], device='cuda:0')
[2024-07-23 21:05:28,617][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[10712],
        [19875],
        [ 8943],
        [11064],
        [20540],
        [18491],
        [12200],
        [12691],
        [17288],
        [16113],
        [19817],
        [15024],
        [21638],
        [19504],
        [17908],
        [22304],
        [22643]], device='cuda:0')
[2024-07-23 21:05:28,620][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341],
        [14341]], device='cuda:0')
[2024-07-23 21:05:28,695][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:28,697][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,698][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,700][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,702][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,703][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,705][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,706][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,707][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,707][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,708][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,709][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,709][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:28,710][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.7899, 0.2101], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,711][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.1164, 0.8836], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,712][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.3386, 0.6614], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,712][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.8943, 0.1057], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,713][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.9020, 0.0980], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,714][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.3480, 0.6520], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,714][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.1168, 0.8832], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,716][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.4816, 0.5184], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,717][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.5867, 0.4133], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,718][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.0548, 0.9452], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,720][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.0272, 0.9728], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,722][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9631, 0.0369], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:28,724][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.6134, 0.2199, 0.1667], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,725][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0849, 0.5292, 0.3859], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,727][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1810, 0.4553, 0.3637], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,729][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.7153, 0.1671, 0.1176], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,731][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.7272, 0.0845, 0.1883], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,733][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.1178, 0.4707, 0.4114], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,735][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0537, 0.5001, 0.4462], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,737][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.2565, 0.3871, 0.3564], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,739][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.1823, 0.6836, 0.1341], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,740][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0071, 0.2722, 0.7207], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,742][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0081, 0.5886, 0.4033], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,743][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.8838, 0.0720, 0.0442], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:28,744][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.6385, 0.2132, 0.1421, 0.0062], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,745][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0660, 0.4725, 0.4212, 0.0403], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,745][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.1344, 0.3255, 0.2607, 0.2793], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,746][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.7331, 0.1143, 0.0900, 0.0626], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,748][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.6150, 0.0776, 0.1981, 0.1094], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,750][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.2334, 0.4440, 0.2689, 0.0537], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,752][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0317, 0.3187, 0.3015, 0.3480], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,753][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.1046, 0.3326, 0.2845, 0.2784], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,755][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.2220, 0.5620, 0.1871, 0.0290], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,757][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0016, 0.0993, 0.1152, 0.7839], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,759][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0063, 0.4194, 0.2927, 0.2817], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,761][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.8593, 0.0274, 0.1109, 0.0023], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:28,763][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.7451, 0.1353, 0.1007, 0.0115, 0.0075], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,764][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.1624, 0.5236, 0.2688, 0.0310, 0.0142], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,766][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0966, 0.2534, 0.1988, 0.2146, 0.2365], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,768][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.5893, 0.1549, 0.1083, 0.0821, 0.0654], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,770][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.4250, 0.0697, 0.2897, 0.1494, 0.0663], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,772][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.1137, 0.3274, 0.3669, 0.1264, 0.0656], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,774][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.0292, 0.2939, 0.2103, 0.2322, 0.2344], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,776][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.1005, 0.2517, 0.2422, 0.2346, 0.1710], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,778][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.3599, 0.3814, 0.2104, 0.0426, 0.0058], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,780][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0056, 0.1021, 0.0954, 0.7815, 0.0153], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,781][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0042, 0.3257, 0.2062, 0.2049, 0.2591], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,783][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.6422, 0.1122, 0.2055, 0.0340, 0.0062], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:28,785][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.7097, 0.1031, 0.0921, 0.0252, 0.0206, 0.0493], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,787][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0498, 0.4660, 0.1993, 0.0531, 0.0294, 0.2024], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,789][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0805, 0.2083, 0.1645, 0.1758, 0.1930, 0.1779], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,791][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.6045, 0.1140, 0.0753, 0.0704, 0.0613, 0.0747], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,793][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.3190, 0.0602, 0.2250, 0.1238, 0.0820, 0.1899], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,795][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.1071, 0.1419, 0.1855, 0.0677, 0.0548, 0.4429], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,796][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0241, 0.2176, 0.1941, 0.1951, 0.1426, 0.2266], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,798][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0806, 0.1961, 0.1665, 0.2189, 0.1828, 0.1551], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,800][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2162, 0.4273, 0.1222, 0.0720, 0.0336, 0.1286], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,802][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.0266, 0.3040, 0.0437, 0.3727, 0.0893, 0.1636], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,804][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0033, 0.2499, 0.1705, 0.1663, 0.2026, 0.2074], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,806][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.6026, 0.1561, 0.2026, 0.0172, 0.0133, 0.0082], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:28,808][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.8096, 0.0656, 0.0277, 0.0108, 0.0088, 0.0203, 0.0573],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,810][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.1589, 0.2372, 0.0804, 0.0099, 0.0066, 0.0531, 0.4538],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,811][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0766, 0.1706, 0.1378, 0.1462, 0.1612, 0.1493, 0.1583],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,811][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.4458, 0.0968, 0.1128, 0.0793, 0.0811, 0.1180, 0.0662],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,812][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.3365, 0.0701, 0.1985, 0.1288, 0.0828, 0.1392, 0.0441],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,813][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.0823, 0.0779, 0.1103, 0.0789, 0.0416, 0.3457, 0.2633],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,814][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0179, 0.1644, 0.1683, 0.1623, 0.1391, 0.1659, 0.1821],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,814][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0819, 0.1938, 0.1353, 0.1970, 0.1443, 0.1430, 0.1047],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,816][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.5715, 0.1006, 0.0187, 0.0321, 0.0093, 0.0910, 0.1769],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,818][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([1.3099e-04, 6.2501e-03, 1.2234e-02, 8.3137e-01, 1.1773e-01, 3.1551e-02,
        7.2366e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,820][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0030, 0.2169, 0.1470, 0.1488, 0.1802, 0.1739, 0.1303],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,821][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.3995, 0.1519, 0.3282, 0.0527, 0.0139, 0.0212, 0.0327],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:28,823][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.8094, 0.0404, 0.0146, 0.0071, 0.0070, 0.0166, 0.0553, 0.0497],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,825][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0820, 0.1101, 0.1159, 0.0271, 0.0241, 0.0668, 0.3774, 0.1967],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,827][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.0650, 0.1465, 0.1185, 0.1256, 0.1382, 0.1281, 0.1369, 0.1412],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,829][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.5451, 0.0961, 0.0711, 0.0608, 0.0548, 0.0639, 0.0436, 0.0646],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,831][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.2318, 0.0648, 0.1975, 0.1280, 0.0647, 0.1568, 0.0470, 0.1094],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,833][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.0454, 0.0496, 0.0712, 0.0497, 0.0319, 0.2868, 0.1905, 0.2749],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,835][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0285, 0.1711, 0.1218, 0.1339, 0.1267, 0.1684, 0.1260, 0.1235],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,837][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0678, 0.1435, 0.1280, 0.1780, 0.1364, 0.1270, 0.0952, 0.1240],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,838][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.4351, 0.1452, 0.0430, 0.0394, 0.0154, 0.0671, 0.1766, 0.0782],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,840][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.0051, 0.0433, 0.0295, 0.3110, 0.5066, 0.0443, 0.0458, 0.0144],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,842][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0031, 0.1903, 0.1291, 0.1265, 0.1513, 0.1545, 0.1101, 0.1350],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,844][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.8567, 0.0191, 0.0664, 0.0237, 0.0056, 0.0121, 0.0119, 0.0045],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:28,847][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.6818, 0.0461, 0.0190, 0.0153, 0.0088, 0.0175, 0.0489, 0.0914, 0.0711],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,848][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0410, 0.0347, 0.0100, 0.0056, 0.0037, 0.0350, 0.2490, 0.1197, 0.5013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,850][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0579, 0.1280, 0.1035, 0.1097, 0.1215, 0.1133, 0.1199, 0.1240, 0.1223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,853][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2958, 0.0888, 0.1222, 0.0719, 0.0756, 0.0797, 0.0665, 0.1178, 0.0816],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,854][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.2081, 0.0580, 0.1881, 0.1479, 0.0756, 0.1456, 0.0307, 0.1036, 0.0422],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,856][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0394, 0.0209, 0.0263, 0.0276, 0.0154, 0.1911, 0.1459, 0.3208, 0.2126],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,858][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0138, 0.1205, 0.1213, 0.1302, 0.1020, 0.1253, 0.1255, 0.0846, 0.1768],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,860][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0448, 0.1353, 0.0993, 0.1616, 0.1159, 0.1214, 0.0898, 0.1236, 0.1083],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,862][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.3460, 0.0306, 0.0056, 0.0125, 0.0036, 0.0605, 0.1088, 0.2937, 0.1386],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,864][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0009, 0.0644, 0.0326, 0.6410, 0.1773, 0.0449, 0.0059, 0.0303, 0.0026],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,866][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0019, 0.1781, 0.1169, 0.1166, 0.1379, 0.1339, 0.1001, 0.1184, 0.0962],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,867][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2429, 0.1772, 0.2696, 0.1835, 0.0283, 0.0160, 0.0251, 0.0149, 0.0424],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:28,869][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.5251, 0.0583, 0.0451, 0.0163, 0.0109, 0.0279, 0.0632, 0.0900, 0.1362,
        0.0270], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,871][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0127, 0.0189, 0.0131, 0.0049, 0.0027, 0.0297, 0.1276, 0.0724, 0.6989,
        0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,873][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0519, 0.1132, 0.0917, 0.0969, 0.1073, 0.1000, 0.1061, 0.1098, 0.1076,
        0.1156], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,875][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.3818, 0.0712, 0.0757, 0.0543, 0.0559, 0.0748, 0.0389, 0.0884, 0.0587,
        0.1003], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,877][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.2922, 0.0523, 0.1283, 0.1172, 0.0625, 0.1297, 0.0307, 0.0979, 0.0330,
        0.0562], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,879][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0719, 0.0315, 0.0271, 0.0211, 0.0107, 0.1534, 0.1047, 0.2996, 0.1593,
        0.1207], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,879][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0198, 0.1056, 0.0964, 0.1080, 0.0946, 0.1150, 0.1124, 0.0780, 0.1509,
        0.1193], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,880][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0453, 0.1091, 0.1093, 0.1394, 0.1013, 0.1051, 0.0778, 0.1094, 0.1119,
        0.0916], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,881][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.2166, 0.0228, 0.0118, 0.0114, 0.0029, 0.0317, 0.2003, 0.1392, 0.2408,
        0.1224], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,882][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0016, 0.0560, 0.0275, 0.5811, 0.0410, 0.1284, 0.0144, 0.0961, 0.0221,
        0.0317], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,884][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.0034, 0.1499, 0.1036, 0.1015, 0.1237, 0.1132, 0.0847, 0.1022, 0.0840,
        0.1337], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,886][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.7035, 0.0675, 0.1075, 0.0159, 0.0143, 0.0232, 0.0122, 0.0109, 0.0400,
        0.0051], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:28,887][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.3348, 0.0426, 0.0350, 0.0183, 0.0109, 0.0237, 0.0667, 0.0705, 0.1482,
        0.0509, 0.1983], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,889][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0361, 0.0588, 0.0293, 0.0075, 0.0020, 0.0113, 0.1575, 0.0429, 0.5070,
        0.0602, 0.0874], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,892][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.0439, 0.1017, 0.0817, 0.0871, 0.0972, 0.0897, 0.0951, 0.0986, 0.0969,
        0.1049, 0.1034], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,893][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.2601, 0.0805, 0.0814, 0.0623, 0.0719, 0.0733, 0.0447, 0.0937, 0.0735,
        0.1208, 0.0379], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,895][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.1359, 0.0426, 0.1432, 0.0891, 0.0654, 0.1476, 0.0328, 0.1143, 0.0389,
        0.0600, 0.1302], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,898][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.0430, 0.0311, 0.0538, 0.0418, 0.0239, 0.1067, 0.0823, 0.1451, 0.1040,
        0.1207, 0.2474], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,899][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0174, 0.1006, 0.1057, 0.1058, 0.0864, 0.1025, 0.0949, 0.0684, 0.1156,
        0.0859, 0.1167], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,901][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0444, 0.1259, 0.0977, 0.1466, 0.0928, 0.0916, 0.0679, 0.0845, 0.0974,
        0.0781, 0.0731], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,903][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.2987, 0.0464, 0.0159, 0.0143, 0.0033, 0.0184, 0.0595, 0.0537, 0.1565,
        0.0820, 0.2512], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,904][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([3.7386e-04, 1.2137e-02, 1.2335e-02, 5.4890e-01, 7.3435e-02, 3.0991e-02,
        5.5767e-02, 1.5237e-01, 2.3673e-02, 8.5722e-02, 4.3025e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,906][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0018, 0.1364, 0.0907, 0.0877, 0.1066, 0.1053, 0.0784, 0.0944, 0.0751,
        0.1131, 0.1105], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,909][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.7004, 0.0444, 0.1522, 0.0135, 0.0057, 0.0184, 0.0107, 0.0122, 0.0248,
        0.0050, 0.0127], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:28,910][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.5119, 0.0129, 0.0050, 0.0035, 0.0026, 0.0049, 0.0183, 0.0342, 0.0299,
        0.0177, 0.2399, 0.1192], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,912][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0238, 0.0129, 0.0040, 0.0020, 0.0010, 0.0113, 0.0797, 0.0439, 0.1954,
        0.0613, 0.1116, 0.4530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,914][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0379, 0.0941, 0.0749, 0.0803, 0.0888, 0.0820, 0.0869, 0.0905, 0.0885,
        0.0966, 0.0948, 0.0847], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,916][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.2490, 0.0632, 0.0950, 0.0552, 0.0600, 0.0591, 0.0518, 0.0909, 0.0627,
        0.1199, 0.0388, 0.0543], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,918][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.1308, 0.0490, 0.1514, 0.1242, 0.0627, 0.1503, 0.0272, 0.0947, 0.0337,
        0.0585, 0.0811, 0.0364], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,920][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0348, 0.0131, 0.0141, 0.0127, 0.0070, 0.0690, 0.0795, 0.1733, 0.0874,
        0.1064, 0.1405, 0.2620], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,922][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0088, 0.0871, 0.1002, 0.0959, 0.0716, 0.0915, 0.0969, 0.0547, 0.1201,
        0.0688, 0.0712, 0.1332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,924][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0345, 0.1061, 0.0785, 0.1174, 0.0838, 0.0878, 0.0640, 0.0882, 0.0816,
        0.0838, 0.0745, 0.0998], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,926][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.2221, 0.0045, 0.0011, 0.0022, 0.0005, 0.0040, 0.0283, 0.0507, 0.0293,
        0.0800, 0.4369, 0.1404], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,927][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [orum] are: tensor([5.1729e-04, 5.8466e-02, 3.3174e-02, 6.0359e-01, 2.1858e-01, 3.0457e-02,
        3.4628e-03, 8.7145e-03, 1.4023e-03, 9.0734e-03, 2.7371e-03, 2.9822e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,929][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0014, 0.1296, 0.0854, 0.0846, 0.1013, 0.0972, 0.0738, 0.0845, 0.0698,
        0.1107, 0.0985, 0.0631], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,931][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2429, 0.1724, 0.2041, 0.1294, 0.0189, 0.0102, 0.0157, 0.0103, 0.0257,
        0.0521, 0.0938, 0.0244], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:28,933][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.5437, 0.0189, 0.0128, 0.0013, 0.0017, 0.0049, 0.0134, 0.0140, 0.0635,
        0.0066, 0.1533, 0.1509, 0.0149], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,935][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0129, 0.0327, 0.0139, 0.0038, 0.0022, 0.0106, 0.0351, 0.0337, 0.2605,
        0.0275, 0.0626, 0.4638, 0.0408], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,937][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0370, 0.0861, 0.0691, 0.0737, 0.0813, 0.0755, 0.0797, 0.0829, 0.0816,
        0.0883, 0.0862, 0.0783, 0.0803], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,939][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.4358, 0.0650, 0.0580, 0.0343, 0.0364, 0.0487, 0.0244, 0.0563, 0.0485,
        0.1102, 0.0304, 0.0399, 0.0122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,941][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.1897, 0.0608, 0.1071, 0.1083, 0.0574, 0.1198, 0.0272, 0.0947, 0.0270,
        0.0443, 0.0830, 0.0330, 0.0476], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,943][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.1647, 0.0479, 0.0407, 0.0121, 0.0078, 0.0611, 0.0685, 0.1047, 0.0742,
        0.0462, 0.2456, 0.1007, 0.0259], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,945][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0162, 0.0806, 0.0925, 0.0840, 0.0731, 0.0859, 0.0877, 0.0521, 0.1006,
        0.0705, 0.0672, 0.1056, 0.0840], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,946][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0150, 0.0922, 0.0800, 0.0864, 0.0846, 0.0756, 0.0604, 0.0769, 0.0812,
        0.0671, 0.0890, 0.0957, 0.0959], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,947][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.2345, 0.0230, 0.0053, 0.0049, 0.0016, 0.0077, 0.0706, 0.0362, 0.1015,
        0.0332, 0.1869, 0.2639, 0.0308], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,948][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0013, 0.0462, 0.0344, 0.5831, 0.2065, 0.0404, 0.0015, 0.0169, 0.0022,
        0.0042, 0.0028, 0.0186, 0.0419], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,949][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0021, 0.1132, 0.0832, 0.0775, 0.0913, 0.0858, 0.0662, 0.0777, 0.0653,
        0.1001, 0.0912, 0.0599, 0.0865], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,950][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.7677, 0.0305, 0.1280, 0.0126, 0.0015, 0.0040, 0.0040, 0.0039, 0.0086,
        0.0069, 0.0214, 0.0097, 0.0013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:28,952][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.7624, 0.0083, 0.0057, 0.0008, 0.0011, 0.0025, 0.0084, 0.0120, 0.0255,
        0.0044, 0.0893, 0.0549, 0.0151, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,954][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0690, 0.0559, 0.0190, 0.0031, 0.0016, 0.0098, 0.0698, 0.0379, 0.2689,
        0.0315, 0.0835, 0.2911, 0.0445, 0.0146], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,955][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0367, 0.0784, 0.0638, 0.0677, 0.0746, 0.0697, 0.0737, 0.0759, 0.0751,
        0.0808, 0.0795, 0.0726, 0.0738, 0.0777], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,957][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.4027, 0.0597, 0.0545, 0.0383, 0.0420, 0.0601, 0.0222, 0.0584, 0.0486,
        0.0963, 0.0288, 0.0408, 0.0148, 0.0329], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,959][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1956, 0.0635, 0.0894, 0.1075, 0.0456, 0.1192, 0.0291, 0.0905, 0.0293,
        0.0397, 0.0661, 0.0340, 0.0435, 0.0469], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,961][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.4891, 0.0520, 0.0393, 0.0116, 0.0065, 0.0299, 0.0290, 0.0488, 0.0316,
        0.0317, 0.1424, 0.0359, 0.0244, 0.0276], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,963][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0113, 0.0706, 0.0860, 0.0804, 0.0689, 0.0778, 0.0803, 0.0548, 0.0881,
        0.0674, 0.0702, 0.0936, 0.0759, 0.0747], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,965][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0226, 0.0865, 0.0660, 0.0798, 0.0786, 0.0680, 0.0513, 0.0659, 0.0750,
        0.0597, 0.0752, 0.0861, 0.0847, 0.1007], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,967][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5227, 0.0145, 0.0043, 0.0041, 0.0012, 0.0035, 0.0331, 0.0156, 0.0534,
        0.0237, 0.1280, 0.1288, 0.0351, 0.0322], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,969][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0010, 0.0606, 0.0181, 0.2706, 0.1666, 0.0326, 0.0033, 0.0164, 0.0140,
        0.0107, 0.0071, 0.1776, 0.1891, 0.0324], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,971][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0020, 0.1075, 0.0765, 0.0719, 0.0857, 0.0782, 0.0629, 0.0704, 0.0598,
        0.0934, 0.0858, 0.0555, 0.0801, 0.0703], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,973][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.8875, 0.0132, 0.0530, 0.0036, 0.0011, 0.0036, 0.0037, 0.0041, 0.0052,
        0.0009, 0.0138, 0.0057, 0.0035, 0.0013], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:28,974][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.3608, 0.0176, 0.0055, 0.0014, 0.0011, 0.0017, 0.0144, 0.0064, 0.0198,
        0.0045, 0.0734, 0.0693, 0.0279, 0.0332, 0.3631], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,977][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0784, 0.1274, 0.0168, 0.0052, 0.0011, 0.0027, 0.0536, 0.0411, 0.1823,
        0.0717, 0.0838, 0.1791, 0.0336, 0.0325, 0.0906], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,978][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0338, 0.0738, 0.0597, 0.0633, 0.0701, 0.0650, 0.0686, 0.0708, 0.0702,
        0.0757, 0.0742, 0.0674, 0.0685, 0.0725, 0.0665], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,981][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.3955, 0.0454, 0.0552, 0.0330, 0.0388, 0.0497, 0.0285, 0.0650, 0.0455,
        0.0921, 0.0255, 0.0393, 0.0115, 0.0335, 0.0416], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,983][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.1037, 0.0641, 0.1205, 0.1065, 0.0541, 0.1106, 0.0362, 0.0851, 0.0349,
        0.0498, 0.0510, 0.0384, 0.0399, 0.0408, 0.0645], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,985][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.2628, 0.0508, 0.0440, 0.0217, 0.0072, 0.0352, 0.0230, 0.0290, 0.0380,
        0.0277, 0.0760, 0.0405, 0.0331, 0.0509, 0.2602], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,987][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0109, 0.0675, 0.0665, 0.0711, 0.0567, 0.0647, 0.0667, 0.0438, 0.0842,
        0.0588, 0.0543, 0.0900, 0.0643, 0.0588, 0.1419], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,989][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0218, 0.0698, 0.0542, 0.0770, 0.0645, 0.0593, 0.0484, 0.0649, 0.0610,
        0.0570, 0.0652, 0.0710, 0.0777, 0.1175, 0.0907], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,990][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.4597, 0.0055, 0.0015, 0.0035, 0.0008, 0.0007, 0.0282, 0.0061, 0.0282,
        0.0145, 0.0891, 0.0434, 0.0167, 0.0567, 0.2453], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,992][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ same] are: tensor([2.5028e-04, 6.2738e-03, 8.8322e-03, 5.3092e-01, 2.2587e-01, 1.6764e-02,
        7.7801e-03, 6.1925e-03, 1.1464e-02, 4.0546e-03, 4.2975e-03, 9.2436e-02,
        3.5694e-02, 4.2841e-02, 6.3304e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,994][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.0018, 0.0964, 0.0715, 0.0677, 0.0801, 0.0755, 0.0579, 0.0635, 0.0571,
        0.0858, 0.0748, 0.0526, 0.0763, 0.0667, 0.0724], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,996][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.8413, 0.0169, 0.0407, 0.0125, 0.0018, 0.0071, 0.0027, 0.0038, 0.0132,
        0.0055, 0.0158, 0.0127, 0.0132, 0.0093, 0.0038], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:28,998][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ as] are: tensor([4.9009e-01, 2.9915e-03, 1.7781e-03, 3.4604e-04, 4.3694e-04, 8.9890e-04,
        3.7865e-03, 2.5505e-03, 1.2474e-02, 1.5551e-03, 3.7902e-02, 2.9136e-02,
        9.0117e-03, 1.1871e-02, 3.7603e-01, 1.9144e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,000][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0456, 0.0351, 0.0128, 0.0040, 0.0018, 0.0062, 0.0231, 0.0518, 0.0945,
        0.0324, 0.0894, 0.1111, 0.0535, 0.0382, 0.3283, 0.0722],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,002][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0310, 0.0693, 0.0556, 0.0594, 0.0657, 0.0611, 0.0642, 0.0671, 0.0662,
        0.0721, 0.0699, 0.0633, 0.0649, 0.0684, 0.0625, 0.0594],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,004][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.3921, 0.0478, 0.0485, 0.0306, 0.0347, 0.0448, 0.0233, 0.0554, 0.0396,
        0.0952, 0.0251, 0.0328, 0.0122, 0.0355, 0.0554, 0.0270],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,006][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.1913, 0.0481, 0.0894, 0.0874, 0.0430, 0.1002, 0.0238, 0.0821, 0.0298,
        0.0309, 0.0626, 0.0328, 0.0327, 0.0340, 0.0650, 0.0469],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,007][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.4874, 0.0186, 0.0208, 0.0104, 0.0056, 0.0281, 0.0117, 0.0269, 0.0186,
        0.0152, 0.0821, 0.0197, 0.0141, 0.0203, 0.1889, 0.0316],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,010][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0087, 0.0525, 0.0613, 0.0663, 0.0521, 0.0574, 0.0606, 0.0425, 0.0742,
        0.0571, 0.0580, 0.0785, 0.0600, 0.0513, 0.1072, 0.1122],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,012][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0164, 0.0667, 0.0519, 0.0641, 0.0690, 0.0697, 0.0415, 0.0651, 0.0634,
        0.0516, 0.0685, 0.0704, 0.0724, 0.0856, 0.0744, 0.0693],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,014][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.1527, 0.0045, 0.0013, 0.0019, 0.0007, 0.0018, 0.0095, 0.0065, 0.0189,
        0.0092, 0.0549, 0.0397, 0.0104, 0.0299, 0.6029, 0.0551],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,015][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ as] are: tensor([3.5157e-04, 6.6833e-03, 2.5400e-02, 5.7744e-01, 2.5000e-01, 1.8011e-02,
        1.2986e-03, 6.7362e-03, 1.9906e-03, 1.4099e-03, 1.4353e-03, 2.8445e-02,
        3.6301e-02, 1.7902e-02, 3.4871e-03, 2.3115e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,015][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0016, 0.0968, 0.0670, 0.0644, 0.0749, 0.0722, 0.0557, 0.0624, 0.0520,
        0.0801, 0.0701, 0.0475, 0.0708, 0.0599, 0.0676, 0.0570],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,016][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.6038, 0.0456, 0.0711, 0.0117, 0.0060, 0.0075, 0.0130, 0.0073, 0.0210,
        0.0177, 0.0361, 0.0257, 0.0163, 0.0326, 0.0813, 0.0033],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,017][circuit_model.py][line:1532][INFO] ##6-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.3974e-01, 1.6668e-03, 1.3149e-03, 2.0493e-04, 2.3282e-04, 5.2443e-04,
        1.3661e-03, 2.0438e-03, 4.2144e-03, 8.3051e-04, 2.6134e-02, 6.7846e-03,
        3.3884e-03, 2.9312e-03, 8.5878e-02, 9.9455e-03, 1.2800e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,020][circuit_model.py][line:1535][INFO] ##6-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1279, 0.0236, 0.0074, 0.0014, 0.0006, 0.0037, 0.0242, 0.0189, 0.0651,
        0.0134, 0.0467, 0.0507, 0.0254, 0.0096, 0.4442, 0.1067, 0.0304],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,021][circuit_model.py][line:1538][INFO] ##6-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0296, 0.0645, 0.0520, 0.0555, 0.0616, 0.0577, 0.0605, 0.0626, 0.0617,
        0.0669, 0.0654, 0.0591, 0.0605, 0.0640, 0.0586, 0.0559, 0.0639],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,023][circuit_model.py][line:1541][INFO] ##6-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.3953, 0.0471, 0.0441, 0.0327, 0.0369, 0.0483, 0.0197, 0.0484, 0.0416,
        0.0856, 0.0246, 0.0349, 0.0124, 0.0278, 0.0462, 0.0292, 0.0252],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,025][circuit_model.py][line:1544][INFO] ##6-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1339, 0.0512, 0.0757, 0.0910, 0.0437, 0.1116, 0.0271, 0.0785, 0.0278,
        0.0361, 0.0596, 0.0313, 0.0340, 0.0373, 0.0624, 0.0476, 0.0512],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,027][circuit_model.py][line:1547][INFO] ##6-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.7648, 0.0156, 0.0121, 0.0058, 0.0025, 0.0102, 0.0063, 0.0132, 0.0078,
        0.0080, 0.0434, 0.0059, 0.0075, 0.0094, 0.0535, 0.0148, 0.0190],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,029][circuit_model.py][line:1550][INFO] ##6-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0083, 0.0550, 0.0621, 0.0596, 0.0528, 0.0629, 0.0600, 0.0457, 0.0665,
        0.0521, 0.0584, 0.0692, 0.0578, 0.0568, 0.0967, 0.0777, 0.0583],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,030][circuit_model.py][line:1553][INFO] ##6-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0267, 0.0618, 0.0494, 0.0615, 0.0612, 0.0576, 0.0386, 0.0547, 0.0609,
        0.0473, 0.0560, 0.0658, 0.0669, 0.0745, 0.0658, 0.0648, 0.0865],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,032][circuit_model.py][line:1556][INFO] ##6-th layer ##Weight##: The head9 weight for token [ the] are: tensor([6.1450e-01, 2.6490e-03, 8.6692e-04, 1.0045e-03, 2.5164e-04, 7.1863e-04,
        5.5424e-03, 2.6291e-03, 8.2784e-03, 5.0262e-03, 3.2764e-02, 1.4844e-02,
        9.2723e-03, 1.0630e-02, 1.9449e-01, 4.4433e-02, 5.2101e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,034][circuit_model.py][line:1559][INFO] ##6-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0012, 0.0345, 0.0128, 0.2816, 0.1395, 0.0155, 0.0018, 0.0096, 0.0087,
        0.0099, 0.0040, 0.0995, 0.1592, 0.0304, 0.0184, 0.1334, 0.0399],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,035][circuit_model.py][line:1562][INFO] ##6-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.0016, 0.0911, 0.0629, 0.0608, 0.0714, 0.0660, 0.0522, 0.0592, 0.0488,
        0.0756, 0.0694, 0.0448, 0.0657, 0.0564, 0.0652, 0.0549, 0.0541],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,037][circuit_model.py][line:1565][INFO] ##6-th layer ##Weight##: The head12 weight for token [ the] are: tensor([9.0500e-01, 7.4590e-03, 3.2264e-02, 2.8904e-03, 8.6031e-04, 2.5374e-03,
        2.3880e-03, 3.1333e-03, 3.0154e-03, 6.7368e-04, 9.7973e-03, 3.0913e-03,
        2.3877e-03, 8.4578e-04, 1.6610e-02, 5.6501e-03, 1.3973e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,108][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:29,108][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,109][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,110][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,110][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,111][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,112][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,112][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,113][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,114][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,114][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,115][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,116][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,117][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.7899, 0.2101], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,119][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.1164, 0.8836], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,121][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.8966, 0.1034], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,123][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.7992, 0.2008], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,124][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.9891, 0.0109], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,126][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.3915, 0.6085], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,128][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.9828, 0.0172], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,130][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.5850, 0.4150], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,131][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.5867, 0.4133], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,134][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.0636, 0.9364], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,135][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.7407, 0.2593], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,137][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.0909, 0.9091], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,139][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.6134, 0.2199, 0.1667], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,141][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0849, 0.5292, 0.3859], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,142][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.7718, 0.1651, 0.0631], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,145][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.5683, 0.1843, 0.2474], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,146][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.9576, 0.0230, 0.0194], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,148][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.1171, 0.6097, 0.2733], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,150][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.9699, 0.0062, 0.0239], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,152][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.3212, 0.3095, 0.3693], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,153][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.1823, 0.6836, 0.1341], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,156][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0094, 0.1913, 0.7993], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,158][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.7508, 0.1811, 0.0681], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,159][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0310, 0.7106, 0.2583], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,161][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.6385, 0.2132, 0.1421, 0.0062], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,163][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0660, 0.4725, 0.4212, 0.0403], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,165][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.7398, 0.1620, 0.0823, 0.0159], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,167][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.7615, 0.1119, 0.1123, 0.0143], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,169][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.9554, 0.0225, 0.0173, 0.0048], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,170][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0911, 0.6177, 0.2054, 0.0858], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,171][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.9870, 0.0034, 0.0069, 0.0026], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,172][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1705, 0.3850, 0.3875, 0.0570], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,172][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.2220, 0.5620, 0.1871, 0.0290], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,173][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0029, 0.0966, 0.2114, 0.6891], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,174][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.7540, 0.1155, 0.0380, 0.0925], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,175][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0176, 0.6897, 0.2742, 0.0185], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,177][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.7451, 0.1353, 0.1007, 0.0115, 0.0075], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,179][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.1624, 0.5236, 0.2688, 0.0310, 0.0142], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,181][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.6461, 0.1896, 0.1029, 0.0391, 0.0222], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,183][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.7386, 0.0960, 0.1374, 0.0165, 0.0115], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,185][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.8740, 0.0357, 0.0485, 0.0183, 0.0235], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,186][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.1161, 0.4607, 0.2389, 0.1306, 0.0538], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,189][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.9791, 0.0031, 0.0108, 0.0026, 0.0043], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,190][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.2023, 0.3034, 0.3843, 0.0598, 0.0503], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,192][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.3599, 0.3814, 0.2104, 0.0426, 0.0058], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,194][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.0084, 0.1411, 0.1998, 0.6342, 0.0164], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,196][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.4538, 0.1517, 0.0596, 0.1411, 0.1938], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,198][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0458, 0.6094, 0.2452, 0.0633, 0.0362], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,200][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.7097, 0.1031, 0.0921, 0.0252, 0.0206, 0.0493], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,202][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.0498, 0.4660, 0.1993, 0.0531, 0.0294, 0.2024], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,203][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.8151, 0.0835, 0.0562, 0.0191, 0.0101, 0.0161], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,205][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.7216, 0.0651, 0.1366, 0.0231, 0.0302, 0.0234], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,207][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.9518, 0.0074, 0.0092, 0.0048, 0.0118, 0.0151], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,209][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.1071, 0.3425, 0.2015, 0.1102, 0.0587, 0.1800], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,212][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.9838, 0.0020, 0.0064, 0.0021, 0.0040, 0.0017], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,213][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.2033, 0.2694, 0.2864, 0.0675, 0.0890, 0.0844], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,215][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.2162, 0.4273, 0.1222, 0.0720, 0.0336, 0.1286], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,217][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.0431, 0.3995, 0.0500, 0.1511, 0.0653, 0.2911], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,219][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.4269, 0.1160, 0.0513, 0.1421, 0.1845, 0.0792], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,221][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.0399, 0.4319, 0.1205, 0.0426, 0.0764, 0.2887], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,223][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.8096, 0.0656, 0.0277, 0.0108, 0.0088, 0.0203, 0.0573],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,225][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.1589, 0.2372, 0.0804, 0.0099, 0.0066, 0.0531, 0.4538],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,226][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.7779, 0.0697, 0.0503, 0.0271, 0.0181, 0.0134, 0.0434],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,229][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.8651, 0.0425, 0.0411, 0.0080, 0.0137, 0.0101, 0.0193],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,230][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.8576, 0.0095, 0.0177, 0.0066, 0.0304, 0.0279, 0.0502],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,232][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.1916, 0.1948, 0.1243, 0.1016, 0.0333, 0.1055, 0.2490],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,233][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([9.8859e-01, 2.2504e-03, 4.9168e-03, 1.2841e-03, 1.1850e-03, 5.5315e-04,
        1.2191e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,235][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.1930, 0.2931, 0.1729, 0.0683, 0.0505, 0.0603, 0.1619],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,237][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.5715, 0.1006, 0.0187, 0.0321, 0.0093, 0.0910, 0.1769],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,238][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.0018, 0.0266, 0.0356, 0.5907, 0.1899, 0.1535, 0.0019],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,239][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.8516, 0.0322, 0.0227, 0.0390, 0.0226, 0.0111, 0.0207],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,239][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.0649, 0.1957, 0.0427, 0.0348, 0.0254, 0.0649, 0.5716],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,240][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.8094, 0.0404, 0.0146, 0.0071, 0.0070, 0.0166, 0.0553, 0.0497],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,242][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0820, 0.1101, 0.1159, 0.0271, 0.0241, 0.0668, 0.3774, 0.1967],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,244][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.8223, 0.0589, 0.0368, 0.0279, 0.0130, 0.0110, 0.0180, 0.0121],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,245][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.7564, 0.0352, 0.0771, 0.0163, 0.0184, 0.0123, 0.0495, 0.0347],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,247][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.7774, 0.0193, 0.0204, 0.0088, 0.0169, 0.0382, 0.0264, 0.0926],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,249][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.0975, 0.1761, 0.0981, 0.0768, 0.0350, 0.0900, 0.2072, 0.2195],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,251][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.9815, 0.0021, 0.0030, 0.0019, 0.0031, 0.0014, 0.0012, 0.0058],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,253][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.1790, 0.1347, 0.1603, 0.0589, 0.0512, 0.0640, 0.1786, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,255][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.4351, 0.1452, 0.0430, 0.0394, 0.0154, 0.0671, 0.1766, 0.0782],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,257][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.0361, 0.1069, 0.0443, 0.0966, 0.3890, 0.2070, 0.0787, 0.0414],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,258][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.3877, 0.0662, 0.0182, 0.0805, 0.1451, 0.0393, 0.0682, 0.1949],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,261][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.0279, 0.0478, 0.0266, 0.0167, 0.0237, 0.1075, 0.3791, 0.3707],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,263][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.6818, 0.0461, 0.0190, 0.0153, 0.0088, 0.0175, 0.0489, 0.0914, 0.0711],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,264][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0410, 0.0347, 0.0100, 0.0056, 0.0037, 0.0350, 0.2490, 0.1197, 0.5013],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,267][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.9059, 0.0268, 0.0220, 0.0114, 0.0048, 0.0056, 0.0120, 0.0033, 0.0082],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,268][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.7440, 0.0174, 0.0403, 0.0122, 0.0104, 0.0113, 0.0534, 0.0584, 0.0525],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,270][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.9208, 0.0042, 0.0099, 0.0067, 0.0111, 0.0167, 0.0040, 0.0205, 0.0061],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,272][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.1284, 0.0656, 0.0401, 0.0358, 0.0154, 0.0690, 0.1336, 0.2061, 0.3061],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,274][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([9.8188e-01, 3.6034e-03, 4.5008e-03, 2.5724e-03, 1.6195e-03, 6.7925e-04,
        1.1084e-03, 1.4190e-03, 2.6222e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,275][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0677, 0.0925, 0.0737, 0.0464, 0.0290, 0.0515, 0.1652, 0.1776, 0.2964],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,278][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.3460, 0.0306, 0.0056, 0.0125, 0.0036, 0.0605, 0.1088, 0.2937, 0.1386],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,279][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0154, 0.1564, 0.0904, 0.2369, 0.1619, 0.1956, 0.0078, 0.1253, 0.0103],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,281][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.8139, 0.0199, 0.0109, 0.0278, 0.0173, 0.0280, 0.0217, 0.0475, 0.0130],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,283][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0137, 0.0218, 0.0130, 0.0188, 0.0124, 0.0473, 0.1872, 0.2266, 0.4593],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,285][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.5251, 0.0583, 0.0451, 0.0163, 0.0109, 0.0279, 0.0632, 0.0900, 0.1362,
        0.0270], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,287][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0127, 0.0189, 0.0131, 0.0049, 0.0027, 0.0297, 0.1276, 0.0724, 0.6989,
        0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,289][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.6463, 0.1123, 0.0544, 0.0498, 0.0177, 0.0172, 0.0332, 0.0188, 0.0377,
        0.0124], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,291][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.4113, 0.0385, 0.0766, 0.0249, 0.0224, 0.0202, 0.0901, 0.0795, 0.1665,
        0.0699], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,293][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.6310, 0.0103, 0.0167, 0.0070, 0.0236, 0.0505, 0.0260, 0.1278, 0.0272,
        0.0799], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,295][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.1146, 0.1051, 0.0413, 0.0357, 0.0154, 0.0649, 0.1148, 0.2009, 0.1973,
        0.1100], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,297][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.9108, 0.0121, 0.0147, 0.0071, 0.0075, 0.0038, 0.0057, 0.0063, 0.0069,
        0.0251], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,299][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0660, 0.0760, 0.1028, 0.0391, 0.0269, 0.0410, 0.0981, 0.1245, 0.3440,
        0.0818], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,301][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.2166, 0.0228, 0.0118, 0.0114, 0.0029, 0.0317, 0.2003, 0.1392, 0.2408,
        0.1224], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,303][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.0040, 0.0779, 0.0473, 0.1413, 0.0250, 0.3628, 0.0242, 0.2102, 0.0544,
        0.0529], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,304][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.5142, 0.0565, 0.0188, 0.0542, 0.0565, 0.0276, 0.0326, 0.1266, 0.0133,
        0.0996], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,304][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0048, 0.0220, 0.0058, 0.0087, 0.0090, 0.0474, 0.1151, 0.1873, 0.5526,
        0.0474], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,305][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.3348, 0.0426, 0.0350, 0.0183, 0.0109, 0.0237, 0.0667, 0.0705, 0.1482,
        0.0509, 0.1983], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,306][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0361, 0.0588, 0.0293, 0.0075, 0.0020, 0.0113, 0.1575, 0.0429, 0.5070,
        0.0602, 0.0874], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,307][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.5317, 0.0820, 0.0524, 0.0642, 0.0383, 0.0453, 0.0468, 0.0270, 0.0509,
        0.0215, 0.0400], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,309][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.3758, 0.0397, 0.0901, 0.0271, 0.0181, 0.0201, 0.0730, 0.0696, 0.1034,
        0.0746, 0.1085], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,311][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.5325, 0.0068, 0.0102, 0.0048, 0.0179, 0.0265, 0.0138, 0.0736, 0.0161,
        0.0462, 0.2516], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,313][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.0672, 0.0824, 0.0655, 0.0539, 0.0252, 0.0426, 0.0866, 0.1083, 0.1515,
        0.1077, 0.2091], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,314][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.8728, 0.0042, 0.0126, 0.0049, 0.0107, 0.0031, 0.0045, 0.0087, 0.0053,
        0.0144, 0.0589], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,316][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0335, 0.0999, 0.0835, 0.0536, 0.0281, 0.0414, 0.0965, 0.0829, 0.2682,
        0.0668, 0.1456], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,318][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.2987, 0.0464, 0.0159, 0.0143, 0.0033, 0.0184, 0.0595, 0.0537, 0.1565,
        0.0820, 0.2512], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,320][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.0018, 0.0230, 0.0173, 0.1575, 0.0365, 0.0891, 0.0687, 0.3718, 0.0823,
        0.1421, 0.0099], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,322][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.3080, 0.0437, 0.0312, 0.0903, 0.0579, 0.0264, 0.0614, 0.0792, 0.0346,
        0.1397, 0.1274], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,324][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0184, 0.0396, 0.0139, 0.0211, 0.0209, 0.0491, 0.1169, 0.1087, 0.3442,
        0.0715, 0.1957], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,326][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.5119, 0.0129, 0.0050, 0.0035, 0.0026, 0.0049, 0.0183, 0.0342, 0.0299,
        0.0177, 0.2399, 0.1192], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,328][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0238, 0.0129, 0.0040, 0.0020, 0.0010, 0.0113, 0.0797, 0.0439, 0.1954,
        0.0613, 0.1116, 0.4530], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,330][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.9091, 0.0319, 0.0169, 0.0092, 0.0030, 0.0028, 0.0087, 0.0024, 0.0045,
        0.0028, 0.0063, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,331][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.7980, 0.0078, 0.0144, 0.0040, 0.0028, 0.0043, 0.0166, 0.0265, 0.0164,
        0.0402, 0.0485, 0.0206], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,334][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.7801, 0.0032, 0.0049, 0.0031, 0.0048, 0.0209, 0.0033, 0.0232, 0.0042,
        0.0237, 0.1253, 0.0031], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,335][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.1230, 0.0387, 0.0191, 0.0165, 0.0066, 0.0237, 0.0650, 0.0992, 0.1172,
        0.0687, 0.1458, 0.2766], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,336][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([9.8558e-01, 1.0567e-03, 1.7820e-03, 6.3097e-04, 5.6097e-04, 2.6920e-04,
        4.9558e-04, 7.3112e-04, 8.4106e-04, 1.5916e-03, 5.3369e-03, 1.1231e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,339][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0570, 0.0469, 0.0355, 0.0196, 0.0114, 0.0180, 0.0632, 0.0708, 0.1387,
        0.0554, 0.1725, 0.3107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,341][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.2221, 0.0045, 0.0011, 0.0022, 0.0005, 0.0040, 0.0283, 0.0507, 0.0293,
        0.0800, 0.4369, 0.1404], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,343][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0152, 0.1307, 0.0867, 0.1349, 0.1409, 0.2323, 0.0085, 0.0672, 0.0108,
        0.0457, 0.0156, 0.1115], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,345][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.9180, 0.0046, 0.0041, 0.0095, 0.0058, 0.0079, 0.0057, 0.0133, 0.0023,
        0.0075, 0.0177, 0.0037], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,347][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0128, 0.0050, 0.0022, 0.0037, 0.0025, 0.0088, 0.0609, 0.0562, 0.1655,
        0.0383, 0.1651, 0.4791], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,349][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.5437, 0.0189, 0.0128, 0.0013, 0.0017, 0.0049, 0.0134, 0.0140, 0.0635,
        0.0066, 0.1533, 0.1509, 0.0149], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,351][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0129, 0.0327, 0.0139, 0.0038, 0.0022, 0.0106, 0.0351, 0.0337, 0.2605,
        0.0275, 0.0626, 0.4638, 0.0408], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,353][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.8794, 0.0328, 0.0132, 0.0135, 0.0054, 0.0070, 0.0114, 0.0042, 0.0106,
        0.0040, 0.0053, 0.0059, 0.0072], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,355][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.2444, 0.0386, 0.0397, 0.0145, 0.0119, 0.0124, 0.0330, 0.0433, 0.1213,
        0.1043, 0.0803, 0.1875, 0.0686], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,357][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.6394, 0.0073, 0.0031, 0.0026, 0.0065, 0.0119, 0.0056, 0.0306, 0.0034,
        0.0405, 0.1602, 0.0065, 0.0824], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,359][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0977, 0.0961, 0.0405, 0.0179, 0.0094, 0.0259, 0.0791, 0.0769, 0.1044,
        0.0463, 0.1781, 0.1570, 0.0706], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,361][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.9303, 0.0042, 0.0084, 0.0036, 0.0037, 0.0013, 0.0028, 0.0037, 0.0037,
        0.0070, 0.0183, 0.0036, 0.0093], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,363][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0180, 0.0408, 0.0464, 0.0114, 0.0143, 0.0169, 0.0582, 0.0510, 0.1686,
        0.0294, 0.1669, 0.3191, 0.0589], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,364][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.2345, 0.0230, 0.0053, 0.0049, 0.0016, 0.0077, 0.0706, 0.0362, 0.1015,
        0.0332, 0.1869, 0.2639, 0.0308], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,367][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0085, 0.0629, 0.0586, 0.1493, 0.1409, 0.2289, 0.0034, 0.1331, 0.0146,
        0.0184, 0.0097, 0.0873, 0.0843], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,369][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.3392, 0.0563, 0.0153, 0.0660, 0.0754, 0.0340, 0.0254, 0.1223, 0.0211,
        0.0831, 0.0817, 0.0283, 0.0518], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,370][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0202, 0.0218, 0.0071, 0.0021, 0.0029, 0.0313, 0.0432, 0.0817, 0.2350,
        0.0202, 0.1530, 0.3737, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,371][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.7624, 0.0083, 0.0057, 0.0008, 0.0011, 0.0025, 0.0084, 0.0120, 0.0255,
        0.0044, 0.0893, 0.0549, 0.0151, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,372][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0690, 0.0559, 0.0190, 0.0031, 0.0016, 0.0098, 0.0698, 0.0379, 0.2689,
        0.0315, 0.0835, 0.2911, 0.0445, 0.0146], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,373][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.8290, 0.0360, 0.0144, 0.0112, 0.0053, 0.0046, 0.0164, 0.0040, 0.0082,
        0.0045, 0.0054, 0.0048, 0.0074, 0.0488], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,374][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.5591, 0.0333, 0.0296, 0.0057, 0.0061, 0.0073, 0.0210, 0.0286, 0.0555,
        0.0522, 0.0628, 0.0708, 0.0315, 0.0364], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,376][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.7917, 0.0053, 0.0017, 0.0016, 0.0027, 0.0126, 0.0055, 0.0212, 0.0034,
        0.0231, 0.0800, 0.0036, 0.0315, 0.0161], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,378][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.1509, 0.0936, 0.0372, 0.0179, 0.0087, 0.0245, 0.0518, 0.0575, 0.0681,
        0.0382, 0.1245, 0.0927, 0.0992, 0.1350], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,379][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([9.7343e-01, 1.2936e-03, 2.3124e-03, 8.1212e-04, 8.7363e-04, 4.9993e-04,
        9.3629e-04, 1.4707e-03, 1.3222e-03, 2.6587e-03, 8.8553e-03, 1.1282e-03,
        2.1478e-03, 2.2593e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,381][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0438, 0.0520, 0.0454, 0.0137, 0.0198, 0.0177, 0.0504, 0.0450, 0.1428,
        0.0340, 0.1846, 0.2316, 0.0600, 0.0592], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,383][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.5227, 0.0145, 0.0043, 0.0041, 0.0012, 0.0035, 0.0331, 0.0156, 0.0534,
        0.0237, 0.1280, 0.1288, 0.0351, 0.0322], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,385][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0043, 0.0706, 0.0238, 0.0697, 0.0650, 0.1124, 0.0046, 0.0513, 0.0415,
        0.0198, 0.0103, 0.3259, 0.1582, 0.0427], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,387][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.6642, 0.0389, 0.0127, 0.0314, 0.0333, 0.0134, 0.0103, 0.0304, 0.0074,
        0.0374, 0.0510, 0.0079, 0.0236, 0.0380], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,389][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0568, 0.0218, 0.0095, 0.0026, 0.0039, 0.0179, 0.0517, 0.0580, 0.1985,
        0.0178, 0.2193, 0.2903, 0.0159, 0.0358], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,391][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.3608, 0.0176, 0.0055, 0.0014, 0.0011, 0.0017, 0.0144, 0.0064, 0.0198,
        0.0045, 0.0734, 0.0693, 0.0279, 0.0332, 0.3631], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,393][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0784, 0.1274, 0.0168, 0.0052, 0.0011, 0.0027, 0.0536, 0.0411, 0.1823,
        0.0717, 0.0838, 0.1791, 0.0336, 0.0325, 0.0906], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,395][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.6552, 0.0581, 0.0342, 0.0251, 0.0100, 0.0117, 0.0285, 0.0072, 0.0201,
        0.0100, 0.0182, 0.0155, 0.0117, 0.0581, 0.0365], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,397][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.5388, 0.0465, 0.0276, 0.0053, 0.0043, 0.0035, 0.0168, 0.0223, 0.0349,
        0.0319, 0.0678, 0.0409, 0.0251, 0.0466, 0.0878], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,399][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.7343, 0.0153, 0.0055, 0.0042, 0.0064, 0.0067, 0.0180, 0.0211, 0.0030,
        0.0751, 0.0524, 0.0023, 0.0240, 0.0142, 0.0176], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,401][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.0975, 0.0617, 0.0314, 0.0169, 0.0051, 0.0110, 0.0270, 0.0220, 0.0527,
        0.0239, 0.0547, 0.0636, 0.0981, 0.1401, 0.2942], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,402][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([9.6020e-01, 3.2154e-03, 8.7786e-03, 1.9953e-03, 1.8322e-03, 5.0940e-04,
        1.3160e-03, 1.5093e-03, 1.3296e-03, 2.5332e-03, 6.9622e-03, 1.0338e-03,
        2.2838e-03, 2.5313e-03, 3.9688e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,404][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.0428, 0.0326, 0.0259, 0.0118, 0.0126, 0.0109, 0.0441, 0.0461, 0.0856,
        0.0324, 0.1608, 0.1465, 0.0545, 0.1120, 0.1814], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,407][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.4597, 0.0055, 0.0015, 0.0035, 0.0008, 0.0007, 0.0282, 0.0061, 0.0282,
        0.0145, 0.0891, 0.0434, 0.0167, 0.0567, 0.2453], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,408][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.0031, 0.0181, 0.0185, 0.1592, 0.0921, 0.0946, 0.0158, 0.0354, 0.0765,
        0.0115, 0.0118, 0.3342, 0.0580, 0.0569, 0.0142], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,411][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.5161, 0.0212, 0.0064, 0.0376, 0.0351, 0.0138, 0.0120, 0.0573, 0.0090,
        0.0803, 0.0744, 0.0106, 0.0301, 0.0600, 0.0362], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,412][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([1.4809e-02, 3.9707e-03, 9.2330e-04, 1.3777e-03, 3.0040e-04, 9.2298e-04,
        7.0001e-03, 8.2834e-03, 2.4379e-02, 4.4867e-03, 4.9002e-02, 6.9676e-02,
        1.7340e-02, 4.3115e-02, 7.5441e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,413][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([4.9009e-01, 2.9915e-03, 1.7781e-03, 3.4604e-04, 4.3694e-04, 8.9890e-04,
        3.7865e-03, 2.5505e-03, 1.2474e-02, 1.5551e-03, 3.7902e-02, 2.9136e-02,
        9.0117e-03, 1.1871e-02, 3.7603e-01, 1.9144e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,416][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0456, 0.0351, 0.0128, 0.0040, 0.0018, 0.0062, 0.0231, 0.0518, 0.0945,
        0.0324, 0.0894, 0.1111, 0.0535, 0.0382, 0.3283, 0.0722],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,418][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.8955, 0.0178, 0.0068, 0.0060, 0.0031, 0.0042, 0.0058, 0.0027, 0.0041,
        0.0029, 0.0042, 0.0023, 0.0032, 0.0218, 0.0123, 0.0073],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,419][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.6952, 0.0123, 0.0125, 0.0022, 0.0022, 0.0017, 0.0065, 0.0087, 0.0186,
        0.0211, 0.0364, 0.0222, 0.0149, 0.0213, 0.1003, 0.0239],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,421][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([8.6913e-01, 2.3205e-03, 8.3339e-04, 8.3085e-04, 1.4447e-03, 5.0144e-03,
        2.6042e-03, 9.1380e-03, 1.5829e-03, 1.0957e-02, 4.7604e-02, 1.4059e-03,
        1.1621e-02, 6.0583e-03, 1.2313e-02, 1.7139e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,423][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.1364, 0.0316, 0.0174, 0.0110, 0.0043, 0.0111, 0.0164, 0.0222, 0.0271,
        0.0152, 0.0599, 0.0380, 0.0498, 0.0825, 0.3487, 0.1285],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,425][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([9.5990e-01, 1.6326e-03, 5.1595e-03, 1.0832e-03, 1.3845e-03, 8.0815e-04,
        8.6338e-04, 1.8904e-03, 1.0092e-03, 3.6894e-03, 9.6679e-03, 8.6083e-04,
        2.5775e-03, 1.5097e-03, 1.6651e-03, 6.2987e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,427][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0456, 0.0348, 0.0288, 0.0083, 0.0183, 0.0238, 0.0335, 0.0508, 0.1040,
        0.0239, 0.1671, 0.1408, 0.0456, 0.0478, 0.1359, 0.0911],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,429][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.1527, 0.0045, 0.0013, 0.0019, 0.0007, 0.0018, 0.0095, 0.0065, 0.0189,
        0.0092, 0.0549, 0.0397, 0.0104, 0.0299, 0.6029, 0.0551],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,430][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.0053, 0.0208, 0.0633, 0.1663, 0.1855, 0.1560, 0.0045, 0.0750, 0.0185,
        0.0047, 0.0071, 0.1193, 0.0703, 0.0413, 0.0103, 0.0519],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,433][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.7583, 0.0209, 0.0054, 0.0197, 0.0155, 0.0117, 0.0070, 0.0174, 0.0044,
        0.0186, 0.0349, 0.0039, 0.0115, 0.0158, 0.0293, 0.0256],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,434][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([2.9137e-03, 6.4879e-04, 2.3363e-04, 1.1352e-04, 1.3441e-04, 5.2253e-04,
        1.3871e-03, 1.6623e-03, 5.8319e-03, 6.5390e-04, 1.1117e-02, 8.7822e-03,
        8.5709e-04, 2.3966e-03, 9.5105e-01, 1.1693e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,436][circuit_model.py][line:1570][INFO] ##6-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([8.3974e-01, 1.6668e-03, 1.3149e-03, 2.0493e-04, 2.3282e-04, 5.2443e-04,
        1.3661e-03, 2.0438e-03, 4.2144e-03, 8.3051e-04, 2.6134e-02, 6.7846e-03,
        3.3884e-03, 2.9312e-03, 8.5878e-02, 9.9455e-03, 1.2800e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,437][circuit_model.py][line:1573][INFO] ##6-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.1279, 0.0236, 0.0074, 0.0014, 0.0006, 0.0037, 0.0242, 0.0189, 0.0651,
        0.0134, 0.0467, 0.0507, 0.0254, 0.0096, 0.4442, 0.1067, 0.0304],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,438][circuit_model.py][line:1576][INFO] ##6-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.8733, 0.0177, 0.0062, 0.0050, 0.0024, 0.0023, 0.0074, 0.0020, 0.0033,
        0.0022, 0.0026, 0.0017, 0.0026, 0.0219, 0.0120, 0.0082, 0.0291],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,439][circuit_model.py][line:1579][INFO] ##6-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.7459, 0.0129, 0.0099, 0.0017, 0.0015, 0.0021, 0.0050, 0.0089, 0.0145,
        0.0180, 0.0283, 0.0171, 0.0104, 0.0141, 0.0584, 0.0263, 0.0249],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,439][circuit_model.py][line:1582][INFO] ##6-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([9.2545e-01, 1.1898e-03, 3.3949e-04, 3.4680e-04, 7.2245e-04, 3.5540e-03,
        1.2960e-03, 5.0725e-03, 6.2185e-04, 6.5612e-03, 2.4064e-02, 5.1508e-04,
        6.1283e-03, 3.2783e-03, 7.2025e-03, 9.1551e-03, 4.5061e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,441][circuit_model.py][line:1585][INFO] ##6-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2024, 0.0293, 0.0131, 0.0084, 0.0033, 0.0097, 0.0143, 0.0193, 0.0197,
        0.0124, 0.0480, 0.0206, 0.0430, 0.0625, 0.2644, 0.0824, 0.1472],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,442][circuit_model.py][line:1588][INFO] ##6-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([9.8576e-01, 5.6358e-04, 1.0598e-03, 3.2373e-04, 3.9681e-04, 2.5586e-04,
        3.2427e-04, 7.2495e-04, 5.0841e-04, 1.1340e-03, 4.5903e-03, 3.8613e-04,
        8.0478e-04, 8.5439e-04, 4.3311e-04, 1.0500e-03, 8.2881e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,444][circuit_model.py][line:1591][INFO] ##6-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0930, 0.0366, 0.0322, 0.0093, 0.0161, 0.0146, 0.0300, 0.0342, 0.0835,
        0.0229, 0.1437, 0.1019, 0.0416, 0.0414, 0.1039, 0.0845, 0.1105],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,445][circuit_model.py][line:1594][INFO] ##6-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([6.1450e-01, 2.6490e-03, 8.6692e-04, 1.0045e-03, 2.5164e-04, 7.1863e-04,
        5.5424e-03, 2.6291e-03, 8.2784e-03, 5.0262e-03, 3.2764e-02, 1.4844e-02,
        9.2723e-03, 1.0630e-02, 1.9449e-01, 4.4433e-02, 5.2101e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,447][circuit_model.py][line:1597][INFO] ##6-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0072, 0.0572, 0.0224, 0.0682, 0.0526, 0.0863, 0.0039, 0.0483, 0.0355,
        0.0197, 0.0091, 0.2193, 0.1479, 0.0387, 0.0377, 0.1132, 0.0325],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,449][circuit_model.py][line:1600][INFO] ##6-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.8107, 0.0119, 0.0036, 0.0100, 0.0102, 0.0057, 0.0030, 0.0109, 0.0023,
        0.0141, 0.0261, 0.0020, 0.0071, 0.0127, 0.0143, 0.0284, 0.0271],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,451][circuit_model.py][line:1603][INFO] ##6-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([3.1160e-02, 1.6775e-03, 6.1639e-04, 2.4188e-04, 3.3553e-04, 1.6207e-03,
        4.0903e-03, 4.2526e-03, 1.3943e-02, 1.3862e-03, 3.1535e-02, 1.5214e-02,
        1.6452e-03, 4.7547e-03, 8.1819e-01, 4.3471e-02, 2.5866e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,454][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:29,457][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 372],
        [4381],
        [2330],
        [1288],
        [3142],
        [8132],
        [3421],
        [3127],
        [6891],
        [1267],
        [1932],
        [2242],
        [1790],
        [ 246],
        [1444],
        [ 543],
        [ 343]], device='cuda:0')
[2024-07-23 21:05:29,458][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  486],
        [15247],
        [ 8904],
        [ 7035],
        [ 9655],
        [18968],
        [11506],
        [13269],
        [23230],
        [ 5370],
        [ 6064],
        [14803],
        [ 6027],
        [ 1342],
        [ 8475],
        [ 2340],
        [ 1533]], device='cuda:0')
[2024-07-23 21:05:29,461][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[26369],
        [41306],
        [41753],
        [41749],
        [39479],
        [37519],
        [35597],
        [33306],
        [34131],
        [35139],
        [33448],
        [33092],
        [34079],
        [32701],
        [21311],
        [18672],
        [26468]], device='cuda:0')
[2024-07-23 21:05:29,463][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 2530],
        [26994],
        [20715],
        [18662],
        [20541],
        [20039],
        [ 8992],
        [ 8731],
        [ 8373],
        [ 8801],
        [ 7954],
        [ 5703],
        [ 6257],
        [ 6572],
        [ 9033],
        [12418],
        [15262]], device='cuda:0')
[2024-07-23 21:05:29,465][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[17073],
        [19917],
        [17880],
        [18800],
        [19260],
        [19142],
        [19690],
        [19370],
        [18757],
        [17927],
        [16859],
        [16496],
        [15937],
        [15407],
        [15191],
        [15105],
        [14633]], device='cuda:0')
[2024-07-23 21:05:29,467][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[7917],
        [1554],
        [1938],
        [2502],
        [2196],
        [2523],
        [3525],
        [3416],
        [5033],
        [4826],
        [4740],
        [5662],
        [5176],
        [5078],
        [5370],
        [5131],
        [5092]], device='cuda:0')
[2024-07-23 21:05:29,469][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 2146],
        [18489],
        [16631],
        [19185],
        [18947],
        [19139],
        [19781],
        [20537],
        [20205],
        [20003],
        [19967],
        [20208],
        [21446],
        [22111],
        [21768],
        [21531],
        [22556]], device='cuda:0')
[2024-07-23 21:05:29,470][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[5593],
        [ 368],
        [1008],
        [ 990],
        [1979],
        [5493],
        [2568],
        [2250],
        [1198],
        [ 832],
        [1108],
        [ 358],
        [ 723],
        [ 890],
        [ 515],
        [ 719],
        [ 970]], device='cuda:0')
[2024-07-23 21:05:29,473][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 5058],
        [23953],
        [15333],
        [12991],
        [13446],
        [12063],
        [10703],
        [10998],
        [10129],
        [10247],
        [ 9692],
        [ 9551],
        [ 9571],
        [ 9321],
        [ 9792],
        [ 9397],
        [ 9223]], device='cuda:0')
[2024-07-23 21:05:29,475][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[16597],
        [29320],
        [16800],
        [13186],
        [10211],
        [ 7469],
        [ 7177],
        [ 5484],
        [ 5102],
        [ 5274],
        [ 5085],
        [ 4474],
        [ 3929],
        [ 4066],
        [ 3217],
        [ 2975],
        [ 3319]], device='cuda:0')
[2024-07-23 21:05:29,476][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 351],
        [1015],
        [2889],
        [2862],
        [2300],
        [2587],
        [1753],
        [2763],
        [4819],
        [9808],
        [3901],
        [4768],
        [9174],
        [3757],
        [4508],
        [8651],
        [2375]], device='cuda:0')
[2024-07-23 21:05:29,478][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[11609],
        [39099],
        [21065],
        [12353],
        [12327],
        [20421],
        [10606],
        [11790],
        [12267],
        [14590],
        [12973],
        [12194],
        [12086],
        [14535],
        [11946],
        [11063],
        [13142]], device='cuda:0')
[2024-07-23 21:05:29,480][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[8645],
        [2571],
        [2927],
        [2767],
        [3192],
        [3416],
        [3688],
        [3887],
        [4049],
        [4287],
        [4774],
        [4867],
        [5029],
        [5369],
        [5324],
        [5173],
        [5481]], device='cuda:0')
[2024-07-23 21:05:29,482][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 1006],
        [  174],
        [  136],
        [ 6413],
        [ 7202],
        [ 3360],
        [17660],
        [ 3096],
        [10717],
        [ 4458],
        [15241],
        [ 9805],
        [12710],
        [ 3182],
        [ 2634],
        [ 4222],
        [ 1529]], device='cuda:0')
[2024-07-23 21:05:29,484][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 270],
        [ 562],
        [ 830],
        [ 409],
        [ 594],
        [ 586],
        [ 196],
        [ 557],
        [ 162],
        [ 232],
        [ 873],
        [ 221],
        [1414],
        [ 721],
        [ 272],
        [ 772],
        [ 539]], device='cuda:0')
[2024-07-23 21:05:29,486][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[9837],
        [1497],
        [ 261],
        [ 341],
        [1348],
        [1260],
        [2921],
        [2753],
        [ 953],
        [ 416],
        [ 244],
        [ 656],
        [ 874],
        [2881],
        [  52],
        [  69],
        [2648]], device='cuda:0')
[2024-07-23 21:05:29,488][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[1641],
        [2199],
        [ 874],
        [ 795],
        [1040],
        [ 922],
        [1084],
        [1018],
        [1570],
        [1808],
        [1408],
        [1955],
        [2052],
        [1753],
        [1787],
        [2652],
        [3370]], device='cuda:0')
[2024-07-23 21:05:29,490][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1454],
        [2043],
        [2664],
        [2832],
        [3691],
        [2505],
        [2931],
        [2407],
        [2264],
        [4137],
        [4788],
        [2317],
        [2671],
        [3149],
        [5306],
        [2841],
        [3006]], device='cuda:0')
[2024-07-23 21:05:29,492][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[1843],
        [ 438],
        [ 642],
        [ 422],
        [ 441],
        [ 381],
        [ 509],
        [1502],
        [2770],
        [2465],
        [1864],
        [1103],
        [1827],
        [1793],
        [2280],
        [2268],
        [2171]], device='cuda:0')
[2024-07-23 21:05:29,494][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[16883],
        [16338],
        [16576],
        [16334],
        [14473],
        [17491],
        [13546],
        [ 9621],
        [16733],
        [ 7434],
        [ 9787],
        [14354],
        [13778],
        [14257],
        [13477],
        [16729],
        [17016]], device='cuda:0')
[2024-07-23 21:05:29,496][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[2840],
        [4490],
        [5316],
        [4983],
        [4723],
        [5357],
        [4266],
        [3215],
        [3545],
        [3708],
        [2559],
        [3333],
        [2961],
        [3532],
        [2924],
        [2662],
        [3063]], device='cuda:0')
[2024-07-23 21:05:29,498][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[1185],
        [1078],
        [ 579],
        [ 908],
        [ 707],
        [ 801],
        [ 943],
        [ 766],
        [ 843],
        [ 164],
        [ 118],
        [ 886],
        [ 207],
        [ 613],
        [ 425],
        [ 390],
        [ 818]], device='cuda:0')
[2024-07-23 21:05:29,500][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 295],
        [ 156],
        [ 224],
        [ 228],
        [ 214],
        [ 224],
        [ 214],
        [ 430],
        [1527],
        [1299],
        [ 832],
        [1179],
        [1218],
        [ 822],
        [ 603],
        [ 494],
        [ 412]], device='cuda:0')
[2024-07-23 21:05:29,502][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[29318],
        [32317],
        [18460],
        [19080],
        [21789],
        [22238],
        [23483],
        [21712],
        [13244],
        [ 9751],
        [14610],
        [13069],
        [ 8370],
        [11652],
        [14302],
        [13362],
        [18002]], device='cuda:0')
[2024-07-23 21:05:29,504][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[18468],
        [ 1077],
        [ 2407],
        [ 1196],
        [ 1162],
        [  238],
        [  422],
        [  107],
        [  137],
        [   41],
        [   19],
        [   61],
        [   54],
        [   55],
        [   53],
        [   55],
        [   47]], device='cuda:0')
[2024-07-23 21:05:29,506][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[1340],
        [ 237],
        [ 304],
        [ 481],
        [ 687],
        [ 562],
        [ 403],
        [ 493],
        [ 373],
        [ 510],
        [ 571],
        [ 798],
        [ 410],
        [ 435],
        [ 533],
        [ 565],
        [ 448]], device='cuda:0')
[2024-07-23 21:05:29,507][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[24032],
        [ 5857],
        [ 7098],
        [ 6975],
        [ 6519],
        [ 5357],
        [ 8591],
        [ 2871],
        [ 3251],
        [ 3181],
        [ 3495],
        [ 3070],
        [ 2884],
        [ 2678],
        [ 1764],
        [ 1676],
        [ 1679]], device='cuda:0')
[2024-07-23 21:05:29,509][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[37096],
        [46245],
        [46620],
        [46765],
        [46692],
        [46832],
        [46721],
        [47015],
        [46430],
        [47167],
        [47545],
        [47065],
        [47164],
        [46900],
        [46375],
        [45422],
        [44365]], device='cuda:0')
[2024-07-23 21:05:29,511][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[42943],
        [40870],
        [39433],
        [40769],
        [38118],
        [42996],
        [46380],
        [45202],
        [46949],
        [43835],
        [41486],
        [46781],
        [44411],
        [43650],
        [48046],
        [49523],
        [49227]], device='cuda:0')
[2024-07-23 21:05:29,513][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411],
        [8411]], device='cuda:0')
[2024-07-23 21:05:29,591][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:29,593][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,594][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,595][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,596][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,598][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,599][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,600][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,602][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,603][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,605][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,606][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,607][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,609][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.3528, 0.6472], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,611][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.8031, 0.1969], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,612][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ original] are: tensor([6.8500e-04, 9.9932e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,613][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.1559, 0.8441], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,615][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.0897, 0.9103], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,616][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.8189, 0.1811], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,618][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.3446, 0.6554], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,620][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.1184, 0.8816], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,622][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.9459, 0.0541], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,623][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.4391, 0.5609], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,624][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ original] are: tensor([1.0860e-04, 9.9989e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,626][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.6011, 0.3989], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,628][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.1100, 0.4987, 0.3913], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,629][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.7550, 0.2422, 0.0029], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,630][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ language] are: tensor([4.3838e-04, 5.8909e-01, 4.1047e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,632][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.0838, 0.4735, 0.4428], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,634][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0127, 0.7323, 0.2550], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,635][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8389, 0.1160, 0.0451], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,637][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.1785, 0.4241, 0.3974], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,639][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0289, 0.6421, 0.3291], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,640][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.9501, 0.0294, 0.0205], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,642][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.2768, 0.2339, 0.4893], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,644][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0008, 0.6707, 0.3286], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,646][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.3581, 0.3148, 0.3271], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,647][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0869, 0.2483, 0.3002, 0.3646], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,647][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.6571, 0.2953, 0.0055, 0.0421], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,647][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ of] are: tensor([3.0836e-04, 3.1869e-01, 2.3033e-01, 4.5067e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,648][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0578, 0.3167, 0.2967, 0.3288], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,648][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0527, 0.7314, 0.1639, 0.0520], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,648][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9214, 0.0584, 0.0179, 0.0023], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,649][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.1601, 0.2797, 0.2687, 0.2914], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,649][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.1407, 0.3664, 0.2944, 0.1986], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,649][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.9573, 0.0195, 0.0154, 0.0078], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,650][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.3189, 0.1804, 0.3874, 0.1133], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,651][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ of] are: tensor([2.7292e-05, 6.5000e-01, 1.5882e-01, 1.9115e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,653][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.5863, 0.1655, 0.0712, 0.1770], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,654][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0687, 0.1608, 0.2048, 0.2701, 0.2957], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,656][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.3952, 0.3020, 0.0115, 0.0565, 0.2348], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,657][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0004, 0.2636, 0.1621, 0.3223, 0.2517], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,659][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.0413, 0.2448, 0.2290, 0.2553, 0.2295], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,661][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0711, 0.6597, 0.1804, 0.0493, 0.0396], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,662][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8630, 0.0780, 0.0425, 0.0065, 0.0099], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,664][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.1060, 0.2246, 0.2136, 0.2371, 0.2186], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,666][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.0174, 0.3326, 0.2572, 0.1698, 0.2231], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,667][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.9074, 0.0372, 0.0287, 0.0157, 0.0111], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,669][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.1970, 0.0925, 0.1933, 0.2580, 0.2593], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,670][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ De] are: tensor([1.4063e-04, 7.8905e-01, 7.6752e-02, 1.0462e-01, 2.9435e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,672][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.2937, 0.1699, 0.1300, 0.2573, 0.1491], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,673][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0560, 0.1165, 0.1027, 0.2479, 0.3021, 0.1748], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,675][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4692, 0.2083, 0.0088, 0.0305, 0.1897, 0.0935], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,676][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([3.1732e-04, 1.7632e-01, 8.3087e-02, 1.8206e-01, 1.3189e-01, 4.2633e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,678][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.0350, 0.1995, 0.1870, 0.2060, 0.1857, 0.1867], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,679][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.0280, 0.5000, 0.1604, 0.0592, 0.0611, 0.1913], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,682][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.8026, 0.0860, 0.0383, 0.0074, 0.0120, 0.0537], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,683][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0797, 0.1853, 0.1763, 0.1985, 0.1820, 0.1782], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,684][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0344, 0.1786, 0.1096, 0.0953, 0.2177, 0.3643], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,686][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.9215, 0.0250, 0.0182, 0.0117, 0.0117, 0.0118], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,688][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.1570, 0.0211, 0.0888, 0.1014, 0.1723, 0.4594], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,690][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0012, 0.7664, 0.0593, 0.0295, 0.0486, 0.0950], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,691][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.1163, 0.1192, 0.0686, 0.3624, 0.1076, 0.2258], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:29,693][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0220, 0.0947, 0.1064, 0.1687, 0.2891, 0.2004, 0.1186],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,695][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.4838, 0.1349, 0.0061, 0.0444, 0.1392, 0.1533, 0.0381],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,696][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([2.0220e-04, 1.5196e-01, 7.6068e-02, 1.5265e-01, 1.3833e-01, 2.4738e-01,
        2.3341e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,698][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.0347, 0.1611, 0.1515, 0.1664, 0.1513, 0.1524, 0.1826],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,699][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.0990, 0.2465, 0.0885, 0.0513, 0.0670, 0.1684, 0.2793],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,701][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.7502, 0.0752, 0.0401, 0.0135, 0.0127, 0.0463, 0.0621],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,703][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0800, 0.1524, 0.1479, 0.1616, 0.1490, 0.1472, 0.1620],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,705][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0063, 0.1036, 0.0889, 0.0838, 0.0690, 0.2085, 0.4399],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,706][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.9305, 0.0153, 0.0135, 0.0106, 0.0091, 0.0096, 0.0114],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,707][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1045, 0.0230, 0.0482, 0.1011, 0.1841, 0.2293, 0.3098],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,710][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0006, 0.3729, 0.1301, 0.1014, 0.2762, 0.0735, 0.0453],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,711][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2889, 0.1512, 0.0584, 0.2066, 0.1741, 0.1043, 0.0166],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:29,713][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.0254, 0.0749, 0.0731, 0.1680, 0.2307, 0.1645, 0.1089, 0.1545],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,714][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.7120, 0.0655, 0.0044, 0.0155, 0.0781, 0.0537, 0.0110, 0.0599],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,714][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([2.2436e-04, 1.5664e-01, 6.3311e-02, 1.4091e-01, 1.5391e-01, 2.3407e-01,
        1.6773e-01, 8.3198e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,714][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.0309, 0.1372, 0.1290, 0.1410, 0.1283, 0.1284, 0.1542, 0.1510],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,715][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0407, 0.1649, 0.0371, 0.0350, 0.0391, 0.1372, 0.1432, 0.4028],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,715][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.7329, 0.0807, 0.0254, 0.0056, 0.0095, 0.0409, 0.0672, 0.0378],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,715][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0705, 0.1278, 0.1270, 0.1399, 0.1296, 0.1273, 0.1396, 0.1384],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,716][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0051, 0.0649, 0.0454, 0.0380, 0.0278, 0.2239, 0.0667, 0.5282],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,716][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.9216, 0.0156, 0.0109, 0.0089, 0.0108, 0.0105, 0.0129, 0.0087],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,717][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.2735, 0.0077, 0.0297, 0.0360, 0.0338, 0.1537, 0.1865, 0.2790],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,717][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([5.5756e-04, 5.7000e-01, 7.7429e-02, 3.8281e-02, 1.5953e-01, 5.3599e-02,
        7.8541e-02, 2.2064e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,717][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.1679, 0.0905, 0.0352, 0.2175, 0.1305, 0.1104, 0.0377, 0.2103],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:29,719][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0152, 0.0694, 0.0847, 0.1214, 0.1712, 0.1340, 0.0930, 0.1877, 0.1234],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,721][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4538, 0.0944, 0.0031, 0.0177, 0.1360, 0.1355, 0.0236, 0.1337, 0.0023],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,723][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0003, 0.1202, 0.0666, 0.1156, 0.0820, 0.2100, 0.1985, 0.0721, 0.1347],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,724][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0250, 0.1189, 0.1126, 0.1237, 0.1113, 0.1126, 0.1342, 0.1322, 0.1295],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,726][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0166, 0.0614, 0.0160, 0.0149, 0.0251, 0.0457, 0.0829, 0.1979, 0.5395],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,727][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.6650, 0.0554, 0.0264, 0.0070, 0.0091, 0.0671, 0.0679, 0.0326, 0.0695],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,729][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0638, 0.1133, 0.1110, 0.1213, 0.1130, 0.1124, 0.1216, 0.1218, 0.1217],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,731][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0345, 0.0803, 0.0388, 0.0402, 0.0408, 0.0951, 0.1731, 0.2851, 0.2121],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,733][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8933, 0.0164, 0.0145, 0.0140, 0.0094, 0.0114, 0.0148, 0.0104, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,734][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0261, 0.0055, 0.0150, 0.0842, 0.0591, 0.0713, 0.1215, 0.3304, 0.2869],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,735][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0006, 0.4703, 0.1042, 0.0866, 0.2078, 0.0432, 0.0294, 0.0236, 0.0342],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,738][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2891, 0.1192, 0.0472, 0.1670, 0.0972, 0.1324, 0.0327, 0.0861, 0.0291],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:29,739][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0136, 0.0620, 0.0864, 0.1004, 0.1438, 0.1505, 0.0862, 0.1693, 0.1269,
        0.0610], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,741][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.6607, 0.0662, 0.0032, 0.0134, 0.0596, 0.0756, 0.0184, 0.0655, 0.0024,
        0.0350], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,742][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ et] are: tensor([1.8447e-04, 1.3136e-01, 5.6778e-02, 9.8905e-02, 9.0516e-02, 2.2153e-01,
        1.3933e-01, 6.9632e-02, 9.0868e-02, 1.0090e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,744][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.0226, 0.1060, 0.0987, 0.1086, 0.0985, 0.0997, 0.1192, 0.1173, 0.1144,
        0.1149], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,745][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0204, 0.0733, 0.0241, 0.0135, 0.0156, 0.0433, 0.0572, 0.1084, 0.5526,
        0.0916], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,746][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ et] are: tensor([9.3909e-01, 1.0689e-02, 2.8174e-03, 4.0346e-04, 9.6397e-04, 6.3713e-03,
        1.7830e-02, 5.4302e-03, 1.5900e-02, 5.0232e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,748][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0473, 0.1029, 0.0985, 0.1100, 0.1024, 0.1004, 0.1116, 0.1138, 0.1129,
        0.1001], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,750][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0045, 0.0537, 0.0092, 0.0188, 0.0649, 0.1561, 0.1187, 0.4519, 0.1115,
        0.0106], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,752][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.7809, 0.0359, 0.0289, 0.0215, 0.0206, 0.0219, 0.0261, 0.0189, 0.0322,
        0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,753][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0617, 0.0012, 0.0050, 0.0056, 0.0086, 0.0284, 0.0494, 0.1229, 0.2136,
        0.5035], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,755][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ et] are: tensor([4.8781e-05, 5.9862e-01, 3.2293e-02, 4.4244e-02, 1.3428e-02, 7.3297e-02,
        3.4663e-02, 6.9041e-02, 1.2687e-01, 7.5037e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,756][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.2783, 0.0858, 0.0286, 0.0706, 0.1238, 0.1224, 0.0212, 0.2131, 0.0147,
        0.0414], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:29,758][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0196, 0.0549, 0.0650, 0.1125, 0.1385, 0.1332, 0.0777, 0.1568, 0.1368,
        0.0590, 0.0460], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,760][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.7238, 0.0777, 0.0025, 0.0101, 0.0518, 0.0632, 0.0064, 0.0264, 0.0013,
        0.0261, 0.0107], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,761][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([1.0608e-04, 7.1071e-02, 3.9174e-02, 1.3207e-01, 9.6080e-02, 2.0295e-01,
        1.6407e-01, 9.3520e-02, 9.6178e-02, 8.8477e-02, 1.6302e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,762][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.0203, 0.0950, 0.0906, 0.0986, 0.0890, 0.0897, 0.1073, 0.1061, 0.1040,
        0.1043, 0.0951], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,764][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0311, 0.0693, 0.0285, 0.0157, 0.0141, 0.0353, 0.0723, 0.0726, 0.3352,
        0.0762, 0.2496], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,766][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.8605, 0.0160, 0.0089, 0.0016, 0.0027, 0.0223, 0.0326, 0.0138, 0.0319,
        0.0018, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,768][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0426, 0.0924, 0.0892, 0.0999, 0.0938, 0.0926, 0.1014, 0.1031, 0.1018,
        0.0911, 0.0920], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,769][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0011, 0.0957, 0.0110, 0.0125, 0.0675, 0.2143, 0.0666, 0.4140, 0.0880,
        0.0074, 0.0219], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,771][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.8792, 0.0163, 0.0136, 0.0111, 0.0094, 0.0094, 0.0120, 0.0083, 0.0129,
        0.0066, 0.0212], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,773][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.1128, 0.0009, 0.0017, 0.0062, 0.0038, 0.0089, 0.0199, 0.0350, 0.0848,
        0.1215, 0.6044], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,774][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.0004, 0.1810, 0.0384, 0.0457, 0.0462, 0.0678, 0.1214, 0.2303, 0.2327,
        0.0335, 0.0025], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,776][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.0979, 0.0777, 0.0510, 0.1659, 0.1348, 0.0840, 0.0342, 0.1140, 0.0276,
        0.1073, 0.1057], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:29,778][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0132, 0.0536, 0.0602, 0.0899, 0.1283, 0.1045, 0.0705, 0.1491, 0.0904,
        0.0668, 0.0676, 0.1058], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,780][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.4103, 0.0842, 0.0035, 0.0165, 0.1039, 0.1027, 0.0270, 0.1165, 0.0033,
        0.1012, 0.0218, 0.0090], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,781][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0003, 0.0862, 0.0456, 0.0768, 0.0665, 0.1802, 0.1414, 0.0504, 0.0873,
        0.0693, 0.0126, 0.1834], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,781][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0168, 0.0875, 0.0822, 0.0902, 0.0812, 0.0822, 0.0979, 0.0963, 0.0944,
        0.0947, 0.0868, 0.0897], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,782][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0194, 0.0190, 0.0045, 0.0031, 0.0039, 0.0103, 0.0171, 0.0412, 0.1452,
        0.0481, 0.1497, 0.5385], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,782][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.8296, 0.0255, 0.0114, 0.0020, 0.0024, 0.0283, 0.0323, 0.0127, 0.0345,
        0.0024, 0.0066, 0.0122], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,782][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0447, 0.0847, 0.0823, 0.0909, 0.0855, 0.0846, 0.0912, 0.0924, 0.0915,
        0.0829, 0.0840, 0.0854], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,783][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0206, 0.0623, 0.0398, 0.0299, 0.0307, 0.0663, 0.1184, 0.1621, 0.1382,
        0.0588, 0.0931, 0.1798], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,783][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.8735, 0.0123, 0.0117, 0.0100, 0.0072, 0.0087, 0.0122, 0.0078, 0.0130,
        0.0071, 0.0247, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,784][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0201, 0.0008, 0.0021, 0.0060, 0.0056, 0.0068, 0.0156, 0.0376, 0.0442,
        0.1019, 0.3633, 0.3960], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,784][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [orum] are: tensor([9.5633e-05, 3.1443e-01, 6.6002e-02, 5.7053e-02, 1.1659e-01, 4.5094e-02,
        2.7803e-02, 2.1502e-02, 3.1382e-02, 1.0842e-02, 1.2564e-03, 3.0796e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,785][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.5108, 0.0656, 0.0253, 0.0663, 0.0565, 0.0993, 0.0192, 0.0523, 0.0148,
        0.0242, 0.0560, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:29,786][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0059, 0.0425, 0.0483, 0.0838, 0.1285, 0.0983, 0.0638, 0.1343, 0.0951,
        0.0618, 0.0568, 0.1104, 0.0706], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,788][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.5998, 0.0787, 0.0034, 0.0204, 0.0601, 0.0682, 0.0148, 0.0412, 0.0022,
        0.0461, 0.0086, 0.0063, 0.0504], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,790][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0006, 0.1114, 0.0537, 0.0958, 0.1188, 0.1756, 0.0852, 0.0609, 0.0502,
        0.0623, 0.0109, 0.0911, 0.0833], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,791][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0158, 0.0805, 0.0752, 0.0828, 0.0750, 0.0751, 0.0900, 0.0890, 0.0870,
        0.0873, 0.0795, 0.0822, 0.0806], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,793][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0160, 0.0734, 0.0227, 0.0073, 0.0059, 0.0216, 0.0135, 0.0372, 0.1549,
        0.0423, 0.1056, 0.3418, 0.1577], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,794][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ is] are: tensor([9.2238e-01, 1.2256e-02, 2.7281e-03, 3.8361e-04, 9.1669e-04, 5.8899e-03,
        1.5328e-02, 6.6733e-03, 2.1231e-02, 4.4280e-04, 2.3944e-03, 8.7507e-03,
        6.2152e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,796][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0375, 0.0784, 0.0759, 0.0841, 0.0796, 0.0782, 0.0848, 0.0853, 0.0849,
        0.0762, 0.0777, 0.0796, 0.0777], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,797][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0087, 0.0642, 0.0190, 0.0181, 0.0308, 0.1094, 0.0546, 0.4238, 0.0691,
        0.0207, 0.0379, 0.0993, 0.0443], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,799][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.6013, 0.0473, 0.0427, 0.0286, 0.0221, 0.0273, 0.0347, 0.0241, 0.0392,
        0.0180, 0.0638, 0.0343, 0.0165], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,801][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0898, 0.0011, 0.0018, 0.0013, 0.0027, 0.0059, 0.0091, 0.0085, 0.0550,
        0.0665, 0.3060, 0.4371, 0.0153], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,802][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.0005, 0.4146, 0.0539, 0.0619, 0.0873, 0.0403, 0.0167, 0.0447, 0.0324,
        0.0062, 0.0010, 0.2320, 0.0085], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,804][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2611, 0.0720, 0.0238, 0.1244, 0.1194, 0.1559, 0.0087, 0.1325, 0.0072,
        0.0177, 0.0481, 0.0068, 0.0224], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:29,806][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0042, 0.0356, 0.0414, 0.0803, 0.1183, 0.0931, 0.0530, 0.1452, 0.1030,
        0.0562, 0.0441, 0.1172, 0.0726, 0.0358], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,808][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.4931, 0.0786, 0.0033, 0.0210, 0.0654, 0.0807, 0.0178, 0.0597, 0.0027,
        0.0487, 0.0084, 0.0060, 0.0558, 0.0588], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,810][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0008, 0.1123, 0.0565, 0.0772, 0.0955, 0.1587, 0.0875, 0.0485, 0.0513,
        0.0555, 0.0102, 0.0973, 0.0809, 0.0680], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,811][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0158, 0.0741, 0.0693, 0.0762, 0.0688, 0.0691, 0.0827, 0.0811, 0.0799,
        0.0798, 0.0730, 0.0759, 0.0741, 0.0804], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,813][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.1544, 0.0762, 0.0100, 0.0039, 0.0036, 0.0169, 0.0136, 0.0274, 0.1196,
        0.0198, 0.1621, 0.2190, 0.1025, 0.0711], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,814][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.8493, 0.0330, 0.0085, 0.0011, 0.0017, 0.0118, 0.0297, 0.0112, 0.0296,
        0.0010, 0.0039, 0.0103, 0.0013, 0.0077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,816][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0391, 0.0727, 0.0704, 0.0777, 0.0735, 0.0717, 0.0784, 0.0781, 0.0784,
        0.0704, 0.0716, 0.0736, 0.0715, 0.0728], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,818][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0022, 0.0304, 0.0097, 0.0186, 0.0148, 0.0720, 0.0780, 0.3708, 0.0897,
        0.0196, 0.0186, 0.1056, 0.0727, 0.0973], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,819][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7098, 0.0284, 0.0278, 0.0185, 0.0163, 0.0175, 0.0252, 0.0151, 0.0280,
        0.0138, 0.0489, 0.0253, 0.0121, 0.0134], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,822][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.4309, 0.0022, 0.0024, 0.0010, 0.0020, 0.0045, 0.0070, 0.0049, 0.0416,
        0.0458, 0.2435, 0.1944, 0.0099, 0.0098], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,823][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([5.8900e-05, 2.8218e-01, 3.4122e-02, 3.8862e-02, 4.8113e-02, 2.6137e-02,
        1.2612e-02, 2.6514e-02, 6.2461e-02, 5.8189e-03, 1.1678e-03, 4.0145e-01,
        1.6172e-02, 4.4331e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,824][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.3425, 0.1084, 0.0238, 0.0576, 0.1025, 0.0709, 0.0106, 0.1119, 0.0165,
        0.0191, 0.0697, 0.0150, 0.0385, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:29,826][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0055, 0.0414, 0.0384, 0.0747, 0.1021, 0.0962, 0.0568, 0.0992, 0.1085,
        0.0572, 0.0367, 0.1265, 0.0753, 0.0402, 0.0412], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,828][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.4056, 0.0940, 0.0021, 0.0186, 0.0676, 0.0773, 0.0139, 0.0496, 0.0018,
        0.0672, 0.0067, 0.0053, 0.0636, 0.1006, 0.0260], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,830][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0002, 0.0630, 0.0387, 0.0801, 0.0765, 0.1417, 0.0995, 0.0397, 0.0708,
        0.0433, 0.0091, 0.1269, 0.0695, 0.0628, 0.0780], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,831][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.0162, 0.0679, 0.0648, 0.0701, 0.0633, 0.0638, 0.0761, 0.0749, 0.0737,
        0.0736, 0.0677, 0.0701, 0.0685, 0.0741, 0.0752], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,833][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0792, 0.0325, 0.0067, 0.0026, 0.0014, 0.0048, 0.0068, 0.0099, 0.0409,
        0.0158, 0.0921, 0.1131, 0.1112, 0.1248, 0.3583], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,834][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ same] are: tensor([8.9421e-01, 2.5469e-02, 4.2827e-03, 6.4152e-04, 8.7095e-04, 3.3525e-03,
        1.1870e-02, 4.9437e-03, 1.4418e-02, 6.3149e-04, 2.7162e-03, 8.9808e-03,
        9.3614e-04, 4.8361e-03, 2.1837e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,836][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0291, 0.0676, 0.0636, 0.0736, 0.0679, 0.0650, 0.0740, 0.0761, 0.0760,
        0.0666, 0.0671, 0.0699, 0.0682, 0.0694, 0.0659], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,838][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0013, 0.0321, 0.0081, 0.0120, 0.0236, 0.0887, 0.0318, 0.2460, 0.1056,
        0.0099, 0.0310, 0.1869, 0.0639, 0.1191, 0.0399], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,840][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.5368, 0.0503, 0.0397, 0.0365, 0.0255, 0.0270, 0.0371, 0.0205, 0.0405,
        0.0142, 0.0604, 0.0340, 0.0182, 0.0226, 0.0367], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,841][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.4671, 0.0011, 0.0017, 0.0015, 0.0012, 0.0027, 0.0058, 0.0051, 0.0192,
        0.0259, 0.1375, 0.0988, 0.0218, 0.0309, 0.1795], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,842][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ same] are: tensor([2.2214e-05, 1.2353e-01, 2.0175e-02, 3.3496e-02, 6.0234e-02, 2.1517e-02,
        3.9849e-02, 2.1978e-02, 5.6058e-02, 4.2243e-03, 9.0627e-04, 4.3666e-01,
        2.7122e-02, 1.5201e-01, 2.2248e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,844][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.2459, 0.0703, 0.0332, 0.0990, 0.1229, 0.0879, 0.0125, 0.0923, 0.0116,
        0.0390, 0.0956, 0.0110, 0.0421, 0.0155, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:29,846][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0042, 0.0367, 0.0419, 0.0621, 0.1054, 0.0948, 0.0585, 0.1041, 0.0971,
        0.0480, 0.0338, 0.1138, 0.0684, 0.0410, 0.0594, 0.0310],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,847][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.5757, 0.0505, 0.0014, 0.0128, 0.0424, 0.0562, 0.0078, 0.0384, 0.0013,
        0.0410, 0.0059, 0.0037, 0.0439, 0.0762, 0.0244, 0.0186],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,848][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0004, 0.0622, 0.0386, 0.0910, 0.0771, 0.1686, 0.0737, 0.0444, 0.0503,
        0.0375, 0.0080, 0.0903, 0.0658, 0.0616, 0.0523, 0.0781],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,849][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0148, 0.0632, 0.0602, 0.0652, 0.0590, 0.0594, 0.0707, 0.0698, 0.0689,
        0.0691, 0.0631, 0.0653, 0.0642, 0.0699, 0.0705, 0.0667],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,849][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.1263, 0.0176, 0.0042, 0.0019, 0.0032, 0.0126, 0.0073, 0.0087, 0.0426,
        0.0157, 0.1070, 0.0990, 0.0565, 0.0722, 0.2940, 0.1311],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,849][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.7481, 0.0379, 0.0070, 0.0014, 0.0014, 0.0052, 0.0155, 0.0102, 0.0186,
        0.0023, 0.0066, 0.0113, 0.0031, 0.0126, 0.0670, 0.0518],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,850][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0294, 0.0632, 0.0600, 0.0687, 0.0629, 0.0613, 0.0686, 0.0702, 0.0701,
        0.0622, 0.0629, 0.0651, 0.0641, 0.0655, 0.0624, 0.0633],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,850][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0023, 0.0700, 0.0094, 0.0216, 0.0503, 0.0870, 0.0674, 0.2112, 0.0933,
        0.0134, 0.0217, 0.1203, 0.0594, 0.1300, 0.0375, 0.0051],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,851][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.5297, 0.0349, 0.0348, 0.0263, 0.0249, 0.0227, 0.0316, 0.0214, 0.0327,
        0.0170, 0.0643, 0.0315, 0.0174, 0.0228, 0.0626, 0.0254],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,851][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ as] are: tensor([6.6707e-01, 4.0077e-04, 9.4809e-04, 4.0188e-04, 6.0520e-04, 1.7491e-03,
        2.3454e-03, 1.9093e-03, 1.8234e-02, 1.4364e-02, 1.2144e-01, 7.6397e-02,
        3.7770e-03, 4.9573e-03, 6.6643e-02, 1.8758e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,852][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0007, 0.0914, 0.0421, 0.0686, 0.1730, 0.0369, 0.0179, 0.0309, 0.0238,
        0.0059, 0.0023, 0.1999, 0.0441, 0.2478, 0.0047, 0.0098],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,854][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.3646, 0.0594, 0.0312, 0.1112, 0.1130, 0.0625, 0.0075, 0.1078, 0.0115,
        0.0154, 0.0587, 0.0077, 0.0196, 0.0120, 0.0103, 0.0077],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:29,856][circuit_model.py][line:1532][INFO] ##7-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0035, 0.0318, 0.0324, 0.0726, 0.1050, 0.0831, 0.0440, 0.1192, 0.0894,
        0.0494, 0.0374, 0.1010, 0.0639, 0.0326, 0.0558, 0.0404, 0.0386],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,857][circuit_model.py][line:1535][INFO] ##7-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.3491, 0.0834, 0.0023, 0.0190, 0.0593, 0.0835, 0.0135, 0.0588, 0.0025,
        0.0483, 0.0071, 0.0052, 0.0499, 0.0697, 0.0308, 0.0267, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,858][circuit_model.py][line:1538][INFO] ##7-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0008, 0.0924, 0.0467, 0.0748, 0.0788, 0.1299, 0.0723, 0.0401, 0.0426,
        0.0451, 0.0086, 0.0772, 0.0635, 0.0564, 0.0749, 0.0617, 0.0342],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,860][circuit_model.py][line:1541][INFO] ##7-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0130, 0.0597, 0.0562, 0.0616, 0.0556, 0.0559, 0.0666, 0.0653, 0.0646,
        0.0645, 0.0587, 0.0612, 0.0601, 0.0656, 0.0665, 0.0628, 0.0621],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,862][circuit_model.py][line:1544][INFO] ##7-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.3085, 0.0172, 0.0028, 0.0013, 0.0013, 0.0057, 0.0025, 0.0068, 0.0230,
        0.0060, 0.0631, 0.0381, 0.0381, 0.0321, 0.1873, 0.1754, 0.0909],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,863][circuit_model.py][line:1547][INFO] ##7-th layer ##Weight##: The head6 weight for token [ the] are: tensor([8.2635e-01, 2.8365e-02, 5.9106e-03, 7.3007e-04, 8.3785e-04, 4.9401e-03,
        1.3869e-02, 5.7463e-03, 1.4176e-02, 6.5993e-04, 2.5203e-03, 5.2741e-03,
        8.0808e-04, 5.0233e-03, 3.0830e-02, 3.5351e-02, 1.8611e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,864][circuit_model.py][line:1550][INFO] ##7-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0293, 0.0596, 0.0566, 0.0643, 0.0594, 0.0575, 0.0646, 0.0653, 0.0657,
        0.0581, 0.0591, 0.0612, 0.0600, 0.0611, 0.0589, 0.0596, 0.0597],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,867][circuit_model.py][line:1553][INFO] ##7-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0042, 0.0367, 0.0100, 0.0224, 0.0171, 0.0633, 0.0752, 0.2719, 0.0922,
        0.0257, 0.0237, 0.1012, 0.0873, 0.0763, 0.0261, 0.0058, 0.0609],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,868][circuit_model.py][line:1556][INFO] ##7-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.7099, 0.0188, 0.0206, 0.0140, 0.0135, 0.0146, 0.0189, 0.0121, 0.0216,
        0.0106, 0.0439, 0.0202, 0.0104, 0.0117, 0.0329, 0.0160, 0.0102],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,869][circuit_model.py][line:1559][INFO] ##7-th layer ##Weight##: The head10 weight for token [ the] are: tensor([8.1203e-01, 9.1704e-04, 1.1316e-03, 4.3280e-04, 5.5396e-04, 1.2235e-03,
        1.6447e-03, 8.7056e-04, 1.0776e-02, 8.0504e-03, 4.0638e-02, 3.6455e-02,
        3.6691e-03, 3.3989e-03, 4.5490e-02, 1.6681e-02, 1.6039e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,871][circuit_model.py][line:1562][INFO] ##7-th layer ##Weight##: The head11 weight for token [ the] are: tensor([2.0199e-04, 2.5298e-01, 3.6504e-02, 5.8536e-02, 7.6340e-02, 3.0802e-02,
        1.7423e-02, 2.3661e-02, 3.5724e-02, 7.4921e-03, 1.7697e-03, 2.3840e-01,
        3.7472e-02, 1.3266e-01, 5.7890e-03, 6.5558e-03, 3.7692e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,873][circuit_model.py][line:1565][INFO] ##7-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.5163, 0.0636, 0.0169, 0.0451, 0.0675, 0.0666, 0.0062, 0.0781, 0.0100,
        0.0094, 0.0564, 0.0071, 0.0192, 0.0079, 0.0093, 0.0107, 0.0097],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:29,919][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:29,920][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,921][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,922][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,922][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,923][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,924][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,924][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,926][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,927][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,928][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,930][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,931][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:29,933][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.0308, 0.9692], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,935][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.5666, 0.4334], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,936][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.0015, 0.9985], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,939][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.3562, 0.6438], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,940][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.1256, 0.8744], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,942][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.9273, 0.0727], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,944][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.9789, 0.0211], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,946][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.6810, 0.3190], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,947][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.1622, 0.8378], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,950][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.3564, 0.6436], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,952][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.0016, 0.9984], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,953][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.6011, 0.3989], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:29,955][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0145, 0.5746, 0.4109], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,957][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.3617, 0.2805, 0.3578], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,959][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0010, 0.6696, 0.3295], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,961][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0899, 0.4717, 0.4384], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,963][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0172, 0.7319, 0.2510], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,964][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.7970, 0.1414, 0.0616], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,967][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.8768, 0.0656, 0.0576], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,968][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.2466, 0.4155, 0.3379], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,970][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.1045, 0.5070, 0.3885], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,972][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0572, 0.5131, 0.4297], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,974][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0052, 0.5038, 0.4910], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,976][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.3581, 0.3148, 0.3271], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:29,978][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0560, 0.2681, 0.6527, 0.0232], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,980][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.3709, 0.4402, 0.1742, 0.0147], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,981][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0005, 0.4024, 0.1075, 0.4896], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,984][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0988, 0.5549, 0.2982, 0.0481], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,985][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0801, 0.7119, 0.1591, 0.0488], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,987][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.9064, 0.0689, 0.0237, 0.0010], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,988][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.8823, 0.0648, 0.0422, 0.0108], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,988][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.5302, 0.2608, 0.1888, 0.0201], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,989][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.1728, 0.3408, 0.4570, 0.0294], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,990][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.1035, 0.4590, 0.3944, 0.0431], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,990][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0008, 0.3294, 0.1852, 0.4846], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,992][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.5863, 0.1655, 0.0712, 0.1770], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:29,994][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0119, 0.2522, 0.6085, 0.0974, 0.0300], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,995][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.2894, 0.2204, 0.3239, 0.0812, 0.0851], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,997][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.0007, 0.4520, 0.1117, 0.3885, 0.0472], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:29,999][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.0689, 0.3987, 0.3501, 0.0538, 0.1286], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,001][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.1081, 0.6416, 0.1737, 0.0453, 0.0313], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,003][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.5927, 0.2196, 0.1492, 0.0133, 0.0252], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,005][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.6272, 0.1725, 0.1200, 0.0551, 0.0253], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,007][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.4045, 0.2392, 0.1836, 0.0710, 0.1018], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,009][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.0192, 0.4368, 0.4150, 0.0733, 0.0557], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,011][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.0780, 0.3952, 0.3203, 0.1443, 0.0623], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,012][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.0016, 0.3589, 0.2041, 0.3761, 0.0592], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,014][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.2937, 0.1699, 0.1300, 0.2573, 0.1491], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,016][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0053, 0.1806, 0.3646, 0.1354, 0.1201, 0.1940], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,018][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.2448, 0.2525, 0.1832, 0.0711, 0.0589, 0.1895], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,020][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.0008, 0.1660, 0.0162, 0.0797, 0.0117, 0.7256], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,022][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.0852, 0.2942, 0.3795, 0.0258, 0.0564, 0.1589], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,024][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0445, 0.5212, 0.1609, 0.0561, 0.0496, 0.1677], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,026][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.7627, 0.0925, 0.0620, 0.0067, 0.0256, 0.0506], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,028][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.6284, 0.1075, 0.1300, 0.0733, 0.0292, 0.0316], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,030][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0541, 0.0995, 0.1394, 0.1557, 0.1264, 0.4248], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,032][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.0919, 0.2152, 0.1636, 0.0448, 0.1777, 0.3068], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,034][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.1107, 0.1585, 0.2805, 0.1062, 0.1077, 0.2364], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,036][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.0077, 0.6159, 0.0995, 0.0638, 0.0754, 0.1378], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,037][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.1163, 0.1192, 0.0686, 0.3624, 0.1076, 0.2258], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,039][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.0207, 0.0361, 0.0524, 0.0478, 0.0305, 0.0942, 0.7183],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,041][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.2225, 0.2207, 0.2007, 0.0894, 0.0704, 0.1496, 0.0466],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,043][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.0009, 0.1917, 0.0401, 0.2429, 0.0637, 0.3606, 0.1001],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,045][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.1490, 0.1349, 0.1892, 0.0265, 0.0508, 0.1631, 0.2865],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,047][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.1332, 0.2589, 0.0906, 0.0515, 0.0559, 0.1594, 0.2505],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,048][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.8797, 0.0404, 0.0275, 0.0076, 0.0104, 0.0243, 0.0100],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,051][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.6508, 0.1119, 0.1258, 0.0588, 0.0185, 0.0134, 0.0209],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,052][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.1376, 0.1240, 0.1282, 0.0619, 0.0669, 0.3039, 0.1776],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,054][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.2784, 0.0628, 0.0969, 0.0506, 0.1145, 0.2020, 0.1948],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,055][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.1437, 0.1401, 0.1084, 0.0642, 0.0900, 0.1558, 0.2978],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,056][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.0032, 0.1874, 0.1686, 0.1840, 0.3599, 0.0786, 0.0182],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,056][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.2889, 0.1512, 0.0584, 0.2066, 0.1741, 0.1043, 0.0166],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,057][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.0230, 0.0462, 0.0693, 0.0247, 0.0145, 0.0227, 0.6305, 0.1690],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,059][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.1423, 0.1326, 0.1728, 0.0753, 0.0871, 0.1878, 0.0993, 0.1027],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,061][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.0009, 0.3379, 0.0169, 0.1394, 0.1105, 0.2962, 0.0596, 0.0386],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,062][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.1069, 0.0858, 0.1289, 0.0099, 0.0255, 0.0872, 0.1066, 0.4493],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,064][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0625, 0.1875, 0.0417, 0.0378, 0.0358, 0.1385, 0.1389, 0.3572],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,066][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.7274, 0.0901, 0.0414, 0.0052, 0.0238, 0.0409, 0.0284, 0.0428],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,068][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.6373, 0.0662, 0.0930, 0.0795, 0.0285, 0.0262, 0.0190, 0.0504],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,070][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.0614, 0.0858, 0.0695, 0.1284, 0.0999, 0.3430, 0.1395, 0.0725],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,072][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.0817, 0.0427, 0.0197, 0.0123, 0.0728, 0.1150, 0.2734, 0.3825],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,074][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.3145, 0.0759, 0.1478, 0.0452, 0.0333, 0.1068, 0.1539, 0.1227],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,075][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.0127, 0.2365, 0.1092, 0.0890, 0.3552, 0.0803, 0.0840, 0.0331],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,078][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.1679, 0.0905, 0.0352, 0.2175, 0.1305, 0.1104, 0.0377, 0.2103],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,080][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0013, 0.0047, 0.0074, 0.0161, 0.0134, 0.0266, 0.2350, 0.1218, 0.5737],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,081][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.1009, 0.2269, 0.1594, 0.1313, 0.0701, 0.0634, 0.0341, 0.0452, 0.1686],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,084][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0025, 0.1406, 0.0358, 0.1097, 0.0191, 0.4336, 0.0627, 0.0488, 0.1472],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,086][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0513, 0.0393, 0.0897, 0.0165, 0.0180, 0.0695, 0.0637, 0.2380, 0.4138],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,087][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0260, 0.0703, 0.0178, 0.0160, 0.0230, 0.0487, 0.0824, 0.1806, 0.5352],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,089][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.8994, 0.0137, 0.0168, 0.0023, 0.0040, 0.0218, 0.0088, 0.0074, 0.0260],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,091][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.5740, 0.0867, 0.0699, 0.1136, 0.0269, 0.0268, 0.0183, 0.0659, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,093][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.2318, 0.0820, 0.0481, 0.0563, 0.0522, 0.1695, 0.0734, 0.0871, 0.1997],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,095][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.0282, 0.0067, 0.0120, 0.0185, 0.0158, 0.0574, 0.1256, 0.2360, 0.4998],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,097][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0429, 0.0185, 0.0362, 0.0467, 0.0404, 0.0682, 0.0985, 0.2951, 0.3537],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,098][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0053, 0.2981, 0.1390, 0.1601, 0.2605, 0.0478, 0.0200, 0.0584, 0.0108],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,101][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.2891, 0.1192, 0.0472, 0.1670, 0.0972, 0.1324, 0.0327, 0.0861, 0.0291],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,102][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0050, 0.0089, 0.0195, 0.0129, 0.0068, 0.0183, 0.1353, 0.1101, 0.4637,
        0.2194], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,104][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0280, 0.2540, 0.1281, 0.0623, 0.0677, 0.0969, 0.0749, 0.0264, 0.1430,
        0.1187], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,105][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([1.5935e-04, 2.1098e-01, 1.0464e-02, 6.5953e-02, 1.8072e-02, 4.9963e-01,
        3.9364e-02, 6.8613e-02, 5.8085e-02, 2.8689e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,107][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.0674, 0.0439, 0.0448, 0.0098, 0.0179, 0.0952, 0.0579, 0.2623, 0.2363,
        0.1645], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,109][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0311, 0.0825, 0.0260, 0.0142, 0.0146, 0.0459, 0.0556, 0.0991, 0.5344,
        0.0967], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,111][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.7349, 0.0439, 0.0267, 0.0031, 0.0109, 0.0180, 0.0242, 0.0234, 0.0790,
        0.0358], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,113][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.4123, 0.1630, 0.1193, 0.0933, 0.0248, 0.0241, 0.0296, 0.0583, 0.0382,
        0.0372], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,115][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.1862, 0.0712, 0.0238, 0.0414, 0.0567, 0.3110, 0.0531, 0.0865, 0.1325,
        0.0377], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,117][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.0213, 0.0262, 0.0233, 0.0114, 0.0345, 0.0622, 0.0798, 0.2656, 0.4408,
        0.0350], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,119][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.0775, 0.0351, 0.0613, 0.0130, 0.0160, 0.0524, 0.1273, 0.1735, 0.3337,
        0.1102], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,121][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.0004, 0.1377, 0.0430, 0.1333, 0.0301, 0.2883, 0.0233, 0.2755, 0.0377,
        0.0308], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,121][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.2783, 0.0858, 0.0286, 0.0706, 0.1238, 0.1224, 0.0212, 0.2131, 0.0147,
        0.0414], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,122][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0197, 0.0146, 0.0261, 0.0107, 0.0046, 0.0123, 0.1472, 0.0577, 0.3966,
        0.0894, 0.2212], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,123][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0288, 0.0870, 0.1591, 0.0545, 0.0370, 0.0809, 0.1225, 0.1048, 0.0943,
        0.1372, 0.0939], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,124][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([9.8328e-05, 7.1375e-02, 8.4494e-03, 1.7013e-01, 2.7055e-02, 2.4551e-01,
        1.5159e-01, 1.4829e-01, 1.1333e-01, 5.7584e-02, 6.5848e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,126][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.0423, 0.0224, 0.0796, 0.0105, 0.0151, 0.0328, 0.0356, 0.1093, 0.1794,
        0.0926, 0.3803], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,128][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0387, 0.0736, 0.0297, 0.0162, 0.0130, 0.0364, 0.0692, 0.0661, 0.3236,
        0.0785, 0.2551], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,130][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.3320, 0.0295, 0.0667, 0.0074, 0.0147, 0.0558, 0.0480, 0.0447, 0.1298,
        0.0434, 0.2280], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,131][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.4134, 0.0738, 0.0878, 0.0623, 0.0441, 0.0431, 0.0321, 0.0665, 0.0403,
        0.0554, 0.0812], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,134][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0234, 0.0811, 0.0940, 0.1296, 0.1112, 0.1228, 0.0897, 0.0682, 0.1036,
        0.0771, 0.0995], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,135][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.0547, 0.0238, 0.0274, 0.0181, 0.0242, 0.0344, 0.0654, 0.1136, 0.1558,
        0.0394, 0.4432], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,137][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.0767, 0.0370, 0.0317, 0.0224, 0.0164, 0.0300, 0.0614, 0.0851, 0.1482,
        0.0569, 0.4342], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,139][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.0009, 0.0419, 0.0434, 0.0704, 0.0782, 0.1170, 0.0767, 0.4259, 0.0467,
        0.0916, 0.0073], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,141][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0979, 0.0777, 0.0510, 0.1659, 0.1348, 0.0840, 0.0342, 0.1140, 0.0276,
        0.1073, 0.1057], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,142][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([6.2010e-03, 5.4056e-04, 8.9492e-04, 1.1782e-03, 9.2405e-04, 2.4037e-03,
        1.5172e-02, 1.4413e-02, 3.1985e-02, 5.8134e-02, 1.3428e-01, 7.3387e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,145][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0563, 0.1838, 0.1052, 0.0640, 0.0551, 0.0445, 0.0229, 0.0358, 0.0886,
        0.0910, 0.0608, 0.1921], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,147][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0029, 0.0536, 0.0124, 0.0267, 0.0110, 0.4636, 0.0228, 0.0218, 0.0444,
        0.0238, 0.0068, 0.3101], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,148][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0637, 0.0121, 0.0318, 0.0037, 0.0044, 0.0174, 0.0129, 0.0412, 0.0788,
        0.0591, 0.3471, 0.3277], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,151][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0308, 0.0214, 0.0051, 0.0033, 0.0036, 0.0112, 0.0172, 0.0389, 0.1465,
        0.0537, 0.1699, 0.4983], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,152][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([8.7157e-01, 8.3351e-03, 8.4103e-03, 6.4947e-04, 1.5534e-03, 7.4319e-03,
        5.0937e-03, 3.5914e-03, 1.9550e-02, 9.1215e-03, 5.2502e-02, 1.2190e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,153][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.6584, 0.0794, 0.0446, 0.0650, 0.0175, 0.0159, 0.0105, 0.0337, 0.0110,
        0.0196, 0.0379, 0.0065], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,156][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.3541, 0.0596, 0.0326, 0.0289, 0.0339, 0.1578, 0.0539, 0.0676, 0.0773,
        0.0167, 0.0609, 0.0568], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,158][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.0206, 0.0010, 0.0022, 0.0015, 0.0015, 0.0049, 0.0201, 0.0216, 0.0663,
        0.0121, 0.3997, 0.4484], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,160][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0255, 0.0027, 0.0050, 0.0023, 0.0037, 0.0056, 0.0098, 0.0333, 0.0354,
        0.0198, 0.3642, 0.4927], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,162][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0059, 0.1601, 0.0929, 0.1095, 0.2085, 0.0975, 0.0202, 0.0885, 0.0103,
        0.1042, 0.0217, 0.0808], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,164][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.5108, 0.0656, 0.0253, 0.0663, 0.0565, 0.0993, 0.0192, 0.0523, 0.0148,
        0.0242, 0.0560, 0.0097], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,166][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0617, 0.0052, 0.0162, 0.0018, 0.0011, 0.0059, 0.0273, 0.0229, 0.1411,
        0.0559, 0.1317, 0.4937, 0.0356], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,168][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.2849, 0.0619, 0.0434, 0.0128, 0.0506, 0.0503, 0.0236, 0.0383, 0.0247,
        0.1148, 0.1129, 0.0487, 0.1332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,170][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0046, 0.1964, 0.0161, 0.0952, 0.0788, 0.4586, 0.0058, 0.0653, 0.0101,
        0.0157, 0.0035, 0.0347, 0.0151], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,172][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0792, 0.0263, 0.0301, 0.0040, 0.0073, 0.0232, 0.0138, 0.0514, 0.1178,
        0.0440, 0.2927, 0.2295, 0.0806], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,174][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0252, 0.0804, 0.0247, 0.0078, 0.0055, 0.0229, 0.0132, 0.0344, 0.1528,
        0.0464, 0.1180, 0.3124, 0.1562], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,175][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([7.7418e-01, 1.1178e-02, 6.8895e-03, 4.0905e-04, 3.2810e-03, 4.4232e-03,
        4.3996e-03, 9.0377e-03, 2.8329e-02, 1.0048e-02, 1.1718e-01, 2.4381e-02,
        6.2606e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,178][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.5832, 0.0526, 0.0491, 0.0310, 0.0131, 0.0198, 0.0209, 0.0264, 0.0342,
        0.0353, 0.0788, 0.0224, 0.0332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,180][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0636, 0.0361, 0.0214, 0.0126, 0.0306, 0.3902, 0.0379, 0.0690, 0.0974,
        0.0292, 0.1022, 0.1009, 0.0088], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,181][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0143, 0.0100, 0.0130, 0.0029, 0.0031, 0.0105, 0.0168, 0.0591, 0.0674,
        0.0114, 0.4604, 0.3104, 0.0208], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,184][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0706, 0.0137, 0.0116, 0.0028, 0.0042, 0.0100, 0.0154, 0.0147, 0.0599,
        0.0225, 0.2336, 0.4881, 0.0529], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,185][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.0146, 0.0890, 0.0540, 0.1572, 0.2378, 0.1523, 0.0095, 0.1989, 0.0059,
        0.0259, 0.0095, 0.0342, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,187][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.2611, 0.0720, 0.0238, 0.1244, 0.1194, 0.1559, 0.0087, 0.1325, 0.0072,
        0.0177, 0.0481, 0.0068, 0.0224], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,188][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.2127, 0.0029, 0.0129, 0.0023, 0.0014, 0.0056, 0.0174, 0.0204, 0.0860,
        0.0410, 0.1620, 0.2751, 0.0572, 0.1032], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,189][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.2971, 0.0758, 0.0479, 0.0074, 0.0260, 0.0548, 0.0186, 0.0346, 0.0516,
        0.0789, 0.0982, 0.0889, 0.0936, 0.0266], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,190][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0020, 0.1990, 0.0225, 0.0310, 0.0380, 0.5320, 0.0092, 0.0340, 0.0189,
        0.0124, 0.0024, 0.0744, 0.0153, 0.0090], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,190][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.2104, 0.0255, 0.0199, 0.0027, 0.0045, 0.0207, 0.0107, 0.0325, 0.0736,
        0.0238, 0.2543, 0.1563, 0.0579, 0.1072], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,192][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.2493, 0.0754, 0.0101, 0.0037, 0.0031, 0.0156, 0.0115, 0.0225, 0.1013,
        0.0193, 0.1661, 0.1726, 0.0904, 0.0591], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,193][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([8.7647e-01, 9.8611e-03, 3.4135e-03, 1.4275e-04, 9.6567e-04, 3.4072e-03,
        3.7991e-03, 4.6657e-03, 1.9120e-02, 4.5528e-03, 5.8039e-02, 1.0203e-02,
        2.7212e-03, 2.6383e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,195][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.6872, 0.0401, 0.0411, 0.0171, 0.0086, 0.0092, 0.0088, 0.0135, 0.0162,
        0.0256, 0.0654, 0.0115, 0.0193, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,197][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.1623, 0.0612, 0.0411, 0.0100, 0.0279, 0.3012, 0.0586, 0.0437, 0.1117,
        0.0182, 0.0910, 0.0636, 0.0055, 0.0041], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,199][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0598, 0.0046, 0.0082, 0.0015, 0.0030, 0.0048, 0.0136, 0.0202, 0.0435,
        0.0130, 0.5459, 0.2291, 0.0191, 0.0337], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,201][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.2084, 0.0170, 0.0123, 0.0032, 0.0051, 0.0095, 0.0102, 0.0147, 0.0360,
        0.0196, 0.2988, 0.2131, 0.0720, 0.0802], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,203][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0053, 0.1000, 0.0543, 0.1265, 0.2024, 0.1641, 0.0129, 0.1079, 0.0228,
        0.0265, 0.0139, 0.1153, 0.0225, 0.0256], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,205][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.3425, 0.1084, 0.0238, 0.0576, 0.1025, 0.0709, 0.0106, 0.1119, 0.0165,
        0.0191, 0.0697, 0.0150, 0.0385, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,207][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([2.6593e-02, 4.0783e-04, 6.4382e-04, 6.7516e-04, 1.1884e-04, 1.8329e-04,
        6.4152e-04, 1.4115e-03, 2.3847e-03, 3.7964e-03, 1.3483e-02, 1.5633e-02,
        1.8026e-02, 1.0355e-01, 8.1245e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,209][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.2237, 0.1103, 0.0924, 0.0295, 0.0646, 0.0506, 0.0292, 0.0396, 0.0185,
        0.0772, 0.0824, 0.0517, 0.0771, 0.0237, 0.0295], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,211][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.0013, 0.0407, 0.0214, 0.0731, 0.0365, 0.3340, 0.0268, 0.0191, 0.0920,
        0.0096, 0.0052, 0.2935, 0.0208, 0.0150, 0.0110], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,213][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.0989, 0.0060, 0.0098, 0.0010, 0.0012, 0.0034, 0.0036, 0.0110, 0.0173,
        0.0079, 0.0952, 0.0603, 0.0493, 0.0598, 0.5752], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,215][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.1102, 0.0343, 0.0072, 0.0028, 0.0012, 0.0049, 0.0065, 0.0090, 0.0397,
        0.0169, 0.0984, 0.1014, 0.1063, 0.1115, 0.3497], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,216][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([7.4055e-01, 2.2785e-02, 6.9405e-03, 6.4939e-04, 3.1012e-03, 2.1683e-03,
        6.6812e-03, 6.9717e-03, 2.4807e-02, 8.9754e-03, 5.6676e-02, 2.5401e-02,
        9.5102e-03, 1.0324e-02, 7.4457e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,218][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.3956, 0.0908, 0.0769, 0.0429, 0.0167, 0.0142, 0.0199, 0.0329, 0.0269,
        0.0233, 0.0817, 0.0191, 0.0250, 0.0394, 0.0947], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,220][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.1262, 0.0261, 0.0227, 0.0607, 0.0313, 0.1724, 0.0524, 0.0760, 0.0709,
        0.0340, 0.1066, 0.0687, 0.0219, 0.0172, 0.1129], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,222][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([1.7421e-02, 2.5255e-03, 1.2486e-03, 9.1270e-04, 6.8147e-04, 7.9689e-04,
        3.0899e-03, 4.3146e-03, 6.3610e-03, 9.8710e-04, 9.2093e-02, 2.5970e-02,
        7.4839e-03, 2.4324e-02, 8.1179e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,224][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.1188, 0.0034, 0.0036, 0.0022, 0.0009, 0.0018, 0.0051, 0.0060, 0.0067,
        0.0049, 0.0991, 0.0477, 0.0661, 0.0855, 0.5481], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,226][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.0005, 0.0477, 0.0526, 0.1275, 0.2897, 0.0904, 0.0501, 0.0906, 0.0312,
        0.0180, 0.0059, 0.1332, 0.0125, 0.0341, 0.0159], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,228][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.2459, 0.0703, 0.0332, 0.0990, 0.1229, 0.0879, 0.0125, 0.0923, 0.0116,
        0.0390, 0.0956, 0.0110, 0.0421, 0.0155, 0.0211], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,229][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([1.4268e-02, 1.3515e-04, 4.0857e-04, 1.1005e-04, 2.9733e-05, 1.4079e-04,
        3.8221e-04, 4.0163e-04, 2.6845e-03, 1.0694e-03, 5.6588e-03, 1.1141e-02,
        3.3084e-03, 2.4177e-02, 9.1023e-01, 2.5857e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,231][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.4982, 0.0764, 0.0364, 0.0032, 0.0107, 0.0294, 0.0069, 0.0188, 0.0319,
        0.0365, 0.1140, 0.0326, 0.0145, 0.0065, 0.0505, 0.0333],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,234][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.0011, 0.0336, 0.0102, 0.0855, 0.0272, 0.6400, 0.0081, 0.0230, 0.0240,
        0.0029, 0.0015, 0.0796, 0.0089, 0.0096, 0.0025, 0.0424],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,235][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([9.7603e-02, 3.9743e-03, 4.3497e-03, 5.0388e-04, 8.1826e-04, 3.1808e-03,
        1.6202e-03, 7.6483e-03, 1.0790e-02, 8.1146e-03, 7.7851e-02, 2.5443e-02,
        1.8365e-02, 4.0845e-02, 5.7077e-01, 1.2812e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,237][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.2039, 0.0173, 0.0045, 0.0020, 0.0027, 0.0119, 0.0066, 0.0075, 0.0377,
        0.0160, 0.1141, 0.0809, 0.0507, 0.0624, 0.2640, 0.1179],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,238][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([8.3316e-01, 6.8304e-03, 2.9065e-03, 1.9721e-04, 1.0135e-03, 1.4668e-03,
        1.1677e-03, 4.2162e-03, 5.7528e-03, 3.6519e-03, 4.6542e-02, 3.6081e-03,
        2.7449e-03, 3.5113e-03, 7.2752e-02, 1.0477e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,240][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.7288, 0.0208, 0.0257, 0.0106, 0.0046, 0.0077, 0.0045, 0.0101, 0.0085,
        0.0140, 0.0449, 0.0042, 0.0082, 0.0210, 0.0557, 0.0306],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,242][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.2597, 0.0255, 0.0312, 0.0106, 0.0241, 0.2126, 0.0232, 0.0304, 0.1134,
        0.0243, 0.0859, 0.0668, 0.0060, 0.0047, 0.0680, 0.0136],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,244][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([2.1274e-03, 1.0285e-04, 1.9685e-04, 5.9871e-05, 1.5155e-04, 1.2569e-04,
        3.5021e-04, 8.4172e-04, 6.9754e-04, 3.6052e-04, 1.8531e-02, 4.9818e-03,
        9.6007e-04, 3.5001e-03, 9.5501e-01, 1.2007e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,245][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([9.5917e-02, 8.9033e-04, 1.5083e-03, 6.2972e-04, 6.8514e-04, 1.2677e-03,
        9.5670e-04, 1.5559e-03, 4.3709e-03, 3.2163e-03, 7.0196e-02, 2.5799e-02,
        1.0498e-02, 2.2763e-02, 7.0707e-01, 5.2672e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,247][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.0086, 0.0191, 0.0353, 0.1581, 0.3644, 0.0853, 0.0058, 0.1109, 0.0064,
        0.0143, 0.0085, 0.0298, 0.0184, 0.0466, 0.0473, 0.0412],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,250][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.3646, 0.0594, 0.0312, 0.1112, 0.1130, 0.0625, 0.0075, 0.1078, 0.0115,
        0.0154, 0.0587, 0.0077, 0.0196, 0.0120, 0.0103, 0.0077],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,251][circuit_model.py][line:1570][INFO] ##7-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.7104e-01, 8.0549e-05, 4.8596e-04, 1.0649e-04, 5.9463e-05, 3.0480e-04,
        3.7423e-04, 6.9885e-04, 2.2852e-03, 1.6638e-03, 1.3995e-02, 6.0579e-03,
        3.7496e-03, 9.3615e-03, 6.8360e-01, 3.8338e-02, 6.7795e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,253][circuit_model.py][line:1573][INFO] ##7-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.4224, 0.0530, 0.0285, 0.0038, 0.0128, 0.0280, 0.0096, 0.0208, 0.0233,
        0.0446, 0.0680, 0.0321, 0.0522, 0.0142, 0.1012, 0.0703, 0.0151],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,254][circuit_model.py][line:1576][INFO] ##7-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0066, 0.1705, 0.0241, 0.0386, 0.0363, 0.5211, 0.0075, 0.0324, 0.0188,
        0.0110, 0.0029, 0.0558, 0.0142, 0.0097, 0.0188, 0.0269, 0.0046],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,255][circuit_model.py][line:1579][INFO] ##7-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([1.8586e-01, 2.9578e-03, 2.7001e-03, 4.2234e-04, 6.1848e-04, 3.4268e-03,
        8.7337e-04, 3.8698e-03, 6.8683e-03, 3.3841e-03, 4.6879e-02, 1.3458e-02,
        1.0237e-02, 2.0787e-02, 5.0805e-01, 1.0680e-01, 8.2816e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,256][circuit_model.py][line:1582][INFO] ##7-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.4632, 0.0148, 0.0027, 0.0011, 0.0010, 0.0046, 0.0019, 0.0051, 0.0174,
        0.0054, 0.0600, 0.0267, 0.0295, 0.0243, 0.1431, 0.1331, 0.0662],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,257][circuit_model.py][line:1585][INFO] ##7-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.3095e-01, 2.4083e-03, 7.1199e-04, 2.6674e-05, 1.8265e-04, 7.0346e-04,
        5.7132e-04, 1.0248e-03, 3.0736e-03, 9.4308e-04, 1.7044e-02, 1.4003e-03,
        5.5626e-04, 6.1799e-04, 3.4287e-02, 4.3952e-03, 1.1042e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,258][circuit_model.py][line:1588][INFO] ##7-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.7152, 0.0203, 0.0177, 0.0077, 0.0037, 0.0050, 0.0032, 0.0064, 0.0063,
        0.0114, 0.0371, 0.0038, 0.0075, 0.0157, 0.0784, 0.0308, 0.0300],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,260][circuit_model.py][line:1591][INFO] ##7-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.3459, 0.0414, 0.0338, 0.0055, 0.0166, 0.2052, 0.0315, 0.0230, 0.0736,
        0.0101, 0.0850, 0.0307, 0.0028, 0.0023, 0.0693, 0.0190, 0.0044],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,261][circuit_model.py][line:1594][INFO] ##7-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([3.1380e-02, 2.3025e-04, 5.4850e-04, 8.9576e-05, 2.0108e-04, 3.2541e-04,
        6.9629e-04, 1.2292e-03, 2.0607e-03, 8.9079e-04, 7.1719e-02, 1.1521e-02,
        1.7527e-03, 3.6328e-03, 8.2481e-01, 3.0039e-02, 1.8874e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,262][circuit_model.py][line:1597][INFO] ##7-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.6717e-01, 1.2571e-03, 1.3311e-03, 4.3355e-04, 5.7864e-04, 1.0013e-03,
        6.5530e-04, 1.1552e-03, 2.6828e-03, 2.1655e-03, 4.8182e-02, 1.5618e-02,
        1.3802e-02, 1.7027e-02, 5.9515e-01, 5.7367e-02, 7.4415e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,264][circuit_model.py][line:1600][INFO] ##7-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.0096, 0.0831, 0.0452, 0.1483, 0.1890, 0.1193, 0.0131, 0.0884, 0.0139,
        0.0252, 0.0120, 0.0580, 0.0251, 0.0349, 0.0896, 0.0254, 0.0200],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,266][circuit_model.py][line:1603][INFO] ##7-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.5163, 0.0636, 0.0169, 0.0451, 0.0675, 0.0666, 0.0062, 0.0781, 0.0100,
        0.0094, 0.0564, 0.0071, 0.0192, 0.0079, 0.0093, 0.0107, 0.0097],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,270][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:30,272][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  405],
        [ 1663],
        [ 5715],
        [  825],
        [ 3504],
        [12984],
        [ 3029],
        [ 6373],
        [12503],
        [ 1562],
        [ 3849],
        [ 5175],
        [ 1957],
        [   56],
        [  945],
        [  557],
        [   55]], device='cuda:0')
[2024-07-23 21:05:30,274][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  387],
        [ 7434],
        [ 4097],
        [ 2356],
        [ 5205],
        [15134],
        [ 4587],
        [ 6825],
        [ 9506],
        [ 2021],
        [ 3266],
        [ 3461],
        [ 3006],
        [  606],
        [ 3890],
        [ 1256],
        [  948]], device='cuda:0')
[2024-07-23 21:05:30,276][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[14810],
        [ 4787],
        [ 7917],
        [ 9466],
        [ 8205],
        [ 6879],
        [ 7180],
        [ 8746],
        [ 9814],
        [ 9790],
        [ 9926],
        [ 9992],
        [ 9755],
        [ 9674],
        [ 9141],
        [ 9236],
        [ 9304]], device='cuda:0')
[2024-07-23 21:05:30,278][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[22908],
        [ 1050],
        [  715],
        [  579],
        [ 1614],
        [ 3054],
        [ 5475],
        [ 8539],
        [ 8221],
        [ 8141],
        [ 7064],
        [ 7815],
        [ 7890],
        [ 9000],
        [ 8342],
        [10759],
        [ 9391]], device='cuda:0')
[2024-07-23 21:05:30,280][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[19152],
        [43267],
        [24365],
        [10544],
        [ 9132],
        [10682],
        [ 8515],
        [ 9810],
        [10560],
        [11101],
        [ 9355],
        [12530],
        [12403],
        [13666],
        [13689],
        [13972],
        [15014]], device='cuda:0')
[2024-07-23 21:05:30,282][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[30445],
        [12382],
        [ 7712],
        [ 5947],
        [ 5233],
        [ 5271],
        [ 5586],
        [ 5634],
        [ 5913],
        [ 5837],
        [ 5893],
        [ 6129],
        [ 6399],
        [ 6261],
        [ 5711],
        [ 5694],
        [ 5478]], device='cuda:0')
[2024-07-23 21:05:30,284][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[21985],
        [18099],
        [19258],
        [18221],
        [18193],
        [18225],
        [20742],
        [21156],
        [36064],
        [37668],
        [32759],
        [34419],
        [31743],
        [28815],
        [17774],
        [18660],
        [17166]], device='cuda:0')
[2024-07-23 21:05:30,286][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[13247],
        [34683],
        [28779],
        [21743],
        [24071],
        [24623],
        [23316],
        [26027],
        [22966],
        [16235],
        [19242],
        [20694],
        [17223],
        [21371],
        [21606],
        [29947],
        [24780]], device='cuda:0')
[2024-07-23 21:05:30,288][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 7951],
        [11691],
        [11107],
        [ 9714],
        [ 8819],
        [ 8396],
        [ 8217],
        [ 8228],
        [ 7918],
        [ 7471],
        [ 6853],
        [ 6617],
        [ 6456],
        [ 6336],
        [ 6330],
        [ 6404],
        [ 6442]], device='cuda:0')
[2024-07-23 21:05:30,290][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[12488],
        [  176],
        [  196],
        [  410],
        [  484],
        [  515],
        [ 1052],
        [ 1594],
        [ 1740],
        [ 1925],
        [ 1468],
        [ 2527],
        [ 2139],
        [ 2536],
        [ 2377],
        [ 1915],
        [ 2442]], device='cuda:0')
[2024-07-23 21:05:30,292][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[1855],
        [2617],
        [2212],
        [2108],
        [2359],
        [2021],
        [1877],
        [1839],
        [1807],
        [2091],
        [1782],
        [1751],
        [3860],
        [2296],
        [5513],
        [5431],
        [2310]], device='cuda:0')
[2024-07-23 21:05:30,294][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[ 5607],
        [13219],
        [27525],
        [28912],
        [35658],
        [38330],
        [35283],
        [37327],
        [37212],
        [31078],
        [31181],
        [28885],
        [27094],
        [27193],
        [24132],
        [23161],
        [17081]], device='cuda:0')
[2024-07-23 21:05:30,296][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 9701],
        [32845],
        [23976],
        [23769],
        [27969],
        [28187],
        [17710],
        [22672],
        [20136],
        [22963],
        [11538],
        [15968],
        [18551],
        [14922],
        [11168],
        [10736],
        [14270]], device='cuda:0')
[2024-07-23 21:05:30,298][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[31580],
        [39847],
        [24828],
        [19271],
        [14332],
        [12218],
        [15855],
        [15806],
        [15860],
        [18603],
        [13875],
        [18845],
        [17571],
        [19354],
        [17106],
        [17395],
        [20582]], device='cuda:0')
[2024-07-23 21:05:30,300][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 4133],
        [ 1765],
        [36245],
        [ 5018],
        [30868],
        [29860],
        [24152],
        [15498],
        [31078],
        [ 9181],
        [21060],
        [28370],
        [ 5622],
        [   70],
        [ 5104],
        [10674],
        [   73]], device='cuda:0')
[2024-07-23 21:05:30,302][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[ 7271],
        [ 6217],
        [ 9458],
        [13032],
        [11741],
        [ 9788],
        [  440],
        [  672],
        [ 4188],
        [ 3189],
        [ 3328],
        [ 3002],
        [ 3614],
        [ 3866],
        [ 1167],
        [ 1086],
        [ 1397]], device='cuda:0')
[2024-07-23 21:05:30,304][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[10438],
        [ 2482],
        [10117],
        [ 6878],
        [11460],
        [12222],
        [13041],
        [13962],
        [14042],
        [12978],
        [11508],
        [12189],
        [11440],
        [13733],
        [12853],
        [12919],
        [12857]], device='cuda:0')
[2024-07-23 21:05:30,306][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[11608],
        [19619],
        [32026],
        [22741],
        [25472],
        [10420],
        [20106],
        [21865],
        [13495],
        [13445],
        [20431],
        [ 7355],
        [15689],
        [12849],
        [ 8432],
        [10354],
        [13577]], device='cuda:0')
[2024-07-23 21:05:30,308][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[ 1472],
        [ 1858],
        [  294],
        [  608],
        [  412],
        [  580],
        [  779],
        [10229],
        [12077],
        [14454],
        [10928],
        [14695],
        [14751],
        [18556],
        [ 4274],
        [ 5771],
        [ 7480]], device='cuda:0')
[2024-07-23 21:05:30,310][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[2709],
        [ 411],
        [ 212],
        [ 234],
        [ 245],
        [ 352],
        [ 297],
        [ 254],
        [ 170],
        [ 140],
        [ 242],
        [ 294],
        [  95],
        [ 122],
        [ 465],
        [ 322],
        [ 116]], device='cuda:0')
[2024-07-23 21:05:30,311][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[26455],
        [12677],
        [ 1491],
        [13728],
        [  143],
        [ 2531],
        [19273],
        [ 1470],
        [24290],
        [ 2625],
        [  913],
        [26656],
        [22372],
        [27580],
        [ 7100],
        [17581],
        [26417]], device='cuda:0')
[2024-07-23 21:05:30,314][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[20131],
        [20655],
        [11650],
        [13301],
        [ 6628],
        [ 4135],
        [ 4338],
        [ 3921],
        [ 5966],
        [ 7939],
        [ 6503],
        [ 7466],
        [ 7222],
        [10235],
        [ 8581],
        [ 8656],
        [ 9564]], device='cuda:0')
[2024-07-23 21:05:30,316][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[2003],
        [4043],
        [6129],
        [6184],
        [5397],
        [4406],
        [4415],
        [3963],
        [3570],
        [4062],
        [3417],
        [4161],
        [4572],
        [4570],
        [4372],
        [4812],
        [5585]], device='cuda:0')
[2024-07-23 21:05:30,317][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[21012],
        [  812],
        [ 4948],
        [ 6840],
        [ 5907],
        [ 9567],
        [17355],
        [15555],
        [10370],
        [10616],
        [20071],
        [13794],
        [17140],
        [20997],
        [10101],
        [ 8793],
        [10485]], device='cuda:0')
[2024-07-23 21:05:30,319][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[8454],
        [9099],
        [7548],
        [7208],
        [6178],
        [7266],
        [6699],
        [3897],
        [2934],
        [5078],
        [2362],
        [3240],
        [4288],
        [3471],
        [1596],
        [1350],
        [1245]], device='cuda:0')
[2024-07-23 21:05:30,321][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[27558],
        [  436],
        [13372],
        [33256],
        [24677],
        [ 1362],
        [ 6398],
        [ 2734],
        [ 5476],
        [ 5171],
        [ 8447],
        [ 4292],
        [ 5957],
        [ 4283],
        [ 4670],
        [ 6878],
        [ 7074]], device='cuda:0')
[2024-07-23 21:05:30,323][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[13889],
        [ 8419],
        [ 7341],
        [11949],
        [ 7392],
        [ 7627],
        [ 8559],
        [ 7737],
        [ 9275],
        [10317],
        [ 9524],
        [16493],
        [10204],
        [12561],
        [11100],
        [11120],
        [16167]], device='cuda:0')
[2024-07-23 21:05:30,325][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[ 9832],
        [39276],
        [35901],
        [26413],
        [37541],
        [39728],
        [37857],
        [40817],
        [32398],
        [37237],
        [37301],
        [28914],
        [29347],
        [26452],
        [38675],
        [35897],
        [32534]], device='cuda:0')
[2024-07-23 21:05:30,327][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[31925],
        [28062],
        [ 5672],
        [18015],
        [ 6851],
        [ 8394],
        [ 7463],
        [10871],
        [ 4196],
        [ 7762],
        [ 5495],
        [ 2825],
        [12232],
        [15757],
        [12873],
        [14127],
        [23946]], device='cuda:0')
[2024-07-23 21:05:30,329][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106],
        [11106]], device='cuda:0')
[2024-07-23 21:05:30,412][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:30,414][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,416][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,417][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,418][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,419][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,419][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,419][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,420][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,420][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,420][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,420][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,421][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,423][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.9806, 0.0194], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,424][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.4537, 0.5463], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,425][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.9373, 0.0627], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,428][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9279, 0.0721], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,429][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.0068, 0.9932], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,431][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9958, 0.0042], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,433][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.6823, 0.3177], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,434][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.0344, 0.9656], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,436][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.5692, 0.4308], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,437][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.4432, 0.5568], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,439][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.6512, 0.3488], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,441][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.2021, 0.7979], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,442][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.9322, 0.0043, 0.0634], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,445][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0206, 0.3634, 0.6160], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,446][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.8067, 0.1170, 0.0763], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,448][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.2148, 0.3442, 0.4410], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,449][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0041, 0.5586, 0.4374], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,451][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.9723, 0.0060, 0.0217], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,453][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.4830, 0.1195, 0.3975], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,455][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0119, 0.4958, 0.4924], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,457][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.2061, 0.5614, 0.2325], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,458][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.3155, 0.4500, 0.2345], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,460][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.1852, 0.0950, 0.7198], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,462][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0119, 0.7562, 0.2318], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,463][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.9354, 0.0062, 0.0451, 0.0133], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,464][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0888, 0.2679, 0.5328, 0.1106], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,467][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.7375, 0.1273, 0.0933, 0.0418], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,468][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.4026, 0.1331, 0.1912, 0.2732], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,470][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0016, 0.1716, 0.6564, 0.1704], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,470][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9748, 0.0047, 0.0180, 0.0026], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,471][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.8854, 0.0197, 0.0876, 0.0072], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,471][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0138, 0.3400, 0.3535, 0.2928], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,471][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.2343, 0.3149, 0.1693, 0.2815], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,472][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.3248, 0.3212, 0.1212, 0.2327], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,472][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.4226, 0.0735, 0.3797, 0.1243], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,473][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0062, 0.5008, 0.2959, 0.1971], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,473][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.9371, 0.0055, 0.0412, 0.0107, 0.0055], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,473][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.0922, 0.2152, 0.4682, 0.1217, 0.1028], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,474][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.6070, 0.1603, 0.1229, 0.0593, 0.0505], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,474][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.6367, 0.0594, 0.1062, 0.1198, 0.0779], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,475][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0011, 0.2664, 0.3637, 0.1494, 0.2193], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,476][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.9505, 0.0076, 0.0257, 0.0095, 0.0068], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,478][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.4707, 0.1171, 0.2425, 0.0663, 0.1034], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,480][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.0138, 0.2624, 0.2680, 0.2502, 0.2057], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,481][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.1985, 0.1895, 0.1394, 0.2646, 0.2081], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,483][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.1705, 0.2425, 0.1774, 0.2303, 0.1793], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,484][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.2516, 0.0610, 0.4187, 0.1253, 0.1434], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,486][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.0047, 0.3224, 0.3810, 0.2452, 0.0467], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,488][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.8535, 0.0091, 0.0872, 0.0189, 0.0101, 0.0212], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,490][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0272, 0.1722, 0.3285, 0.2748, 0.0549, 0.1424], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,491][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.8124, 0.0645, 0.0460, 0.0242, 0.0170, 0.0358], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,492][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.3667, 0.0793, 0.1608, 0.1731, 0.1418, 0.0784], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,494][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([1.7716e-04, 3.8530e-02, 2.8027e-02, 3.7747e-01, 5.4524e-01, 1.0552e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,496][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.8861, 0.0068, 0.0292, 0.0187, 0.0187, 0.0405], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,497][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0398, 0.1491, 0.3090, 0.1970, 0.2624, 0.0427], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,499][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0106, 0.2227, 0.2099, 0.2009, 0.1752, 0.1808], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,501][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.2049, 0.1137, 0.0890, 0.1613, 0.1232, 0.3080], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,502][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.1328, 0.1792, 0.0723, 0.1850, 0.1426, 0.2882], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,504][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.2523, 0.0484, 0.3396, 0.1549, 0.1419, 0.0628], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,506][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.0090, 0.2888, 0.2620, 0.1770, 0.1327, 0.1306], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,507][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.7310, 0.0174, 0.1545, 0.0385, 0.0220, 0.0313, 0.0053],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,509][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0836, 0.0876, 0.2222, 0.2175, 0.1413, 0.1223, 0.1255],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,511][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.6339, 0.1111, 0.0651, 0.0383, 0.0284, 0.0655, 0.0577],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,513][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.5508, 0.0589, 0.0991, 0.1129, 0.0784, 0.0484, 0.0514],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,514][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.0022, 0.0331, 0.0445, 0.4163, 0.3459, 0.0073, 0.1506],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,516][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.9252, 0.0042, 0.0183, 0.0149, 0.0143, 0.0167, 0.0063],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,518][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.2189, 0.2473, 0.1249, 0.1579, 0.2213, 0.0235, 0.0062],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,519][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0071, 0.1825, 0.1927, 0.1669, 0.1525, 0.1669, 0.1313],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,521][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.0787, 0.1394, 0.0831, 0.1560, 0.1239, 0.3269, 0.0920],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,523][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.1763, 0.1651, 0.0965, 0.1683, 0.1144, 0.1255, 0.1538],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,525][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.5642, 0.0417, 0.2008, 0.0665, 0.0540, 0.0347, 0.0381],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,526][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.0341, 0.2040, 0.2017, 0.2028, 0.1429, 0.0922, 0.1224],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,528][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.8956, 0.0049, 0.0481, 0.0125, 0.0062, 0.0126, 0.0014, 0.0188],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,530][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0202, 0.0660, 0.1773, 0.1420, 0.0839, 0.1288, 0.1834, 0.1982],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,531][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.4154, 0.1379, 0.1009, 0.0648, 0.0450, 0.0844, 0.0768, 0.0748],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,533][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.7768, 0.0277, 0.0456, 0.0466, 0.0308, 0.0188, 0.0295, 0.0243],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,535][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0010, 0.0393, 0.1446, 0.1712, 0.4574, 0.0069, 0.1191, 0.0604],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,536][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.9521, 0.0020, 0.0107, 0.0037, 0.0055, 0.0127, 0.0037, 0.0096],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,537][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.1070, 0.1016, 0.1967, 0.1773, 0.2452, 0.0384, 0.0069, 0.1270],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,538][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0064, 0.1525, 0.1557, 0.1472, 0.1288, 0.1432, 0.1104, 0.1559],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,538][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.0941, 0.0996, 0.0736, 0.1384, 0.1055, 0.2931, 0.0850, 0.1106],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,539][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.1122, 0.1244, 0.0791, 0.1483, 0.1200, 0.1426, 0.1141, 0.1594],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,539][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.5406, 0.0293, 0.1793, 0.0499, 0.0914, 0.0299, 0.0216, 0.0580],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,539][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.0137, 0.1041, 0.1185, 0.1008, 0.0480, 0.0483, 0.1609, 0.4058],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,540][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.7929, 0.0110, 0.0861, 0.0271, 0.0180, 0.0200, 0.0031, 0.0261, 0.0157],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,540][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0174, 0.0449, 0.1725, 0.1643, 0.0535, 0.0633, 0.0618, 0.1157, 0.3065],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,541][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.5954, 0.0896, 0.0480, 0.0366, 0.0245, 0.0480, 0.0494, 0.0342, 0.0743],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,541][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5601, 0.0454, 0.0594, 0.0997, 0.0650, 0.0380, 0.0500, 0.0457, 0.0368],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,542][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0006, 0.0223, 0.0123, 0.5807, 0.1751, 0.0094, 0.1549, 0.0187, 0.0260],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,543][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.6782, 0.0087, 0.0361, 0.0530, 0.0374, 0.0427, 0.0259, 0.0484, 0.0695],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,545][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.2383, 0.1804, 0.1436, 0.1975, 0.1147, 0.0381, 0.0063, 0.0649, 0.0162],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,547][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0031, 0.1409, 0.1476, 0.1257, 0.1168, 0.1241, 0.1031, 0.1406, 0.0981],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,548][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0823, 0.1074, 0.0605, 0.1212, 0.0882, 0.2360, 0.0622, 0.1078, 0.1344],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,550][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.1620, 0.1511, 0.0780, 0.1215, 0.0821, 0.1096, 0.0976, 0.1209, 0.0772],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,551][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.3058, 0.0488, 0.2581, 0.0808, 0.0541, 0.0332, 0.0340, 0.0832, 0.1018],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,553][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0507, 0.1074, 0.0912, 0.1060, 0.0322, 0.0458, 0.0534, 0.3220, 0.1913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,555][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.7891, 0.0115, 0.0909, 0.0236, 0.0156, 0.0156, 0.0029, 0.0256, 0.0140,
        0.0111], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,557][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0167, 0.0234, 0.0524, 0.0465, 0.0296, 0.0897, 0.0932, 0.2171, 0.3029,
        0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,558][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.7542, 0.0355, 0.0284, 0.0179, 0.0154, 0.0264, 0.0357, 0.0204, 0.0400,
        0.0261], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,560][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.4857, 0.0455, 0.0701, 0.1010, 0.0670, 0.0355, 0.0484, 0.0447, 0.0326,
        0.0694], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,562][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0012, 0.0390, 0.0309, 0.3043, 0.1466, 0.0104, 0.2036, 0.0422, 0.0379,
        0.1839], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,564][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.8304, 0.0043, 0.0223, 0.0099, 0.0114, 0.0252, 0.0084, 0.0168, 0.0358,
        0.0354], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,565][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.3138, 0.0993, 0.1345, 0.1088, 0.1136, 0.0257, 0.0065, 0.1296, 0.0075,
        0.0607], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,567][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0056, 0.1241, 0.1312, 0.1162, 0.1003, 0.1203, 0.0884, 0.1254, 0.0892,
        0.0993], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,569][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.1136, 0.0934, 0.0444, 0.0808, 0.0813, 0.2581, 0.0500, 0.0738, 0.1383,
        0.0665], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,570][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.1273, 0.1120, 0.0777, 0.1019, 0.0747, 0.0922, 0.0917, 0.1130, 0.0781,
        0.1313], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,572][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.3682, 0.0616, 0.2037, 0.0553, 0.0454, 0.0179, 0.0233, 0.0450, 0.0411,
        0.1383], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,574][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.0154, 0.1973, 0.1131, 0.0975, 0.0584, 0.0496, 0.0748, 0.1613, 0.1265,
        0.1062], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,575][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.8146, 0.0037, 0.0529, 0.0114, 0.0071, 0.0123, 0.0013, 0.0167, 0.0078,
        0.0051, 0.0672], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,577][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0016, 0.0194, 0.0769, 0.1236, 0.0434, 0.0679, 0.1650, 0.0855, 0.2589,
        0.1202, 0.0374], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,579][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.8431, 0.0219, 0.0164, 0.0106, 0.0079, 0.0128, 0.0237, 0.0142, 0.0280,
        0.0133, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,581][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.7895, 0.0142, 0.0391, 0.0304, 0.0170, 0.0159, 0.0187, 0.0170, 0.0115,
        0.0322, 0.0145], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,583][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0024, 0.0223, 0.0307, 0.2341, 0.5038, 0.0051, 0.0498, 0.0285, 0.0161,
        0.0882, 0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,584][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.8975, 0.0027, 0.0130, 0.0086, 0.0059, 0.0076, 0.0022, 0.0078, 0.0074,
        0.0123, 0.0352], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,586][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0425, 0.0984, 0.2277, 0.0978, 0.2037, 0.0729, 0.0164, 0.0631, 0.0236,
        0.0550, 0.0988], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,587][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0037, 0.1117, 0.1165, 0.1074, 0.0897, 0.1056, 0.0857, 0.1158, 0.0805,
        0.0989, 0.0844], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,590][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.0300, 0.0699, 0.0492, 0.0819, 0.0916, 0.2113, 0.0576, 0.0948, 0.1496,
        0.0960, 0.0681], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,591][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.0714, 0.1014, 0.0754, 0.1107, 0.0713, 0.0838, 0.0675, 0.0922, 0.0653,
        0.1270, 0.1339], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,593][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.2380, 0.0190, 0.0846, 0.0262, 0.0349, 0.0303, 0.0221, 0.0476, 0.0427,
        0.0809, 0.3737], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,595][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.0066, 0.0964, 0.0555, 0.0749, 0.0394, 0.0727, 0.0788, 0.2414, 0.2252,
        0.0661, 0.0428], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,596][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.7769, 0.0069, 0.0626, 0.0188, 0.0129, 0.0144, 0.0021, 0.0156, 0.0121,
        0.0089, 0.0504, 0.0185], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,598][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0105, 0.0188, 0.0693, 0.0731, 0.0257, 0.0639, 0.0459, 0.0587, 0.1916,
        0.0342, 0.0365, 0.3719], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,600][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.5562, 0.0698, 0.0349, 0.0300, 0.0203, 0.0388, 0.0432, 0.0282, 0.0589,
        0.0376, 0.0143, 0.0679], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,602][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.5797, 0.0294, 0.0433, 0.0716, 0.0535, 0.0240, 0.0356, 0.0333, 0.0275,
        0.0506, 0.0242, 0.0272], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,603][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0010, 0.0184, 0.0106, 0.4042, 0.1692, 0.0067, 0.1525, 0.0102, 0.0152,
        0.1345, 0.0165, 0.0610], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,604][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.5268, 0.0055, 0.0276, 0.0307, 0.0210, 0.0219, 0.0107, 0.0273, 0.0294,
        0.0512, 0.1987, 0.0491], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,605][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.4117, 0.0841, 0.0890, 0.1008, 0.1116, 0.0333, 0.0042, 0.0511, 0.0098,
        0.0370, 0.0503, 0.0169], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,605][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0018, 0.1050, 0.1124, 0.0929, 0.0875, 0.0924, 0.0787, 0.1056, 0.0736,
        0.0928, 0.0794, 0.0780], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,606][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0542, 0.0808, 0.0476, 0.0938, 0.0697, 0.1917, 0.0538, 0.0883, 0.1125,
        0.0660, 0.0320, 0.1096], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,606][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.1154, 0.1076, 0.0585, 0.0853, 0.0564, 0.0750, 0.0734, 0.0841, 0.0576,
        0.1035, 0.1207, 0.0625], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,606][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.5098, 0.0125, 0.0728, 0.0161, 0.0163, 0.0082, 0.0064, 0.0159, 0.0185,
        0.0336, 0.2329, 0.0569], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,607][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0669, 0.0769, 0.0492, 0.0625, 0.0160, 0.0305, 0.0373, 0.2070, 0.0926,
        0.0701, 0.0793, 0.2117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,607][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.7549, 0.0058, 0.0597, 0.0134, 0.0088, 0.0178, 0.0017, 0.0207, 0.0092,
        0.0074, 0.0576, 0.0135, 0.0294], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,608][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0930, 0.0392, 0.0750, 0.0631, 0.0899, 0.1349, 0.0218, 0.1456, 0.0412,
        0.0759, 0.0497, 0.1088, 0.0617], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,609][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.7132, 0.0283, 0.0264, 0.0119, 0.0112, 0.0213, 0.0280, 0.0205, 0.0403,
        0.0247, 0.0167, 0.0453, 0.0124], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,611][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.4587, 0.0339, 0.0506, 0.0792, 0.0628, 0.0315, 0.0380, 0.0450, 0.0270,
        0.0675, 0.0335, 0.0269, 0.0454], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,612][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0003, 0.0807, 0.0214, 0.0427, 0.0415, 0.0025, 0.1704, 0.0473, 0.0562,
        0.1133, 0.1272, 0.2724, 0.0240], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,614][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.6344, 0.0035, 0.0175, 0.0035, 0.0059, 0.0198, 0.0069, 0.0096, 0.0421,
        0.0144, 0.1622, 0.0598, 0.0203], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,614][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.4265, 0.0822, 0.0476, 0.0500, 0.1148, 0.0441, 0.0021, 0.1014, 0.0025,
        0.0157, 0.0973, 0.0026, 0.0132], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,616][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0027, 0.0936, 0.0968, 0.0848, 0.0775, 0.0852, 0.0709, 0.0938, 0.0693,
        0.0846, 0.0760, 0.0761, 0.0888], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,618][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0721, 0.0691, 0.0361, 0.0672, 0.0695, 0.1951, 0.0444, 0.0649, 0.1105,
        0.0753, 0.0504, 0.1089, 0.0365], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,620][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0996, 0.0945, 0.0536, 0.0726, 0.0564, 0.0651, 0.0715, 0.0762, 0.0554,
        0.1194, 0.0924, 0.0603, 0.0829], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,622][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.3527, 0.0175, 0.1061, 0.0239, 0.0330, 0.0125, 0.0071, 0.0256, 0.0123,
        0.0371, 0.2179, 0.0277, 0.1267], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,623][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0171, 0.0411, 0.0438, 0.0366, 0.0277, 0.0250, 0.0511, 0.1829, 0.0974,
        0.0484, 0.0599, 0.2821, 0.0868], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,624][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([8.0264e-01, 5.8246e-03, 4.7333e-02, 1.1257e-02, 7.4371e-03, 1.1603e-02,
        1.4748e-03, 1.7519e-02, 6.8190e-03, 6.0881e-03, 5.2703e-02, 9.3095e-03,
        1.9955e-02, 3.7038e-05], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,626][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1089, 0.0323, 0.0884, 0.0414, 0.0385, 0.1597, 0.0426, 0.0733, 0.0881,
        0.0558, 0.0547, 0.1388, 0.0643, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,628][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.6569, 0.0298, 0.0279, 0.0131, 0.0117, 0.0233, 0.0350, 0.0274, 0.0439,
        0.0208, 0.0225, 0.0507, 0.0139, 0.0230], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,630][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2465, 0.0414, 0.0503, 0.1062, 0.0802, 0.0391, 0.0473, 0.0556, 0.0364,
        0.0757, 0.0356, 0.0367, 0.0700, 0.0790], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,631][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0008, 0.0375, 0.0253, 0.0449, 0.0791, 0.0060, 0.1803, 0.0170, 0.0781,
        0.0242, 0.1350, 0.2908, 0.0582, 0.0229], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,633][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.7084, 0.0030, 0.0139, 0.0057, 0.0062, 0.0116, 0.0056, 0.0069, 0.0279,
        0.0168, 0.0918, 0.0403, 0.0318, 0.0302], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,635][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.6095, 0.0431, 0.0377, 0.0357, 0.1082, 0.0510, 0.0007, 0.0381, 0.0020,
        0.0057, 0.0560, 0.0015, 0.0050, 0.0058], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,637][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0029, 0.0841, 0.0899, 0.0782, 0.0722, 0.0766, 0.0638, 0.0842, 0.0637,
        0.0740, 0.0674, 0.0701, 0.0874, 0.0856], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,639][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0434, 0.0682, 0.0335, 0.0651, 0.0648, 0.1661, 0.0420, 0.0738, 0.1054,
        0.0672, 0.0498, 0.1106, 0.0435, 0.0666], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,640][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1211, 0.0870, 0.0396, 0.0607, 0.0508, 0.0593, 0.0558, 0.0781, 0.0435,
        0.0921, 0.0800, 0.0453, 0.0643, 0.1224], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,642][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.5250, 0.0122, 0.0513, 0.0120, 0.0192, 0.0098, 0.0048, 0.0128, 0.0085,
        0.0161, 0.1748, 0.0198, 0.0631, 0.0705], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,643][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0172, 0.0372, 0.0395, 0.0292, 0.0172, 0.0286, 0.0447, 0.1428, 0.0710,
        0.0487, 0.0676, 0.1686, 0.0902, 0.1976], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,645][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ same] are: tensor([7.6421e-01, 6.3400e-03, 5.5754e-02, 1.4655e-02, 9.1468e-03, 1.0625e-02,
        1.9408e-03, 1.7124e-02, 9.4524e-03, 7.9373e-03, 5.0968e-02, 1.6816e-02,
        3.2145e-02, 3.2789e-05, 2.8588e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,647][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0127, 0.0479, 0.0779, 0.1970, 0.0497, 0.0375, 0.0307, 0.0370, 0.0655,
        0.0288, 0.0518, 0.1750, 0.0795, 0.0280, 0.0811], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,648][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.6040, 0.0315, 0.0251, 0.0174, 0.0143, 0.0214, 0.0308, 0.0232, 0.0400,
        0.0277, 0.0141, 0.0498, 0.0223, 0.0366, 0.0418], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,650][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.3758, 0.0341, 0.0703, 0.0798, 0.0572, 0.0330, 0.0349, 0.0401, 0.0248,
        0.0530, 0.0274, 0.0241, 0.0513, 0.0595, 0.0349], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,651][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ same] are: tensor([1.8958e-04, 5.7282e-03, 1.8519e-02, 4.6117e-02, 7.8693e-02, 2.2503e-03,
        3.3385e-02, 8.8785e-03, 1.2988e-02, 2.0370e-01, 1.1470e-02, 4.2815e-02,
        2.7609e-01, 7.9795e-02, 1.7938e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,653][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.5914, 0.0030, 0.0091, 0.0118, 0.0033, 0.0043, 0.0049, 0.0050, 0.0147,
        0.0107, 0.0475, 0.0247, 0.0351, 0.0623, 0.1723], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,654][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0980, 0.1123, 0.1509, 0.2298, 0.1149, 0.0315, 0.0030, 0.0715, 0.0030,
        0.0432, 0.0657, 0.0034, 0.0304, 0.0341, 0.0082], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,657][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0033, 0.0784, 0.0829, 0.0740, 0.0643, 0.0732, 0.0571, 0.0787, 0.0587,
        0.0693, 0.0609, 0.0667, 0.0776, 0.0805, 0.0744], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,658][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.0329, 0.0530, 0.0313, 0.0644, 0.0593, 0.1559, 0.0418, 0.0624, 0.0985,
        0.0616, 0.0343, 0.0962, 0.0392, 0.0567, 0.1126], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,660][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.0572, 0.0670, 0.0461, 0.0753, 0.0543, 0.0742, 0.0543, 0.0679, 0.0459,
        0.0889, 0.0842, 0.0501, 0.0725, 0.1065, 0.0557], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,662][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.3370, 0.0140, 0.0906, 0.0163, 0.0160, 0.0049, 0.0045, 0.0120, 0.0054,
        0.0222, 0.1471, 0.0155, 0.1026, 0.0853, 0.1266], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,664][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.0086, 0.0619, 0.0403, 0.0295, 0.0076, 0.0083, 0.0359, 0.1203, 0.0792,
        0.0455, 0.0428, 0.1587, 0.0834, 0.1125, 0.1655], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,665][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ as] are: tensor([6.6557e-01, 9.9868e-03, 8.0298e-02, 1.7794e-02, 1.3784e-02, 1.6136e-02,
        2.7667e-03, 2.3279e-02, 1.2592e-02, 9.6424e-03, 7.9579e-02, 1.9824e-02,
        3.8485e-02, 5.9245e-05, 3.9060e-03, 6.3005e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,667][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0449, 0.0291, 0.0791, 0.1083, 0.0570, 0.1044, 0.0336, 0.1110, 0.0565,
        0.0216, 0.0352, 0.0811, 0.0684, 0.0174, 0.0748, 0.0777],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,668][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.5190, 0.0389, 0.0284, 0.0172, 0.0145, 0.0253, 0.0438, 0.0250, 0.0425,
        0.0275, 0.0166, 0.0512, 0.0213, 0.0386, 0.0516, 0.0386],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,670][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.1182, 0.0411, 0.0557, 0.0862, 0.0612, 0.0370, 0.0462, 0.0535, 0.0350,
        0.0885, 0.0318, 0.0297, 0.0747, 0.0841, 0.0495, 0.1076],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,671][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ as] are: tensor([1.1341e-04, 5.1849e-03, 5.8899e-03, 6.3496e-03, 8.9965e-03, 5.1884e-04,
        1.0922e-02, 1.4649e-03, 1.6862e-03, 2.7713e-03, 3.2585e-03, 7.1306e-03,
        8.9150e-03, 1.3911e-02, 9.1307e-01, 9.8162e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,672][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ as] are: tensor([8.0780e-01, 6.1947e-04, 2.9006e-03, 1.9095e-03, 1.1234e-03, 3.1041e-03,
        1.0199e-03, 1.8275e-03, 5.8826e-03, 2.8673e-03, 2.6861e-02, 8.3660e-03,
        8.8678e-03, 1.6477e-02, 8.9312e-02, 2.1057e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,672][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.2815, 0.0517, 0.0604, 0.0635, 0.1558, 0.0462, 0.0025, 0.0975, 0.0046,
        0.0264, 0.0994, 0.0045, 0.0182, 0.0212, 0.0245, 0.0421],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,673][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0018, 0.0715, 0.0805, 0.0668, 0.0631, 0.0664, 0.0581, 0.0715, 0.0554,
        0.0650, 0.0603, 0.0623, 0.0780, 0.0785, 0.0736, 0.0474],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,673][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0223, 0.0545, 0.0257, 0.0515, 0.0531, 0.1423, 0.0345, 0.0549, 0.0866,
        0.0543, 0.0424, 0.1018, 0.0414, 0.0601, 0.1139, 0.0609],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,673][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.0831, 0.0683, 0.0325, 0.0636, 0.0586, 0.0687, 0.0550, 0.0667, 0.0362,
        0.0706, 0.0720, 0.0392, 0.0530, 0.0996, 0.0589, 0.0739],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,674][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.4789, 0.0057, 0.0331, 0.0063, 0.0092, 0.0039, 0.0025, 0.0074, 0.0042,
        0.0126, 0.1055, 0.0089, 0.0382, 0.0462, 0.0911, 0.1465],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,674][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.0484, 0.0115, 0.0114, 0.0065, 0.0042, 0.0072, 0.0109, 0.0529, 0.0360,
        0.0155, 0.0481, 0.1026, 0.0345, 0.0639, 0.2230, 0.3233],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,675][circuit_model.py][line:1532][INFO] ##8-th layer ##Weight##: The head1 weight for token [ the] are: tensor([7.8847e-01, 6.2065e-03, 4.5647e-02, 1.2410e-02, 8.3117e-03, 1.1452e-02,
        1.7300e-03, 1.8739e-02, 7.7045e-03, 6.7060e-03, 5.3741e-02, 1.0558e-02,
        2.1512e-02, 4.9688e-05, 2.6360e-03, 4.0686e-03, 5.2526e-05],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,676][circuit_model.py][line:1535][INFO] ##8-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0972, 0.0275, 0.0679, 0.0546, 0.0379, 0.1490, 0.0338, 0.0567, 0.0646,
        0.0326, 0.0374, 0.0790, 0.0443, 0.0114, 0.1174, 0.0703, 0.0183],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,678][circuit_model.py][line:1538][INFO] ##8-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.5723, 0.0278, 0.0277, 0.0128, 0.0115, 0.0216, 0.0347, 0.0276, 0.0405,
        0.0198, 0.0214, 0.0471, 0.0140, 0.0231, 0.0366, 0.0344, 0.0271],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,679][circuit_model.py][line:1541][INFO] ##8-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1856, 0.0294, 0.0475, 0.0824, 0.0544, 0.0325, 0.0399, 0.0415, 0.0256,
        0.0683, 0.0253, 0.0223, 0.0592, 0.0743, 0.0440, 0.1051, 0.0626],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,681][circuit_model.py][line:1544][INFO] ##8-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0008, 0.0151, 0.0087, 0.0197, 0.0354, 0.0027, 0.0664, 0.0057, 0.0303,
        0.0139, 0.0416, 0.1083, 0.0324, 0.0107, 0.4801, 0.0953, 0.0329],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,682][circuit_model.py][line:1547][INFO] ##8-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.7235, 0.0010, 0.0050, 0.0029, 0.0024, 0.0044, 0.0017, 0.0022, 0.0093,
        0.0054, 0.0362, 0.0122, 0.0167, 0.0157, 0.0919, 0.0389, 0.0306],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,684][circuit_model.py][line:1550][INFO] ##8-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.5408, 0.0376, 0.0299, 0.0437, 0.1284, 0.0482, 0.0006, 0.0395, 0.0017,
        0.0040, 0.0612, 0.0013, 0.0052, 0.0073, 0.0059, 0.0336, 0.0112],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,686][circuit_model.py][line:1553][INFO] ##8-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0021, 0.0664, 0.0716, 0.0641, 0.0594, 0.0609, 0.0520, 0.0676, 0.0513,
        0.0605, 0.0545, 0.0561, 0.0715, 0.0710, 0.0688, 0.0469, 0.0753],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,687][circuit_model.py][line:1556][INFO] ##8-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0269, 0.0489, 0.0262, 0.0537, 0.0504, 0.1247, 0.0339, 0.0601, 0.0757,
        0.0507, 0.0379, 0.0822, 0.0380, 0.0584, 0.1053, 0.0653, 0.0616],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,690][circuit_model.py][line:1559][INFO] ##8-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.1000, 0.0671, 0.0281, 0.0471, 0.0430, 0.0506, 0.0409, 0.0600, 0.0286,
        0.0623, 0.0672, 0.0293, 0.0403, 0.0859, 0.0594, 0.0819, 0.1084],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,691][circuit_model.py][line:1562][INFO] ##8-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.5869, 0.0039, 0.0150, 0.0042, 0.0055, 0.0032, 0.0011, 0.0033, 0.0019,
        0.0048, 0.0576, 0.0052, 0.0271, 0.0332, 0.0618, 0.0943, 0.0910],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,693][circuit_model.py][line:1565][INFO] ##8-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0315, 0.0122, 0.0125, 0.0102, 0.0054, 0.0086, 0.0105, 0.0476, 0.0137,
        0.0144, 0.0262, 0.0282, 0.0304, 0.0708, 0.1257, 0.2681, 0.2840],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,739][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:30,740][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,740][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,740][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,741][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,741][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,741][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,742][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,743][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,743][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,744][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,744][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,744][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:30,746][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.4991, 0.5009], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,746][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.4537, 0.5463], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,747][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.9941, 0.0059], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,747][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.2111, 0.7889], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,747][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.1120, 0.8880], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,748][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.9529, 0.0471], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,748][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.9584, 0.0416], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,748][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.0744, 0.9256], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,748][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.9413, 0.0587], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,749][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.8806, 0.1194], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,749][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.6512, 0.3488], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,749][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.2021, 0.7979], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:30,751][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.7058, 0.2896, 0.0046], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,752][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0206, 0.3634, 0.6160], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,754][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.9158, 0.0223, 0.0619], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,755][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.0664, 0.3162, 0.6174], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,757][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0219, 0.2248, 0.7533], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,759][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.7331, 0.0837, 0.1832], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,760][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.8492, 0.0431, 0.1077], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,762][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0175, 0.1514, 0.8311], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,764][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.6411, 0.0330, 0.3259], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,765][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.4676, 0.1017, 0.4307], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,767][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.1852, 0.0950, 0.7198], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,769][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0119, 0.7562, 0.2318], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:30,770][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.5196, 0.4046, 0.0080, 0.0678], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,772][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0888, 0.2679, 0.5328, 0.1106], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,774][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.8718, 0.0345, 0.0862, 0.0075], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,775][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0826, 0.3618, 0.4724, 0.0833], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,777][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0488, 0.1623, 0.7454, 0.0435], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,779][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.9166, 0.0200, 0.0591, 0.0043], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,780][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([9.9161e-01, 2.0970e-03, 5.9629e-03, 3.3251e-04], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,781][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.1232, 0.0500, 0.7950, 0.0318], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,783][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.4822, 0.0541, 0.4321, 0.0316], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,785][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.6501, 0.1322, 0.2083, 0.0094], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,786][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.4226, 0.0735, 0.3797, 0.1243], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,788][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0062, 0.5008, 0.2959, 0.1971], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:30,790][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.3551, 0.4580, 0.0177, 0.0803, 0.0889], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,791][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0922, 0.2152, 0.4682, 0.1217, 0.1028], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,793][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.5928, 0.0877, 0.2099, 0.0642, 0.0454], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,795][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.1244, 0.2257, 0.5066, 0.1151, 0.0282], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,797][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0130, 0.2075, 0.5332, 0.0522, 0.1941], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,798][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.8559, 0.0290, 0.0782, 0.0181, 0.0188], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,800][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.9223, 0.0196, 0.0392, 0.0051, 0.0138], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,802][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0266, 0.0469, 0.7898, 0.0813, 0.0555], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,803][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.3886, 0.0394, 0.4791, 0.0258, 0.0671], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,805][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.5122, 0.1065, 0.3487, 0.0130, 0.0196], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,807][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.2516, 0.0610, 0.4187, 0.1253, 0.1434], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,809][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0047, 0.3224, 0.3810, 0.2452, 0.0467], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:30,810][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.4677, 0.3306, 0.0096, 0.0720, 0.1123, 0.0077], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,810][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.0272, 0.1722, 0.3285, 0.2748, 0.0549, 0.1424], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,811][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.8620, 0.0111, 0.0263, 0.0070, 0.0128, 0.0807], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,811][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.1811, 0.1491, 0.4662, 0.0911, 0.0909, 0.0217], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,812][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0100, 0.0683, 0.2234, 0.0784, 0.3061, 0.3138], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,812][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.8045, 0.0187, 0.0621, 0.0259, 0.0426, 0.0463], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,812][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.3797, 0.1500, 0.2409, 0.0549, 0.1283, 0.0461], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,813][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0070, 0.0393, 0.4997, 0.2404, 0.1261, 0.0874], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,813][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.5059, 0.0376, 0.2585, 0.0288, 0.1094, 0.0597], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,813][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.5429, 0.1340, 0.2257, 0.0176, 0.0256, 0.0542], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,814][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.2523, 0.0484, 0.3396, 0.1549, 0.1419, 0.0628], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,816][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.0090, 0.2888, 0.2620, 0.1770, 0.1327, 0.1306], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:30,818][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.6443, 0.2321, 0.0032, 0.0547, 0.0620, 0.0013, 0.0024],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,819][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.0836, 0.0876, 0.2222, 0.2175, 0.1413, 0.1223, 0.1255],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,820][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.6467, 0.0430, 0.1211, 0.0226, 0.0360, 0.1113, 0.0193],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,822][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.3961, 0.1261, 0.3373, 0.0513, 0.0360, 0.0258, 0.0273],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,824][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.0398, 0.0305, 0.2099, 0.0519, 0.1178, 0.1724, 0.3777],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,825][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.8711, 0.0104, 0.0288, 0.0155, 0.0299, 0.0246, 0.0198],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,827][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.8694, 0.0505, 0.0315, 0.0094, 0.0305, 0.0076, 0.0009],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,829][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.1127, 0.0257, 0.2170, 0.1834, 0.1095, 0.1424, 0.2093],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,831][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.5616, 0.0238, 0.1720, 0.0178, 0.0621, 0.0476, 0.1151],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,832][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.6704, 0.0630, 0.1561, 0.0090, 0.0157, 0.0285, 0.0573],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,834][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.5642, 0.0417, 0.2008, 0.0665, 0.0540, 0.0347, 0.0381],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,836][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.0341, 0.2040, 0.2017, 0.2028, 0.1429, 0.0922, 0.1224],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:30,837][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.5464, 0.3263, 0.0067, 0.0590, 0.0543, 0.0029, 0.0011, 0.0034],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,839][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0202, 0.0660, 0.1773, 0.1420, 0.0839, 0.1288, 0.1834, 0.1982],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,841][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.7896, 0.0149, 0.0487, 0.0083, 0.0112, 0.0668, 0.0118, 0.0487],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,842][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.5845, 0.0695, 0.1710, 0.0411, 0.0603, 0.0110, 0.0230, 0.0396],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,844][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.0834, 0.0287, 0.2237, 0.0207, 0.1109, 0.1142, 0.1810, 0.2374],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,846][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.9141, 0.0045, 0.0154, 0.0035, 0.0106, 0.0118, 0.0099, 0.0302],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,848][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.7835, 0.0387, 0.0586, 0.0204, 0.0581, 0.0171, 0.0023, 0.0212],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,850][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.1039, 0.0281, 0.2292, 0.0778, 0.0894, 0.0590, 0.1557, 0.2570],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,851][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.5155, 0.0178, 0.2595, 0.0194, 0.0494, 0.0389, 0.0522, 0.0473],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,853][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.8669, 0.0182, 0.0486, 0.0026, 0.0058, 0.0139, 0.0210, 0.0229],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,854][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.5406, 0.0293, 0.1793, 0.0499, 0.0914, 0.0299, 0.0216, 0.0580],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,856][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.0137, 0.1041, 0.1185, 0.1008, 0.0480, 0.0483, 0.1609, 0.4058],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:30,858][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.5580, 0.2531, 0.0036, 0.0733, 0.0989, 0.0020, 0.0015, 0.0024, 0.0071],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,859][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0174, 0.0449, 0.1725, 0.1643, 0.0535, 0.0633, 0.0618, 0.1157, 0.3065],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,861][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.8054, 0.0132, 0.0377, 0.0041, 0.0228, 0.0682, 0.0098, 0.0269, 0.0117],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,863][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.1004, 0.1456, 0.2246, 0.0934, 0.0444, 0.0178, 0.0661, 0.0962, 0.2116],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,865][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0370, 0.0138, 0.0868, 0.0723, 0.0537, 0.1105, 0.1321, 0.2591, 0.2348],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,867][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.7542, 0.0062, 0.0204, 0.0173, 0.0304, 0.0248, 0.0175, 0.0650, 0.0642],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,869][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.8123, 0.0638, 0.0496, 0.0136, 0.0234, 0.0127, 0.0018, 0.0092, 0.0135],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,870][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0058, 0.0089, 0.0465, 0.2368, 0.0825, 0.0520, 0.0979, 0.2475, 0.2223],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,871][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.4151, 0.0179, 0.1307, 0.0151, 0.0412, 0.0269, 0.0618, 0.0553, 0.2361],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,874][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.5516, 0.0297, 0.1654, 0.0117, 0.0097, 0.0259, 0.0926, 0.0395, 0.0739],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,875][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.3058, 0.0488, 0.2581, 0.0808, 0.0541, 0.0332, 0.0340, 0.0832, 0.1018],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,876][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0507, 0.1074, 0.0912, 0.1060, 0.0322, 0.0458, 0.0534, 0.3220, 0.1913],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:30,877][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.4556, 0.2849, 0.0052, 0.0431, 0.0467, 0.0029, 0.0025, 0.0059, 0.0105,
        0.1428], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,877][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0167, 0.0234, 0.0524, 0.0465, 0.0296, 0.0897, 0.0932, 0.2171, 0.3029,
        0.1284], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,878][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.7814, 0.0174, 0.0305, 0.0067, 0.0114, 0.0511, 0.0111, 0.0555, 0.0061,
        0.0289], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,878][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.1812, 0.1046, 0.1512, 0.0472, 0.0323, 0.0311, 0.0634, 0.0890, 0.2288,
        0.0712], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,879][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.0643, 0.0247, 0.0978, 0.0233, 0.0500, 0.0805, 0.1565, 0.1539, 0.1775,
        0.1715], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,879][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.8331, 0.0040, 0.0139, 0.0033, 0.0093, 0.0114, 0.0070, 0.0208, 0.0346,
        0.0627], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,879][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.9100, 0.0174, 0.0238, 0.0079, 0.0129, 0.0056, 0.0012, 0.0114, 0.0031,
        0.0067], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,880][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.1047, 0.0112, 0.1245, 0.0694, 0.0533, 0.0421, 0.0515, 0.1049, 0.1748,
        0.2636], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,880][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.2555, 0.0353, 0.2064, 0.0228, 0.0488, 0.0398, 0.0607, 0.0510, 0.1785,
        0.1012], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,881][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.7566, 0.0246, 0.0721, 0.0052, 0.0076, 0.0181, 0.0328, 0.0277, 0.0291,
        0.0262], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,883][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.3682, 0.0616, 0.2037, 0.0553, 0.0454, 0.0179, 0.0233, 0.0450, 0.0411,
        0.1383], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,884][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0154, 0.1973, 0.1131, 0.0975, 0.0584, 0.0496, 0.0748, 0.1613, 0.1265,
        0.1062], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:30,886][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.3023, 0.3506, 0.0061, 0.0677, 0.0939, 0.0039, 0.0029, 0.0055, 0.0087,
        0.1329, 0.0254], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,887][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0016, 0.0194, 0.0769, 0.1236, 0.0434, 0.0679, 0.1650, 0.0855, 0.2589,
        0.1202, 0.0374], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,889][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.6339, 0.0132, 0.0298, 0.0053, 0.0092, 0.0918, 0.0068, 0.0451, 0.0088,
        0.0122, 0.1438], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,891][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.1108, 0.0678, 0.1097, 0.0531, 0.0366, 0.0386, 0.0388, 0.0732, 0.1382,
        0.0504, 0.2829], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,892][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.0500, 0.0214, 0.0896, 0.0436, 0.0767, 0.0540, 0.0647, 0.0852, 0.0793,
        0.0847, 0.3507], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,894][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.7431, 0.0040, 0.0147, 0.0060, 0.0078, 0.0083, 0.0047, 0.0185, 0.0149,
        0.0405, 0.1375], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,896][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.4363, 0.0768, 0.1256, 0.0378, 0.0886, 0.0549, 0.0103, 0.0321, 0.0189,
        0.0280, 0.0908], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,897][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0752, 0.0120, 0.0656, 0.0325, 0.0172, 0.0152, 0.0185, 0.0404, 0.0527,
        0.0552, 0.6155], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,899][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.2553, 0.0068, 0.1486, 0.0077, 0.0255, 0.0279, 0.0382, 0.0322, 0.1052,
        0.0413, 0.3113], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,901][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.6244, 0.0216, 0.0592, 0.0044, 0.0066, 0.0214, 0.0290, 0.0313, 0.0300,
        0.0256, 0.1465], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,903][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.2380, 0.0190, 0.0846, 0.0262, 0.0349, 0.0303, 0.0221, 0.0476, 0.0427,
        0.0809, 0.3737], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,904][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0066, 0.0964, 0.0555, 0.0749, 0.0394, 0.0727, 0.0788, 0.2414, 0.2252,
        0.0661, 0.0428], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:30,905][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([5.8805e-01, 2.0437e-01, 2.5382e-03, 4.9475e-02, 4.5503e-02, 8.7412e-04,
        5.0876e-04, 1.1775e-03, 3.0378e-03, 6.6969e-02, 1.6692e-02, 2.0802e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,907][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0105, 0.0188, 0.0693, 0.0731, 0.0257, 0.0639, 0.0459, 0.0587, 0.1916,
        0.0342, 0.0365, 0.3719], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,909][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.6336, 0.0075, 0.0359, 0.0042, 0.0150, 0.0459, 0.0073, 0.0196, 0.0076,
        0.0221, 0.1918, 0.0095], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,910][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.2375, 0.0554, 0.1391, 0.0255, 0.0187, 0.0075, 0.0209, 0.0306, 0.0662,
        0.0281, 0.2460, 0.1245], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,913][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0726, 0.0033, 0.0209, 0.0103, 0.0101, 0.0157, 0.0293, 0.0310, 0.0283,
        0.0540, 0.3336, 0.3911], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,914][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.6434, 0.0014, 0.0058, 0.0035, 0.0053, 0.0045, 0.0023, 0.0132, 0.0085,
        0.0460, 0.2179, 0.0481], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,915][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([9.3027e-01, 1.3914e-02, 1.2527e-02, 3.3479e-03, 9.1431e-03, 4.4851e-03,
        5.3616e-04, 3.1759e-03, 3.0947e-03, 2.8065e-03, 1.1533e-02, 5.1707e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,917][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0216, 0.0010, 0.0051, 0.0132, 0.0037, 0.0026, 0.0034, 0.0133, 0.0096,
        0.0568, 0.5110, 0.3587], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,919][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.2791, 0.0067, 0.0327, 0.0040, 0.0102, 0.0068, 0.0191, 0.0162, 0.0556,
        0.0192, 0.2487, 0.3019], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,921][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.7403, 0.0088, 0.0435, 0.0029, 0.0028, 0.0065, 0.0158, 0.0091, 0.0139,
        0.0103, 0.1119, 0.0344], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,922][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.5098, 0.0125, 0.0728, 0.0161, 0.0163, 0.0082, 0.0064, 0.0159, 0.0185,
        0.0336, 0.2329, 0.0569], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,924][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0669, 0.0769, 0.0492, 0.0625, 0.0160, 0.0305, 0.0373, 0.2070, 0.0926,
        0.0701, 0.0793, 0.2117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:30,926][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.6985, 0.1494, 0.0027, 0.0235, 0.0241, 0.0015, 0.0016, 0.0014, 0.0065,
        0.0537, 0.0130, 0.0215, 0.0027], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,928][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0930, 0.0392, 0.0750, 0.0631, 0.0899, 0.1349, 0.0218, 0.1456, 0.0412,
        0.0759, 0.0497, 0.1088, 0.0617], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,930][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.6703, 0.0113, 0.0206, 0.0043, 0.0066, 0.0287, 0.0062, 0.0552, 0.0059,
        0.0196, 0.1526, 0.0102, 0.0085], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,931][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.2355, 0.0541, 0.0499, 0.0094, 0.0249, 0.0129, 0.0135, 0.0341, 0.0657,
        0.0312, 0.2892, 0.1384, 0.0413], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,932][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0602, 0.0087, 0.0171, 0.0037, 0.0076, 0.0154, 0.0347, 0.0378, 0.0437,
        0.0203, 0.3964, 0.3236, 0.0310], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,935][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.6493, 0.0019, 0.0073, 0.0007, 0.0020, 0.0061, 0.0027, 0.0085, 0.0188,
        0.0134, 0.2155, 0.0587, 0.0151], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,936][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([9.5117e-01, 7.5269e-03, 5.2374e-03, 1.7481e-03, 5.3232e-03, 4.6066e-03,
        1.4061e-04, 4.7070e-03, 3.4148e-04, 9.6324e-04, 1.7076e-02, 3.4170e-04,
        8.2032e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,937][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.1550, 0.0031, 0.0194, 0.0032, 0.0017, 0.0028, 0.0033, 0.0071, 0.0113,
        0.0203, 0.6324, 0.0973, 0.0430], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,939][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.2176, 0.0071, 0.0297, 0.0029, 0.0062, 0.0095, 0.0152, 0.0165, 0.0670,
        0.0325, 0.2217, 0.3521, 0.0219], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,941][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.5565, 0.0353, 0.0536, 0.0032, 0.0059, 0.0095, 0.0209, 0.0140, 0.0252,
        0.0195, 0.1828, 0.0607, 0.0130], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,942][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.3527, 0.0175, 0.1061, 0.0239, 0.0330, 0.0125, 0.0071, 0.0256, 0.0123,
        0.0371, 0.2179, 0.0277, 0.1267], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,944][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0171, 0.0411, 0.0438, 0.0366, 0.0277, 0.0250, 0.0511, 0.1829, 0.0974,
        0.0484, 0.0599, 0.2821, 0.0868], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:30,944][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([1.4337e-01, 2.8590e-02, 4.5305e-04, 4.0590e-03, 3.5132e-03, 1.4242e-04,
        1.5220e-04, 1.4122e-04, 8.0505e-04, 6.5579e-03, 1.7095e-03, 3.0783e-03,
        3.8225e-04, 8.0704e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,945][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.1089, 0.0323, 0.0884, 0.0414, 0.0385, 0.1597, 0.0426, 0.0733, 0.0881,
        0.0558, 0.0547, 0.1388, 0.0643, 0.0131], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,945][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.6462, 0.0144, 0.0361, 0.0053, 0.0073, 0.0366, 0.0063, 0.0386, 0.0055,
        0.0145, 0.1669, 0.0079, 0.0058, 0.0085], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,946][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.3906, 0.0684, 0.0351, 0.0089, 0.0090, 0.0186, 0.0130, 0.0231, 0.0669,
        0.0169, 0.1685, 0.1201, 0.0282, 0.0327], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,946][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.1684, 0.0069, 0.0234, 0.0033, 0.0078, 0.0135, 0.0201, 0.0148, 0.0265,
        0.0129, 0.4081, 0.1433, 0.0211, 0.1300], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,947][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([8.7494e-01, 6.3076e-04, 2.2079e-03, 3.8198e-04, 9.3232e-04, 1.4535e-03,
        7.7537e-04, 2.2500e-03, 4.9453e-03, 6.4352e-03, 6.7339e-02, 1.5134e-02,
        9.5832e-03, 1.2995e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,947][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([9.7355e-01, 3.8062e-03, 3.4145e-03, 8.5773e-04, 3.0974e-03, 3.4778e-03,
        6.2694e-05, 1.4509e-03, 2.3114e-04, 4.0077e-04, 8.8314e-03, 1.6489e-04,
        2.9763e-04, 3.5382e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,947][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.4326, 0.0012, 0.0109, 0.0027, 0.0013, 0.0023, 0.0014, 0.0044, 0.0047,
        0.0067, 0.3923, 0.0431, 0.0258, 0.0708], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,949][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2558, 0.0040, 0.0228, 0.0017, 0.0047, 0.0064, 0.0138, 0.0082, 0.0572,
        0.0168, 0.2818, 0.2767, 0.0109, 0.0391], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,951][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.7456, 0.0114, 0.0248, 0.0012, 0.0027, 0.0053, 0.0097, 0.0063, 0.0111,
        0.0078, 0.1313, 0.0277, 0.0033, 0.0118], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,952][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.5250, 0.0122, 0.0513, 0.0120, 0.0192, 0.0098, 0.0048, 0.0128, 0.0085,
        0.0161, 0.1748, 0.0198, 0.0631, 0.0705], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,954][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0172, 0.0372, 0.0395, 0.0292, 0.0172, 0.0286, 0.0447, 0.1428, 0.0710,
        0.0487, 0.0676, 0.1686, 0.0902, 0.1976], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:30,955][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([2.0695e-01, 2.7538e-02, 1.3815e-04, 2.6379e-03, 3.5177e-03, 1.2059e-04,
        8.9609e-05, 8.5186e-05, 3.7748e-04, 2.9862e-03, 1.2520e-03, 1.6697e-03,
        2.7661e-04, 7.3212e-01, 2.0243e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,957][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0127, 0.0479, 0.0779, 0.1970, 0.0497, 0.0375, 0.0307, 0.0370, 0.0655,
        0.0288, 0.0518, 0.1750, 0.0795, 0.0280, 0.0811], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,959][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.7095, 0.0182, 0.0159, 0.0021, 0.0039, 0.0169, 0.0051, 0.0338, 0.0058,
        0.0082, 0.1334, 0.0087, 0.0033, 0.0058, 0.0294], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,960][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.1608, 0.1470, 0.1057, 0.0478, 0.0228, 0.0168, 0.0128, 0.0283, 0.0441,
        0.0170, 0.0974, 0.0664, 0.0620, 0.0566, 0.1146], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,961][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([1.2343e-02, 1.2271e-04, 9.3294e-04, 2.4794e-04, 1.8989e-04, 9.3519e-05,
        1.9953e-04, 3.5710e-04, 1.7005e-04, 7.0529e-04, 3.9491e-03, 1.8929e-03,
        4.5246e-03, 1.7737e-02, 9.5653e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,963][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([8.1511e-01, 2.7923e-04, 6.3738e-04, 2.7465e-04, 1.7305e-04, 2.1162e-04,
        2.7016e-04, 6.8070e-04, 1.0734e-03, 1.8106e-03, 2.5700e-02, 4.5551e-03,
        6.3269e-03, 1.6797e-02, 1.2610e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,964][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([8.4372e-01, 2.8667e-02, 3.6434e-02, 1.6317e-02, 1.2361e-02, 1.0167e-02,
        4.7833e-04, 8.3444e-03, 7.8722e-04, 4.5352e-03, 2.9388e-02, 7.5732e-04,
        2.7464e-03, 3.8454e-03, 1.4475e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,965][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([2.3107e-01, 5.7624e-04, 1.9945e-03, 1.2560e-03, 2.1979e-04, 3.2306e-04,
        2.7055e-04, 7.1980e-04, 5.0626e-04, 3.2379e-03, 1.1153e-01, 1.2425e-02,
        3.2995e-02, 2.1716e-01, 3.8572e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,967][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.3035, 0.0044, 0.0269, 0.0023, 0.0040, 0.0047, 0.0203, 0.0112, 0.0207,
        0.0228, 0.1401, 0.0615, 0.0152, 0.0573, 0.3051], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,969][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.4754, 0.0334, 0.1203, 0.0036, 0.0028, 0.0088, 0.0252, 0.0117, 0.0164,
        0.0154, 0.1335, 0.0353, 0.0067, 0.0215, 0.0902], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,970][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.3370, 0.0140, 0.0906, 0.0163, 0.0160, 0.0049, 0.0045, 0.0120, 0.0054,
        0.0222, 0.1471, 0.0155, 0.1026, 0.0853, 0.1266], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,971][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.0086, 0.0619, 0.0403, 0.0295, 0.0076, 0.0083, 0.0359, 0.1203, 0.0792,
        0.0455, 0.0428, 0.1587, 0.0834, 0.1125, 0.1655], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:30,973][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([2.1132e-01, 2.6128e-02, 3.7316e-04, 3.7921e-03, 5.7277e-03, 3.1219e-04,
        1.8763e-04, 1.2525e-04, 9.0811e-04, 5.0648e-03, 2.6042e-03, 3.5078e-03,
        5.0117e-04, 7.1203e-01, 1.8178e-02, 9.2326e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,975][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0449, 0.0291, 0.0791, 0.1083, 0.0570, 0.1044, 0.0336, 0.1110, 0.0565,
        0.0216, 0.0352, 0.0811, 0.0684, 0.0174, 0.0748, 0.0777],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,976][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([8.3893e-01, 4.0979e-03, 6.0633e-03, 5.9582e-04, 1.5673e-03, 6.6761e-03,
        1.3773e-03, 8.4342e-03, 1.1946e-03, 3.3914e-03, 7.9377e-02, 2.3325e-03,
        1.7284e-03, 2.9278e-03, 2.6945e-02, 1.4362e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,977][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.2885, 0.0440, 0.0424, 0.0080, 0.0114, 0.0101, 0.0065, 0.0216, 0.0580,
        0.0142, 0.1576, 0.0806, 0.0277, 0.0268, 0.0971, 0.1054],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,978][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([3.6237e-03, 3.8906e-05, 1.4119e-04, 1.7388e-05, 5.0637e-05, 6.2819e-05,
        7.5421e-05, 1.1103e-04, 7.3383e-05, 1.0000e-04, 2.9408e-03, 8.5812e-04,
        1.8998e-04, 3.0649e-03, 9.6660e-01, 2.2055e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,980][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([8.4595e-01, 8.4689e-05, 3.4561e-04, 8.8136e-05, 1.1721e-04, 3.6314e-04,
        1.1691e-04, 4.0010e-04, 8.3845e-04, 8.9292e-04, 2.1386e-02, 2.9510e-03,
        2.4999e-03, 6.9923e-03, 1.0447e-01, 1.2503e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,981][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([9.3967e-01, 6.0691e-03, 6.6689e-03, 2.3608e-03, 8.3833e-03, 5.0670e-03,
        2.3183e-04, 4.2934e-03, 5.5741e-04, 1.5815e-03, 1.6388e-02, 4.2465e-04,
        9.5198e-04, 1.3407e-03, 1.7726e-03, 4.2367e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,982][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([1.3747e-01, 1.0161e-04, 6.6489e-04, 3.8365e-04, 1.1050e-04, 2.0684e-04,
        9.0765e-05, 2.2155e-04, 2.3714e-04, 8.6388e-04, 5.0434e-02, 3.8791e-03,
        6.2874e-03, 5.3798e-02, 5.4914e-01, 1.9611e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,984][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.1055, 0.0013, 0.0054, 0.0007, 0.0021, 0.0036, 0.0056, 0.0044, 0.0216,
        0.0065, 0.0658, 0.0968, 0.0059, 0.0303, 0.4328, 0.2120],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,985][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.7166, 0.0097, 0.0183, 0.0007, 0.0021, 0.0042, 0.0052, 0.0052, 0.0078,
        0.0060, 0.1006, 0.0174, 0.0021, 0.0083, 0.0625, 0.0333],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,987][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.4789, 0.0057, 0.0331, 0.0063, 0.0092, 0.0039, 0.0025, 0.0074, 0.0042,
        0.0126, 0.1055, 0.0089, 0.0382, 0.0462, 0.0911, 0.1465],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,989][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.0484, 0.0115, 0.0114, 0.0065, 0.0042, 0.0072, 0.0109, 0.0529, 0.0360,
        0.0155, 0.0481, 0.1026, 0.0345, 0.0639, 0.2230, 0.3233],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:30,990][circuit_model.py][line:1570][INFO] ##8-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([9.0689e-02, 1.0540e-02, 1.4980e-04, 1.7092e-03, 1.4105e-03, 6.5207e-05,
        5.5354e-05, 4.4152e-05, 3.3695e-04, 2.1624e-03, 6.4831e-04, 1.3560e-03,
        1.5089e-04, 4.4448e-01, 7.6980e-03, 2.6913e-03, 4.3581e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,992][circuit_model.py][line:1573][INFO] ##8-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0972, 0.0275, 0.0679, 0.0546, 0.0379, 0.1490, 0.0338, 0.0567, 0.0646,
        0.0326, 0.0374, 0.0790, 0.0443, 0.0114, 0.1174, 0.0703, 0.0183],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,993][circuit_model.py][line:1576][INFO] ##8-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.7071, 0.0054, 0.0165, 0.0024, 0.0033, 0.0209, 0.0027, 0.0268, 0.0028,
        0.0082, 0.1179, 0.0044, 0.0035, 0.0051, 0.0385, 0.0244, 0.0101],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,995][circuit_model.py][line:1579][INFO] ##8-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.4836, 0.0366, 0.0183, 0.0046, 0.0043, 0.0115, 0.0067, 0.0139, 0.0301,
        0.0087, 0.1052, 0.0518, 0.0143, 0.0180, 0.0728, 0.0859, 0.0338],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,996][circuit_model.py][line:1582][INFO] ##8-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([6.1714e-02, 1.3772e-04, 5.4661e-04, 9.7710e-05, 1.4971e-04, 2.8076e-04,
        3.1610e-04, 2.7408e-04, 4.1650e-04, 3.9281e-04, 1.7984e-02, 2.4490e-03,
        8.5145e-04, 6.8509e-03, 7.8511e-01, 6.1405e-02, 6.1028e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,997][circuit_model.py][line:1585][INFO] ##8-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.3160e-01, 4.8531e-05, 1.9996e-04, 4.2282e-05, 8.5784e-05, 1.6936e-04,
        4.6647e-05, 1.7524e-04, 3.6784e-04, 6.3526e-04, 1.1883e-02, 1.1151e-03,
        1.5179e-03, 2.5022e-03, 3.4073e-02, 7.2037e-03, 8.3296e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,998][circuit_model.py][line:1588][INFO] ##8-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([9.8140e-01, 2.2479e-03, 1.6918e-03, 5.8562e-04, 2.2331e-03, 1.8782e-03,
        2.8034e-05, 8.5198e-04, 9.5527e-05, 1.5361e-04, 6.0744e-03, 6.5350e-05,
        1.5805e-04, 2.4253e-04, 2.1198e-04, 1.6330e-03, 4.5373e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:30,999][circuit_model.py][line:1591][INFO] ##8-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([7.2914e-01, 3.5987e-05, 3.4538e-04, 8.6786e-05, 3.2930e-05, 9.2911e-05,
        2.1780e-05, 1.0581e-04, 9.5293e-05, 2.5608e-04, 3.7226e-02, 8.8361e-04,
        1.7408e-03, 5.7086e-03, 1.2098e-01, 4.7239e-02, 5.6006e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,002][circuit_model.py][line:1594][INFO] ##8-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.3289, 0.0013, 0.0060, 0.0006, 0.0015, 0.0021, 0.0043, 0.0027, 0.0148,
        0.0053, 0.1151, 0.0720, 0.0036, 0.0154, 0.2602, 0.0917, 0.0746],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,003][circuit_model.py][line:1597][INFO] ##8-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([7.8344e-01, 7.8516e-03, 1.3008e-02, 5.7641e-04, 1.2717e-03, 2.7270e-03,
        4.9726e-03, 3.2208e-03, 5.8465e-03, 3.8758e-03, 8.1695e-02, 1.4070e-02,
        1.5056e-03, 6.4437e-03, 3.9710e-02, 1.5445e-02, 1.4337e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,004][circuit_model.py][line:1600][INFO] ##8-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.5869, 0.0039, 0.0150, 0.0042, 0.0055, 0.0032, 0.0011, 0.0033, 0.0019,
        0.0048, 0.0576, 0.0052, 0.0271, 0.0332, 0.0618, 0.0943, 0.0910],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,006][circuit_model.py][line:1603][INFO] ##8-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0315, 0.0122, 0.0125, 0.0102, 0.0054, 0.0086, 0.0105, 0.0476, 0.0137,
        0.0144, 0.0262, 0.0282, 0.0304, 0.0708, 0.1257, 0.2681, 0.2840],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,007][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:31,010][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  337],
        [ 9174],
        [16466],
        [ 5504],
        [ 5352],
        [10836],
        [ 2104],
        [ 3836],
        [ 3554],
        [ 1846],
        [ 2336],
        [ 1905],
        [ 3792],
        [  598],
        [ 1880],
        [ 1570],
        [  431]], device='cuda:0')
[2024-07-23 21:05:31,011][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  373],
        [13829],
        [18713],
        [ 8723],
        [10170],
        [19830],
        [ 6210],
        [13359],
        [10567],
        [ 4407],
        [ 6390],
        [ 6321],
        [ 8908],
        [ 1542],
        [ 6559],
        [ 4004],
        [ 1505]], device='cuda:0')
[2024-07-23 21:05:31,012][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 830],
        [ 817],
        [1104],
        [1104],
        [1104],
        [1684],
        [2447],
        [1369],
        [2063],
        [2148],
        [1763],
        [1914],
        [1884],
        [1727],
        [1833],
        [2077],
        [1762]], device='cuda:0')
[2024-07-23 21:05:31,013][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[26611],
        [48346],
        [12868],
        [ 9076],
        [ 7445],
        [ 8491],
        [ 9074],
        [14415],
        [ 8725],
        [13434],
        [11152],
        [13255],
        [17919],
        [18534],
        [10182],
        [11572],
        [12212]], device='cuda:0')
[2024-07-23 21:05:31,014][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[14173],
        [13096],
        [15117],
        [17408],
        [20542],
        [17420],
        [21058],
        [25642],
        [23852],
        [22072],
        [20092],
        [23917],
        [22545],
        [23412],
        [22660],
        [23805],
        [23677]], device='cuda:0')
[2024-07-23 21:05:31,016][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[  382],
        [ 1652],
        [26127],
        [18892],
        [10421],
        [16585],
        [13277],
        [ 5745],
        [13734],
        [15392],
        [ 4793],
        [12048],
        [15116],
        [18557],
        [16714],
        [21332],
        [20208]], device='cuda:0')
[2024-07-23 21:05:31,018][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 1496],
        [31680],
        [28374],
        [21031],
        [23083],
        [16882],
        [19169],
        [18255],
        [21889],
        [23900],
        [17919],
        [23464],
        [33617],
        [32301],
        [27469],
        [12418],
        [19002]], device='cuda:0')
[2024-07-23 21:05:31,019][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 752],
        [ 798],
        [ 848],
        [ 804],
        [ 852],
        [ 851],
        [ 749],
        [ 693],
        [2216],
        [1242],
        [ 728],
        [3023],
        [2329],
        [2016],
        [3107],
        [1281],
        [2036]], device='cuda:0')
[2024-07-23 21:05:31,021][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[30685],
        [48279],
        [32756],
        [35181],
        [27764],
        [19515],
        [27824],
        [15055],
        [26182],
        [19552],
        [16576],
        [20282],
        [17734],
        [18338],
        [17695],
        [12019],
        [14054]], device='cuda:0')
[2024-07-23 21:05:31,022][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[2885],
        [  40],
        [ 130],
        [ 273],
        [ 435],
        [ 483],
        [ 549],
        [ 596],
        [ 627],
        [ 626],
        [ 636],
        [ 648],
        [ 680],
        [ 775],
        [ 807],
        [ 864],
        [ 941]], device='cuda:0')
[2024-07-23 21:05:31,024][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[21198],
        [  127],
        [  383],
        [ 1151],
        [ 1445],
        [ 2514],
        [ 2351],
        [ 2773],
        [ 2734],
        [ 2875],
        [ 2921],
        [ 2980],
        [ 3187],
        [ 3374],
        [ 3525],
        [ 3787],
        [ 4171]], device='cuda:0')
[2024-07-23 21:05:31,025][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[41391],
        [43008],
        [39616],
        [35325],
        [28904],
        [24885],
        [24630],
        [18057],
        [21702],
        [18679],
        [16249],
        [17544],
        [15696],
        [16467],
        [15561],
        [15273],
        [17034]], device='cuda:0')
[2024-07-23 21:05:31,027][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[3704],
        [   7],
        [3088],
        [2769],
        [6659],
        [8291],
        [4335],
        [7315],
        [5812],
        [3144],
        [4821],
        [3483],
        [6143],
        [5685],
        [5333],
        [6248],
        [7684]], device='cuda:0')
[2024-07-23 21:05:31,029][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[10698],
        [48032],
        [46982],
        [45594],
        [42466],
        [41980],
        [41376],
        [30707],
        [39343],
        [43271],
        [41411],
        [42166],
        [41454],
        [38246],
        [36794],
        [24177],
        [21636]], device='cuda:0')
[2024-07-23 21:05:31,030][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[475],
        [528],
        [717],
        [442],
        [483],
        [501],
        [443],
        [457],
        [441],
        [520],
        [477],
        [411],
        [408],
        [363],
        [559],
        [493],
        [411]], device='cuda:0')
[2024-07-23 21:05:31,032][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[47072],
        [ 2103],
        [ 9485],
        [ 2769],
        [ 1658],
        [ 2996],
        [ 8175],
        [ 3827],
        [ 5295],
        [ 4807],
        [ 2804],
        [ 8582],
        [15915],
        [46482],
        [46427],
        [46208],
        [45119]], device='cuda:0')
[2024-07-23 21:05:31,033][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 1732],
        [35282],
        [30196],
        [31277],
        [33244],
        [35488],
        [33780],
        [31212],
        [26589],
        [27274],
        [26445],
        [24664],
        [33051],
        [30915],
        [27174],
        [29527],
        [29198]], device='cuda:0')
[2024-07-23 21:05:31,035][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[12474],
        [12877],
        [11296],
        [11458],
        [ 9590],
        [15406],
        [12146],
        [14251],
        [12519],
        [12640],
        [13930],
        [11397],
        [13172],
        [12689],
        [12603],
        [14169],
        [14530]], device='cuda:0')
[2024-07-23 21:05:31,037][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[28611],
        [ 2155],
        [ 1935],
        [ 2055],
        [ 2029],
        [ 2175],
        [ 2457],
        [ 3880],
        [ 3846],
        [ 5627],
        [ 4431],
        [ 4177],
        [ 4732],
        [ 5333],
        [ 3586],
        [ 5719],
        [ 6424]], device='cuda:0')
[2024-07-23 21:05:31,038][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[21245],
        [ 2202],
        [ 5541],
        [ 6023],
        [ 3096],
        [ 6088],
        [16941],
        [13906],
        [11223],
        [10664],
        [ 6835],
        [ 5192],
        [ 4780],
        [ 6237],
        [15564],
        [15710],
        [15209]], device='cuda:0')
[2024-07-23 21:05:31,040][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 9280],
        [ 8389],
        [11716],
        [11721],
        [12521],
        [13605],
        [12289],
        [11997],
        [12780],
        [10266],
        [12587],
        [11063],
        [11079],
        [10650],
        [14459],
        [13952],
        [11794]], device='cuda:0')
[2024-07-23 21:05:31,042][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[ 186],
        [  48],
        [ 116],
        [ 149],
        [  54],
        [3997],
        [  40],
        [ 300],
        [ 153],
        [  53],
        [2360],
        [  49],
        [  56],
        [  84],
        [  73],
        [  44],
        [ 102]], device='cuda:0')
[2024-07-23 21:05:31,043][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[17809],
        [34337],
        [37492],
        [36717],
        [35776],
        [33216],
        [28057],
        [23268],
        [17218],
        [17105],
        [30167],
        [26608],
        [28993],
        [25950],
        [ 7111],
        [ 5822],
        [11696]], device='cuda:0')
[2024-07-23 21:05:31,045][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[14173],
        [12886],
        [13263],
        [10084],
        [ 9116],
        [14368],
        [17475],
        [15681],
        [18792],
        [14739],
        [17127],
        [23880],
        [23527],
        [23806],
        [14252],
        [12679],
        [15805]], device='cuda:0')
[2024-07-23 21:05:31,046][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[2999],
        [2699],
        [ 639],
        [ 686],
        [ 679],
        [ 690],
        [ 948],
        [2099],
        [1889],
        [2163],
        [3546],
        [5027],
        [4657],
        [6564],
        [2577],
        [6371],
        [7058]], device='cuda:0')
[2024-07-23 21:05:31,048][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[4714],
        [  40],
        [5493],
        [5616],
        [5198],
        [5333],
        [5609],
        [7557],
        [6415],
        [4716],
        [3156],
        [2281],
        [4521],
        [3565],
        [3909],
        [5983],
        [6938]], device='cuda:0')
[2024-07-23 21:05:31,050][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[40609],
        [ 1335],
        [ 1698],
        [ 2015],
        [ 3434],
        [ 3546],
        [ 2655],
        [ 5598],
        [ 5133],
        [ 3969],
        [ 7215],
        [13532],
        [20830],
        [24401],
        [27637],
        [29724],
        [28895]], device='cuda:0')
[2024-07-23 21:05:31,051][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[19891],
        [47176],
        [39859],
        [42295],
        [44607],
        [31246],
        [39447],
        [33150],
        [37839],
        [42476],
        [32581],
        [38129],
        [34242],
        [28093],
        [37570],
        [32181],
        [26046]], device='cuda:0')
[2024-07-23 21:05:31,053][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[46306],
        [47043],
        [45292],
        [45623],
        [45233],
        [45675],
        [44513],
        [45031],
        [45683],
        [43406],
        [43217],
        [42203],
        [40364],
        [44105],
        [44039],
        [44093],
        [43924]], device='cuda:0')
[2024-07-23 21:05:31,054][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049],
        [12049]], device='cuda:0')
[2024-07-23 21:05:31,108][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:31,110][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,111][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,113][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,114][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,115][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,116][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,118][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,119][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,120][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,122][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,123][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,124][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,126][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.3550, 0.6450], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,128][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.8607, 0.1393], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,129][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.7574, 0.2426], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,131][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.6443, 0.3557], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,132][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.0465, 0.9535], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,134][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.9720, 0.0280], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,135][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ original] are: tensor([5.1898e-04, 9.9948e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,136][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.1210, 0.8790], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,138][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.0370, 0.9630], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,139][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ original] are: tensor([1.7276e-05, 9.9998e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,141][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.0678, 0.9322], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,142][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.9607, 0.0393], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,144][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.2040, 0.4301, 0.3659], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,146][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.3609, 0.1798, 0.4593], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,147][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.1403, 0.4290, 0.4307], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,148][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.6020, 0.1613, 0.2367], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,151][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0459, 0.9107, 0.0434], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,152][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.8729, 0.0389, 0.0882], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,153][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ language] are: tensor([1.1172e-04, 2.5505e-01, 7.4483e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,154][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0696, 0.6514, 0.2790], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,155][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0415, 0.6576, 0.3009], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,155][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ language] are: tensor([2.4803e-06, 7.7559e-01, 2.2441e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,155][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.0242, 0.3298, 0.6460], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,156][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.7250, 0.1915, 0.0836], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,156][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.5567, 0.1792, 0.2421, 0.0220], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,156][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.4494, 0.1021, 0.3201, 0.1283], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,157][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.4705, 0.2044, 0.3158, 0.0093], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,157][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.7389, 0.0332, 0.0478, 0.1801], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,157][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0285, 0.6828, 0.0608, 0.2279], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,158][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.9662, 0.0118, 0.0193, 0.0027], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,158][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ of] are: tensor([3.2319e-04, 1.2136e-01, 6.8068e-01, 1.9764e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,160][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0265, 0.3217, 0.5890, 0.0628], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,162][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0253, 0.6300, 0.2316, 0.1131], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,163][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ of] are: tensor([1.0861e-06, 5.5997e-01, 1.7410e-01, 2.6593e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,164][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.0545, 0.2032, 0.5595, 0.1829], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,165][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.7628, 0.0809, 0.1264, 0.0299], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,167][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.1867, 0.2338, 0.5316, 0.0358, 0.0121], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,169][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.3723, 0.0721, 0.2213, 0.0872, 0.2472], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,170][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.1874, 0.4319, 0.3153, 0.0193, 0.0461], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,172][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.2977, 0.0403, 0.0689, 0.4001, 0.1930], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,174][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0410, 0.6305, 0.0290, 0.2207, 0.0788], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,175][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.8763, 0.0184, 0.0984, 0.0032, 0.0037], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,176][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ De] are: tensor([2.9836e-04, 1.8726e-01, 4.1453e-01, 1.7909e-01, 2.1883e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,178][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.0102, 0.3603, 0.4748, 0.1350, 0.0198], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,180][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.0050, 0.4705, 0.1840, 0.1294, 0.2111], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,181][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ De] are: tensor([2.2073e-06, 5.2947e-01, 2.1202e-01, 1.7811e-01, 8.0400e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,183][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.0292, 0.1773, 0.4616, 0.2523, 0.0795], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,184][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.2208, 0.1781, 0.1916, 0.1185, 0.2910], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,186][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.5972, 0.1610, 0.1670, 0.0210, 0.0097, 0.0442], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,188][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.3474, 0.0426, 0.1435, 0.0593, 0.1405, 0.2666], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,190][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.2439, 0.3770, 0.2241, 0.0139, 0.0315, 0.1096], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,191][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.1603, 0.0324, 0.0481, 0.3500, 0.2893, 0.1197], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,192][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.0694, 0.5466, 0.0344, 0.2278, 0.0820, 0.0398], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,195][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.8796, 0.0174, 0.0752, 0.0148, 0.0059, 0.0071], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,196][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([9.8526e-05, 1.1612e-01, 2.3127e-01, 3.1351e-01, 2.9587e-01, 4.3123e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,197][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.0265, 0.1882, 0.4456, 0.2309, 0.0666, 0.0422], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,199][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.0066, 0.3388, 0.1375, 0.0968, 0.2848, 0.1354], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,200][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([4.3333e-05, 2.7228e-01, 1.6816e-01, 2.7145e-01, 1.2563e-01, 1.6244e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,202][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.0087, 0.0225, 0.0871, 0.7591, 0.0569, 0.0658], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,203][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.1953, 0.1672, 0.1611, 0.0556, 0.1923, 0.2286], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,205][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.5101, 0.1654, 0.1922, 0.0313, 0.0057, 0.0481, 0.0473],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,207][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.4911, 0.0356, 0.1094, 0.0413, 0.1068, 0.1390, 0.0768],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,208][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.2064, 0.2704, 0.2165, 0.0158, 0.0407, 0.0916, 0.1587],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,210][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.1388, 0.0314, 0.0541, 0.3001, 0.2246, 0.1408, 0.1102],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,212][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.0366, 0.4236, 0.0265, 0.1631, 0.0574, 0.0328, 0.2599],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,213][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([9.5778e-01, 1.1439e-02, 2.5461e-02, 2.3751e-03, 1.3050e-03, 8.0234e-04,
        8.4117e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,214][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([2.3080e-04, 9.5972e-02, 3.0275e-01, 1.4073e-01, 3.8636e-01, 2.5252e-02,
        4.8707e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,216][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0052, 0.4356, 0.2631, 0.1278, 0.0337, 0.0477, 0.0869],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,218][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.0092, 0.2255, 0.0960, 0.0992, 0.2977, 0.0952, 0.1773],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,219][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([6.4596e-06, 3.9689e-01, 8.8687e-02, 1.4120e-01, 1.0887e-01, 3.9907e-02,
        2.2443e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,220][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.0258, 0.0142, 0.0835, 0.5665, 0.0727, 0.1394, 0.0979],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,221][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.2585, 0.1474, 0.0898, 0.0313, 0.1212, 0.1581, 0.1937],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,221][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.6303, 0.1081, 0.1485, 0.0160, 0.0066, 0.0174, 0.0507, 0.0224],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,221][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.1433, 0.0231, 0.0981, 0.0396, 0.1105, 0.1593, 0.1065, 0.3196],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,222][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2153, 0.2456, 0.1332, 0.0262, 0.0712, 0.0730, 0.1071, 0.1285],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,222][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.1895, 0.0243, 0.0381, 0.2565, 0.2361, 0.1169, 0.0871, 0.0515],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,223][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.0725, 0.3743, 0.0261, 0.1742, 0.0899, 0.0350, 0.1450, 0.0831],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,223][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([9.5273e-01, 1.6359e-02, 2.3232e-02, 2.7900e-03, 1.6004e-03, 2.0744e-03,
        5.0997e-04, 7.0540e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,223][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([2.3522e-04, 7.4491e-02, 2.4028e-01, 1.9279e-01, 3.9849e-01, 2.4119e-02,
        4.5296e-02, 2.4301e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,224][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.0059, 0.1208, 0.3900, 0.1880, 0.0624, 0.0446, 0.1167, 0.0715],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,224][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.0047, 0.1749, 0.0392, 0.0589, 0.1310, 0.0558, 0.0783, 0.4573],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,225][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([8.5814e-06, 2.4104e-01, 2.2534e-01, 1.3828e-01, 9.1864e-02, 3.0653e-02,
        1.9090e-01, 8.1920e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,227][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.0880, 0.0436, 0.0855, 0.2811, 0.0822, 0.1185, 0.1531, 0.1481],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,228][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.2033, 0.0571, 0.1358, 0.0365, 0.0717, 0.2178, 0.0811, 0.1967],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,230][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.3908, 0.1606, 0.1609, 0.0273, 0.0065, 0.0659, 0.0724, 0.0411, 0.0745],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,231][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.1955, 0.0230, 0.0771, 0.0263, 0.0967, 0.1338, 0.0528, 0.1842, 0.2105],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,233][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.3522, 0.1220, 0.1355, 0.0137, 0.0208, 0.0541, 0.1064, 0.0621, 0.1331],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,234][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0721, 0.0228, 0.0356, 0.3096, 0.1907, 0.1244, 0.0923, 0.0661, 0.0864],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,236][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0373, 0.3487, 0.0173, 0.1296, 0.0348, 0.0202, 0.1652, 0.0988, 0.1481],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,237][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [orum] are: tensor([8.4819e-01, 2.9014e-02, 1.0161e-01, 7.9266e-03, 2.8939e-03, 3.2357e-03,
        1.6654e-03, 6.6732e-04, 4.7936e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,238][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [orum] are: tensor([1.0989e-04, 9.1539e-02, 2.1486e-01, 1.3148e-01, 3.4596e-01, 1.6087e-02,
        3.1920e-02, 3.9137e-02, 1.2891e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,240][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0081, 0.2692, 0.1589, 0.0803, 0.0148, 0.0352, 0.0908, 0.0564, 0.2864],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,242][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0039, 0.1215, 0.0519, 0.0560, 0.1278, 0.0532, 0.0739, 0.4369, 0.0749],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,242][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [orum] are: tensor([6.4348e-06, 2.0037e-01, 5.9924e-02, 9.0053e-02, 4.1829e-02, 1.5092e-02,
        7.5932e-02, 7.9024e-02, 4.3777e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,245][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0077, 0.0041, 0.0151, 0.6100, 0.0518, 0.0599, 0.0329, 0.0668, 0.1516],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,246][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.2124, 0.0648, 0.1069, 0.0176, 0.0647, 0.1393, 0.0829, 0.1929, 0.1187],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,248][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.3776, 0.1886, 0.3018, 0.0257, 0.0046, 0.0150, 0.0305, 0.0188, 0.0246,
        0.0128], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,250][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.1267, 0.0192, 0.0497, 0.0253, 0.0780, 0.0988, 0.0601, 0.2467, 0.1906,
        0.1049], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,251][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.5111, 0.1032, 0.0665, 0.0065, 0.0206, 0.0364, 0.0482, 0.0386, 0.0643,
        0.1045], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,253][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.4071, 0.0130, 0.0212, 0.1727, 0.1209, 0.0878, 0.0512, 0.0281, 0.0554,
        0.0425], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,255][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.0467, 0.4114, 0.0176, 0.1081, 0.0346, 0.0205, 0.1386, 0.0678, 0.1358,
        0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,256][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ et] are: tensor([9.2654e-01, 2.9322e-02, 3.1793e-02, 1.3907e-03, 1.0743e-03, 1.3001e-03,
        1.3447e-03, 6.3995e-04, 2.1767e-03, 4.4195e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,258][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0003, 0.1074, 0.1950, 0.1242, 0.2736, 0.0346, 0.0335, 0.0722, 0.1038,
        0.0554], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,259][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.0269, 0.2876, 0.2836, 0.0613, 0.0204, 0.0256, 0.0636, 0.0370, 0.1283,
        0.0657], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,261][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.0101, 0.1357, 0.0441, 0.0353, 0.0818, 0.0433, 0.0759, 0.4153, 0.0817,
        0.0767], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,262][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ et] are: tensor([5.5446e-06, 2.4706e-01, 1.3048e-01, 1.3685e-01, 4.3137e-02, 3.4105e-02,
        1.0192e-01, 4.9493e-02, 1.4916e-01, 1.0779e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,264][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.1172, 0.0248, 0.0750, 0.1581, 0.0348, 0.1050, 0.0465, 0.1858, 0.1222,
        0.1304], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,265][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.1039, 0.0913, 0.1227, 0.0619, 0.1734, 0.0480, 0.1193, 0.0950, 0.0855,
        0.0989], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,267][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.4930, 0.0679, 0.1208, 0.0135, 0.0058, 0.0282, 0.0399, 0.0303, 0.0320,
        0.0190, 0.1495], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,269][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0178, 0.0101, 0.0315, 0.0159, 0.0626, 0.0866, 0.0758, 0.2645, 0.1797,
        0.0691, 0.1864], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,270][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.1132, 0.0771, 0.0674, 0.0174, 0.0380, 0.0955, 0.1086, 0.0488, 0.1653,
        0.2155, 0.0532], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,272][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.2371, 0.0095, 0.0245, 0.2161, 0.1922, 0.0911, 0.0610, 0.0312, 0.0597,
        0.0536, 0.0239], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,274][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.0987, 0.3557, 0.0080, 0.0953, 0.0297, 0.0183, 0.1592, 0.0793, 0.1426,
        0.0106, 0.0026], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,276][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.8045, 0.0425, 0.0864, 0.0063, 0.0039, 0.0050, 0.0043, 0.0019, 0.0068,
        0.0069, 0.0314], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,277][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([9.7865e-05, 7.6913e-02, 1.0775e-01, 1.6565e-01, 2.6177e-01, 3.0514e-02,
        7.3292e-02, 9.0490e-02, 8.0326e-02, 7.5347e-02, 3.7847e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,279][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.0122, 0.1622, 0.2453, 0.0686, 0.0212, 0.0717, 0.0698, 0.0865, 0.1489,
        0.0690, 0.0445], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,280][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.0040, 0.0826, 0.0444, 0.0564, 0.0822, 0.0658, 0.0547, 0.4323, 0.0541,
        0.0790, 0.0447], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,281][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([2.5275e-06, 1.0582e-01, 5.8982e-02, 1.1175e-01, 3.4775e-02, 3.1375e-02,
        1.1722e-01, 5.6857e-02, 3.4751e-01, 1.0656e-01, 2.9152e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,283][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.1084, 0.0113, 0.0332, 0.2070, 0.0340, 0.0463, 0.0298, 0.0460, 0.0691,
        0.0472, 0.3677], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,285][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.0413, 0.0369, 0.0834, 0.0483, 0.0524, 0.0745, 0.0813, 0.2206, 0.2072,
        0.0789, 0.0752], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,287][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.4219, 0.0830, 0.1929, 0.0179, 0.0045, 0.0302, 0.0417, 0.0224, 0.0430,
        0.0171, 0.1085, 0.0169], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,288][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.1166, 0.0151, 0.0393, 0.0128, 0.0554, 0.0773, 0.0379, 0.1208, 0.1363,
        0.0502, 0.2383, 0.1001], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,288][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.1511, 0.0672, 0.0851, 0.0115, 0.0269, 0.0614, 0.1286, 0.0496, 0.1260,
        0.1094, 0.0495, 0.1337], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,288][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0318, 0.0151, 0.0254, 0.2915, 0.1620, 0.1013, 0.0784, 0.0527, 0.0762,
        0.0758, 0.0351, 0.0548], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,289][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0357, 0.2637, 0.0141, 0.0924, 0.0254, 0.0181, 0.1360, 0.0845, 0.1450,
        0.0152, 0.0057, 0.1642], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,289][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [orum] are: tensor([9.5578e-01, 6.2359e-03, 2.4176e-02, 8.0677e-04, 6.2961e-04, 8.6109e-04,
        4.4050e-04, 1.3054e-04, 1.2689e-03, 8.2767e-04, 7.1524e-03, 1.6863e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,290][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [orum] are: tensor([1.2262e-04, 5.1411e-02, 1.5042e-01, 6.5740e-02, 2.7697e-01, 1.8919e-02,
        1.9748e-02, 4.2167e-02, 6.7650e-02, 3.7489e-02, 3.6909e-02, 2.3245e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,290][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.0421, 0.1832, 0.1660, 0.0505, 0.0067, 0.0169, 0.0572, 0.0203, 0.1462,
        0.0383, 0.0424, 0.2302], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,290][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0055, 0.1226, 0.0494, 0.0583, 0.0977, 0.0525, 0.0672, 0.3053, 0.0592,
        0.0818, 0.0476, 0.0531], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,291][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [orum] are: tensor([6.4877e-06, 9.1794e-02, 3.3512e-02, 4.9442e-02, 2.8716e-02, 1.4520e-02,
        4.9235e-02, 4.0123e-02, 2.0432e-01, 6.5519e-02, 1.0402e-02, 4.1241e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,292][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0425, 0.0032, 0.0117, 0.2553, 0.0109, 0.0152, 0.0085, 0.0166, 0.0300,
        0.0305, 0.1065, 0.4692], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,294][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1657, 0.0327, 0.0630, 0.0100, 0.0477, 0.0924, 0.0550, 0.1264, 0.0943,
        0.1324, 0.1114, 0.0690], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,295][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.4018, 0.1000, 0.1871, 0.0169, 0.0054, 0.0163, 0.0352, 0.0382, 0.0481,
        0.0121, 0.0931, 0.0278, 0.0179], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,297][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.1168, 0.0180, 0.0462, 0.0224, 0.0462, 0.0837, 0.0499, 0.1500, 0.1195,
        0.0404, 0.1609, 0.0802, 0.0658], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,298][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.2996, 0.0899, 0.0633, 0.0088, 0.0223, 0.0602, 0.0562, 0.0299, 0.0738,
        0.1413, 0.0491, 0.0822, 0.0236], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,300][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.2080, 0.0165, 0.0329, 0.1763, 0.1327, 0.0932, 0.0613, 0.0309, 0.0716,
        0.0496, 0.0294, 0.0420, 0.0556], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,301][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0361, 0.1870, 0.0182, 0.0906, 0.0380, 0.0208, 0.1212, 0.0622, 0.1017,
        0.0169, 0.0068, 0.1408, 0.1597], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,302][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ is] are: tensor([9.7522e-01, 5.3423e-03, 1.0235e-02, 6.3017e-04, 6.2836e-04, 7.3462e-04,
        2.6307e-04, 2.3009e-04, 4.7466e-04, 6.6767e-04, 4.4067e-03, 5.7105e-04,
        6.0049e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,303][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ is] are: tensor([2.9417e-04, 3.2615e-02, 9.0794e-02, 7.8680e-02, 3.9138e-01, 2.2931e-02,
        2.3905e-02, 4.5344e-02, 3.6291e-02, 2.7857e-02, 2.9361e-02, 1.6166e-01,
        5.8881e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,305][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0198, 0.1705, 0.1352, 0.0412, 0.0109, 0.0206, 0.0295, 0.0195, 0.0882,
        0.0485, 0.0296, 0.1293, 0.2573], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,307][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0104, 0.1377, 0.0357, 0.0395, 0.0763, 0.0309, 0.0535, 0.2509, 0.0554,
        0.0457, 0.0880, 0.0688, 0.1071], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,308][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ is] are: tensor([7.1271e-06, 1.7202e-01, 6.8734e-02, 6.4904e-02, 3.9345e-02, 1.7563e-02,
        6.8093e-02, 3.0406e-02, 1.2185e-01, 9.4101e-02, 7.7598e-03, 2.4730e-01,
        6.7909e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,309][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.1366, 0.0092, 0.0193, 0.0381, 0.0062, 0.0221, 0.0075, 0.0254, 0.0250,
        0.0167, 0.2774, 0.2198, 0.1968], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,312][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.2854, 0.0427, 0.0800, 0.0223, 0.0616, 0.0339, 0.0767, 0.1203, 0.0520,
        0.0545, 0.1106, 0.0412, 0.0187], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,313][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.6496, 0.0407, 0.1156, 0.0077, 0.0017, 0.0133, 0.0152, 0.0173, 0.0245,
        0.0111, 0.0710, 0.0133, 0.0072, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,315][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0817, 0.0213, 0.0431, 0.0211, 0.0450, 0.0726, 0.0468, 0.1205, 0.1265,
        0.0535, 0.1680, 0.0799, 0.0668, 0.0531], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,317][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.5334, 0.0558, 0.0413, 0.0035, 0.0134, 0.0350, 0.0425, 0.0168, 0.0495,
        0.0986, 0.0356, 0.0467, 0.0098, 0.0180], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,318][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.2126, 0.0099, 0.0228, 0.1743, 0.1311, 0.0896, 0.0513, 0.0288, 0.0640,
        0.0416, 0.0276, 0.0390, 0.0562, 0.0512], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,320][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0309, 0.1668, 0.0166, 0.0477, 0.0160, 0.0207, 0.0970, 0.0495, 0.1235,
        0.0106, 0.0105, 0.1764, 0.1259, 0.1077], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,321][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.8307e-01, 3.0567e-03, 7.5088e-03, 2.7657e-04, 3.4352e-04, 4.7664e-04,
        1.4403e-04, 8.8502e-05, 4.2592e-04, 2.9662e-04, 3.4012e-03, 4.7506e-04,
        1.7935e-04, 2.5239e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,322][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([1.4205e-04, 5.3314e-02, 1.0484e-01, 4.1976e-02, 1.7633e-01, 2.4620e-02,
        2.4292e-02, 4.9766e-02, 6.4450e-02, 3.0129e-02, 4.3890e-02, 2.8965e-01,
        6.2887e-02, 3.3710e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,324][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1111, 0.2139, 0.1450, 0.0328, 0.0043, 0.0202, 0.0255, 0.0079, 0.0866,
        0.0227, 0.0296, 0.1293, 0.0851, 0.0859], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,326][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0075, 0.1150, 0.0322, 0.0335, 0.0512, 0.0267, 0.0390, 0.2252, 0.0476,
        0.0423, 0.0596, 0.0476, 0.0716, 0.2010], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,327][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([2.9024e-06, 1.6017e-01, 5.5187e-02, 4.8528e-02, 3.3051e-02, 2.1223e-02,
        6.9522e-02, 3.5205e-02, 1.3994e-01, 5.1206e-02, 9.6599e-03, 2.4858e-01,
        5.3853e-02, 7.3868e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,329][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2639, 0.0058, 0.0124, 0.0233, 0.0044, 0.0092, 0.0029, 0.0078, 0.0074,
        0.0068, 0.0993, 0.0551, 0.1142, 0.3875], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,330][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1781, 0.0264, 0.0640, 0.0180, 0.0463, 0.0364, 0.1194, 0.1250, 0.0592,
        0.0866, 0.1769, 0.0406, 0.0128, 0.0102], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,332][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.2907, 0.0757, 0.1098, 0.0184, 0.0048, 0.0249, 0.0740, 0.0310, 0.0259,
        0.0089, 0.1603, 0.0148, 0.0096, 0.0097, 0.1414], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,334][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.1847, 0.0138, 0.0326, 0.0135, 0.0366, 0.0503, 0.0457, 0.1362, 0.0734,
        0.0549, 0.1971, 0.0357, 0.0619, 0.0263, 0.0373], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,336][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0480, 0.1049, 0.0460, 0.0108, 0.0237, 0.0448, 0.0866, 0.0270, 0.0959,
        0.1221, 0.0367, 0.0711, 0.0147, 0.0367, 0.2312], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,337][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.1621, 0.0097, 0.0221, 0.1743, 0.0946, 0.0777, 0.0535, 0.0320, 0.0539,
        0.0358, 0.0181, 0.0318, 0.0546, 0.0852, 0.0946], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,339][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.0462, 0.1914, 0.0150, 0.0742, 0.0272, 0.0145, 0.0881, 0.0350, 0.0829,
        0.0117, 0.0031, 0.1217, 0.1256, 0.1263, 0.0371], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,340][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ same] are: tensor([9.7880e-01, 5.7347e-03, 8.6156e-03, 5.5181e-04, 1.3785e-04, 1.4399e-04,
        1.7499e-04, 1.4578e-04, 2.4060e-04, 2.8234e-04, 2.3798e-03, 2.3390e-04,
        4.1397e-04, 4.6495e-04, 1.6828e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,342][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ same] are: tensor([1.0734e-04, 7.1271e-02, 2.0299e-01, 8.8289e-02, 2.6813e-01, 1.2022e-02,
        2.0376e-02, 1.3441e-02, 2.7755e-02, 1.1758e-02, 2.4766e-02, 1.1953e-01,
        3.7591e-02, 1.8359e-02, 8.3615e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,343][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.0893, 0.2467, 0.0353, 0.0412, 0.0030, 0.0125, 0.0050, 0.0067, 0.0303,
        0.0222, 0.0114, 0.0523, 0.1262, 0.1541, 0.1639], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,345][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.0050, 0.1100, 0.0298, 0.0456, 0.0504, 0.0330, 0.0548, 0.2249, 0.0519,
        0.0486, 0.0265, 0.0400, 0.0780, 0.1779, 0.0237], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,346][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ same] are: tensor([5.3629e-06, 1.1692e-01, 6.1225e-02, 3.5517e-02, 2.9232e-02, 1.4110e-02,
        7.7884e-02, 2.5948e-02, 1.7919e-01, 7.2622e-02, 6.1388e-03, 1.8136e-01,
        6.9699e-02, 6.4357e-02, 6.5788e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,347][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ same] are: tensor([2.6616e-02, 4.9515e-04, 5.5187e-04, 3.4405e-03, 1.5540e-04, 2.0913e-04,
        1.3278e-04, 3.3066e-04, 1.5736e-04, 2.1480e-04, 6.0340e-03, 2.5184e-03,
        1.7586e-02, 2.3458e-01, 7.0697e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,349][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.0964, 0.0352, 0.0556, 0.0468, 0.0421, 0.0949, 0.0540, 0.0708, 0.0893,
        0.0771, 0.1201, 0.0465, 0.0129, 0.0124, 0.1458], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,351][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.6009, 0.0214, 0.0695, 0.0044, 0.0015, 0.0091, 0.0114, 0.0114, 0.0116,
        0.0064, 0.0524, 0.0053, 0.0027, 0.0077, 0.1333, 0.0510],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,353][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.2484, 0.0104, 0.0344, 0.0120, 0.0329, 0.0659, 0.0299, 0.1271, 0.0578,
        0.0288, 0.1613, 0.0308, 0.0376, 0.0274, 0.0320, 0.0633],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,354][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.1919, 0.0595, 0.0614, 0.0063, 0.0131, 0.0330, 0.0514, 0.0197, 0.0574,
        0.0664, 0.0320, 0.0579, 0.0177, 0.0424, 0.2658, 0.0243],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,355][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.2026, 0.0097, 0.0222, 0.1148, 0.0803, 0.0679, 0.0397, 0.0224, 0.0427,
        0.0268, 0.0150, 0.0260, 0.0377, 0.0577, 0.1010, 0.1334],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,355][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0268, 0.1822, 0.0140, 0.0597, 0.0219, 0.0127, 0.0767, 0.0396, 0.0775,
        0.0115, 0.0048, 0.1196, 0.1042, 0.1401, 0.0503, 0.0584],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,356][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ as] are: tensor([9.8691e-01, 1.6464e-03, 3.9156e-03, 3.5346e-04, 2.2758e-04, 2.5981e-04,
        1.1064e-04, 1.4101e-04, 2.8957e-04, 2.8533e-04, 2.4974e-03, 2.7740e-04,
        2.7551e-04, 3.6421e-04, 1.5214e-03, 9.2018e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,356][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ as] are: tensor([1.7786e-04, 4.0196e-02, 5.2265e-02, 4.4274e-02, 1.7746e-01, 1.2328e-02,
        1.5520e-02, 1.8061e-02, 3.1116e-02, 1.4254e-02, 2.8413e-02, 1.0291e-01,
        3.3411e-02, 3.0571e-02, 5.4813e-02, 3.4423e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,357][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.1183, 0.0546, 0.0243, 0.0096, 0.0019, 0.0121, 0.0081, 0.0058, 0.0199,
        0.0074, 0.0171, 0.0353, 0.0543, 0.0519, 0.3809, 0.1986],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,357][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0096, 0.1029, 0.0287, 0.0421, 0.0408, 0.0262, 0.0516, 0.2127, 0.0503,
        0.0338, 0.0304, 0.0374, 0.0546, 0.1889, 0.0331, 0.0569],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,358][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ as] are: tensor([3.2653e-06, 6.9633e-02, 3.3555e-02, 3.1120e-02, 2.2958e-02, 1.4116e-02,
        5.4259e-02, 5.1101e-02, 1.5287e-01, 7.4453e-02, 7.2197e-03, 2.1108e-01,
        4.7292e-02, 6.2686e-02, 4.7121e-02, 1.2053e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,359][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ as] are: tensor([1.3443e-02, 1.5615e-04, 1.2497e-04, 5.0378e-04, 2.7874e-05, 1.5151e-04,
        1.8899e-05, 1.0250e-04, 1.2731e-04, 1.0837e-04, 2.2130e-03, 1.5333e-03,
        3.0260e-03, 2.7927e-02, 3.1184e-01, 6.3869e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,361][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.2385, 0.0248, 0.0495, 0.0184, 0.0694, 0.0399, 0.0669, 0.0560, 0.0379,
        0.0998, 0.0861, 0.0230, 0.0125, 0.0154, 0.1216, 0.0403],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,362][circuit_model.py][line:1532][INFO] ##9-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.6829, 0.0177, 0.0495, 0.0035, 0.0008, 0.0059, 0.0080, 0.0094, 0.0103,
        0.0054, 0.0366, 0.0060, 0.0038, 0.0062, 0.0988, 0.0418, 0.0134],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,364][circuit_model.py][line:1535][INFO] ##9-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.1327, 0.0136, 0.0301, 0.0142, 0.0346, 0.0523, 0.0327, 0.0894, 0.0803,
        0.0374, 0.1435, 0.0480, 0.0497, 0.0408, 0.0463, 0.1018, 0.0527],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,366][circuit_model.py][line:1538][INFO] ##9-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.4148, 0.0476, 0.0369, 0.0032, 0.0118, 0.0260, 0.0354, 0.0149, 0.0400,
        0.0677, 0.0257, 0.0377, 0.0088, 0.0135, 0.1762, 0.0200, 0.0198],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,368][circuit_model.py][line:1541][INFO] ##9-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.1733, 0.0054, 0.0140, 0.1183, 0.0875, 0.0718, 0.0299, 0.0191, 0.0407,
        0.0246, 0.0151, 0.0245, 0.0398, 0.0464, 0.0824, 0.1300, 0.0771],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,369][circuit_model.py][line:1544][INFO] ##9-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0230, 0.1249, 0.0127, 0.0395, 0.0124, 0.0158, 0.0734, 0.0375, 0.0859,
        0.0090, 0.0073, 0.1311, 0.0904, 0.0863, 0.0617, 0.0682, 0.1208],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,370][circuit_model.py][line:1547][INFO] ##9-th layer ##Weight##: The head6 weight for token [ the] are: tensor([9.9327e-01, 8.5669e-04, 2.8708e-03, 1.1102e-04, 1.0845e-04, 1.3303e-04,
        4.0000e-05, 2.8444e-05, 1.4508e-04, 9.3296e-05, 1.2149e-03, 1.4794e-04,
        7.0596e-05, 1.0450e-04, 4.6852e-04, 1.5768e-04, 1.7844e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,371][circuit_model.py][line:1550][INFO] ##9-th layer ##Weight##: The head7 weight for token [ the] are: tensor([1.5589e-04, 2.7560e-02, 4.2960e-02, 2.8398e-02, 9.8503e-02, 1.4547e-02,
        1.3339e-02, 2.9592e-02, 3.7058e-02, 1.3058e-02, 2.5098e-02, 1.5460e-01,
        3.4635e-02, 2.7827e-02, 7.6357e-02, 2.9170e-01, 8.4603e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,373][circuit_model.py][line:1553][INFO] ##9-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.1655, 0.0564, 0.0225, 0.0079, 0.0007, 0.0062, 0.0046, 0.0016, 0.0163,
        0.0046, 0.0084, 0.0259, 0.0212, 0.0245, 0.3137, 0.1506, 0.1694],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,375][circuit_model.py][line:1556][INFO] ##9-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0097, 0.0869, 0.0247, 0.0309, 0.0371, 0.0248, 0.0416, 0.2001, 0.0472,
        0.0339, 0.0354, 0.0428, 0.0548, 0.1660, 0.0253, 0.0399, 0.0989],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,376][circuit_model.py][line:1559][INFO] ##9-th layer ##Weight##: The head10 weight for token [ the] are: tensor([4.8750e-06, 1.0911e-01, 3.0250e-02, 4.4699e-02, 3.2245e-02, 1.9266e-02,
        6.5895e-02, 3.5202e-02, 1.2772e-01, 4.8489e-02, 6.9023e-03, 1.7921e-01,
        4.4729e-02, 6.9118e-02, 5.7399e-02, 5.9158e-02, 7.0614e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,377][circuit_model.py][line:1562][INFO] ##9-th layer ##Weight##: The head11 weight for token [ the] are: tensor([6.8019e-02, 1.0930e-04, 1.8399e-04, 4.6361e-04, 5.6731e-05, 1.3631e-04,
        2.7294e-05, 8.8065e-05, 1.0629e-04, 1.0256e-04, 3.2856e-03, 9.6657e-04,
        4.9565e-03, 1.8555e-02, 1.3587e-01, 4.7758e-01, 2.8949e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,379][circuit_model.py][line:1565][INFO] ##9-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.1780, 0.0156, 0.0516, 0.0135, 0.0321, 0.0302, 0.0775, 0.0974, 0.0445,
        0.0789, 0.1534, 0.0299, 0.0087, 0.0068, 0.1469, 0.0278, 0.0072],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,434][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:31,436][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,437][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,439][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,440][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,441][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,442][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,444][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,445][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,447][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,448][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,449][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,451][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,452][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.3550, 0.6450], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,453][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.2183, 0.7817], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,456][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.7382, 0.2618], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,457][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.8683, 0.1317], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,458][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.0381, 0.9619], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,460][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.9720, 0.0280], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,462][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([5.1898e-04, 9.9948e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,463][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.2180, 0.7820], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,464][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.7970, 0.2030], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,466][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([2.2487e-04, 9.9978e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,467][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.0678, 0.9322], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,469][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.0950, 0.9050], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,470][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.2040, 0.4301, 0.3659], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,472][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0273, 0.2559, 0.7168], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,474][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.4456, 0.3419, 0.2125], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,475][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.7492, 0.0445, 0.2063], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,477][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0408, 0.5348, 0.4244], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,479][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.8729, 0.0389, 0.0882], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,480][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([1.1172e-04, 2.5505e-01, 7.4483e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,481][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.1428, 0.5804, 0.2768], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,483][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.4545, 0.2744, 0.2711], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,484][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([5.2796e-05, 7.9090e-01, 2.0905e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,486][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0242, 0.3298, 0.6460], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,487][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.1824, 0.4382, 0.3794], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,487][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.5567, 0.1792, 0.2421, 0.0220], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,488][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0611, 0.1713, 0.6151, 0.1525], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,488][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.4763, 0.3037, 0.1977, 0.0223], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,488][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.8838, 0.0092, 0.1036, 0.0035], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,489][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.2778, 0.4057, 0.2486, 0.0679], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,489][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.9662, 0.0118, 0.0193, 0.0027], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,489][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([3.2319e-04, 1.2136e-01, 6.8068e-01, 1.9764e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,490][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0686, 0.2818, 0.5793, 0.0702], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,490][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.4743, 0.3221, 0.1911, 0.0124], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,490][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([2.9607e-05, 7.0591e-01, 1.2161e-01, 1.7245e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,491][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.0545, 0.2032, 0.5595, 0.1829], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,491][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.1058, 0.1368, 0.0774, 0.6800], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,493][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.1867, 0.2338, 0.5316, 0.0358, 0.0121], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,495][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0849, 0.0863, 0.3877, 0.0972, 0.3439], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,496][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.0894, 0.3068, 0.5127, 0.0440, 0.0472], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,498][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.3273, 0.0338, 0.5579, 0.0501, 0.0308], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,499][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.2127, 0.4016, 0.1407, 0.1833, 0.0617], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,501][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.8763, 0.0184, 0.0984, 0.0032, 0.0037], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,502][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([2.9836e-04, 1.8726e-01, 4.1453e-01, 1.7909e-01, 2.1883e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,504][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.0238, 0.3134, 0.4928, 0.1518, 0.0181], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,505][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.1176, 0.4386, 0.3126, 0.0617, 0.0695], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,507][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([4.1458e-05, 6.1472e-01, 1.7286e-01, 1.6601e-01, 4.6373e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,508][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.0292, 0.1773, 0.4616, 0.2523, 0.0795], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,509][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0561, 0.1338, 0.1285, 0.4858, 0.1957], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,512][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.5972, 0.1610, 0.1670, 0.0210, 0.0097, 0.0442], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,513][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.1687, 0.0623, 0.2727, 0.0803, 0.2273, 0.1887], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,514][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.2229, 0.3188, 0.2380, 0.0522, 0.0676, 0.1005], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,516][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.5037, 0.0241, 0.1086, 0.0635, 0.0932, 0.2070], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,518][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.2848, 0.1683, 0.2801, 0.1314, 0.0655, 0.0699], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,520][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.8796, 0.0174, 0.0752, 0.0148, 0.0059, 0.0071], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,520][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([9.8526e-05, 1.1612e-01, 2.3127e-01, 3.1351e-01, 2.9587e-01, 4.3123e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,523][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.0369, 0.1910, 0.4571, 0.2441, 0.0494, 0.0214], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,524][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.3294, 0.2257, 0.1597, 0.0351, 0.1155, 0.1346], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,525][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([2.0666e-04, 1.9784e-01, 1.4156e-01, 1.9645e-01, 6.5581e-02, 3.9836e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,526][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.0087, 0.0225, 0.0871, 0.7591, 0.0569, 0.0658], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,529][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.0212, 0.0938, 0.0778, 0.4327, 0.0976, 0.2770], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,530][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.5101, 0.1654, 0.1922, 0.0313, 0.0057, 0.0481, 0.0473],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,531][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.0939, 0.1495, 0.2525, 0.0975, 0.2661, 0.0932, 0.0473],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,534][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.6108, 0.0938, 0.1212, 0.0280, 0.0404, 0.0623, 0.0435],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,535][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.5893, 0.0052, 0.0562, 0.0140, 0.0242, 0.2470, 0.0641],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,537][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.7233, 0.0770, 0.0634, 0.0309, 0.0156, 0.0339, 0.0559],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,538][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([9.5778e-01, 1.1439e-02, 2.5461e-02, 2.3751e-03, 1.3050e-03, 8.0234e-04,
        8.4117e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,539][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([2.3080e-04, 9.5972e-02, 3.0275e-01, 1.4073e-01, 3.8636e-01, 2.5252e-02,
        4.8707e-02], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,541][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0124, 0.3953, 0.3196, 0.1634, 0.0346, 0.0225, 0.0522],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,542][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.4600, 0.1155, 0.0917, 0.0381, 0.1422, 0.0596, 0.0930],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,544][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.0004, 0.3712, 0.0782, 0.1111, 0.0745, 0.0828, 0.2817],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,546][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.0258, 0.0142, 0.0835, 0.5665, 0.0727, 0.1394, 0.0979],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,547][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.0174, 0.1771, 0.0787, 0.2687, 0.0867, 0.1933, 0.1781],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,548][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.6303, 0.1081, 0.1485, 0.0160, 0.0066, 0.0174, 0.0507, 0.0224],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,551][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0583, 0.0215, 0.1991, 0.0543, 0.2415, 0.1119, 0.0360, 0.2773],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,552][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.7862, 0.0312, 0.0868, 0.0086, 0.0202, 0.0296, 0.0163, 0.0210],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,554][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.7583, 0.0091, 0.0495, 0.0060, 0.0121, 0.0787, 0.0188, 0.0676],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,554][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.8333, 0.0402, 0.0355, 0.0341, 0.0281, 0.0120, 0.0086, 0.0081],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,555][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([9.5273e-01, 1.6359e-02, 2.3232e-02, 2.7900e-03, 1.6004e-03, 2.0744e-03,
        5.0997e-04, 7.0540e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,555][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([2.3522e-04, 7.4491e-02, 2.4028e-01, 1.9279e-01, 3.9849e-01, 2.4119e-02,
        4.5296e-02, 2.4301e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,555][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.0155, 0.1347, 0.4339, 0.2149, 0.0563, 0.0250, 0.0782, 0.0416],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,556][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.3218, 0.2269, 0.0570, 0.0487, 0.0910, 0.0765, 0.0436, 0.1346],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,556][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([2.1006e-04, 2.0614e-01, 2.3745e-01, 1.0317e-01, 6.1062e-02, 4.9295e-02,
        2.8020e-01, 6.2478e-02], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,557][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.0880, 0.0436, 0.0855, 0.2811, 0.0822, 0.1185, 0.1531, 0.1481],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,557][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.0123, 0.0882, 0.0606, 0.2096, 0.1071, 0.0876, 0.1124, 0.3223],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,558][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.3908, 0.1606, 0.1609, 0.0273, 0.0065, 0.0659, 0.0724, 0.0411, 0.0745],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,560][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0198, 0.0330, 0.1231, 0.0405, 0.2669, 0.0764, 0.0301, 0.1477, 0.2625],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,561][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.6272, 0.1105, 0.0705, 0.0227, 0.0358, 0.0248, 0.0156, 0.0577, 0.0352],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,563][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.4067, 0.0027, 0.0205, 0.0369, 0.0240, 0.1974, 0.0380, 0.2493, 0.0245],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,564][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.4373, 0.1066, 0.1327, 0.0719, 0.0156, 0.0208, 0.0794, 0.0439, 0.0917],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,565][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([8.4819e-01, 2.9014e-02, 1.0161e-01, 7.9266e-03, 2.8939e-03, 3.2357e-03,
        1.6654e-03, 6.6732e-04, 4.7936e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,566][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([1.0989e-04, 9.1539e-02, 2.1486e-01, 1.3148e-01, 3.4596e-01, 1.6087e-02,
        3.1920e-02, 3.9137e-02, 1.2891e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,568][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.0202, 0.3031, 0.2207, 0.1222, 0.0165, 0.0231, 0.0675, 0.0412, 0.1855],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,569][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.5117, 0.0938, 0.0799, 0.0341, 0.0715, 0.0508, 0.0292, 0.1187, 0.0102],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,570][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([2.5245e-04, 1.5495e-01, 6.1044e-02, 7.0153e-02, 2.2477e-02, 2.1426e-02,
        6.7104e-02, 5.9535e-02, 5.4306e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,572][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0077, 0.0041, 0.0151, 0.6100, 0.0518, 0.0599, 0.0329, 0.0668, 0.1516],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,574][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0045, 0.0583, 0.0420, 0.1976, 0.0799, 0.1214, 0.1453, 0.1197, 0.2314],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,575][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.3776, 0.1886, 0.3018, 0.0257, 0.0046, 0.0150, 0.0305, 0.0188, 0.0246,
        0.0128], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,578][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0990, 0.0364, 0.0998, 0.0319, 0.1798, 0.0560, 0.0229, 0.2275, 0.1442,
        0.1024], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,579][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.4958, 0.0665, 0.1479, 0.0142, 0.0137, 0.0522, 0.0486, 0.0533, 0.0644,
        0.0434], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,580][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([9.4071e-01, 8.7815e-04, 5.9681e-03, 1.2680e-03, 8.6544e-04, 1.8545e-02,
        1.5809e-03, 8.2076e-03, 2.6469e-03, 1.9333e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,582][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.3481, 0.1313, 0.1189, 0.0501, 0.0154, 0.0356, 0.0601, 0.0507, 0.1097,
        0.0800], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,583][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([9.2654e-01, 2.9322e-02, 3.1793e-02, 1.3907e-03, 1.0743e-03, 1.3001e-03,
        1.3447e-03, 6.3995e-04, 2.1767e-03, 4.4195e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,585][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0003, 0.1074, 0.1950, 0.1242, 0.2736, 0.0346, 0.0335, 0.0722, 0.1038,
        0.0554], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,586][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.0836, 0.2617, 0.3295, 0.0800, 0.0208, 0.0158, 0.0427, 0.0261, 0.0752,
        0.0644], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,588][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.5971, 0.0886, 0.0631, 0.0167, 0.0310, 0.0346, 0.0355, 0.0589, 0.0141,
        0.0604], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,589][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([1.3198e-04, 2.3416e-01, 1.1819e-01, 1.1038e-01, 2.2874e-02, 7.9565e-02,
        1.3805e-01, 4.5360e-02, 1.6149e-01, 8.9800e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,591][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.1172, 0.0248, 0.0750, 0.1581, 0.0348, 0.1050, 0.0465, 0.1858, 0.1222,
        0.1304], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,592][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0896, 0.0586, 0.0146, 0.1688, 0.1089, 0.0823, 0.0367, 0.2887, 0.1273,
        0.0246], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,595][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.4930, 0.0679, 0.1208, 0.0135, 0.0058, 0.0282, 0.0399, 0.0303, 0.0320,
        0.0190, 0.1495], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,596][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0076, 0.0086, 0.0495, 0.0187, 0.1291, 0.0620, 0.0326, 0.3092, 0.1492,
        0.0521, 0.1814], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,598][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.2133, 0.0399, 0.0871, 0.0135, 0.0175, 0.0394, 0.0154, 0.0331, 0.0411,
        0.0490, 0.4507], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,600][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.7378, 0.0025, 0.0188, 0.0053, 0.0054, 0.0203, 0.0048, 0.0156, 0.0042,
        0.0366, 0.1488], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,601][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.3658, 0.0482, 0.0653, 0.0576, 0.0324, 0.0445, 0.0936, 0.0618, 0.0509,
        0.0341, 0.1460], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,603][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.8045, 0.0425, 0.0864, 0.0063, 0.0039, 0.0050, 0.0043, 0.0019, 0.0068,
        0.0069, 0.0314], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,604][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([9.7865e-05, 7.6913e-02, 1.0775e-01, 1.6565e-01, 2.6177e-01, 3.0514e-02,
        7.3292e-02, 9.0490e-02, 8.0326e-02, 7.5347e-02, 3.7847e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,606][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.0247, 0.1680, 0.3016, 0.0953, 0.0238, 0.0541, 0.0524, 0.0675, 0.1005,
        0.0697, 0.0425], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,608][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1565, 0.0548, 0.1123, 0.0599, 0.0585, 0.1391, 0.0342, 0.1536, 0.0192,
        0.0742, 0.1378], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,609][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([9.0678e-06, 4.5737e-02, 3.2767e-02, 6.6047e-02, 1.5839e-02, 5.4682e-02,
        1.6562e-01, 4.1732e-02, 4.7752e-01, 6.8111e-02, 3.1931e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,610][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.1084, 0.0113, 0.0332, 0.2070, 0.0340, 0.0463, 0.0298, 0.0460, 0.0691,
        0.0472, 0.3677], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,612][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0098, 0.0498, 0.0453, 0.2380, 0.1028, 0.0916, 0.0983, 0.1037, 0.1745,
        0.0315, 0.0546], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,614][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.4219, 0.0830, 0.1929, 0.0179, 0.0045, 0.0302, 0.0417, 0.0224, 0.0430,
        0.0171, 0.1085, 0.0169], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,616][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0265, 0.0222, 0.0643, 0.0176, 0.1395, 0.0538, 0.0204, 0.1086, 0.1821,
        0.0387, 0.1773, 0.1489], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,618][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.4085, 0.0712, 0.0727, 0.0163, 0.0141, 0.0209, 0.0103, 0.0242, 0.0233,
        0.0169, 0.2869, 0.0348], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,619][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([5.6995e-01, 4.9328e-04, 4.2237e-03, 8.8853e-03, 3.0533e-03, 2.2752e-02,
        3.7916e-03, 2.2307e-02, 3.1148e-03, 5.8090e-02, 2.7349e-01, 2.9846e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,620][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.7577, 0.0226, 0.0351, 0.0125, 0.0031, 0.0052, 0.0236, 0.0100, 0.0327,
        0.0068, 0.0580, 0.0327], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,621][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([9.5578e-01, 6.2359e-03, 2.4176e-02, 8.0677e-04, 6.2961e-04, 8.6109e-04,
        4.4050e-04, 1.3054e-04, 1.2689e-03, 8.2767e-04, 7.1524e-03, 1.6863e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,621][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([1.2262e-04, 5.1411e-02, 1.5042e-01, 6.5740e-02, 2.7697e-01, 1.8919e-02,
        1.9748e-02, 4.2167e-02, 6.7650e-02, 3.7489e-02, 3.6909e-02, 2.3245e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,621][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.1180, 0.2031, 0.2279, 0.0789, 0.0080, 0.0119, 0.0427, 0.0172, 0.0929,
        0.0440, 0.0413, 0.1140], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,622][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.4736, 0.0589, 0.0761, 0.0283, 0.0309, 0.0496, 0.0188, 0.0498, 0.0058,
        0.0485, 0.1447, 0.0149], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,622][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([1.5853e-04, 4.6019e-02, 2.1736e-02, 2.5590e-02, 1.0880e-02, 2.1954e-02,
        3.3902e-02, 2.6178e-02, 1.9284e-01, 3.7217e-02, 1.9743e-02, 5.6378e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,623][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0425, 0.0032, 0.0117, 0.2553, 0.0109, 0.0152, 0.0085, 0.0166, 0.0300,
        0.0305, 0.1065, 0.4692], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,623][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0242, 0.0261, 0.0399, 0.1488, 0.0655, 0.1084, 0.0820, 0.0761, 0.1892,
        0.0140, 0.0864, 0.1395], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:31,624][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.4018, 0.1000, 0.1871, 0.0169, 0.0054, 0.0163, 0.0352, 0.0382, 0.0481,
        0.0121, 0.0931, 0.0278, 0.0179], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,624][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0832, 0.0332, 0.0980, 0.0328, 0.0865, 0.0733, 0.0176, 0.1034, 0.0947,
        0.0252, 0.1421, 0.0857, 0.1242], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,626][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.3723, 0.0293, 0.0689, 0.0051, 0.0057, 0.0158, 0.0092, 0.0141, 0.0117,
        0.0128, 0.4054, 0.0262, 0.0236], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,627][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([8.7746e-01, 5.4198e-04, 7.3788e-03, 2.7908e-04, 2.6071e-04, 4.1878e-03,
        2.6991e-04, 1.4924e-03, 7.5394e-04, 2.3141e-03, 9.6732e-02, 2.8628e-03,
        5.4706e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,628][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.3896, 0.0310, 0.0706, 0.0264, 0.0156, 0.0259, 0.0378, 0.0318, 0.0397,
        0.0234, 0.1712, 0.0666, 0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,629][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([9.7522e-01, 5.3423e-03, 1.0235e-02, 6.3017e-04, 6.2836e-04, 7.3462e-04,
        2.6307e-04, 2.3009e-04, 4.7466e-04, 6.6767e-04, 4.4067e-03, 5.7105e-04,
        6.0049e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,630][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([2.9417e-04, 3.2615e-02, 9.0794e-02, 7.8680e-02, 3.9138e-01, 2.2931e-02,
        2.3905e-02, 4.5344e-02, 3.6291e-02, 2.7857e-02, 2.9361e-02, 1.6166e-01,
        5.8881e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,631][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0501, 0.1666, 0.1789, 0.0596, 0.0109, 0.0141, 0.0244, 0.0162, 0.0624,
        0.0476, 0.0288, 0.0755, 0.2649], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,634][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.3059, 0.0829, 0.0609, 0.0201, 0.0281, 0.0236, 0.0198, 0.0358, 0.0098,
        0.0304, 0.2743, 0.0423, 0.0660], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,635][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([2.1281e-04, 1.4357e-01, 5.1694e-02, 4.5175e-02, 2.0766e-02, 3.1340e-02,
        7.3923e-02, 3.0060e-02, 1.3415e-01, 7.1636e-02, 1.5868e-02, 2.8302e-01,
        9.8579e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,636][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.1366, 0.0092, 0.0193, 0.0381, 0.0062, 0.0221, 0.0075, 0.0254, 0.0250,
        0.0167, 0.2774, 0.2198, 0.1968], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,638][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0223, 0.0524, 0.0195, 0.3380, 0.0753, 0.1414, 0.0323, 0.1194, 0.0587,
        0.0065, 0.0542, 0.0438, 0.0363], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:31,640][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.6496, 0.0407, 0.1156, 0.0077, 0.0017, 0.0133, 0.0152, 0.0173, 0.0245,
        0.0111, 0.0710, 0.0133, 0.0072, 0.0117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,642][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0302, 0.0466, 0.0781, 0.0290, 0.0546, 0.0460, 0.0143, 0.0816, 0.1122,
        0.0412, 0.1625, 0.0779, 0.1146, 0.1112], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,643][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.3818, 0.0270, 0.0719, 0.0043, 0.0038, 0.0127, 0.0089, 0.0107, 0.0154,
        0.0082, 0.3762, 0.0362, 0.0121, 0.0309], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,645][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([9.0071e-01, 2.4411e-04, 3.6648e-03, 2.9846e-04, 3.9558e-04, 3.5397e-03,
        2.1083e-04, 1.4034e-03, 5.5081e-04, 1.9718e-03, 7.2187e-02, 1.8930e-03,
        5.1404e-03, 7.7923e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,646][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.6350, 0.0293, 0.0213, 0.0035, 0.0012, 0.0119, 0.0224, 0.0127, 0.0387,
        0.0042, 0.1233, 0.0520, 0.0292, 0.0152], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,647][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.8307e-01, 3.0567e-03, 7.5088e-03, 2.7657e-04, 3.4352e-04, 4.7664e-04,
        1.4403e-04, 8.8502e-05, 4.2592e-04, 2.9662e-04, 3.4012e-03, 4.7506e-04,
        1.7935e-04, 2.5239e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,648][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([1.4205e-04, 5.3314e-02, 1.0484e-01, 4.1976e-02, 1.7633e-01, 2.4620e-02,
        2.4292e-02, 4.9766e-02, 6.4450e-02, 3.0129e-02, 4.3890e-02, 2.8965e-01,
        6.2887e-02, 3.3710e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,650][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.2335, 0.1673, 0.1671, 0.0385, 0.0042, 0.0132, 0.0184, 0.0070, 0.0521,
        0.0235, 0.0277, 0.0670, 0.0925, 0.0880], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,652][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.1985, 0.0733, 0.0469, 0.0168, 0.0167, 0.0237, 0.0137, 0.0336, 0.0094,
        0.0337, 0.2560, 0.0379, 0.0547, 0.1853], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,653][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([8.0016e-05, 1.2102e-01, 3.7603e-02, 2.9281e-02, 1.5672e-02, 5.2108e-02,
        7.0215e-02, 2.9834e-02, 1.8688e-01, 3.4921e-02, 1.8075e-02, 2.9750e-01,
        6.5154e-02, 4.1654e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,655][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([0.2639, 0.0058, 0.0124, 0.0233, 0.0044, 0.0092, 0.0029, 0.0078, 0.0074,
        0.0068, 0.0993, 0.0551, 0.1142, 0.3875], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,657][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0227, 0.0346, 0.0125, 0.2638, 0.0481, 0.1527, 0.0283, 0.0841, 0.0998,
        0.0066, 0.0834, 0.0617, 0.0217, 0.0801], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:31,658][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.2907, 0.0757, 0.1098, 0.0184, 0.0048, 0.0249, 0.0740, 0.0310, 0.0259,
        0.0089, 0.1603, 0.0148, 0.0096, 0.0097, 0.1414], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,660][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.1457, 0.0326, 0.0610, 0.0196, 0.0580, 0.0220, 0.0222, 0.0921, 0.0738,
        0.0452, 0.1690, 0.0363, 0.1066, 0.0365, 0.0793], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,662][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.3998, 0.0522, 0.0403, 0.0039, 0.0050, 0.0095, 0.0058, 0.0074, 0.0044,
        0.0089, 0.3180, 0.0095, 0.0118, 0.0218, 0.1017], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,663][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([9.2498e-01, 6.7336e-05, 7.1389e-04, 1.4610e-04, 1.9812e-05, 1.8364e-04,
        3.4307e-05, 2.4526e-04, 3.2836e-05, 4.5548e-04, 1.6062e-02, 2.4462e-04,
        3.3258e-03, 3.4503e-02, 1.8990e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,665][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.5702, 0.0292, 0.0767, 0.0264, 0.0108, 0.0092, 0.0175, 0.0066, 0.0186,
        0.0067, 0.0828, 0.0402, 0.0425, 0.0379, 0.0246], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,666][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([9.7880e-01, 5.7347e-03, 8.6156e-03, 5.5181e-04, 1.3785e-04, 1.4399e-04,
        1.7499e-04, 1.4578e-04, 2.4060e-04, 2.8234e-04, 2.3798e-03, 2.3390e-04,
        4.1397e-04, 4.6495e-04, 1.6828e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,667][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([1.0734e-04, 7.1271e-02, 2.0299e-01, 8.8289e-02, 2.6813e-01, 1.2022e-02,
        2.0376e-02, 1.3441e-02, 2.7755e-02, 1.1758e-02, 2.4766e-02, 1.1953e-01,
        3.7591e-02, 1.8359e-02, 8.3615e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,669][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.2421, 0.1743, 0.0469, 0.0478, 0.0032, 0.0071, 0.0037, 0.0053, 0.0159,
        0.0222, 0.0117, 0.0233, 0.1367, 0.1489, 0.1107], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,670][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.2836, 0.1091, 0.0252, 0.0179, 0.0127, 0.0181, 0.0115, 0.0296, 0.0036,
        0.0289, 0.1234, 0.0115, 0.0533, 0.2264, 0.0451], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,672][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([6.2820e-05, 7.2140e-02, 4.5507e-02, 1.8231e-02, 1.0770e-02, 1.3729e-02,
        8.2477e-02, 1.2966e-02, 2.1744e-01, 4.9990e-02, 7.7950e-03, 1.8438e-01,
        9.7224e-02, 3.5388e-02, 1.5191e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,673][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([2.6616e-02, 4.9515e-04, 5.5187e-04, 3.4405e-03, 1.5540e-04, 2.0913e-04,
        1.3278e-04, 3.3066e-04, 1.5736e-04, 2.1480e-04, 6.0340e-03, 2.5184e-03,
        1.7586e-02, 2.3458e-01, 7.0697e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,675][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.0093, 0.0599, 0.0233, 0.4306, 0.0393, 0.0486, 0.0355, 0.0601, 0.0462,
        0.0063, 0.0501, 0.0418, 0.0253, 0.0918, 0.0317], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:31,676][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.6009, 0.0214, 0.0695, 0.0044, 0.0015, 0.0091, 0.0114, 0.0114, 0.0116,
        0.0064, 0.0524, 0.0053, 0.0027, 0.0077, 0.1333, 0.0510],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,678][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0639, 0.0231, 0.0585, 0.0162, 0.0470, 0.0497, 0.0128, 0.1203, 0.0513,
        0.0169, 0.1490, 0.0342, 0.0506, 0.0446, 0.0630, 0.1990],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,680][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.4943, 0.0063, 0.0105, 0.0006, 0.0012, 0.0033, 0.0022, 0.0052, 0.0030,
        0.0041, 0.1731, 0.0061, 0.0028, 0.0107, 0.1576, 0.1192],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,681][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([9.3009e-01, 1.8011e-05, 3.3591e-04, 2.6327e-05, 1.8338e-05, 2.2068e-04,
        1.5724e-05, 9.6212e-05, 2.6027e-05, 2.0887e-04, 1.0852e-02, 1.4176e-04,
        8.1125e-04, 4.1227e-03, 2.4226e-02, 2.8789e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,683][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.4085, 0.0688, 0.0393, 0.0145, 0.0067, 0.0083, 0.0139, 0.0129, 0.0250,
        0.0080, 0.1514, 0.0533, 0.0330, 0.0449, 0.0563, 0.0553],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,684][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([9.8691e-01, 1.6464e-03, 3.9156e-03, 3.5346e-04, 2.2758e-04, 2.5981e-04,
        1.1064e-04, 1.4101e-04, 2.8957e-04, 2.8533e-04, 2.4974e-03, 2.7740e-04,
        2.7551e-04, 3.6421e-04, 1.5214e-03, 9.2018e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,685][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([1.7786e-04, 4.0196e-02, 5.2265e-02, 4.4274e-02, 1.7746e-01, 1.2328e-02,
        1.5520e-02, 1.8061e-02, 3.1116e-02, 1.4254e-02, 2.8413e-02, 1.0291e-01,
        3.3411e-02, 3.0571e-02, 5.4813e-02, 3.4423e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,687][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.2062, 0.0490, 0.0343, 0.0125, 0.0019, 0.0074, 0.0056, 0.0048, 0.0115,
        0.0080, 0.0153, 0.0185, 0.0604, 0.0571, 0.2707, 0.2368],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,688][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.5791, 0.0324, 0.0135, 0.0095, 0.0048, 0.0059, 0.0057, 0.0097, 0.0019,
        0.0081, 0.0920, 0.0069, 0.0187, 0.1315, 0.0352, 0.0452],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,688][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([6.3415e-05, 3.3659e-02, 2.0850e-02, 1.4788e-02, 7.6021e-03, 1.3891e-02,
        3.6728e-02, 2.5339e-02, 1.4755e-01, 3.0550e-02, 9.1403e-03, 2.1351e-01,
        3.6285e-02, 2.2929e-02, 6.8161e-02, 3.1895e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,689][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([1.3443e-02, 1.5615e-04, 1.2497e-04, 5.0378e-04, 2.7874e-05, 1.5151e-04,
        1.8899e-05, 1.0250e-04, 1.2731e-04, 1.0837e-04, 2.2130e-03, 1.5333e-03,
        3.0260e-03, 2.7927e-02, 3.1184e-01, 6.3869e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,689][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.0429, 0.0223, 0.0062, 0.2357, 0.0418, 0.0882, 0.0237, 0.0747, 0.0776,
        0.0049, 0.0635, 0.0707, 0.0190, 0.1031, 0.0222, 0.1034],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:31,690][circuit_model.py][line:1570][INFO] ##9-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.6829, 0.0177, 0.0495, 0.0035, 0.0008, 0.0059, 0.0080, 0.0094, 0.0103,
        0.0054, 0.0366, 0.0060, 0.0038, 0.0062, 0.0988, 0.0418, 0.0134],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,690][circuit_model.py][line:1573][INFO] ##9-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0238, 0.0213, 0.0315, 0.0151, 0.0273, 0.0232, 0.0086, 0.0468, 0.0651,
        0.0204, 0.0918, 0.0440, 0.0516, 0.0556, 0.0646, 0.2641, 0.1454],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,691][circuit_model.py][line:1576][INFO] ##9-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.4048, 0.0141, 0.0227, 0.0019, 0.0014, 0.0041, 0.0021, 0.0040, 0.0029,
        0.0024, 0.1443, 0.0073, 0.0047, 0.0147, 0.1391, 0.1796, 0.0500],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,691][circuit_model.py][line:1579][INFO] ##9-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([9.6523e-01, 6.4134e-06, 1.6109e-04, 1.1347e-05, 1.3681e-05, 1.5154e-04,
        3.9403e-06, 3.9958e-05, 1.7377e-05, 7.7992e-05, 5.6358e-03, 6.0050e-05,
        4.2237e-04, 7.4171e-04, 5.1304e-03, 1.4431e-02, 7.8610e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,692][circuit_model.py][line:1582][INFO] ##9-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.5336, 0.0312, 0.0148, 0.0032, 0.0007, 0.0086, 0.0135, 0.0082, 0.0266,
        0.0023, 0.0828, 0.0482, 0.0229, 0.0125, 0.0861, 0.0697, 0.0353],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,693][circuit_model.py][line:1585][INFO] ##9-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([9.9327e-01, 8.5669e-04, 2.8708e-03, 1.1102e-04, 1.0845e-04, 1.3303e-04,
        4.0000e-05, 2.8444e-05, 1.4508e-04, 9.3296e-05, 1.2149e-03, 1.4794e-04,
        7.0596e-05, 1.0450e-04, 4.6852e-04, 1.5768e-04, 1.7844e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,695][circuit_model.py][line:1588][INFO] ##9-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([1.5589e-04, 2.7560e-02, 4.2960e-02, 2.8398e-02, 9.8503e-02, 1.4547e-02,
        1.3339e-02, 2.9592e-02, 3.7058e-02, 1.3058e-02, 2.5098e-02, 1.5460e-01,
        3.4635e-02, 2.7827e-02, 7.6357e-02, 2.9170e-01, 8.4603e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,696][circuit_model.py][line:1591][INFO] ##9-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.3159, 0.0402, 0.0269, 0.0087, 0.0007, 0.0037, 0.0029, 0.0014, 0.0084,
        0.0046, 0.0072, 0.0125, 0.0231, 0.0249, 0.1959, 0.1590, 0.1639],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,698][circuit_model.py][line:1594][INFO] ##9-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2936, 0.0403, 0.0203, 0.0092, 0.0062, 0.0090, 0.0061, 0.0159, 0.0036,
        0.0148, 0.1452, 0.0148, 0.0304, 0.1367, 0.0373, 0.0469, 0.1698],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,699][circuit_model.py][line:1597][INFO] ##9-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([1.0842e-04, 7.6291e-02, 1.9910e-02, 2.2181e-02, 1.1692e-02, 3.3833e-02,
        5.1888e-02, 2.2545e-02, 1.3774e-01, 2.7438e-02, 9.6673e-03, 1.5962e-01,
        4.5963e-02, 3.3805e-02, 1.0758e-01, 1.7033e-01, 6.9405e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,700][circuit_model.py][line:1600][INFO] ##9-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([6.8019e-02, 1.0930e-04, 1.8399e-04, 4.6361e-04, 5.6731e-05, 1.3631e-04,
        2.7294e-05, 8.8065e-05, 1.0629e-04, 1.0256e-04, 3.2856e-03, 9.6657e-04,
        4.9565e-03, 1.8555e-02, 1.3587e-01, 4.7758e-01, 2.8949e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,702][circuit_model.py][line:1603][INFO] ##9-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0208, 0.0183, 0.0060, 0.1888, 0.0361, 0.1482, 0.0160, 0.0511, 0.0833,
        0.0027, 0.0696, 0.0503, 0.0103, 0.0494, 0.0144, 0.0773, 0.1573],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:31,703][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:31,706][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[  266],
        [10420],
        [ 6502],
        [ 1208],
        [ 2156],
        [ 6940],
        [  856],
        [ 1991],
        [ 1858],
        [  493],
        [ 1125],
        [ 2212],
        [  738],
        [   13],
        [  965],
        [  387],
        [   13]], device='cuda:0')
[2024-07-23 21:05:31,707][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[  252],
        [15988],
        [ 6248],
        [ 2884],
        [ 3429],
        [ 6089],
        [  927],
        [ 1519],
        [ 1524],
        [ 1081],
        [ 1986],
        [ 2309],
        [ 1287],
        [  249],
        [ 1489],
        [  855],
        [  253]], device='cuda:0')
[2024-07-23 21:05:31,709][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[31355],
        [31469],
        [25063],
        [24129],
        [20272],
        [21950],
        [20183],
        [24717],
        [18590],
        [21170],
        [15767],
        [15874],
        [17578],
        [18571],
        [11616],
        [12307],
        [14064]], device='cuda:0')
[2024-07-23 21:05:31,710][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 1371],
        [50254],
        [50097],
        [49493],
        [49407],
        [43578],
        [44271],
        [41716],
        [40466],
        [42509],
        [34724],
        [29475],
        [35278],
        [36355],
        [35858],
        [37217],
        [40409]], device='cuda:0')
[2024-07-23 21:05:31,712][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[7537],
        [ 673],
        [2745],
        [3785],
        [2288],
        [2563],
        [3424],
        [3625],
        [5212],
        [4684],
        [7293],
        [7420],
        [6670],
        [6556],
        [8173],
        [9924],
        [9250]], device='cuda:0')
[2024-07-23 21:05:31,714][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[10034],
        [25050],
        [18304],
        [13089],
        [15462],
        [16552],
        [19399],
        [19110],
        [20424],
        [19562],
        [20197],
        [22569],
        [22616],
        [23555],
        [26242],
        [27048],
        [27953]], device='cuda:0')
[2024-07-23 21:05:31,715][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[35688],
        [   93],
        [   96],
        [   81],
        [   75],
        [   69],
        [   88],
        [   69],
        [   77],
        [   87],
        [   95],
        [   85],
        [  101],
        [   77],
        [   79],
        [   76],
        [   69]], device='cuda:0')
[2024-07-23 21:05:31,717][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 386],
        [ 141],
        [ 573],
        [ 230],
        [ 818],
        [ 634],
        [ 250],
        [ 210],
        [ 879],
        [ 236],
        [1306],
        [ 354],
        [ 311],
        [ 347],
        [ 298],
        [ 371],
        [ 382]], device='cuda:0')
[2024-07-23 21:05:31,718][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[ 6133],
        [21748],
        [ 5152],
        [ 4486],
        [ 7077],
        [ 8214],
        [ 7889],
        [ 8375],
        [ 8513],
        [ 7919],
        [ 7889],
        [ 7518],
        [ 8718],
        [ 7260],
        [ 8215],
        [ 8011],
        [ 7866]], device='cuda:0')
[2024-07-23 21:05:31,720][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[ 6971],
        [ 7077],
        [10140],
        [14850],
        [13192],
        [15165],
        [15301],
        [22324],
        [25026],
        [22158],
        [28128],
        [26685],
        [28737],
        [24829],
        [23910],
        [31860],
        [31175]], device='cuda:0')
[2024-07-23 21:05:31,722][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 5324],
        [14260],
        [14552],
        [15659],
        [16927],
        [18371],
        [17437],
        [16115],
        [15989],
        [15452],
        [15584],
        [15372],
        [15007],
        [15890],
        [16285],
        [16394],
        [16399]], device='cuda:0')
[2024-07-23 21:05:31,723][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[36079],
        [29374],
        [23493],
        [18853],
        [17341],
        [13162],
        [13881],
        [10971],
        [14162],
        [12092],
        [10415],
        [12653],
        [12207],
        [12565],
        [11680],
        [10799],
        [11359]], device='cuda:0')
[2024-07-23 21:05:31,725][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[11432],
        [13634],
        [14344],
        [12014],
        [10631],
        [ 7150],
        [ 8134],
        [12507],
        [ 8487],
        [15986],
        [11379],
        [ 8656],
        [ 8763],
        [ 7091],
        [18863],
        [10500],
        [ 8437]], device='cuda:0')
[2024-07-23 21:05:31,727][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[29859],
        [30328],
        [26228],
        [23655],
        [15173],
        [14291],
        [16455],
        [12300],
        [12803],
        [10910],
        [ 9143],
        [ 7913],
        [ 9961],
        [ 8188],
        [ 8009],
        [ 8197],
        [ 7442]], device='cuda:0')
[2024-07-23 21:05:31,728][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[ 6499],
        [ 2091],
        [22809],
        [ 2534],
        [ 5329],
        [17164],
        [12770],
        [12416],
        [10242],
        [ 2332],
        [ 5263],
        [ 7659],
        [ 5129],
        [  281],
        [ 5156],
        [ 2610],
        [  246]], device='cuda:0')
[2024-07-23 21:05:31,730][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[2128],
        [ 642],
        [1580],
        [1307],
        [2573],
        [1489],
        [1622],
        [1148],
        [2293],
        [2010],
        [5202],
        [4001],
        [4115],
        [3220],
        [3915],
        [2116],
        [2174]], device='cuda:0')
[2024-07-23 21:05:31,731][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[7156],
        [   5],
        [  53],
        [ 125],
        [ 610],
        [ 807],
        [ 446],
        [ 920],
        [ 692],
        [ 902],
        [2995],
        [3315],
        [2913],
        [2673],
        [2657],
        [2597],
        [1472]], device='cuda:0')
[2024-07-23 21:05:31,733][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1916],
        [  60],
        [1674],
        [1551],
        [5907],
        [2414],
        [1628],
        [ 634],
        [1506],
        [4368],
        [4260],
        [4056],
        [3832],
        [3783],
        [2662],
        [1802],
        [1909]], device='cuda:0')
[2024-07-23 21:05:31,735][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[4679],
        [1082],
        [2914],
        [3479],
        [3573],
        [2308],
        [1660],
        [2242],
        [1727],
        [2825],
        [1959],
        [1518],
        [3212],
        [3179],
        [2162],
        [2173],
        [3160]], device='cuda:0')
[2024-07-23 21:05:31,736][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[  399],
        [12205],
        [10745],
        [11225],
        [11999],
        [10698],
        [ 6832],
        [ 4159],
        [ 9296],
        [ 9258],
        [10435],
        [ 7255],
        [10249],
        [ 9842],
        [ 8344],
        [10436],
        [ 9665]], device='cuda:0')
[2024-07-23 21:05:31,738][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[43903],
        [42554],
        [33510],
        [43610],
        [34573],
        [33325],
        [43256],
        [42690],
        [28084],
        [39700],
        [17099],
        [42475],
        [43368],
        [43626],
        [43479],
        [43486],
        [43800]], device='cuda:0')
[2024-07-23 21:05:31,739][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[31125],
        [29033],
        [25645],
        [25851],
        [26228],
        [26064],
        [20946],
        [21079],
        [17792],
        [20487],
        [20689],
        [17475],
        [17600],
        [18649],
        [23778],
        [28740],
        [30487]], device='cuda:0')
[2024-07-23 21:05:31,741][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[ 3849],
        [10733],
        [22176],
        [35585],
        [34718],
        [36114],
        [27634],
        [31963],
        [21469],
        [25445],
        [20906],
        [20589],
        [17754],
        [17730],
        [11780],
        [ 8227],
        [ 9115]], device='cuda:0')
[2024-07-23 21:05:31,743][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[5319],
        [7106],
        [8167],
        [7409],
        [7738],
        [8815],
        [8642],
        [7468],
        [7873],
        [7860],
        [9112],
        [8078],
        [7410],
        [5984],
        [5043],
        [5162],
        [4836]], device='cuda:0')
[2024-07-23 21:05:31,744][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[  564],
        [21718],
        [25599],
        [26395],
        [26570],
        [27192],
        [14430],
        [15107],
        [10072],
        [15347],
        [ 7165],
        [ 7667],
        [11834],
        [10536],
        [10258],
        [10910],
        [11454]], device='cuda:0')
[2024-07-23 21:05:31,746][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[ 7337],
        [ 7409],
        [15597],
        [10599],
        [ 7287],
        [ 2033],
        [ 1553],
        [ 1507],
        [ 1540],
        [ 1875],
        [  874],
        [  974],
        [  923],
        [ 1789],
        [ 2197],
        [ 2125],
        [ 2852]], device='cuda:0')
[2024-07-23 21:05:31,748][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[ 9227],
        [ 3207],
        [ 7979],
        [11829],
        [12390],
        [ 7661],
        [ 5987],
        [ 6432],
        [ 5594],
        [ 6288],
        [ 7173],
        [ 6565],
        [ 7776],
        [ 7910],
        [ 9556],
        [ 9129],
        [ 8924]], device='cuda:0')
[2024-07-23 21:05:31,749][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[31671],
        [48691],
        [34641],
        [33024],
        [27768],
        [36594],
        [42016],
        [43011],
        [41607],
        [36293],
        [38641],
        [38974],
        [38626],
        [36791],
        [39080],
        [38474],
        [37413]], device='cuda:0')
[2024-07-23 21:05:31,751][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[26472],
        [31360],
        [10397],
        [29116],
        [21728],
        [19801],
        [26471],
        [26565],
        [32842],
        [35743],
        [22451],
        [38643],
        [31237],
        [44998],
        [29364],
        [39626],
        [46479]], device='cuda:0')
[2024-07-23 21:05:31,752][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272],
        [9272]], device='cuda:0')
[2024-07-23 21:05:31,816][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:31,817][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,819][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,820][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,821][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,823][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,829][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,830][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,830][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,830][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,831][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,831][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,832][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:31,834][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.0161, 0.9839], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,835][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.0139, 0.9861], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,837][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.0273, 0.9727], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,838][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.9718, 0.0282], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,840][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.0706, 0.9294], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,842][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.0356, 0.9644], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,843][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ original] are: tensor([4.8730e-04, 9.9951e-01], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,844][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.5532, 0.4468], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,846][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.0740, 0.9260], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,848][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.0496, 0.9504], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,849][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.6416, 0.3584], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,851][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.1340, 0.8660], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:31,853][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.0061, 0.6502, 0.3436], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,854][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0010, 0.1840, 0.8150], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,855][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0019, 0.5101, 0.4879], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,857][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.8490, 0.0687, 0.0823], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,859][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0669, 0.8332, 0.0998], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,860][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0023, 0.2373, 0.7605], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,861][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ language] are: tensor([2.1059e-04, 5.7164e-01, 4.2815e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,863][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.3307, 0.0338, 0.6355], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,865][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0458, 0.6258, 0.3284], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,866][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0079, 0.7600, 0.2322], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,868][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.5467, 0.2779, 0.1754], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,870][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0180, 0.4061, 0.5758], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:31,871][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0014, 0.4038, 0.5285, 0.0664], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,872][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ of] are: tensor([1.2122e-04, 7.6490e-02, 3.3324e-01, 5.9015e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,874][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0020, 0.2466, 0.2446, 0.5068], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,876][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.9025, 0.0270, 0.0507, 0.0198], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,877][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ of] are: tensor([0.0096, 0.1133, 0.0943, 0.7828], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,879][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ of] are: tensor([0.0432, 0.0803, 0.5979, 0.2786], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,880][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ of] are: tensor([1.2050e-04, 1.8357e-01, 3.1946e-01, 4.9685e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,882][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.3664, 0.0633, 0.2702, 0.3001], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,883][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0188, 0.3880, 0.1866, 0.4066], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,885][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0063, 0.6428, 0.1255, 0.2255], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,887][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.4695, 0.1999, 0.1520, 0.1787], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,888][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.1467, 0.0193, 0.5332, 0.3007], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:31,890][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0015, 0.4261, 0.4047, 0.0453, 0.1224], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,892][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.0030, 0.0825, 0.3403, 0.3438, 0.2304], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,894][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0283, 0.2153, 0.4830, 0.2078, 0.0655], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,895][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.7475, 0.0489, 0.1018, 0.0534, 0.0483], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,895][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ De] are: tensor([0.0009, 0.1063, 0.0219, 0.3467, 0.5242], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,896][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.0251, 0.0509, 0.5181, 0.1280, 0.2779], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,896][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ De] are: tensor([1.0851e-04, 2.5210e-01, 2.8549e-01, 2.2407e-01, 2.3823e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,896][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.3269, 0.0485, 0.3879, 0.0928, 0.1439], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,897][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ De] are: tensor([0.0146, 0.3037, 0.1294, 0.2818, 0.2704], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,897][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0008, 0.5243, 0.1373, 0.2762, 0.0614], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,897][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.2230, 0.1780, 0.1165, 0.1668, 0.3157], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,898][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.0304, 0.0310, 0.2713, 0.6317, 0.0356], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:31,898][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0246, 0.2599, 0.2700, 0.0439, 0.2960, 0.1056], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,898][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.0011, 0.0233, 0.1516, 0.5341, 0.2740, 0.0158], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,900][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0341, 0.1952, 0.3142, 0.3086, 0.0725, 0.0754], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,902][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.9028, 0.0164, 0.0350, 0.0149, 0.0167, 0.0143], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,903][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([0.0297, 0.0414, 0.0110, 0.2507, 0.6315, 0.0357], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,904][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.0199, 0.0376, 0.1655, 0.1803, 0.4261, 0.1706], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,905][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([3.3479e-04, 1.5572e-01, 1.4713e-01, 3.9114e-01, 2.4514e-01, 6.0532e-02],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,907][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.6329, 0.0287, 0.1278, 0.0754, 0.1048, 0.0304], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,909][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([0.0263, 0.3098, 0.1252, 0.2680, 0.1951, 0.0756], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,910][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.0012, 0.6865, 0.0713, 0.1535, 0.0381, 0.0494], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,912][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.3234, 0.1083, 0.0584, 0.1160, 0.1989, 0.1950], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,914][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.0135, 0.0178, 0.0792, 0.7770, 0.0485, 0.0639], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:31,916][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0049, 0.2748, 0.2724, 0.0491, 0.2079, 0.0247, 0.1661],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,917][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0015, 0.0220, 0.1231, 0.2948, 0.4379, 0.0114, 0.1093],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,919][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0537, 0.2065, 0.2989, 0.1373, 0.0802, 0.0487, 0.1746],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,921][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.9312, 0.0115, 0.0267, 0.0075, 0.0103, 0.0103, 0.0025],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,922][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([0.0631, 0.0684, 0.0179, 0.3118, 0.4574, 0.0485, 0.0330],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,924][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.0314, 0.0321, 0.2575, 0.1429, 0.3506, 0.1408, 0.0448],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,925][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([1.4498e-04, 1.3624e-01, 6.9797e-02, 2.6351e-01, 3.2539e-01, 6.6044e-02,
        1.3887e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,927][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.5788, 0.0587, 0.0950, 0.1507, 0.0674, 0.0416, 0.0078],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,928][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([0.0174, 0.2513, 0.0966, 0.2003, 0.1821, 0.0627, 0.1895],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,930][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.0123, 0.3256, 0.2228, 0.1481, 0.0669, 0.0676, 0.1567],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,932][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.3724, 0.0986, 0.0626, 0.0854, 0.1595, 0.1439, 0.0777],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,933][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.3100, 0.0291, 0.1090, 0.2891, 0.0779, 0.1342, 0.0508],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:31,935][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.0498, 0.1268, 0.4187, 0.0270, 0.2115, 0.0224, 0.1124, 0.0315],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,937][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.0064, 0.0055, 0.1748, 0.1074, 0.4623, 0.0142, 0.0443, 0.1851],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,939][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.2720, 0.1109, 0.2591, 0.1234, 0.0249, 0.0256, 0.0734, 0.1107],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,940][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.9648, 0.0046, 0.0157, 0.0028, 0.0047, 0.0034, 0.0012, 0.0028],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,942][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([0.1992, 0.0332, 0.0407, 0.1395, 0.4560, 0.0636, 0.0431, 0.0248],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,944][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.0984, 0.0181, 0.2585, 0.0530, 0.2211, 0.1978, 0.0322, 0.1209],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,945][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0035, 0.0430, 0.1393, 0.1174, 0.3729, 0.0565, 0.1069, 0.1604],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,946][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([9.7369e-01, 3.3961e-03, 5.8890e-03, 9.1452e-03, 5.1881e-03, 1.8262e-03,
        1.6538e-04, 7.0018e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,948][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([0.0189, 0.2073, 0.1059, 0.1983, 0.1695, 0.0503, 0.1438, 0.1060],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,950][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.0096, 0.3102, 0.1093, 0.1905, 0.0564, 0.0582, 0.1182, 0.1476],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,952][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.6273, 0.0494, 0.0301, 0.0533, 0.0782, 0.0685, 0.0353, 0.0579],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,953][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.7242, 0.0065, 0.0518, 0.0865, 0.0413, 0.0455, 0.0060, 0.0383],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:31,955][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0111, 0.1771, 0.2548, 0.0284, 0.1711, 0.0491, 0.1409, 0.0719, 0.0955],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,957][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0009, 0.0112, 0.0672, 0.1500, 0.2614, 0.0067, 0.0855, 0.1635, 0.2535],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,959][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0074, 0.0699, 0.2792, 0.1272, 0.0744, 0.0394, 0.1373, 0.1771, 0.0882],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,960][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.9390, 0.0072, 0.0209, 0.0055, 0.0074, 0.0064, 0.0020, 0.0039, 0.0077],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,962][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0430, 0.0438, 0.0071, 0.3596, 0.4571, 0.0276, 0.0283, 0.0177, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,962][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0163, 0.0102, 0.0562, 0.0781, 0.2799, 0.1013, 0.0295, 0.2546, 0.1739],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,962][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0004, 0.0608, 0.0561, 0.0942, 0.1494, 0.0319, 0.1157, 0.2033, 0.2880],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,963][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.6997, 0.0256, 0.0636, 0.0943, 0.0497, 0.0292, 0.0050, 0.0131, 0.0199],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,963][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0119, 0.1808, 0.0794, 0.1533, 0.1286, 0.0468, 0.1323, 0.0963, 0.1706],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,964][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0007, 0.2917, 0.2017, 0.1513, 0.0351, 0.0284, 0.1352, 0.1194, 0.0365],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,964][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.5487, 0.0539, 0.0320, 0.0499, 0.0801, 0.0779, 0.0380, 0.0550, 0.0644],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,964][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0447, 0.0120, 0.0274, 0.6955, 0.0530, 0.0467, 0.0327, 0.0584, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:31,965][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0097, 0.2610, 0.4310, 0.0265, 0.0826, 0.0082, 0.0584, 0.0133, 0.0376,
        0.0716], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,965][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.0007, 0.0168, 0.0963, 0.0728, 0.1990, 0.0025, 0.0436, 0.1068, 0.0890,
        0.3727], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,967][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0643, 0.1480, 0.2877, 0.0771, 0.0424, 0.0188, 0.0759, 0.1678, 0.0213,
        0.0967], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,969][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.9052, 0.0125, 0.0241, 0.0103, 0.0114, 0.0101, 0.0033, 0.0072, 0.0085,
        0.0075], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,970][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ et] are: tensor([0.2154, 0.0331, 0.0462, 0.1655, 0.3666, 0.0836, 0.0380, 0.0171, 0.0172,
        0.0173], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,972][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.3424, 0.0059, 0.0455, 0.0131, 0.0407, 0.0505, 0.0069, 0.0224, 0.0340,
        0.4386], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,972][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ et] are: tensor([1.8945e-04, 8.0119e-02, 7.5986e-02, 8.5960e-02, 6.4668e-02, 1.4200e-02,
        4.3810e-02, 1.0011e-01, 4.5039e-02, 4.8992e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,974][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ et] are: tensor([8.5125e-01, 2.6221e-02, 6.2699e-02, 1.8782e-02, 1.3603e-02, 3.4335e-03,
        8.1171e-04, 9.0491e-04, 3.6628e-03, 1.8628e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,976][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ et] are: tensor([0.0145, 0.1556, 0.0754, 0.1411, 0.1431, 0.0449, 0.1141, 0.1002, 0.1440,
        0.0671], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,977][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0087, 0.1565, 0.0612, 0.0901, 0.0180, 0.0241, 0.0692, 0.0513, 0.0329,
        0.4881], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,979][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.5674, 0.0467, 0.0270, 0.0403, 0.0594, 0.0712, 0.0282, 0.0492, 0.0424,
        0.0681], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,981][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.0674, 0.0383, 0.0932, 0.2751, 0.0254, 0.0363, 0.0227, 0.0593, 0.0129,
        0.3695], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:31,982][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0065, 0.1103, 0.1407, 0.0144, 0.0742, 0.0603, 0.1128, 0.0270, 0.0693,
        0.0357, 0.3487], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,984][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([0.0005, 0.0024, 0.0307, 0.0598, 0.1933, 0.0046, 0.0622, 0.1089, 0.0651,
        0.0733, 0.3993], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,986][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.0747, 0.0753, 0.2625, 0.0954, 0.0241, 0.0262, 0.1250, 0.0931, 0.0499,
        0.0578, 0.1160], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,987][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([0.7618, 0.0152, 0.0447, 0.0171, 0.0207, 0.0215, 0.0078, 0.0157, 0.0117,
        0.0138, 0.0700], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,989][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([0.4896, 0.0130, 0.0209, 0.1024, 0.2210, 0.0258, 0.0128, 0.0085, 0.0092,
        0.0130, 0.0839], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,991][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.1460, 0.0066, 0.0224, 0.0194, 0.0361, 0.0246, 0.0059, 0.0343, 0.0189,
        0.3016, 0.3843], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,993][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0004, 0.0294, 0.0490, 0.0446, 0.1091, 0.0309, 0.0660, 0.1409, 0.0723,
        0.2469, 0.2105], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,994][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([9.5435e-01, 1.3602e-03, 9.6245e-03, 2.6725e-03, 3.7097e-03, 1.0752e-03,
        9.9894e-05, 2.4520e-04, 1.0006e-03, 2.3332e-03, 2.3528e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,995][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([0.0146, 0.1546, 0.0764, 0.1407, 0.1372, 0.0446, 0.1264, 0.0859, 0.1466,
        0.0425, 0.0305], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,997][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.0035, 0.1451, 0.0584, 0.1856, 0.0144, 0.0248, 0.0722, 0.0793, 0.0221,
        0.3236, 0.0711], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:31,999][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.5011, 0.0450, 0.0227, 0.0337, 0.0687, 0.0701, 0.0345, 0.0553, 0.0491,
        0.0692, 0.0505], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,000][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.0287, 0.0074, 0.0362, 0.5232, 0.0267, 0.0261, 0.0428, 0.0402, 0.0114,
        0.1540, 0.1032], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,002][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0052, 0.0709, 0.1214, 0.0168, 0.0707, 0.0326, 0.0584, 0.0349, 0.0458,
        0.0753, 0.3273, 0.1407], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,004][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.0008, 0.0039, 0.0377, 0.0586, 0.1586, 0.0035, 0.0299, 0.0716, 0.0802,
        0.0786, 0.2283, 0.2483], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,005][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0093, 0.0398, 0.3372, 0.0948, 0.0366, 0.0278, 0.0834, 0.1036, 0.0640,
        0.0639, 0.0906, 0.0489], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,007][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.8715, 0.0099, 0.0236, 0.0066, 0.0075, 0.0077, 0.0024, 0.0049, 0.0082,
        0.0049, 0.0398, 0.0130], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,009][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [orum] are: tensor([0.0559, 0.0424, 0.0117, 0.3671, 0.3743, 0.0328, 0.0274, 0.0128, 0.0156,
        0.0096, 0.0393, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,011][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0256, 0.0034, 0.0133, 0.0161, 0.0340, 0.0160, 0.0044, 0.0244, 0.0238,
        0.2533, 0.2827, 0.3030], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,012][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [orum] are: tensor([1.6926e-04, 1.6578e-02, 3.5823e-02, 5.7261e-02, 4.3259e-02, 1.7576e-02,
        4.5815e-02, 8.8280e-02, 9.8319e-02, 2.7027e-01, 9.2416e-02, 2.3424e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,014][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.7030, 0.0157, 0.0717, 0.0601, 0.0427, 0.0176, 0.0015, 0.0062, 0.0066,
        0.0142, 0.0531, 0.0076], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,016][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [orum] are: tensor([0.0129, 0.1254, 0.0618, 0.1010, 0.0992, 0.0393, 0.0937, 0.0719, 0.1228,
        0.0545, 0.0301, 0.1874], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,017][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0005, 0.1504, 0.1285, 0.0930, 0.0132, 0.0126, 0.0622, 0.0606, 0.0154,
        0.3046, 0.0669, 0.0919], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,019][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.5091, 0.0375, 0.0244, 0.0385, 0.0575, 0.0540, 0.0293, 0.0388, 0.0503,
        0.0722, 0.0508, 0.0376], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,021][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0437, 0.0080, 0.0308, 0.6479, 0.0352, 0.0265, 0.0243, 0.0185, 0.0165,
        0.1183, 0.0224, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,022][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0046, 0.0898, 0.3578, 0.0402, 0.0568, 0.0139, 0.0258, 0.0178, 0.0273,
        0.0435, 0.1259, 0.1194, 0.0772], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,024][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0022, 0.0051, 0.0613, 0.0671, 0.1235, 0.0023, 0.0146, 0.0487, 0.0315,
        0.0772, 0.1933, 0.0997, 0.2735], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,026][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0123, 0.1091, 0.3871, 0.1031, 0.0243, 0.0341, 0.0569, 0.0912, 0.0161,
        0.0631, 0.0489, 0.0204, 0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,028][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.7701, 0.0172, 0.0243, 0.0133, 0.0166, 0.0124, 0.0070, 0.0104, 0.0118,
        0.0076, 0.0667, 0.0193, 0.0233], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,029][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ is] are: tensor([0.0535, 0.0310, 0.0271, 0.1986, 0.2432, 0.0423, 0.0265, 0.0127, 0.0213,
        0.0185, 0.0870, 0.0222, 0.2159], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,029][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0232, 0.0028, 0.0332, 0.0191, 0.0153, 0.0183, 0.0016, 0.0076, 0.0053,
        0.1019, 0.1107, 0.0691, 0.5919], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,030][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ is] are: tensor([1.8379e-04, 2.3592e-02, 1.2563e-01, 5.7541e-02, 6.4283e-02, 1.5639e-02,
        4.6537e-02, 6.1812e-02, 3.8154e-02, 1.3653e-01, 1.1141e-01, 9.6042e-02,
        2.2264e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,030][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.8114, 0.0211, 0.0573, 0.0257, 0.0172, 0.0060, 0.0012, 0.0015, 0.0045,
        0.0108, 0.0172, 0.0072, 0.0188], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,030][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ is] are: tensor([0.0100, 0.1133, 0.0550, 0.1169, 0.1019, 0.0325, 0.0818, 0.0685, 0.0859,
        0.0394, 0.0200, 0.1304, 0.1445], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,031][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0057, 0.1272, 0.0381, 0.0675, 0.0094, 0.0092, 0.0443, 0.0224, 0.0095,
        0.1812, 0.0333, 0.0379, 0.4143], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,031][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.3096, 0.0475, 0.0353, 0.0475, 0.0720, 0.0717, 0.0377, 0.0514, 0.0664,
        0.0803, 0.0709, 0.0516, 0.0581], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,032][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ is] are: tensor([7.7606e-01, 1.3834e-03, 4.8721e-02, 2.6591e-02, 4.4432e-03, 9.3851e-03,
        7.5200e-04, 4.0930e-03, 1.2903e-03, 1.4489e-02, 4.9203e-02, 5.3155e-04,
        6.3054e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,032][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0013, 0.1020, 0.2494, 0.0475, 0.0308, 0.0110, 0.0148, 0.0126, 0.0150,
        0.0257, 0.1519, 0.0599, 0.0911, 0.1872], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,033][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([3.7873e-05, 3.4477e-03, 1.5488e-02, 2.5947e-02, 3.2088e-02, 1.2148e-03,
        7.1497e-03, 2.4538e-02, 1.3749e-02, 2.4662e-02, 1.0427e-01, 6.0523e-02,
        9.6539e-02, 5.9035e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,034][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0130, 0.1041, 0.3023, 0.1083, 0.0125, 0.0363, 0.0779, 0.0911, 0.0240,
        0.0410, 0.0976, 0.0322, 0.0353, 0.0243], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,036][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.6519, 0.0275, 0.0452, 0.0238, 0.0172, 0.0177, 0.0090, 0.0142, 0.0156,
        0.0104, 0.0713, 0.0217, 0.0261, 0.0481], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,038][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0134, 0.0245, 0.0215, 0.2846, 0.1650, 0.0452, 0.0267, 0.0097, 0.0250,
        0.0129, 0.1119, 0.0175, 0.0821, 0.1599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,039][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.2500, 0.0019, 0.0166, 0.0121, 0.0117, 0.0078, 0.0007, 0.0046, 0.0025,
        0.0245, 0.0551, 0.0143, 0.2636, 0.3346], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,040][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([2.1244e-05, 8.9556e-03, 4.4868e-02, 5.5251e-02, 2.2805e-02, 1.2364e-02,
        1.6304e-02, 3.4240e-02, 2.9726e-02, 6.7634e-02, 1.0767e-01, 9.5449e-02,
        1.6302e-01, 3.4169e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,041][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.7545e-01, 9.9542e-03, 2.9586e-02, 2.0257e-02, 7.7669e-03, 2.5763e-03,
        3.9903e-04, 4.7897e-04, 2.0952e-03, 4.3495e-03, 2.2042e-02, 3.4570e-03,
        8.7518e-03, 1.2835e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,043][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0068, 0.1309, 0.0531, 0.1081, 0.0906, 0.0275, 0.0720, 0.0626, 0.0726,
        0.0327, 0.0158, 0.1118, 0.1176, 0.0979], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,045][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0035, 0.0544, 0.0101, 0.0324, 0.0048, 0.0035, 0.0209, 0.0100, 0.0048,
        0.0387, 0.0077, 0.0164, 0.1505, 0.6421], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,046][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.4015, 0.0426, 0.0308, 0.0366, 0.0588, 0.0611, 0.0352, 0.0437, 0.0567,
        0.0583, 0.0547, 0.0462, 0.0362, 0.0377], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,048][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.3311, 0.0017, 0.4267, 0.0584, 0.0055, 0.0241, 0.0017, 0.0039, 0.0018,
        0.0147, 0.0513, 0.0008, 0.0545, 0.0238], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,050][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0228, 0.1102, 0.2345, 0.0121, 0.0529, 0.0129, 0.0401, 0.0252, 0.0233,
        0.0381, 0.2926, 0.0464, 0.0310, 0.0281, 0.0298], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,052][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.0020, 0.0096, 0.0864, 0.0599, 0.1198, 0.0029, 0.0146, 0.0317, 0.0205,
        0.0305, 0.2242, 0.0488, 0.1467, 0.1795, 0.0230], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,054][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0070, 0.0666, 0.2496, 0.0891, 0.0269, 0.0325, 0.0759, 0.1249, 0.0188,
        0.0435, 0.0764, 0.0200, 0.0315, 0.0257, 0.1117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,055][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.7293, 0.0242, 0.0358, 0.0164, 0.0121, 0.0103, 0.0046, 0.0089, 0.0103,
        0.0091, 0.0517, 0.0131, 0.0175, 0.0280, 0.0288], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,057][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ same] are: tensor([0.1413, 0.0336, 0.0123, 0.1600, 0.1456, 0.0205, 0.0172, 0.0163, 0.0186,
        0.0128, 0.0535, 0.0208, 0.1473, 0.1191, 0.0812], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,058][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ same] are: tensor([2.3221e-02, 1.1879e-03, 3.2907e-03, 2.0120e-03, 1.1571e-03, 9.9365e-04,
        2.6558e-04, 1.1681e-03, 7.7601e-04, 1.4433e-02, 1.6817e-02, 1.1647e-02,
        2.1424e-01, 3.9036e-01, 3.1843e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,060][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0003, 0.0437, 0.0843, 0.0947, 0.0775, 0.0110, 0.0197, 0.0564, 0.0196,
        0.1161, 0.0634, 0.0457, 0.2145, 0.1280, 0.0252], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,061][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ same] are: tensor([9.1104e-01, 4.2258e-03, 2.1245e-02, 1.3774e-02, 8.6025e-03, 1.6772e-03,
        4.7757e-04, 1.4444e-03, 2.3732e-03, 8.8850e-03, 1.0497e-02, 2.2792e-03,
        6.8925e-03, 4.2856e-03, 2.3067e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,062][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ same] are: tensor([0.0060, 0.0846, 0.0516, 0.0843, 0.0885, 0.0242, 0.0731, 0.0556, 0.0724,
        0.0387, 0.0148, 0.1022, 0.0894, 0.0699, 0.1447], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,063][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ same] are: tensor([1.0323e-04, 2.5137e-02, 9.0298e-03, 1.9094e-02, 1.1801e-03, 1.3856e-03,
        4.4935e-03, 5.5105e-03, 1.5155e-03, 1.9068e-02, 1.0381e-02, 8.7172e-03,
        1.5934e-01, 7.1873e-01, 1.6314e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,065][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.1556, 0.0463, 0.0319, 0.0447, 0.0768, 0.0819, 0.0353, 0.0606, 0.0594,
        0.0850, 0.0770, 0.0429, 0.0602, 0.0573, 0.0849], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,066][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ same] are: tensor([9.1313e-01, 9.1074e-04, 3.0127e-02, 1.3549e-02, 3.3273e-03, 2.2723e-03,
        2.6738e-04, 9.0116e-04, 9.2487e-05, 3.9836e-03, 1.3841e-02, 6.3312e-05,
        7.4852e-03, 8.6906e-03, 1.3560e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,068][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0055, 0.0682, 0.2313, 0.0314, 0.0414, 0.0114, 0.0304, 0.0095, 0.0136,
        0.0252, 0.1049, 0.0344, 0.0599, 0.1352, 0.0488, 0.1489],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,069][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ as] are: tensor([1.1773e-04, 1.8038e-03, 1.2481e-02, 1.7385e-02, 4.4646e-02, 9.2731e-04,
        1.1110e-02, 2.2596e-02, 1.7759e-02, 3.7910e-02, 4.8233e-02, 3.1355e-02,
        7.2181e-02, 1.7842e-01, 1.4793e-02, 4.8828e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,071][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0052, 0.0440, 0.2279, 0.0857, 0.0354, 0.0274, 0.0678, 0.1153, 0.0171,
        0.0275, 0.0786, 0.0189, 0.0299, 0.0221, 0.0774, 0.1198],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,072][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.8528, 0.0076, 0.0134, 0.0049, 0.0047, 0.0046, 0.0024, 0.0037, 0.0038,
        0.0028, 0.0253, 0.0075, 0.0093, 0.0147, 0.0128, 0.0296],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,074][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ as] are: tensor([0.0752, 0.0187, 0.0161, 0.0934, 0.1311, 0.0308, 0.0226, 0.0103, 0.0112,
        0.0100, 0.0496, 0.0101, 0.0653, 0.1321, 0.0539, 0.2696],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,076][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ as] are: tensor([1.0753e-02, 1.5466e-04, 8.2605e-04, 1.4547e-03, 3.9579e-04, 9.5457e-04,
        1.5839e-04, 5.5324e-04, 5.8190e-04, 5.3008e-03, 4.4144e-03, 8.6302e-03,
        8.2479e-02, 1.8913e-01, 1.6274e-01, 5.3147e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,077][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0002, 0.0204, 0.0615, 0.0473, 0.0376, 0.0108, 0.0276, 0.0584, 0.0447,
        0.0956, 0.1046, 0.0769, 0.1272, 0.1108, 0.0362, 0.1402],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,079][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.7869, 0.0127, 0.0575, 0.0252, 0.0139, 0.0053, 0.0014, 0.0020, 0.0056,
        0.0127, 0.0196, 0.0053, 0.0156, 0.0138, 0.0048, 0.0178],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,081][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ as] are: tensor([0.0078, 0.0810, 0.0331, 0.0700, 0.0645, 0.0213, 0.0556, 0.0491, 0.0684,
        0.0283, 0.0128, 0.1027, 0.0822, 0.0703, 0.1357, 0.1172],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,082][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ as] are: tensor([4.8370e-04, 2.0467e-02, 7.3856e-03, 1.6191e-02, 4.2023e-03, 2.7718e-03,
        1.7532e-02, 8.1037e-03, 3.1573e-03, 3.8522e-02, 5.9548e-03, 1.0227e-02,
        1.1900e-01, 6.6248e-01, 2.0262e-02, 6.3263e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,084][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.1696, 0.0501, 0.0331, 0.0405, 0.0694, 0.0760, 0.0362, 0.0548, 0.0562,
        0.0701, 0.0641, 0.0405, 0.0474, 0.0484, 0.0725, 0.0710],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,086][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.0812, 0.0090, 0.0621, 0.0547, 0.0107, 0.0080, 0.0032, 0.0094, 0.0019,
        0.0145, 0.0285, 0.0008, 0.0929, 0.0406, 0.0103, 0.5723],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,088][circuit_model.py][line:1532][INFO] ##10-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0027, 0.0650, 0.1527, 0.0342, 0.0221, 0.0067, 0.0110, 0.0107, 0.0075,
        0.0168, 0.0581, 0.0237, 0.0733, 0.1593, 0.0435, 0.0516, 0.2612],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,089][circuit_model.py][line:1535][INFO] ##10-th layer ##Weight##: The head2 weight for token [ the] are: tensor([1.2543e-05, 1.6135e-03, 4.1807e-03, 1.3505e-02, 1.7494e-02, 5.2614e-04,
        4.0156e-03, 1.3931e-02, 6.4758e-03, 1.3739e-02, 2.3222e-02, 2.0213e-02,
        4.6861e-02, 3.4277e-01, 1.2752e-02, 1.7286e-01, 3.0583e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,090][circuit_model.py][line:1538][INFO] ##10-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0046, 0.0694, 0.1743, 0.1083, 0.0129, 0.0322, 0.0701, 0.0919, 0.0202,
        0.0339, 0.0542, 0.0234, 0.0276, 0.0217, 0.0595, 0.1087, 0.0871],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,092][circuit_model.py][line:1541][INFO] ##10-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.5568, 0.0282, 0.0337, 0.0187, 0.0137, 0.0122, 0.0075, 0.0104, 0.0106,
        0.0066, 0.0486, 0.0150, 0.0222, 0.0393, 0.0290, 0.0782, 0.0692],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,094][circuit_model.py][line:1544][INFO] ##10-th layer ##Weight##: The head5 weight for token [ the] are: tensor([0.0083, 0.0143, 0.0087, 0.1730, 0.1046, 0.0300, 0.0171, 0.0064, 0.0159,
        0.0073, 0.0390, 0.0099, 0.0705, 0.0942, 0.0374, 0.1125, 0.2508],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,095][circuit_model.py][line:1547][INFO] ##10-th layer ##Weight##: The head6 weight for token [ the] are: tensor([3.3664e-02, 9.8155e-05, 5.9593e-04, 8.1503e-04, 3.5911e-04, 4.1197e-04,
        3.8724e-05, 2.5207e-04, 2.1528e-04, 1.2368e-03, 3.0014e-03, 1.5410e-03,
        3.4863e-02, 4.8842e-02, 3.4624e-02, 2.1413e-01, 6.2531e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,096][circuit_model.py][line:1550][INFO] ##10-th layer ##Weight##: The head7 weight for token [ the] are: tensor([9.1422e-05, 9.2011e-03, 3.1781e-02, 4.2910e-02, 1.7628e-02, 9.4933e-03,
        1.2392e-02, 2.4621e-02, 2.5219e-02, 5.0513e-02, 6.6488e-02, 6.3434e-02,
        1.1254e-01, 2.1212e-01, 3.1308e-02, 1.2100e-01, 1.6926e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,096][circuit_model.py][line:1553][INFO] ##10-th layer ##Weight##: The head8 weight for token [ the] are: tensor([8.3080e-01, 9.0461e-03, 2.9549e-02, 2.3137e-02, 9.9997e-03, 1.7053e-03,
        2.9987e-04, 3.8811e-04, 1.5331e-03, 4.8190e-03, 1.2996e-02, 2.6974e-03,
        1.3409e-02, 1.4997e-02, 2.8048e-03, 1.4707e-02, 2.7115e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,097][circuit_model.py][line:1556][INFO] ##10-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.0040, 0.0834, 0.0284, 0.0629, 0.0538, 0.0183, 0.0476, 0.0450, 0.0497,
        0.0228, 0.0096, 0.0762, 0.0750, 0.0619, 0.1198, 0.0996, 0.1419],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,097][circuit_model.py][line:1559][INFO] ##10-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0007, 0.0246, 0.0042, 0.0170, 0.0035, 0.0026, 0.0127, 0.0074, 0.0026,
        0.0206, 0.0048, 0.0081, 0.0708, 0.3934, 0.0147, 0.1059, 0.3065],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,098][circuit_model.py][line:1562][INFO] ##10-th layer ##Weight##: The head11 weight for token [ the] are: tensor([0.2262, 0.0421, 0.0304, 0.0372, 0.0594, 0.0656, 0.0371, 0.0487, 0.0588,
        0.0605, 0.0574, 0.0476, 0.0368, 0.0386, 0.0575, 0.0532, 0.0428],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,098][circuit_model.py][line:1565][INFO] ##10-th layer ##Weight##: The head12 weight for token [ the] are: tensor([4.2572e-01, 1.7220e-03, 2.6880e-01, 2.4310e-02, 3.7430e-03, 1.5042e-02,
        8.8485e-04, 2.5635e-03, 1.0865e-03, 5.9202e-03, 2.2842e-02, 3.7759e-04,
        2.0217e-02, 7.1125e-03, 1.7950e-03, 1.7649e-01, 2.1384e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,168][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:32,169][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,169][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,169][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,170][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,170][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,170][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,170][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,171][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,172][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,174][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,175][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,176][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,178][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.0161, 0.9839], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,179][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.0139, 0.9861], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,181][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.0273, 0.9727], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,182][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.2788, 0.7212], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,182][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.0706, 0.9294], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,183][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.0356, 0.9644], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,183][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.0052, 0.9948], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,183][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.5532, 0.4468], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,183][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.1411, 0.8589], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,184][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.0055, 0.9945], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,184][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.8740, 0.1260], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,184][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.1340, 0.8660], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,185][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.0061, 0.6502, 0.3436], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,185][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0010, 0.1840, 0.8150], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,185][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0019, 0.5101, 0.4879], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,187][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.2445, 0.6279, 0.1276], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,189][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0669, 0.8332, 0.0998], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,190][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0023, 0.2373, 0.7605], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,191][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0023, 0.3222, 0.6755], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,193][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.3307, 0.0338, 0.6355], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,194][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.0172, 0.1107, 0.8721], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,196][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([4.7656e-04, 1.4962e-01, 8.4990e-01], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,197][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.0533, 0.0246, 0.9221], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,198][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0180, 0.4061, 0.5758], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,201][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0014, 0.4038, 0.5285, 0.0664], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,202][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([1.2122e-04, 7.6490e-02, 3.3324e-01, 5.9015e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,203][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0020, 0.2466, 0.2446, 0.5068], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,204][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.2301, 0.4617, 0.2190, 0.0892], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,207][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([0.0096, 0.1133, 0.0943, 0.7828], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,208][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([0.0432, 0.0803, 0.5979, 0.2786], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,210][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0022, 0.1625, 0.5541, 0.2812], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,211][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.3664, 0.0633, 0.2702, 0.3001], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,213][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.0128, 0.0901, 0.4037, 0.4935], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,215][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0015, 0.0452, 0.7826, 0.1707], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,215][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([5.7616e-02, 1.7264e-02, 9.2509e-01, 3.2072e-05], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,218][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.1467, 0.0193, 0.5332, 0.3007], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,219][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0015, 0.4261, 0.4047, 0.0453, 0.1224], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,221][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0030, 0.0825, 0.3403, 0.3438, 0.2304], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,223][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.0283, 0.2153, 0.4830, 0.2078, 0.0655], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,224][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.1954, 0.2650, 0.2583, 0.1610, 0.1202], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,226][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([0.0009, 0.1063, 0.0219, 0.3467, 0.5242], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,228][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.0251, 0.0509, 0.5181, 0.1280, 0.2779], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,229][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.0022, 0.2102, 0.5261, 0.1228, 0.1387], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,231][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.3269, 0.0485, 0.3879, 0.0928, 0.1439], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,232][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.0624, 0.0611, 0.3110, 0.1956, 0.3699], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,234][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([3.9790e-04, 5.1546e-02, 6.6879e-01, 2.0316e-01, 7.6103e-02],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,235][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([1.6924e-01, 1.7094e-02, 8.1320e-01, 2.3705e-05, 4.4337e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,236][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0304, 0.0310, 0.2713, 0.6317, 0.0356], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,238][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0246, 0.2599, 0.2700, 0.0439, 0.2960, 0.1056], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,240][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.0011, 0.0233, 0.1516, 0.5341, 0.2740, 0.0158], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,242][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.0341, 0.1952, 0.3142, 0.3086, 0.0725, 0.0754], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,243][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.4518, 0.1517, 0.1205, 0.0840, 0.1121, 0.0799], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,245][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([0.0297, 0.0414, 0.0110, 0.2507, 0.6315, 0.0357], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,247][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.0199, 0.0376, 0.1655, 0.1803, 0.4261, 0.1706], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,248][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.0096, 0.1761, 0.2859, 0.2813, 0.1907, 0.0564], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,249][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.6329, 0.0287, 0.1278, 0.0754, 0.1048, 0.0304], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,250][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.3304, 0.0402, 0.2868, 0.1636, 0.1557, 0.0234], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,250][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.0005, 0.0734, 0.3047, 0.1605, 0.0957, 0.3651], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,250][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([3.5353e-01, 9.5261e-03, 6.3184e-01, 1.7646e-05, 5.4682e-04, 4.5435e-03],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,251][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.0135, 0.0178, 0.0792, 0.7770, 0.0485, 0.0639], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,251][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.0049, 0.2748, 0.2724, 0.0491, 0.2079, 0.0247, 0.1661],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,252][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.0015, 0.0220, 0.1231, 0.2948, 0.4379, 0.0114, 0.1093],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,252][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.0537, 0.2065, 0.2989, 0.1373, 0.0802, 0.0487, 0.1746],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,252][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.4214, 0.1742, 0.1632, 0.0609, 0.0871, 0.0725, 0.0207],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,253][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([0.0631, 0.0684, 0.0179, 0.3118, 0.4574, 0.0485, 0.0330],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,253][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.0314, 0.0321, 0.2575, 0.1429, 0.3506, 0.1408, 0.0448],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,253][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.0029, 0.1523, 0.1577, 0.2283, 0.2980, 0.0701, 0.0907],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,254][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.5788, 0.0587, 0.0950, 0.1507, 0.0674, 0.0416, 0.0078],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,256][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.0859, 0.1086, 0.4501, 0.1335, 0.1538, 0.0172, 0.0509],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,258][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.0165, 0.0290, 0.2397, 0.1479, 0.1106, 0.2754, 0.1808],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,258][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([2.2777e-01, 9.7877e-03, 7.5702e-01, 1.5111e-05, 4.9312e-04, 2.8039e-03,
        2.1096e-03], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,260][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.3100, 0.0291, 0.1090, 0.2891, 0.0779, 0.1342, 0.0508],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,262][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.0498, 0.1268, 0.4187, 0.0270, 0.2115, 0.0224, 0.1124, 0.0315],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,263][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.0064, 0.0055, 0.1748, 0.1074, 0.4623, 0.0142, 0.0443, 0.1851],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,265][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.2720, 0.1109, 0.2591, 0.1234, 0.0249, 0.0256, 0.0734, 0.1107],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,266][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([9.3954e-01, 8.9834e-03, 3.4118e-02, 5.2867e-03, 6.4999e-03, 3.5417e-03,
        6.8804e-04, 1.3423e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,268][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([0.1992, 0.0332, 0.0407, 0.1395, 0.4560, 0.0636, 0.0431, 0.0248],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,269][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.0984, 0.0181, 0.2585, 0.0530, 0.2211, 0.1978, 0.0322, 0.1209],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,271][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.0954, 0.0509, 0.2554, 0.0790, 0.3006, 0.0520, 0.0688, 0.0978],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,272][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([9.7369e-01, 3.3961e-03, 5.8890e-03, 9.1452e-03, 5.1881e-03, 1.8262e-03,
        1.6538e-04, 7.0018e-04], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,274][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.4075, 0.0240, 0.4096, 0.0618, 0.0710, 0.0051, 0.0078, 0.0132],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,276][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.0022, 0.0203, 0.1891, 0.0380, 0.0320, 0.2345, 0.1080, 0.3758],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,277][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([5.5758e-01, 2.7387e-03, 4.3350e-01, 1.3631e-05, 4.5033e-04, 1.7116e-03,
        7.2123e-04, 3.2849e-03], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,278][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.7242, 0.0065, 0.0518, 0.0865, 0.0413, 0.0455, 0.0060, 0.0383],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,280][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0111, 0.1771, 0.2548, 0.0284, 0.1711, 0.0491, 0.1409, 0.0719, 0.0955],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,282][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0009, 0.0112, 0.0672, 0.1500, 0.2614, 0.0067, 0.0855, 0.1635, 0.2535],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,284][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0074, 0.0699, 0.2792, 0.1272, 0.0744, 0.0394, 0.1373, 0.1771, 0.0882],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,286][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.5126, 0.0803, 0.1437, 0.0463, 0.0850, 0.0448, 0.0095, 0.0155, 0.0624],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,287][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0430, 0.0438, 0.0071, 0.3596, 0.4571, 0.0276, 0.0283, 0.0177, 0.0158],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,288][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0163, 0.0102, 0.0562, 0.0781, 0.2799, 0.1013, 0.0295, 0.2546, 0.1739],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,291][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0121, 0.0711, 0.1407, 0.0834, 0.1620, 0.0403, 0.0868, 0.1437, 0.2598],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,292][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.6997, 0.0256, 0.0636, 0.0943, 0.0497, 0.0292, 0.0050, 0.0131, 0.0199],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,294][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.0540, 0.0698, 0.3916, 0.1262, 0.1225, 0.0110, 0.0417, 0.0488, 0.1345],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,296][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0019, 0.0075, 0.0599, 0.0460, 0.0291, 0.1495, 0.1015, 0.5152, 0.0895],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,297][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([5.1724e-01, 9.6113e-03, 4.2389e-01, 1.7193e-05, 6.2011e-04, 3.3989e-03,
        3.0158e-03, 3.5585e-02, 6.6215e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,298][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0447, 0.0120, 0.0274, 0.6955, 0.0530, 0.0467, 0.0327, 0.0584, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,300][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0097, 0.2610, 0.4310, 0.0265, 0.0826, 0.0082, 0.0584, 0.0133, 0.0376,
        0.0716], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,302][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.0007, 0.0168, 0.0963, 0.0728, 0.1990, 0.0025, 0.0436, 0.1068, 0.0890,
        0.3727], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,304][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.0643, 0.1480, 0.2877, 0.0771, 0.0424, 0.0188, 0.0759, 0.1678, 0.0213,
        0.0967], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,305][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.7960, 0.0527, 0.0603, 0.0175, 0.0229, 0.0160, 0.0031, 0.0073, 0.0129,
        0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,307][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([0.2154, 0.0331, 0.0462, 0.1655, 0.3666, 0.0836, 0.0380, 0.0171, 0.0172,
        0.0173], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,309][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.3424, 0.0059, 0.0455, 0.0131, 0.0407, 0.0505, 0.0069, 0.0224, 0.0340,
        0.4386], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,310][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0048, 0.1013, 0.2058, 0.0704, 0.0528, 0.0147, 0.0254, 0.0546, 0.0284,
        0.4418], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,312][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([8.5125e-01, 2.6221e-02, 6.2699e-02, 1.8782e-02, 1.3603e-02, 3.4335e-03,
        8.1171e-04, 9.0491e-04, 3.6628e-03, 1.8628e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,314][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.3428, 0.0254, 0.3262, 0.0731, 0.0903, 0.0051, 0.0088, 0.0249, 0.0307,
        0.0728], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,315][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.0038, 0.0138, 0.0549, 0.0132, 0.0114, 0.1169, 0.0377, 0.2415, 0.0361,
        0.4706], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,316][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([4.9715e-01, 1.3658e-02, 4.6542e-01, 1.2443e-05, 2.2692e-04, 2.3629e-03,
        1.6819e-03, 1.5103e-02, 3.9749e-03, 4.0643e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,318][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0674, 0.0383, 0.0932, 0.2751, 0.0254, 0.0363, 0.0227, 0.0593, 0.0129,
        0.3695], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,320][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0065, 0.1103, 0.1407, 0.0144, 0.0742, 0.0603, 0.1128, 0.0270, 0.0693,
        0.0357, 0.3487], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,321][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([0.0005, 0.0024, 0.0307, 0.0598, 0.1933, 0.0046, 0.0622, 0.1089, 0.0651,
        0.0733, 0.3993], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,322][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.0747, 0.0753, 0.2625, 0.0954, 0.0241, 0.0262, 0.1250, 0.0931, 0.0499,
        0.0578, 0.1160], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,323][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([0.7292, 0.0185, 0.0600, 0.0167, 0.0128, 0.0177, 0.0033, 0.0069, 0.0046,
        0.0092, 0.1210], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,323][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([0.4896, 0.0130, 0.0209, 0.1024, 0.2210, 0.0258, 0.0128, 0.0085, 0.0092,
        0.0130, 0.0839], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,324][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.1460, 0.0066, 0.0224, 0.0194, 0.0361, 0.0246, 0.0059, 0.0343, 0.0189,
        0.3016, 0.3843], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,324][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.0090, 0.0352, 0.1063, 0.0313, 0.0870, 0.0310, 0.0416, 0.0705, 0.0487,
        0.1905, 0.3490], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,324][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([9.5435e-01, 1.3602e-03, 9.6245e-03, 2.6725e-03, 3.7097e-03, 1.0752e-03,
        9.9894e-05, 2.4520e-04, 1.0006e-03, 2.3332e-03, 2.3528e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,325][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1666, 0.0102, 0.0893, 0.0244, 0.0552, 0.0061, 0.0060, 0.0079, 0.0228,
        0.0105, 0.6009], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,325][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.0013, 0.0123, 0.0707, 0.0392, 0.0078, 0.0364, 0.0225, 0.1505, 0.0156,
        0.3283, 0.3154], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,326][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([3.4863e-01, 2.9746e-03, 5.8002e-01, 5.9858e-06, 2.2707e-04, 2.8884e-03,
        2.3345e-03, 1.0884e-02, 3.6735e-03, 2.3642e-04, 4.8132e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,328][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.0287, 0.0074, 0.0362, 0.5232, 0.0267, 0.0261, 0.0428, 0.0402, 0.0114,
        0.1540, 0.1032], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,329][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0052, 0.0709, 0.1214, 0.0168, 0.0707, 0.0326, 0.0584, 0.0349, 0.0458,
        0.0753, 0.3273, 0.1407], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,331][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.0008, 0.0039, 0.0377, 0.0586, 0.1586, 0.0035, 0.0299, 0.0716, 0.0802,
        0.0786, 0.2283, 0.2483], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,332][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0093, 0.0398, 0.3372, 0.0948, 0.0366, 0.0278, 0.0834, 0.1036, 0.0640,
        0.0639, 0.0906, 0.0489], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,334][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.5978, 0.0443, 0.0949, 0.0278, 0.0265, 0.0221, 0.0031, 0.0056, 0.0189,
        0.0068, 0.1139, 0.0383], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,336][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([0.0559, 0.0424, 0.0117, 0.3671, 0.3743, 0.0328, 0.0274, 0.0128, 0.0156,
        0.0096, 0.0393, 0.0113], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,337][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0256, 0.0034, 0.0133, 0.0161, 0.0340, 0.0160, 0.0044, 0.0244, 0.0238,
        0.2533, 0.2827, 0.3030], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,339][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0049, 0.0221, 0.0958, 0.0448, 0.0393, 0.0197, 0.0310, 0.0500, 0.0727,
        0.2489, 0.2156, 0.1550], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,341][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.7030, 0.0157, 0.0717, 0.0601, 0.0427, 0.0176, 0.0015, 0.0062, 0.0066,
        0.0142, 0.0531, 0.0076], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,342][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([0.1120, 0.0173, 0.1263, 0.0285, 0.0375, 0.0039, 0.0063, 0.0090, 0.0261,
        0.0368, 0.5496, 0.0466], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,344][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0022, 0.0035, 0.0395, 0.0353, 0.0099, 0.0448, 0.0366, 0.1257, 0.0215,
        0.2672, 0.2974, 0.1164], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,345][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([5.9125e-01, 2.9080e-03, 3.7098e-01, 4.8495e-06, 2.3097e-04, 2.0444e-03,
        9.5140e-04, 1.1552e-02, 2.8597e-03, 1.6521e-04, 1.5767e-02, 1.2826e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,347][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0437, 0.0080, 0.0308, 0.6479, 0.0352, 0.0265, 0.0243, 0.0185, 0.0165,
        0.1183, 0.0224, 0.0080], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,349][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0046, 0.0898, 0.3578, 0.0402, 0.0568, 0.0139, 0.0258, 0.0178, 0.0273,
        0.0435, 0.1259, 0.1194, 0.0772], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,351][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0022, 0.0051, 0.0613, 0.0671, 0.1235, 0.0023, 0.0146, 0.0487, 0.0315,
        0.0772, 0.1933, 0.0997, 0.2735], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,352][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0123, 0.1091, 0.3871, 0.1031, 0.0243, 0.0341, 0.0569, 0.0912, 0.0161,
        0.0631, 0.0489, 0.0204, 0.0335], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,354][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.3264, 0.0460, 0.0631, 0.0466, 0.0337, 0.0243, 0.0070, 0.0109, 0.0171,
        0.0056, 0.2387, 0.0439, 0.1368], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,356][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([0.0535, 0.0310, 0.0271, 0.1986, 0.2432, 0.0423, 0.0265, 0.0127, 0.0213,
        0.0185, 0.0870, 0.0222, 0.2159], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,358][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0232, 0.0028, 0.0332, 0.0191, 0.0153, 0.0183, 0.0016, 0.0076, 0.0053,
        0.1019, 0.1107, 0.0691, 0.5919], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,359][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0091, 0.0285, 0.2841, 0.0375, 0.0424, 0.0149, 0.0261, 0.0343, 0.0238,
        0.1024, 0.2260, 0.0565, 0.1144], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,361][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.8114, 0.0211, 0.0573, 0.0257, 0.0172, 0.0060, 0.0012, 0.0015, 0.0045,
        0.0108, 0.0172, 0.0072, 0.0188], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,363][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.1440, 0.0247, 0.2554, 0.1517, 0.0698, 0.0028, 0.0050, 0.0118, 0.0126,
        0.0206, 0.2376, 0.0186, 0.0454], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,365][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0098, 0.0048, 0.0305, 0.0079, 0.0020, 0.0125, 0.0056, 0.0237, 0.0066,
        0.0578, 0.3186, 0.0361, 0.4840], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,366][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([3.8787e-01, 4.9753e-03, 5.6713e-01, 2.4760e-05, 4.6412e-04, 2.4471e-03,
        1.2389e-03, 6.8162e-03, 2.9570e-03, 3.3884e-04, 2.1441e-02, 2.5832e-03,
        1.7138e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,367][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([7.7606e-01, 1.3834e-03, 4.8721e-02, 2.6591e-02, 4.4432e-03, 9.3851e-03,
        7.5200e-04, 4.0930e-03, 1.2903e-03, 1.4489e-02, 4.9203e-02, 5.3155e-04,
        6.3054e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,369][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0013, 0.1020, 0.2494, 0.0475, 0.0308, 0.0110, 0.0148, 0.0126, 0.0150,
        0.0257, 0.1519, 0.0599, 0.0911, 0.1872], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,370][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([3.7873e-05, 3.4477e-03, 1.5488e-02, 2.5947e-02, 3.2088e-02, 1.2148e-03,
        7.1497e-03, 2.4538e-02, 1.3749e-02, 2.4662e-02, 1.0427e-01, 6.0523e-02,
        9.6539e-02, 5.9035e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,371][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0130, 0.1041, 0.3023, 0.1083, 0.0125, 0.0363, 0.0779, 0.0911, 0.0240,
        0.0410, 0.0976, 0.0322, 0.0353, 0.0243], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,374][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.1445, 0.1365, 0.1177, 0.0860, 0.0138, 0.0257, 0.0053, 0.0087, 0.0116,
        0.0046, 0.1254, 0.0197, 0.0874, 0.2130], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,375][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0134, 0.0245, 0.0215, 0.2846, 0.1650, 0.0452, 0.0267, 0.0097, 0.0250,
        0.0129, 0.1119, 0.0175, 0.0821, 0.1599], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,377][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.2500, 0.0019, 0.0166, 0.0121, 0.0117, 0.0078, 0.0007, 0.0046, 0.0025,
        0.0245, 0.0551, 0.0143, 0.2636, 0.3346], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,379][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0008, 0.0217, 0.1543, 0.0547, 0.0245, 0.0175, 0.0147, 0.0254, 0.0289,
        0.0653, 0.2566, 0.0752, 0.1079, 0.1526], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,380][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([8.7545e-01, 9.9542e-03, 2.9586e-02, 2.0257e-02, 7.7669e-03, 2.5763e-03,
        3.9903e-04, 4.7897e-04, 2.0952e-03, 4.3495e-03, 2.2042e-02, 3.4570e-03,
        8.7518e-03, 1.2835e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,382][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0691, 0.0508, 0.3051, 0.1161, 0.0599, 0.0029, 0.0034, 0.0109, 0.0119,
        0.0195, 0.2536, 0.0130, 0.0368, 0.0469], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,383][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0287, 0.0042, 0.0243, 0.0112, 0.0007, 0.0114, 0.0025, 0.0179, 0.0031,
        0.0362, 0.1103, 0.0109, 0.2421, 0.4966], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,384][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([2.6416e-01, 8.9326e-03, 6.5739e-01, 1.7480e-05, 2.2095e-04, 2.2955e-03,
        1.5411e-03, 7.6162e-03, 4.5464e-03, 2.9719e-04, 4.7251e-02, 3.6690e-03,
        1.9397e-03, 1.2220e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,386][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.3311, 0.0017, 0.4267, 0.0584, 0.0055, 0.0241, 0.0017, 0.0039, 0.0018,
        0.0147, 0.0513, 0.0008, 0.0545, 0.0238], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,388][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.0228, 0.1102, 0.2345, 0.0121, 0.0529, 0.0129, 0.0401, 0.0252, 0.0233,
        0.0381, 0.2926, 0.0464, 0.0310, 0.0281, 0.0298], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,389][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.0020, 0.0096, 0.0864, 0.0599, 0.1198, 0.0029, 0.0146, 0.0317, 0.0205,
        0.0305, 0.2242, 0.0488, 0.1467, 0.1795, 0.0230], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,389][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.0070, 0.0666, 0.2496, 0.0891, 0.0269, 0.0325, 0.0759, 0.1249, 0.0188,
        0.0435, 0.0764, 0.0200, 0.0315, 0.0257, 0.1117], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,390][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.3078, 0.1155, 0.0720, 0.0483, 0.0235, 0.0170, 0.0052, 0.0111, 0.0116,
        0.0097, 0.1125, 0.0171, 0.0734, 0.0815, 0.0938], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,390][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([0.1413, 0.0336, 0.0123, 0.1600, 0.1456, 0.0205, 0.0172, 0.0163, 0.0186,
        0.0128, 0.0535, 0.0208, 0.1473, 0.1191, 0.0812], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,391][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([2.3221e-02, 1.1879e-03, 3.2907e-03, 2.0120e-03, 1.1571e-03, 9.9365e-04,
        2.6558e-04, 1.1681e-03, 7.7601e-04, 1.4433e-02, 1.6817e-02, 1.1647e-02,
        2.1424e-01, 3.9036e-01, 3.1843e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,391][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0084, 0.0566, 0.2272, 0.0703, 0.0647, 0.0116, 0.0120, 0.0311, 0.0140,
        0.1112, 0.1544, 0.0287, 0.1273, 0.0584, 0.0241], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,391][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([9.1104e-01, 4.2258e-03, 2.1245e-02, 1.3774e-02, 8.6025e-03, 1.6772e-03,
        4.7757e-04, 1.4444e-03, 2.3732e-03, 8.8850e-03, 1.0497e-02, 2.2792e-03,
        6.8925e-03, 4.2856e-03, 2.3067e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,392][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([0.0395, 0.0220, 0.3750, 0.0758, 0.0782, 0.0019, 0.0082, 0.0082, 0.0128,
        0.0356, 0.1699, 0.0228, 0.0335, 0.0459, 0.0704], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,392][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([4.9308e-03, 7.1837e-04, 1.2192e-03, 8.6372e-04, 5.9427e-05, 3.2712e-04,
        1.5714e-04, 7.0051e-04, 1.5008e-04, 1.9567e-03, 1.2673e-02, 1.0446e-03,
        4.7686e-02, 7.7959e-02, 8.4955e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,393][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([2.6970e-01, 8.6548e-03, 6.7489e-01, 1.5198e-05, 7.3788e-04, 2.3177e-03,
        1.0824e-03, 1.1318e-02, 1.9915e-03, 2.0185e-04, 2.4173e-02, 1.1582e-03,
        9.0472e-04, 3.4612e-05, 2.8186e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,394][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([9.1313e-01, 9.1074e-04, 3.0127e-02, 1.3549e-02, 3.3273e-03, 2.2723e-03,
        2.6738e-04, 9.0116e-04, 9.2487e-05, 3.9836e-03, 1.3841e-02, 6.3312e-05,
        7.4852e-03, 8.6906e-03, 1.3560e-03], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,396][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.0055, 0.0682, 0.2313, 0.0314, 0.0414, 0.0114, 0.0304, 0.0095, 0.0136,
        0.0252, 0.1049, 0.0344, 0.0599, 0.1352, 0.0488, 0.1489],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,397][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([1.1773e-04, 1.8038e-03, 1.2481e-02, 1.7385e-02, 4.4646e-02, 9.2731e-04,
        1.1110e-02, 2.2596e-02, 1.7759e-02, 3.7910e-02, 4.8233e-02, 3.1355e-02,
        7.2181e-02, 1.7842e-01, 1.4793e-02, 4.8828e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,399][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.0052, 0.0440, 0.2279, 0.0857, 0.0354, 0.0274, 0.0678, 0.1153, 0.0171,
        0.0275, 0.0786, 0.0189, 0.0299, 0.0221, 0.0774, 0.1198],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,401][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.2180, 0.0476, 0.0586, 0.0260, 0.0123, 0.0126, 0.0054, 0.0066, 0.0118,
        0.0027, 0.0633, 0.0232, 0.0631, 0.0740, 0.1026, 0.2722],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,403][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([0.0752, 0.0187, 0.0161, 0.0934, 0.1311, 0.0308, 0.0226, 0.0103, 0.0112,
        0.0100, 0.0496, 0.0101, 0.0653, 0.1321, 0.0539, 0.2696],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,404][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([1.0753e-02, 1.5466e-04, 8.2605e-04, 1.4547e-03, 3.9579e-04, 9.5457e-04,
        1.5839e-04, 5.5324e-04, 5.8190e-04, 5.3008e-03, 4.4144e-03, 8.6302e-03,
        8.2479e-02, 1.8913e-01, 1.6274e-01, 5.3147e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,405][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.0033, 0.0311, 0.1800, 0.0363, 0.0364, 0.0109, 0.0218, 0.0341, 0.0362,
        0.0741, 0.2075, 0.0526, 0.0645, 0.0392, 0.0388, 0.1334],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,407][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.7869, 0.0127, 0.0575, 0.0252, 0.0139, 0.0053, 0.0014, 0.0020, 0.0056,
        0.0127, 0.0196, 0.0053, 0.0156, 0.0138, 0.0048, 0.0178],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,408][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([1.3230e-02, 8.9279e-03, 4.6890e-02, 2.9833e-02, 2.4394e-02, 5.7441e-04,
        1.5564e-03, 4.4444e-03, 5.6237e-03, 4.2360e-03, 3.7514e-02, 1.1483e-02,
        1.4504e-02, 3.5595e-02, 2.7366e-02, 7.3383e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,409][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([1.0479e-03, 1.3320e-04, 1.4374e-04, 2.4283e-04, 1.7277e-05, 1.9580e-04,
        7.3380e-05, 2.5582e-04, 1.1380e-04, 5.9864e-04, 2.7895e-03, 5.6955e-04,
        1.4478e-02, 2.9144e-02, 3.8435e-01, 5.6584e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,410][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([4.7385e-01, 8.3382e-03, 4.7332e-01, 3.3196e-05, 3.6110e-04, 2.1781e-03,
        1.9286e-03, 8.5342e-03, 3.3311e-03, 2.8619e-04, 2.1541e-02, 1.7865e-03,
        1.2655e-03, 9.6863e-05, 2.6620e-03, 4.8522e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,412][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.0812, 0.0090, 0.0621, 0.0547, 0.0107, 0.0080, 0.0032, 0.0094, 0.0019,
        0.0145, 0.0285, 0.0008, 0.0929, 0.0406, 0.0103, 0.5723],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,414][circuit_model.py][line:1570][INFO] ##10-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0027, 0.0650, 0.1527, 0.0342, 0.0221, 0.0067, 0.0110, 0.0107, 0.0075,
        0.0168, 0.0581, 0.0237, 0.0733, 0.1593, 0.0435, 0.0516, 0.2612],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,415][circuit_model.py][line:1573][INFO] ##10-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([1.2543e-05, 1.6135e-03, 4.1807e-03, 1.3505e-02, 1.7494e-02, 5.2614e-04,
        4.0156e-03, 1.3931e-02, 6.4758e-03, 1.3739e-02, 2.3222e-02, 2.0213e-02,
        4.6861e-02, 3.4277e-01, 1.2752e-02, 1.7286e-01, 3.0583e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,417][circuit_model.py][line:1576][INFO] ##10-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0046, 0.0694, 0.1743, 0.1083, 0.0129, 0.0322, 0.0701, 0.0919, 0.0202,
        0.0339, 0.0542, 0.0234, 0.0276, 0.0217, 0.0595, 0.1087, 0.0871],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,419][circuit_model.py][line:1579][INFO] ##10-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0331, 0.0724, 0.0348, 0.0388, 0.0080, 0.0078, 0.0032, 0.0045, 0.0048,
        0.0017, 0.0312, 0.0081, 0.0470, 0.0995, 0.0547, 0.2669, 0.2835],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,421][circuit_model.py][line:1582][INFO] ##10-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([0.0083, 0.0143, 0.0087, 0.1730, 0.1046, 0.0300, 0.0171, 0.0064, 0.0159,
        0.0073, 0.0390, 0.0099, 0.0705, 0.0942, 0.0374, 0.1125, 0.2508],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,422][circuit_model.py][line:1585][INFO] ##10-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([3.3664e-02, 9.8155e-05, 5.9593e-04, 8.1503e-04, 3.5911e-04, 4.1197e-04,
        3.8724e-05, 2.5207e-04, 2.1528e-04, 1.2368e-03, 3.0014e-03, 1.5410e-03,
        3.4863e-02, 4.8842e-02, 3.4624e-02, 2.1413e-01, 6.2531e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,424][circuit_model.py][line:1588][INFO] ##10-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0019, 0.0203, 0.1077, 0.0399, 0.0212, 0.0125, 0.0138, 0.0215, 0.0289,
        0.0464, 0.1635, 0.0615, 0.0712, 0.0920, 0.0451, 0.1443, 0.1082],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,425][circuit_model.py][line:1591][INFO] ##10-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([8.3080e-01, 9.0461e-03, 2.9549e-02, 2.3137e-02, 9.9997e-03, 1.7053e-03,
        2.9987e-04, 3.8811e-04, 1.5331e-03, 4.8190e-03, 1.2996e-02, 2.6974e-03,
        1.3409e-02, 1.4997e-02, 2.8048e-03, 1.4707e-02, 2.7115e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,426][circuit_model.py][line:1594][INFO] ##10-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.0067, 0.0195, 0.0512, 0.0306, 0.0135, 0.0007, 0.0010, 0.0037, 0.0036,
        0.0042, 0.0370, 0.0043, 0.0100, 0.0164, 0.0233, 0.6471, 0.1271],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,427][circuit_model.py][line:1597][INFO] ##10-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([8.5895e-03, 1.2295e-04, 4.4642e-04, 3.4192e-04, 1.3832e-05, 3.6543e-04,
        6.1329e-05, 3.3236e-04, 1.1621e-04, 7.4610e-04, 2.5215e-03, 4.2176e-04,
        1.5954e-02, 2.6043e-02, 1.2092e-01, 4.4865e-01, 3.7435e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,429][circuit_model.py][line:1600][INFO] ##10-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([5.0259e-01, 1.4388e-02, 4.2415e-01, 1.7774e-05, 2.2173e-04, 2.0725e-03,
        1.6450e-03, 1.0511e-02, 5.3226e-03, 3.0048e-04, 3.1044e-02, 3.2623e-03,
        1.6804e-03, 1.0695e-04, 2.2910e-03, 2.1411e-04, 1.8309e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,430][circuit_model.py][line:1603][INFO] ##10-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([4.2572e-01, 1.7220e-03, 2.6880e-01, 2.4310e-02, 3.7430e-03, 1.5042e-02,
        8.8485e-04, 2.5635e-03, 1.0865e-03, 5.9202e-03, 2.2842e-02, 3.7759e-04,
        2.0217e-02, 7.1125e-03, 1.7950e-03, 1.7649e-01, 2.1384e-02],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,431][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:32,434][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[ 225],
        [1553],
        [ 211],
        [  24],
        [  15],
        [ 102],
        [   8],
        [  10],
        [  12],
        [   4],
        [  56],
        [  18],
        [  17],
        [   7],
        [  87],
        [  12],
        [  18]], device='cuda:0')
[2024-07-23 21:05:32,436][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[237],
        [800],
        [160],
        [ 32],
        [ 23],
        [136],
        [ 10],
        [ 30],
        [ 13],
        [  9],
        [ 93],
        [ 32],
        [ 37],
        [ 20],
        [205],
        [ 33],
        [ 45]], device='cuda:0')
[2024-07-23 21:05:32,438][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[22079],
        [26198],
        [24192],
        [21664],
        [21538],
        [19706],
        [20748],
        [18949],
        [20049],
        [19834],
        [16259],
        [14744],
        [16699],
        [15533],
        [15788],
        [15049],
        [13765]], device='cuda:0')
[2024-07-23 21:05:32,439][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 6493],
        [50144],
        [25585],
        [18928],
        [16274],
        [11726],
        [10136],
        [17216],
        [24083],
        [24846],
        [22197],
        [22830],
        [20629],
        [13231],
        [17408],
        [15694],
        [ 9996]], device='cuda:0')
[2024-07-23 21:05:32,441][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[20952],
        [26800],
        [25756],
        [20983],
        [23877],
        [24376],
        [28022],
        [27083],
        [28237],
        [28908],
        [30323],
        [29397],
        [28338],
        [29351],
        [31292],
        [31345],
        [29964]], device='cuda:0')
[2024-07-23 21:05:32,442][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[ 3934],
        [10848],
        [30443],
        [16281],
        [26353],
        [10744],
        [ 8151],
        [ 5716],
        [ 7293],
        [10358],
        [16892],
        [11623],
        [18078],
        [22849],
        [22352],
        [13122],
        [26331]], device='cuda:0')
[2024-07-23 21:05:32,444][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[ 1417],
        [38777],
        [37344],
        [20616],
        [30191],
        [30821],
        [29658],
        [30348],
        [29021],
        [29124],
        [28777],
        [29088],
        [30523],
        [28466],
        [26724],
        [25776],
        [23278]], device='cuda:0')
[2024-07-23 21:05:32,445][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[40215],
        [12282],
        [10381],
        [11884],
        [11182],
        [12446],
        [11820],
        [12876],
        [10499],
        [18366],
        [13833],
        [ 8314],
        [15892],
        [17741],
        [11360],
        [10366],
        [13412]], device='cuda:0')
[2024-07-23 21:05:32,447][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[34654],
        [15524],
        [12747],
        [ 6949],
        [ 7899],
        [ 6166],
        [ 5957],
        [ 6661],
        [ 8447],
        [ 9642],
        [ 9361],
        [ 9619],
        [ 8307],
        [ 7396],
        [ 7435],
        [ 8496],
        [ 7498]], device='cuda:0')
[2024-07-23 21:05:32,449][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[15543],
        [  673],
        [ 8860],
        [ 9816],
        [11902],
        [13355],
        [10741],
        [14755],
        [12379],
        [10254],
        [15391],
        [13858],
        [11793],
        [13910],
        [14228],
        [13241],
        [15838]], device='cuda:0')
[2024-07-23 21:05:32,450][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[ 1880],
        [15554],
        [16684],
        [12476],
        [14369],
        [13489],
        [12595],
        [12099],
        [11713],
        [11790],
        [11959],
        [11858],
        [10069],
        [10145],
        [10257],
        [ 9073],
        [ 8339]], device='cuda:0')
[2024-07-23 21:05:32,452][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[16249],
        [20767],
        [18919],
        [20112],
        [20410],
        [20495],
        [20529],
        [22186],
        [20887],
        [14999],
        [17282],
        [15481],
        [15209],
        [21626],
        [21968],
        [22072],
        [23334]], device='cuda:0')
[2024-07-23 21:05:32,454][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[7976],
        [2520],
        [2938],
        [2453],
        [1990],
        [1995],
        [2049],
        [2263],
        [2056],
        [1892],
        [1863],
        [1787],
        [1684],
        [1659],
        [1669],
        [1663],
        [1605]], device='cuda:0')
[2024-07-23 21:05:32,455][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[ 5265],
        [49425],
        [46898],
        [38427],
        [38942],
        [37670],
        [37963],
        [27812],
        [38453],
        [39193],
        [38720],
        [37781],
        [23666],
        [37396],
        [11600],
        [39189],
        [36668]], device='cuda:0')
[2024-07-23 21:05:32,457][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[1425],
        [3781],
        [1900],
        [1134],
        [2045],
        [1995],
        [2249],
        [1360],
        [1669],
        [1675],
        [1556],
        [1421],
        [1218],
        [1075],
        [1113],
        [1042],
        [ 939]], device='cuda:0')
[2024-07-23 21:05:32,458][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[2336],
        [2401],
        [3561],
        [4105],
        [4311],
        [5666],
        [5361],
        [5458],
        [5397],
        [4667],
        [3097],
        [2346],
        [3557],
        [4099],
        [3174],
        [4785],
        [4556]], device='cuda:0')
[2024-07-23 21:05:32,459][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[1204],
        [1575],
        [2782],
        [2888],
        [3385],
        [3765],
        [3726],
        [3215],
        [2564],
        [3315],
        [2157],
        [2822],
        [2706],
        [3599],
        [3202],
        [2264],
        [2829]], device='cuda:0')
[2024-07-23 21:05:32,460][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[4917],
        [7481],
        [3787],
        [2472],
        [2490],
        [2858],
        [3598],
        [4678],
        [4531],
        [5416],
        [3951],
        [3632],
        [3860],
        [3450],
        [3259],
        [2836],
        [2339]], device='cuda:0')
[2024-07-23 21:05:32,461][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[6432],
        [1592],
        [1861],
        [2302],
        [3122],
        [2741],
        [2728],
        [6004],
        [2792],
        [2487],
        [4072],
        [3143],
        [2642],
        [3464],
        [1874],
        [3608],
        [5902]], device='cuda:0')
[2024-07-23 21:05:32,462][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[1959],
        [2893],
        [2953],
        [1892],
        [ 995],
        [ 727],
        [1013],
        [ 829],
        [1012],
        [ 973],
        [ 628],
        [ 964],
        [ 510],
        [ 583],
        [ 615],
        [ 488],
        [ 597]], device='cuda:0')
[2024-07-23 21:05:32,464][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[ 8352],
        [33074],
        [23941],
        [23666],
        [19734],
        [17679],
        [17955],
        [15596],
        [ 9994],
        [ 8011],
        [ 2675],
        [ 2317],
        [ 5523],
        [ 5641],
        [ 4579],
        [ 7469],
        [ 9693]], device='cuda:0')
[2024-07-23 21:05:32,465][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[3612],
        [1669],
        [7139],
        [5886],
        [5885],
        [4325],
        [3727],
        [3890],
        [4453],
        [3061],
        [2206],
        [2938],
        [3474],
        [2887],
        [3762],
        [2875],
        [2793]], device='cuda:0')
[2024-07-23 21:05:32,467][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[1066],
        [1160],
        [2460],
        [2173],
        [2916],
        [2466],
        [2147],
        [1183],
        [2073],
        [1660],
        [1466],
        [2429],
        [1936],
        [1716],
        [1514],
        [2066],
        [1852]], device='cuda:0')
[2024-07-23 21:05:32,469][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[2164],
        [4823],
        [1974],
        [1715],
        [1229],
        [1502],
        [1520],
        [1858],
        [1471],
        [1913],
        [2543],
        [2314],
        [1463],
        [1477],
        [1174],
        [1186],
        [1086]], device='cuda:0')
[2024-07-23 21:05:32,470][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 4831],
        [13572],
        [20651],
        [20814],
        [18692],
        [13227],
        [ 9910],
        [ 7075],
        [ 5344],
        [ 4589],
        [ 3518],
        [ 3512],
        [ 8583],
        [ 7169],
        [ 5438],
        [ 4944],
        [ 5115]], device='cuda:0')
[2024-07-23 21:05:32,472][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[2302],
        [2654],
        [ 881],
        [ 876],
        [ 888],
        [ 911],
        [ 892],
        [ 968],
        [ 940],
        [ 954],
        [ 896],
        [ 985],
        [ 926],
        [ 905],
        [ 894],
        [ 967],
        [1005]], device='cuda:0')
[2024-07-23 21:05:32,474][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[4192],
        [ 166],
        [  92],
        [  39],
        [  31],
        [  45],
        [  56],
        [ 286],
        [  68],
        [ 109],
        [  87],
        [  73],
        [ 427],
        [  53],
        [1743],
        [ 182],
        [  97]], device='cuda:0')
[2024-07-23 21:05:32,475][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[15090],
        [23458],
        [15014],
        [16954],
        [19291],
        [20349],
        [19053],
        [15315],
        [21555],
        [19595],
        [25673],
        [28231],
        [18737],
        [24643],
        [15890],
        [25481],
        [26062]], device='cuda:0')
[2024-07-23 21:05:32,477][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[29115],
        [26911],
        [33916],
        [32921],
        [28171],
        [30907],
        [32029],
        [35230],
        [34195],
        [38591],
        [40973],
        [38307],
        [38456],
        [39570],
        [41171],
        [38047],
        [38819]], device='cuda:0')
[2024-07-23 21:05:32,478][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541],
        [1541]], device='cuda:0')
[2024-07-23 21:05:32,541][circuit_model.py][line:1111][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:32,542][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,543][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,544][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,545][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,545][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,546][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,547][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,549][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,550][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,551][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,553][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,555][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:32,557][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ original] are: tensor([0.0840, 0.9160], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,559][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ original] are: tensor([0.1298, 0.8702], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,559][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ original] are: tensor([0.0153, 0.9847], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,560][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ original] are: tensor([0.5481, 0.4519], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,561][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ original] are: tensor([0.0067, 0.9933], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,561][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ original] are: tensor([0.0142, 0.9858], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,562][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ original] are: tensor([0.0393, 0.9607], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,564][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ original] are: tensor([0.1013, 0.8987], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,566][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ original] are: tensor([0.2085, 0.7915], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,567][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ original] are: tensor([0.0130, 0.9870], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,570][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ original] are: tensor([0.1057, 0.8943], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,571][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ original] are: tensor([0.0913, 0.9087], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:32,573][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ language] are: tensor([0.1187, 0.4592, 0.4221], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,575][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ language] are: tensor([0.0347, 0.4642, 0.5011], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,577][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ language] are: tensor([0.0098, 0.5825, 0.4077], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,579][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ language] are: tensor([0.4944, 0.1803, 0.3253], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,581][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ language] are: tensor([0.0032, 0.3351, 0.6617], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,583][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ language] are: tensor([0.0096, 0.7746, 0.2157], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,584][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ language] are: tensor([0.0022, 0.0611, 0.9367], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,587][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ language] are: tensor([0.0489, 0.4163, 0.5348], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,588][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ language] are: tensor([0.0746, 0.6012, 0.3242], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,590][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ language] are: tensor([0.0065, 0.2296, 0.7639], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,592][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ language] are: tensor([0.1172, 0.4393, 0.4436], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,594][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ language] are: tensor([0.0241, 0.8023, 0.1736], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:32,595][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ of] are: tensor([0.0132, 0.3428, 0.1413, 0.5027], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,598][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ of] are: tensor([0.0105, 0.2164, 0.4700, 0.3030], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,600][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ of] are: tensor([0.0035, 0.1986, 0.5594, 0.2384], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,601][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ of] are: tensor([0.0654, 0.0883, 0.6839, 0.1623], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,603][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ of] are: tensor([4.1505e-04, 1.1550e-01, 2.7422e-01, 6.0987e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,604][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ of] are: tensor([5.4984e-04, 2.3638e-01, 1.2819e-01, 6.3488e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,606][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ of] are: tensor([0.0022, 0.1019, 0.7796, 0.1163], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,608][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ of] are: tensor([0.0109, 0.1751, 0.3620, 0.4520], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,610][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ of] are: tensor([0.0383, 0.0715, 0.8688, 0.0214], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,611][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ of] are: tensor([0.0011, 0.1791, 0.2854, 0.5343], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,613][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ of] are: tensor([0.3850, 0.1768, 0.2009, 0.2374], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,614][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ of] are: tensor([0.0249, 0.3868, 0.5183, 0.0700], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:32,614][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ De] are: tensor([0.0147, 0.3189, 0.1569, 0.4096, 0.0998], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,615][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ De] are: tensor([0.0648, 0.0769, 0.5989, 0.2065, 0.0528], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,616][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ De] are: tensor([0.0367, 0.2157, 0.4780, 0.1538, 0.1158], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,617][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ De] are: tensor([0.0153, 0.0652, 0.7413, 0.1345, 0.0437], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,618][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ De] are: tensor([2.5773e-04, 8.9879e-02, 1.0343e-01, 5.1926e-02, 7.5451e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,620][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ De] are: tensor([0.0063, 0.3736, 0.1584, 0.4105, 0.0513], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,622][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ De] are: tensor([0.0224, 0.0985, 0.7919, 0.0478, 0.0394], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,623][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ De] are: tensor([0.1394, 0.1730, 0.3271, 0.2592, 0.1013], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,625][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ De] are: tensor([4.1984e-03, 2.3724e-03, 9.9260e-01, 3.4578e-04, 4.8307e-04],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,627][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ De] are: tensor([0.0122, 0.1807, 0.3110, 0.4122, 0.0840], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,628][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ De] are: tensor([0.1362, 0.1886, 0.4484, 0.1918, 0.0349], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,630][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ De] are: tensor([0.0428, 0.3276, 0.5914, 0.0212, 0.0169], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:32,632][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ fin] are: tensor([0.0189, 0.2502, 0.1425, 0.1998, 0.1203, 0.2683], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,634][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ fin] are: tensor([0.4144, 0.0870, 0.2714, 0.0913, 0.0512, 0.0847], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,637][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ fin] are: tensor([0.0553, 0.2249, 0.4286, 0.0489, 0.0608, 0.1815], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,638][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ fin] are: tensor([0.1928, 0.1798, 0.3752, 0.0898, 0.1377, 0.0247], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,640][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ fin] are: tensor([2.3973e-04, 1.6799e-01, 1.6989e-01, 3.4591e-02, 5.0856e-01, 1.1874e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,642][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ fin] are: tensor([0.0147, 0.1783, 0.1261, 0.3797, 0.2484, 0.0528], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,643][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ fin] are: tensor([0.0101, 0.0799, 0.6466, 0.0476, 0.0260, 0.1898], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,645][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ fin] are: tensor([0.2365, 0.2735, 0.1587, 0.1198, 0.1113, 0.1003], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,647][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ fin] are: tensor([2.2993e-03, 3.2308e-03, 9.9391e-01, 1.0976e-04, 2.0688e-04, 2.3912e-04],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,649][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ fin] are: tensor([0.0250, 0.1565, 0.4245, 0.2744, 0.0736, 0.0461], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,651][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ fin] are: tensor([0.1231, 0.4708, 0.1423, 0.0956, 0.0446, 0.1236], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,653][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ fin] are: tensor([0.1274, 0.5170, 0.1692, 0.0442, 0.0472, 0.0951], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:32,655][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ibus] are: tensor([0.0281, 0.3094, 0.0922, 0.1509, 0.1041, 0.1190, 0.1963],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,656][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ibus] are: tensor([0.0888, 0.1448, 0.5248, 0.0718, 0.0459, 0.0945, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,659][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ibus] are: tensor([0.0095, 0.2596, 0.2875, 0.0992, 0.0615, 0.1126, 0.1700],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,661][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ibus] are: tensor([0.0265, 0.0610, 0.6402, 0.1057, 0.0973, 0.0379, 0.0314],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,662][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ibus] are: tensor([1.7045e-04, 1.6336e-01, 1.9448e-01, 4.1654e-02, 3.5795e-01, 1.2661e-01,
        1.1577e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,664][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ibus] are: tensor([0.0266, 0.2340, 0.1418, 0.3139, 0.1352, 0.0607, 0.0879],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,666][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ibus] are: tensor([0.0049, 0.0998, 0.6875, 0.0566, 0.0189, 0.0807, 0.0516],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,668][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ibus] are: tensor([0.0569, 0.2207, 0.2525, 0.1864, 0.0912, 0.0830, 0.1093],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,669][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ibus] are: tensor([6.3754e-03, 4.5202e-03, 9.8785e-01, 1.9979e-04, 2.0600e-04, 4.8839e-04,
        3.5592e-04], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,671][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ibus] are: tensor([0.0164, 0.1481, 0.2597, 0.3263, 0.0973, 0.0704, 0.0818],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,673][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ibus] are: tensor([0.2030, 0.2651, 0.1271, 0.1319, 0.0185, 0.0284, 0.2261],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,675][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ibus] are: tensor([0.0496, 0.4399, 0.2214, 0.0164, 0.0168, 0.0950, 0.1609],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:32,677][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ bon] are: tensor([0.1324, 0.3510, 0.1047, 0.1153, 0.0450, 0.1080, 0.0571, 0.0865],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,678][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ bon] are: tensor([0.6005, 0.0210, 0.2434, 0.0228, 0.0226, 0.0726, 0.0107, 0.0064],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,679][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ bon] are: tensor([0.1615, 0.0850, 0.5039, 0.0297, 0.0230, 0.0771, 0.0909, 0.0290],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,680][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ bon] are: tensor([0.1269, 0.0616, 0.7325, 0.0389, 0.0212, 0.0069, 0.0100, 0.0022],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,681][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ bon] are: tensor([2.1883e-04, 6.2585e-02, 7.7859e-02, 2.0887e-02, 3.0344e-01, 9.1982e-02,
        4.5779e-02, 3.9725e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,682][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ bon] are: tensor([0.1664, 0.1165, 0.1475, 0.2782, 0.1269, 0.0613, 0.0338, 0.0694],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,682][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ bon] are: tensor([0.0309, 0.0781, 0.7021, 0.0409, 0.0346, 0.0698, 0.0270, 0.0166],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,684][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ bon] are: tensor([0.5087, 0.0821, 0.1159, 0.0460, 0.0783, 0.0667, 0.0303, 0.0721],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,686][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ bon] are: tensor([3.1506e-04, 2.7093e-03, 9.9683e-01, 4.7228e-06, 1.5570e-05, 7.1237e-05,
        5.7661e-05, 1.4690e-08], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,688][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ bon] are: tensor([0.1436, 0.0428, 0.2362, 0.2561, 0.1153, 0.0943, 0.0411, 0.0705],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,689][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ bon] are: tensor([0.8531, 0.0648, 0.0149, 0.0080, 0.0023, 0.0083, 0.0163, 0.0322],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,691][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ bon] are: tensor([0.4816, 0.2106, 0.2560, 0.0069, 0.0028, 0.0173, 0.0207, 0.0041],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:32,693][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0103, 0.3112, 0.0758, 0.1008, 0.0833, 0.1082, 0.0922, 0.1344, 0.0837],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,695][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.1353, 0.0551, 0.4888, 0.0785, 0.0407, 0.1348, 0.0198, 0.0114, 0.0357],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,697][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0100, 0.1581, 0.2676, 0.0663, 0.0862, 0.1395, 0.1010, 0.0746, 0.0966],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,699][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0346, 0.0355, 0.6461, 0.1503, 0.0673, 0.0199, 0.0167, 0.0117, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,700][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [orum] are: tensor([4.1858e-05, 4.9461e-02, 8.4907e-02, 2.2796e-02, 3.0607e-01, 4.9212e-02,
        4.7706e-02, 3.7648e-01, 6.3327e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,703][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0097, 0.0914, 0.1315, 0.3915, 0.1327, 0.0401, 0.0512, 0.0553, 0.0965],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,704][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0058, 0.0708, 0.5299, 0.0911, 0.0480, 0.0844, 0.0642, 0.0629, 0.0429],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,706][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.1248, 0.1754, 0.2202, 0.1389, 0.0643, 0.0774, 0.0495, 0.0665, 0.0830],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,708][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [orum] are: tensor([1.2589e-05, 2.8886e-04, 9.9968e-01, 2.2047e-06, 1.8526e-06, 9.4342e-06,
        2.7664e-06, 1.7339e-11, 8.9802e-11], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,709][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0066, 0.0424, 0.2870, 0.2146, 0.1141, 0.0563, 0.0545, 0.0860, 0.1385],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,711][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0616, 0.2320, 0.1157, 0.0879, 0.0194, 0.0513, 0.1443, 0.1178, 0.1699],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,714][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.0635, 0.1398, 0.1462, 0.0179, 0.0292, 0.1611, 0.1319, 0.1553, 0.1553],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:32,715][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ et] are: tensor([0.0211, 0.2207, 0.0940, 0.1335, 0.0299, 0.0792, 0.0571, 0.0819, 0.0970,
        0.1856], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,717][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ et] are: tensor([0.1383, 0.0830, 0.6289, 0.0312, 0.0235, 0.0555, 0.0106, 0.0062, 0.0103,
        0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,719][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ et] are: tensor([0.0341, 0.2169, 0.2841, 0.0591, 0.0323, 0.0457, 0.1083, 0.0646, 0.0747,
        0.0800], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,721][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ et] are: tensor([0.0508, 0.1313, 0.4718, 0.1624, 0.0212, 0.0176, 0.0260, 0.0145, 0.0175,
        0.0870], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,722][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ et] are: tensor([1.0702e-04, 1.0187e-01, 7.9714e-02, 2.1807e-02, 2.3467e-01, 4.4045e-02,
        2.7927e-02, 3.9281e-01, 2.8675e-02, 6.8375e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,725][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ et] are: tensor([0.0061, 0.2416, 0.2918, 0.1769, 0.0743, 0.0233, 0.0250, 0.0237, 0.0362,
        0.1011], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,727][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ et] are: tensor([0.0095, 0.1360, 0.6654, 0.0282, 0.0214, 0.0387, 0.0186, 0.0233, 0.0129,
        0.0459], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,728][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ et] are: tensor([0.1249, 0.1292, 0.1693, 0.0632, 0.0315, 0.0983, 0.0839, 0.1277, 0.0934,
        0.0785], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,730][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ et] are: tensor([1.0749e-01, 3.7788e-03, 8.5878e-01, 1.6815e-03, 8.4296e-03, 2.4835e-03,
        9.4072e-04, 3.3830e-06, 6.1647e-06, 1.6403e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,731][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ et] are: tensor([0.0102, 0.0819, 0.3701, 0.1046, 0.0543, 0.0224, 0.0269, 0.0935, 0.0485,
        0.1877], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,733][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ et] are: tensor([0.3954, 0.1388, 0.1501, 0.1086, 0.0032, 0.0219, 0.0445, 0.0147, 0.0677,
        0.0551], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,736][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ et] are: tensor([0.0622, 0.3440, 0.3692, 0.0173, 0.0130, 0.0536, 0.0368, 0.0541, 0.0398,
        0.0100], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:32,738][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ mal] are: tensor([0.0652, 0.1700, 0.1056, 0.1070, 0.0502, 0.0780, 0.0189, 0.0215, 0.0439,
        0.0884, 0.2512], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,739][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ mal] are: tensor([8.0167e-01, 2.6628e-02, 1.2564e-01, 9.0452e-03, 1.9551e-03, 2.1034e-02,
        2.0243e-03, 3.6362e-04, 4.2824e-03, 1.4812e-03, 5.8820e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,741][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ mal] are: tensor([0.4106, 0.0418, 0.2386, 0.0138, 0.0052, 0.0404, 0.0237, 0.0057, 0.0221,
        0.0082, 0.1898], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,742][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ mal] are: tensor([1.3736e-02, 4.5374e-02, 8.8460e-01, 4.3517e-02, 1.9571e-03, 3.2493e-03,
        1.3788e-03, 2.5099e-04, 7.3080e-04, 2.9815e-03, 2.2269e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,744][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ mal] are: tensor([1.7877e-04, 1.4257e-01, 7.3839e-02, 1.0475e-02, 7.6644e-02, 8.4053e-02,
        4.0133e-02, 2.4182e-01, 4.0744e-02, 2.4364e-02, 2.6518e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,745][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ mal] are: tensor([0.1001, 0.3248, 0.2833, 0.1095, 0.0217, 0.0202, 0.0255, 0.0174, 0.0243,
        0.0571, 0.0159], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,746][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ mal] are: tensor([0.0245, 0.0328, 0.6386, 0.0063, 0.0045, 0.0536, 0.0075, 0.0110, 0.0066,
        0.0053, 0.2093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,747][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ mal] are: tensor([0.8246, 0.0764, 0.0572, 0.0059, 0.0024, 0.0066, 0.0061, 0.0026, 0.0091,
        0.0035, 0.0056], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,747][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ mal] are: tensor([1.8136e-02, 9.9097e-03, 9.4986e-01, 5.8978e-04, 1.3136e-03, 1.5918e-03,
        1.0267e-03, 2.0271e-06, 4.7840e-06, 1.1093e-02, 6.4726e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,748][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ mal] are: tensor([0.1543, 0.1160, 0.3290, 0.1004, 0.0234, 0.0424, 0.0735, 0.0285, 0.0339,
        0.0222, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,750][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ mal] are: tensor([0.4360, 0.1443, 0.0869, 0.0336, 0.0021, 0.0081, 0.0127, 0.0085, 0.0142,
        0.0054, 0.2483], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,752][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ mal] are: tensor([0.1421, 0.3379, 0.3561, 0.0049, 0.0005, 0.0555, 0.0204, 0.0033, 0.0137,
        0.0007, 0.0649], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:32,754][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [orum] are: tensor([0.0044, 0.2094, 0.0600, 0.0694, 0.0283, 0.0510, 0.0317, 0.0311, 0.0271,
        0.0584, 0.2472, 0.1820], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,755][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [orum] are: tensor([0.2442, 0.0323, 0.4465, 0.0624, 0.0186, 0.0687, 0.0133, 0.0043, 0.0234,
        0.0102, 0.0333, 0.0429], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,757][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [orum] are: tensor([0.0123, 0.0493, 0.2332, 0.0423, 0.0319, 0.0930, 0.0318, 0.0307, 0.0355,
        0.0118, 0.3553, 0.0729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,757][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [orum] are: tensor([0.0122, 0.0242, 0.6510, 0.1977, 0.0204, 0.0111, 0.0050, 0.0026, 0.0050,
        0.0157, 0.0358, 0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,758][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [orum] are: tensor([6.0897e-05, 4.6741e-02, 9.3669e-02, 1.3678e-02, 1.4702e-01, 4.3599e-02,
        2.7556e-02, 2.2997e-01, 4.2233e-02, 4.7089e-02, 1.9469e-01, 1.1370e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,760][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [orum] are: tensor([0.0132, 0.0928, 0.1861, 0.2286, 0.0825, 0.0377, 0.0289, 0.0292, 0.0655,
        0.1325, 0.0380, 0.0650], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,762][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [orum] are: tensor([0.0133, 0.0402, 0.5567, 0.0391, 0.0151, 0.0490, 0.0293, 0.0404, 0.0202,
        0.0117, 0.1503, 0.0348], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,764][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [orum] are: tensor([0.2358, 0.1338, 0.2258, 0.1172, 0.0409, 0.0564, 0.0292, 0.0250, 0.0542,
        0.0151, 0.0375, 0.0290], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,765][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [orum] are: tensor([7.8161e-05, 6.1922e-04, 9.9856e-01, 7.8604e-06, 3.1038e-06, 1.8551e-05,
        3.7840e-06, 5.4475e-11, 1.7589e-10, 6.1439e-04, 9.4365e-05, 6.6863e-10],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,767][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [orum] are: tensor([0.0176, 0.0425, 0.2350, 0.1468, 0.0542, 0.0492, 0.0343, 0.0468, 0.0658,
        0.0547, 0.1302, 0.1231], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,769][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [orum] are: tensor([0.0920, 0.1647, 0.1444, 0.1178, 0.0069, 0.0158, 0.0511, 0.0271, 0.0729,
        0.0166, 0.1614, 0.1293], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,771][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [orum] are: tensor([0.1046, 0.1951, 0.2186, 0.0137, 0.0083, 0.0909, 0.0489, 0.0485, 0.0434,
        0.0050, 0.1530, 0.0700], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:32,773][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ is] are: tensor([0.0412, 0.2221, 0.0711, 0.1434, 0.0215, 0.0344, 0.0354, 0.0250, 0.0241,
        0.0484, 0.2066, 0.0875, 0.0394], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,775][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ is] are: tensor([0.0698, 0.0465, 0.6350, 0.0507, 0.0123, 0.0447, 0.0067, 0.0039, 0.0164,
        0.0071, 0.0485, 0.0421, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,777][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ is] are: tensor([0.0036, 0.0181, 0.2422, 0.0443, 0.0161, 0.0254, 0.0141, 0.0200, 0.0318,
        0.0260, 0.4362, 0.0829, 0.0393], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,779][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ is] are: tensor([0.0452, 0.0651, 0.4081, 0.1422, 0.0188, 0.0212, 0.0079, 0.0053, 0.0091,
        0.0299, 0.0253, 0.0473, 0.1747], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,780][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ is] are: tensor([4.5980e-05, 1.9232e-02, 4.4785e-02, 3.1051e-02, 2.3505e-01, 3.7571e-02,
        1.5042e-02, 1.5741e-01, 1.8338e-02, 5.0972e-02, 1.3100e-01, 6.2305e-02,
        1.9719e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,782][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ is] are: tensor([0.0070, 0.0589, 0.1792, 0.2867, 0.0624, 0.0255, 0.0115, 0.0136, 0.0553,
        0.0650, 0.1306, 0.0664, 0.0379], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,784][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ is] are: tensor([0.0097, 0.0533, 0.7003, 0.0309, 0.0125, 0.0210, 0.0089, 0.0173, 0.0070,
        0.0135, 0.0904, 0.0153, 0.0200], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,786][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ is] are: tensor([0.0457, 0.0429, 0.1794, 0.1262, 0.0740, 0.0635, 0.0209, 0.0625, 0.0479,
        0.0330, 0.2155, 0.0308, 0.0578], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,788][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ is] are: tensor([1.6030e-02, 4.6212e-02, 8.1790e-01, 4.0660e-03, 3.0221e-03, 3.2104e-03,
        4.7063e-03, 2.2072e-05, 5.5309e-05, 1.2927e-02, 1.1949e-02, 1.2678e-04,
        7.9775e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,790][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ is] are: tensor([0.0361, 0.0675, 0.2555, 0.2131, 0.0381, 0.0379, 0.0172, 0.0304, 0.0471,
        0.0597, 0.1072, 0.0421, 0.0482], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,792][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ is] are: tensor([0.3535, 0.1175, 0.1427, 0.1369, 0.0012, 0.0119, 0.0182, 0.0070, 0.0337,
        0.0109, 0.0425, 0.0835, 0.0406], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,794][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ is] are: tensor([0.0373, 0.1711, 0.2524, 0.0197, 0.0134, 0.0303, 0.0263, 0.0608, 0.0451,
        0.0159, 0.1966, 0.0984, 0.0328], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:32,796][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0125, 0.0524, 0.0348, 0.1843, 0.0055, 0.0058, 0.0025, 0.0015, 0.0045,
        0.0100, 0.0278, 0.0180, 0.0142, 0.6263], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,798][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0122, 0.1379, 0.6458, 0.0886, 0.0024, 0.0224, 0.0016, 0.0010, 0.0054,
        0.0014, 0.0267, 0.0129, 0.0084, 0.0332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,800][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0022, 0.0378, 0.2383, 0.0582, 0.0044, 0.0070, 0.0029, 0.0035, 0.0067,
        0.0049, 0.3331, 0.0231, 0.0260, 0.2520], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,801][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([4.5439e-03, 1.1110e-02, 4.1970e-01, 1.4468e-01, 3.0556e-03, 6.4931e-03,
        5.1997e-04, 2.6623e-04, 7.2541e-04, 7.0115e-03, 2.6737e-02, 6.9071e-03,
        7.6594e-02, 2.9165e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,803][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([3.3159e-05, 3.9044e-02, 2.7527e-02, 2.5571e-02, 7.0494e-02, 1.5624e-02,
        3.3222e-03, 6.9528e-02, 5.6183e-03, 2.0894e-02, 1.1842e-01, 2.3453e-02,
        1.9934e-01, 3.8113e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,805][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0028, 0.2042, 0.0981, 0.1705, 0.0111, 0.0083, 0.0025, 0.0046, 0.0214,
        0.0149, 0.0185, 0.0141, 0.0310, 0.3981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,807][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0080, 0.0843, 0.7126, 0.0183, 0.0032, 0.0096, 0.0019, 0.0034, 0.0017,
        0.0030, 0.1112, 0.0043, 0.0097, 0.0288], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,809][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0737, 0.2403, 0.1802, 0.1313, 0.0168, 0.0291, 0.0078, 0.0374, 0.0117,
        0.0088, 0.1955, 0.0059, 0.0290, 0.0326], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,811][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.3616, 0.0387, 0.1173, 0.1375, 0.0379, 0.0309, 0.0097, 0.0005, 0.0013,
        0.0116, 0.0400, 0.0027, 0.0607, 0.1497], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,813][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0112, 0.2196, 0.2448, 0.1814, 0.0133, 0.0304, 0.0087, 0.0131, 0.0172,
        0.0192, 0.0800, 0.0150, 0.0230, 0.1232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,814][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([7.3482e-01, 3.0500e-02, 1.0029e-01, 5.6077e-02, 1.5261e-04, 2.4743e-03,
        1.2462e-03, 5.2888e-04, 3.4889e-03, 1.2400e-03, 1.4581e-02, 1.1559e-02,
        1.1300e-02, 3.1742e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,815][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0384, 0.2506, 0.5173, 0.0101, 0.0020, 0.0292, 0.0049, 0.0110, 0.0086,
        0.0019, 0.0719, 0.0202, 0.0041, 0.0297], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:32,816][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ same] are: tensor([0.0264, 0.0609, 0.0489, 0.0454, 0.0115, 0.0150, 0.0122, 0.0134, 0.0081,
        0.0206, 0.2694, 0.0359, 0.0142, 0.3894, 0.0287], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,817][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ same] are: tensor([0.3139, 0.0432, 0.4133, 0.0316, 0.0157, 0.0337, 0.0072, 0.0055, 0.0096,
        0.0071, 0.0760, 0.0226, 0.0103, 0.0073, 0.0030], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,817][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ same] are: tensor([0.0101, 0.0589, 0.2942, 0.0323, 0.0118, 0.0274, 0.0126, 0.0134, 0.0134,
        0.0137, 0.4303, 0.0301, 0.0141, 0.0178, 0.0199], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,820][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ same] are: tensor([0.2642, 0.0263, 0.2884, 0.0334, 0.0072, 0.0044, 0.0039, 0.0020, 0.0016,
        0.0090, 0.0107, 0.0070, 0.1679, 0.1409, 0.0330], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,821][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ same] are: tensor([6.2015e-05, 2.0901e-02, 4.6084e-02, 2.6588e-02, 1.4156e-01, 1.4465e-02,
        8.0887e-03, 1.0424e-01, 1.2154e-02, 3.7011e-02, 1.2799e-01, 3.9202e-02,
        1.1230e-01, 1.0778e-01, 2.0158e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,823][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ same] are: tensor([0.0304, 0.0898, 0.1144, 0.1844, 0.0971, 0.0263, 0.0249, 0.0488, 0.0248,
        0.1285, 0.0243, 0.0171, 0.0917, 0.0802, 0.0171], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,824][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ same] are: tensor([0.0095, 0.0578, 0.6737, 0.0404, 0.0141, 0.0246, 0.0087, 0.0114, 0.0039,
        0.0084, 0.0826, 0.0068, 0.0141, 0.0164, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,827][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ same] are: tensor([0.1924, 0.0924, 0.1766, 0.1062, 0.0457, 0.0280, 0.0262, 0.0564, 0.0265,
        0.0145, 0.1426, 0.0165, 0.0406, 0.0186, 0.0168], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,828][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ same] are: tensor([5.2771e-04, 1.0185e-02, 8.9855e-01, 1.7965e-05, 1.0456e-04, 2.2911e-04,
        8.6606e-04, 2.0115e-07, 4.7243e-07, 1.5006e-03, 1.9805e-03, 1.3315e-06,
        1.9810e-02, 6.6222e-02, 5.1404e-06], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,830][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ same] are: tensor([0.0569, 0.1215, 0.2868, 0.1419, 0.0511, 0.0660, 0.0230, 0.0960, 0.0137,
        0.0409, 0.0386, 0.0089, 0.0317, 0.0152, 0.0078], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,832][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ same] are: tensor([0.3898, 0.1017, 0.1769, 0.0314, 0.0026, 0.0077, 0.0196, 0.0421, 0.0159,
        0.0064, 0.0978, 0.0261, 0.0243, 0.0134, 0.0444], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,834][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ same] are: tensor([0.1726, 0.1966, 0.2864, 0.0133, 0.0036, 0.0669, 0.0375, 0.0514, 0.0261,
        0.0015, 0.0522, 0.0399, 0.0059, 0.0299, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:32,836][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ as] are: tensor([0.0233, 0.1015, 0.0270, 0.0784, 0.0096, 0.0062, 0.0091, 0.0054, 0.0090,
        0.0253, 0.0587, 0.0352, 0.0131, 0.5273, 0.0331, 0.0377],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,839][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ as] are: tensor([0.0272, 0.1054, 0.4722, 0.0918, 0.0110, 0.0315, 0.0058, 0.0042, 0.0131,
        0.0065, 0.0361, 0.0413, 0.0148, 0.0383, 0.0046, 0.0961],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,841][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ as] are: tensor([0.0048, 0.0538, 0.2172, 0.0652, 0.0124, 0.0177, 0.0132, 0.0175, 0.0197,
        0.0206, 0.2731, 0.0409, 0.0344, 0.1024, 0.0275, 0.0798],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,843][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ as] are: tensor([0.0346, 0.0253, 0.1607, 0.1168, 0.0045, 0.0063, 0.0041, 0.0021, 0.0022,
        0.0083, 0.0135, 0.0115, 0.0809, 0.2796, 0.0776, 0.1718],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,844][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ as] are: tensor([1.3908e-04, 2.7885e-02, 6.2837e-02, 2.7246e-02, 8.4213e-02, 1.6446e-02,
        7.8695e-03, 7.6291e-02, 1.0483e-02, 2.4130e-02, 1.1748e-01, 3.6402e-02,
        1.0393e-01, 1.8628e-01, 1.7042e-01, 4.7961e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,846][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ as] are: tensor([0.0026, 0.0510, 0.0872, 0.1607, 0.0466, 0.0234, 0.0271, 0.0273, 0.0318,
        0.0514, 0.0208, 0.0217, 0.0413, 0.1872, 0.0411, 0.1787],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,848][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ as] are: tensor([0.0074, 0.0445, 0.5437, 0.0448, 0.0084, 0.0262, 0.0122, 0.0276, 0.0063,
        0.0131, 0.0742, 0.0113, 0.0208, 0.0496, 0.0395, 0.0705],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,850][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ as] are: tensor([0.0216, 0.0582, 0.1680, 0.1870, 0.0509, 0.0421, 0.0140, 0.0324, 0.0157,
        0.0090, 0.1413, 0.0103, 0.0301, 0.0464, 0.0325, 0.1405],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,851][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ as] are: tensor([1.8945e-01, 8.8998e-02, 4.1463e-01, 1.5061e-02, 8.2744e-03, 3.8209e-03,
        4.7260e-03, 1.1136e-04, 8.8789e-05, 6.0690e-03, 7.9072e-03, 1.9087e-04,
        3.5838e-02, 1.4954e-01, 4.1072e-03, 7.1188e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,854][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ as] are: tensor([0.0141, 0.0933, 0.1587, 0.1537, 0.0257, 0.0626, 0.0294, 0.0609, 0.0277,
        0.0381, 0.0401, 0.0186, 0.0256, 0.0506, 0.0277, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,856][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ as] are: tensor([0.0790, 0.0885, 0.1749, 0.0822, 0.0015, 0.0072, 0.0098, 0.0095, 0.0203,
        0.0045, 0.0281, 0.0430, 0.0407, 0.0743, 0.0507, 0.2858],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,857][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ as] are: tensor([0.0448, 0.2604, 0.1830, 0.0216, 0.0089, 0.0366, 0.0276, 0.0933, 0.0320,
        0.0044, 0.0599, 0.0611, 0.0134, 0.0448, 0.0659, 0.0424],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:32,860][circuit_model.py][line:1532][INFO] ##11-th layer ##Weight##: The head1 weight for token [ the] are: tensor([0.0044, 0.0171, 0.0123, 0.1210, 0.0037, 0.0025, 0.0012, 0.0006, 0.0024,
        0.0078, 0.0160, 0.0101, 0.0096, 0.5220, 0.0084, 0.0357, 0.2252],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,862][circuit_model.py][line:1535][INFO] ##11-th layer ##Weight##: The head2 weight for token [ the] are: tensor([0.0093, 0.1508, 0.3298, 0.0841, 0.0041, 0.0291, 0.0020, 0.0027, 0.0079,
        0.0031, 0.0322, 0.0219, 0.0152, 0.0478, 0.0049, 0.1041, 0.1509],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,864][circuit_model.py][line:1538][INFO] ##11-th layer ##Weight##: The head3 weight for token [ the] are: tensor([0.0026, 0.0508, 0.1264, 0.0637, 0.0063, 0.0066, 0.0039, 0.0059, 0.0093,
        0.0110, 0.1675, 0.0254, 0.0374, 0.2257, 0.0232, 0.0654, 0.1691],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,866][circuit_model.py][line:1541][INFO] ##11-th layer ##Weight##: The head4 weight for token [ the] are: tensor([0.0183, 0.0071, 0.1206, 0.0609, 0.0022, 0.0041, 0.0008, 0.0003, 0.0007,
        0.0085, 0.0131, 0.0038, 0.0940, 0.1393, 0.0140, 0.2232, 0.2891],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,867][circuit_model.py][line:1544][INFO] ##11-th layer ##Weight##: The head5 weight for token [ the] are: tensor([2.2844e-05, 2.0840e-02, 2.0651e-02, 1.4958e-02, 4.4770e-02, 8.4550e-03,
        2.6255e-03, 4.3059e-02, 4.8451e-03, 1.3783e-02, 7.5932e-02, 1.7403e-02,
        1.1201e-01, 2.2265e-01, 1.5925e-01, 3.9620e-02, 1.9912e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,869][circuit_model.py][line:1547][INFO] ##11-th layer ##Weight##: The head6 weight for token [ the] are: tensor([0.0013, 0.0951, 0.0332, 0.1089, 0.0095, 0.0045, 0.0021, 0.0051, 0.0120,
        0.0126, 0.0099, 0.0075, 0.0225, 0.2825, 0.0321, 0.0601, 0.3012],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,871][circuit_model.py][line:1550][INFO] ##11-th layer ##Weight##: The head7 weight for token [ the] are: tensor([0.0084, 0.0899, 0.4386, 0.0376, 0.0050, 0.0132, 0.0038, 0.0061, 0.0028,
        0.0064, 0.0631, 0.0049, 0.0155, 0.0637, 0.0429, 0.0656, 0.1324],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,873][circuit_model.py][line:1553][INFO] ##11-th layer ##Weight##: The head8 weight for token [ the] are: tensor([0.0464, 0.1541, 0.1159, 0.1203, 0.0229, 0.0244, 0.0066, 0.0426, 0.0111,
        0.0097, 0.1720, 0.0054, 0.0250, 0.0379, 0.0183, 0.0984, 0.0890],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,875][circuit_model.py][line:1556][INFO] ##11-th layer ##Weight##: The head9 weight for token [ the] are: tensor([0.5107, 0.0407, 0.0766, 0.0814, 0.0240, 0.0237, 0.0073, 0.0005, 0.0012,
        0.0081, 0.0297, 0.0024, 0.0524, 0.0603, 0.0031, 0.0272, 0.0508],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,878][circuit_model.py][line:1559][INFO] ##11-th layer ##Weight##: The head10 weight for token [ the] are: tensor([0.0038, 0.1017, 0.0675, 0.1112, 0.0104, 0.0205, 0.0086, 0.0166, 0.0120,
        0.0203, 0.0274, 0.0099, 0.0183, 0.1327, 0.0197, 0.0960, 0.3235],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,879][circuit_model.py][line:1562][INFO] ##11-th layer ##Weight##: The head11 weight for token [ the] are: tensor([5.6408e-01, 2.6050e-02, 5.7554e-02, 3.2140e-02, 1.8916e-04, 2.4809e-03,
        1.4379e-03, 8.8648e-04, 4.6736e-03, 1.7134e-03, 4.7993e-03, 9.7530e-03,
        1.5825e-02, 3.9678e-02, 6.8294e-03, 1.2343e-01, 1.0848e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:32,880][circuit_model.py][line:1565][INFO] ##11-th layer ##Weight##: The head12 weight for token [ the] are: tensor([0.0890, 0.2058, 0.1899, 0.0105, 0.0046, 0.0410, 0.0105, 0.0385, 0.0268,
        0.0062, 0.0545, 0.0466, 0.0160, 0.0662, 0.0183, 0.0491, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,000][circuit_model.py][line:1216][INFO] ############showing the attention weight of each circuit
[2024-07-23 21:05:33,002][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,003][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,004][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,005][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,006][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,006][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,006][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,007][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,007][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,007][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,007][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,008][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [The] are: tensor([1.], device='cuda:0') for source tokens [The]
[2024-07-23 21:05:33,008][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ original] are: tensor([0.0840, 0.9160], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,008][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ original] are: tensor([0.1298, 0.8702], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,009][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ original] are: tensor([0.0153, 0.9847], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,009][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ original] are: tensor([0.5481, 0.4519], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,009][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ original] are: tensor([0.0067, 0.9933], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,010][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ original] are: tensor([0.0142, 0.9858], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,010][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ original] are: tensor([0.0393, 0.9607], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,010][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ original] are: tensor([0.1013, 0.8987], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,011][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ original] are: tensor([0.4947, 0.5053], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,011][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ original] are: tensor([0.0130, 0.9870], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,011][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ original] are: tensor([0.1057, 0.8943], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,011][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ original] are: tensor([0.0913, 0.9087], device='cuda:0') for source tokens [The original]
[2024-07-23 21:05:33,012][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ language] are: tensor([0.1187, 0.4592, 0.4221], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,012][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ language] are: tensor([0.0347, 0.4642, 0.5011], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,012][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ language] are: tensor([0.0098, 0.5825, 0.4077], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,013][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ language] are: tensor([0.4944, 0.1803, 0.3253], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,013][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ language] are: tensor([0.0032, 0.3351, 0.6617], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,013][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ language] are: tensor([0.0096, 0.7746, 0.2157], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,014][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ language] are: tensor([0.0022, 0.0611, 0.9367], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,014][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ language] are: tensor([0.0489, 0.4163, 0.5348], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,014][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ language] are: tensor([0.2517, 0.2673, 0.4810], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,015][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ language] are: tensor([0.0065, 0.2296, 0.7639], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,016][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ language] are: tensor([0.1172, 0.4393, 0.4436], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,018][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ language] are: tensor([0.0241, 0.8023, 0.1736], device='cuda:0') for source tokens [The original language]
[2024-07-23 21:05:33,019][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ of] are: tensor([0.0132, 0.3428, 0.1413, 0.5027], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,021][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ of] are: tensor([0.0105, 0.2164, 0.4700, 0.3030], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,023][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ of] are: tensor([0.0035, 0.1986, 0.5594, 0.2384], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,025][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ of] are: tensor([0.0654, 0.0883, 0.6839, 0.1623], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,025][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ of] are: tensor([4.1505e-04, 1.1550e-01, 2.7422e-01, 6.0987e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,026][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ of] are: tensor([5.4984e-04, 2.3638e-01, 1.2819e-01, 6.3488e-01], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,026][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ of] are: tensor([0.0022, 0.1019, 0.7796, 0.1163], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,027][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ of] are: tensor([0.0109, 0.1751, 0.3620, 0.4520], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,027][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ of] are: tensor([0.2089, 0.1457, 0.5809, 0.0644], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,027][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ of] are: tensor([0.0011, 0.1791, 0.2854, 0.5343], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,028][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ of] are: tensor([0.3850, 0.1768, 0.2009, 0.2374], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,028][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ of] are: tensor([0.0249, 0.3868, 0.5183, 0.0700], device='cuda:0') for source tokens [The original language of]
[2024-07-23 21:05:33,028][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ De] are: tensor([0.0147, 0.3189, 0.1569, 0.4096, 0.0998], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,029][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ De] are: tensor([0.0648, 0.0769, 0.5989, 0.2065, 0.0528], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,029][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ De] are: tensor([0.0367, 0.2157, 0.4780, 0.1538, 0.1158], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,030][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ De] are: tensor([0.0153, 0.0652, 0.7413, 0.1345, 0.0437], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,031][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ De] are: tensor([2.5773e-04, 8.9879e-02, 1.0343e-01, 5.1926e-02, 7.5451e-01],
       device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,033][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ De] are: tensor([0.0063, 0.3736, 0.1584, 0.4105, 0.0513], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,034][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ De] are: tensor([0.0224, 0.0985, 0.7919, 0.0478, 0.0394], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,035][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ De] are: tensor([0.1394, 0.1730, 0.3271, 0.2592, 0.1013], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,036][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ De] are: tensor([0.1125, 0.0841, 0.6743, 0.0429, 0.0862], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,038][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ De] are: tensor([0.0122, 0.1807, 0.3110, 0.4122, 0.0840], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,040][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ De] are: tensor([0.1362, 0.1886, 0.4484, 0.1918, 0.0349], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,041][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ De] are: tensor([0.0428, 0.3276, 0.5914, 0.0212, 0.0169], device='cuda:0') for source tokens [The original language of De]
[2024-07-23 21:05:33,043][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ fin] are: tensor([0.0189, 0.2502, 0.1425, 0.1998, 0.1203, 0.2683], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,045][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ fin] are: tensor([0.4144, 0.0870, 0.2714, 0.0913, 0.0512, 0.0847], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,047][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ fin] are: tensor([0.0553, 0.2249, 0.4286, 0.0489, 0.0608, 0.1815], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,048][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ fin] are: tensor([0.1928, 0.1798, 0.3752, 0.0898, 0.1377, 0.0247], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,049][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ fin] are: tensor([2.3973e-04, 1.6799e-01, 1.6989e-01, 3.4591e-02, 5.0856e-01, 1.1874e-01],
       device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,051][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ fin] are: tensor([0.0147, 0.1783, 0.1261, 0.3797, 0.2484, 0.0528], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,053][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ fin] are: tensor([0.0101, 0.0799, 0.6466, 0.0476, 0.0260, 0.1898], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,054][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ fin] are: tensor([0.2365, 0.2735, 0.1587, 0.1198, 0.1113, 0.1003], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,056][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ fin] are: tensor([0.0790, 0.0608, 0.6127, 0.0298, 0.0742, 0.1435], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,058][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ fin] are: tensor([0.0250, 0.1565, 0.4245, 0.2744, 0.0736, 0.0461], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,059][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ fin] are: tensor([0.1231, 0.4708, 0.1423, 0.0956, 0.0446, 0.1236], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,061][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ fin] are: tensor([0.1274, 0.5170, 0.1692, 0.0442, 0.0472, 0.0951], device='cuda:0') for source tokens [The original language of De fin]
[2024-07-23 21:05:33,063][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ibus] are: tensor([0.0281, 0.3094, 0.0922, 0.1509, 0.1041, 0.1190, 0.1963],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,064][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ibus] are: tensor([0.0888, 0.1448, 0.5248, 0.0718, 0.0459, 0.0945, 0.0294],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,066][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ibus] are: tensor([0.0095, 0.2596, 0.2875, 0.0992, 0.0615, 0.1126, 0.1700],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,068][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ibus] are: tensor([0.0265, 0.0610, 0.6402, 0.1057, 0.0973, 0.0379, 0.0314],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,069][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ibus] are: tensor([1.7045e-04, 1.6336e-01, 1.9448e-01, 4.1654e-02, 3.5795e-01, 1.2661e-01,
        1.1577e-01], device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,070][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ibus] are: tensor([0.0266, 0.2340, 0.1418, 0.3139, 0.1352, 0.0607, 0.0879],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,073][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ibus] are: tensor([0.0049, 0.0998, 0.6875, 0.0566, 0.0189, 0.0807, 0.0516],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,074][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ibus] are: tensor([0.0569, 0.2207, 0.2525, 0.1864, 0.0912, 0.0830, 0.1093],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,076][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ibus] are: tensor([0.1183, 0.0497, 0.4786, 0.0268, 0.0899, 0.2098, 0.0268],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,078][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ibus] are: tensor([0.0164, 0.1481, 0.2597, 0.3263, 0.0973, 0.0704, 0.0818],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,079][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ibus] are: tensor([0.2030, 0.2651, 0.1271, 0.1319, 0.0185, 0.0284, 0.2261],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,081][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ibus] are: tensor([0.0496, 0.4399, 0.2214, 0.0164, 0.0168, 0.0950, 0.1609],
       device='cuda:0') for source tokens [The original language of De finibus]
[2024-07-23 21:05:33,083][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ bon] are: tensor([0.1324, 0.3510, 0.1047, 0.1153, 0.0450, 0.1080, 0.0571, 0.0865],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,085][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ bon] are: tensor([0.6005, 0.0210, 0.2434, 0.0228, 0.0226, 0.0726, 0.0107, 0.0064],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,086][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ bon] are: tensor([0.1615, 0.0850, 0.5039, 0.0297, 0.0230, 0.0771, 0.0909, 0.0290],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,088][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ bon] are: tensor([0.1269, 0.0616, 0.7325, 0.0389, 0.0212, 0.0069, 0.0100, 0.0022],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,089][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ bon] are: tensor([2.1883e-04, 6.2585e-02, 7.7859e-02, 2.0887e-02, 3.0344e-01, 9.1982e-02,
        4.5779e-02, 3.9725e-01], device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,091][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ bon] are: tensor([0.1664, 0.1165, 0.1475, 0.2782, 0.1269, 0.0613, 0.0338, 0.0694],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,092][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ bon] are: tensor([0.0309, 0.0781, 0.7021, 0.0409, 0.0346, 0.0698, 0.0270, 0.0166],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,093][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ bon] are: tensor([0.5087, 0.0821, 0.1159, 0.0460, 0.0783, 0.0667, 0.0303, 0.0721],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,093][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ bon] are: tensor([0.0387, 0.0720, 0.6344, 0.0112, 0.0518, 0.1727, 0.0180, 0.0011],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,094][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ bon] are: tensor([0.1436, 0.0428, 0.2362, 0.2561, 0.1153, 0.0943, 0.0411, 0.0705],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,094][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ bon] are: tensor([0.8531, 0.0648, 0.0149, 0.0080, 0.0023, 0.0083, 0.0163, 0.0322],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,094][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ bon] are: tensor([0.4816, 0.2106, 0.2560, 0.0069, 0.0028, 0.0173, 0.0207, 0.0041],
       device='cuda:0') for source tokens [The original language of De finibus bon]
[2024-07-23 21:05:33,095][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0103, 0.3112, 0.0758, 0.1008, 0.0833, 0.1082, 0.0922, 0.1344, 0.0837],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,095][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.1353, 0.0551, 0.4888, 0.0785, 0.0407, 0.1348, 0.0198, 0.0114, 0.0357],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,096][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0100, 0.1581, 0.2676, 0.0663, 0.0862, 0.1395, 0.1010, 0.0746, 0.0966],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,096][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0346, 0.0355, 0.6461, 0.1503, 0.0673, 0.0199, 0.0167, 0.0117, 0.0180],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,097][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([4.1858e-05, 4.9461e-02, 8.4907e-02, 2.2796e-02, 3.0607e-01, 4.9212e-02,
        4.7706e-02, 3.7648e-01, 6.3327e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,098][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0097, 0.0914, 0.1315, 0.3915, 0.1327, 0.0401, 0.0512, 0.0553, 0.0965],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,100][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0058, 0.0708, 0.5299, 0.0911, 0.0480, 0.0844, 0.0642, 0.0629, 0.0429],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,102][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.1248, 0.1754, 0.2202, 0.1389, 0.0643, 0.0774, 0.0495, 0.0665, 0.0830],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,103][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([2.6479e-02, 3.4675e-02, 7.2209e-01, 1.0133e-02, 5.3071e-02, 1.3865e-01,
        1.4140e-02, 4.0312e-04, 3.5928e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,104][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0066, 0.0424, 0.2870, 0.2146, 0.1141, 0.0563, 0.0545, 0.0860, 0.1385],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,106][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0616, 0.2320, 0.1157, 0.0879, 0.0194, 0.0513, 0.1443, 0.1178, 0.1699],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,108][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.0635, 0.1398, 0.1462, 0.0179, 0.0292, 0.1611, 0.1319, 0.1553, 0.1553],
       device='cuda:0') for source tokens [The original language of De finibus bonorum]
[2024-07-23 21:05:33,109][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ et] are: tensor([0.0211, 0.2207, 0.0940, 0.1335, 0.0299, 0.0792, 0.0571, 0.0819, 0.0970,
        0.1856], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,111][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ et] are: tensor([0.1383, 0.0830, 0.6289, 0.0312, 0.0235, 0.0555, 0.0106, 0.0062, 0.0103,
        0.0123], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,113][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ et] are: tensor([0.0341, 0.2169, 0.2841, 0.0591, 0.0323, 0.0457, 0.1083, 0.0646, 0.0747,
        0.0800], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,114][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ et] are: tensor([0.0508, 0.1313, 0.4718, 0.1624, 0.0212, 0.0176, 0.0260, 0.0145, 0.0175,
        0.0870], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,116][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ et] are: tensor([1.0702e-04, 1.0187e-01, 7.9714e-02, 2.1807e-02, 2.3467e-01, 4.4045e-02,
        2.7927e-02, 3.9281e-01, 2.8675e-02, 6.8375e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,117][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ et] are: tensor([0.0061, 0.2416, 0.2918, 0.1769, 0.0743, 0.0233, 0.0250, 0.0237, 0.0362,
        0.1011], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,119][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ et] are: tensor([0.0095, 0.1360, 0.6654, 0.0282, 0.0214, 0.0387, 0.0186, 0.0233, 0.0129,
        0.0459], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,120][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ et] are: tensor([0.1249, 0.1292, 0.1693, 0.0632, 0.0315, 0.0983, 0.0839, 0.1277, 0.0934,
        0.0785], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,122][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ et] are: tensor([0.2772, 0.0467, 0.2118, 0.0266, 0.0696, 0.1343, 0.0280, 0.0046, 0.0032,
        0.1982], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,124][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ et] are: tensor([0.0102, 0.0819, 0.3701, 0.1046, 0.0543, 0.0224, 0.0269, 0.0935, 0.0485,
        0.1877], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,125][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ et] are: tensor([0.3954, 0.1388, 0.1501, 0.1086, 0.0032, 0.0219, 0.0445, 0.0147, 0.0677,
        0.0551], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,127][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ et] are: tensor([0.0622, 0.3440, 0.3692, 0.0173, 0.0130, 0.0536, 0.0368, 0.0541, 0.0398,
        0.0100], device='cuda:0') for source tokens [The original language of De finibus bonorum et]
[2024-07-23 21:05:33,129][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ mal] are: tensor([0.0652, 0.1700, 0.1056, 0.1070, 0.0502, 0.0780, 0.0189, 0.0215, 0.0439,
        0.0884, 0.2512], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,130][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ mal] are: tensor([8.0167e-01, 2.6628e-02, 1.2564e-01, 9.0452e-03, 1.9551e-03, 2.1034e-02,
        2.0243e-03, 3.6362e-04, 4.2824e-03, 1.4812e-03, 5.8820e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,132][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ mal] are: tensor([0.4106, 0.0418, 0.2386, 0.0138, 0.0052, 0.0404, 0.0237, 0.0057, 0.0221,
        0.0082, 0.1898], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,133][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ mal] are: tensor([1.3736e-02, 4.5374e-02, 8.8460e-01, 4.3517e-02, 1.9571e-03, 3.2493e-03,
        1.3788e-03, 2.5099e-04, 7.3080e-04, 2.9815e-03, 2.2269e-03],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,134][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ mal] are: tensor([1.7877e-04, 1.4257e-01, 7.3839e-02, 1.0475e-02, 7.6644e-02, 8.4053e-02,
        4.0133e-02, 2.4182e-01, 4.0744e-02, 2.4364e-02, 2.6518e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,136][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ mal] are: tensor([0.1001, 0.3248, 0.2833, 0.1095, 0.0217, 0.0202, 0.0255, 0.0174, 0.0243,
        0.0571, 0.0159], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,137][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ mal] are: tensor([0.0245, 0.0328, 0.6386, 0.0063, 0.0045, 0.0536, 0.0075, 0.0110, 0.0066,
        0.0053, 0.2093], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,139][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ mal] are: tensor([0.8246, 0.0764, 0.0572, 0.0059, 0.0024, 0.0066, 0.0061, 0.0026, 0.0091,
        0.0035, 0.0056], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,141][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ mal] are: tensor([0.1062, 0.0512, 0.2689, 0.0212, 0.0555, 0.1190, 0.0241, 0.0027, 0.0021,
        0.1708, 0.1782], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,142][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ mal] are: tensor([0.1543, 0.1160, 0.3290, 0.1004, 0.0234, 0.0424, 0.0735, 0.0285, 0.0339,
        0.0222, 0.0763], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,145][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ mal] are: tensor([0.4360, 0.1443, 0.0869, 0.0336, 0.0021, 0.0081, 0.0127, 0.0085, 0.0142,
        0.0054, 0.2483], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,146][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ mal] are: tensor([0.1421, 0.3379, 0.3561, 0.0049, 0.0005, 0.0555, 0.0204, 0.0033, 0.0137,
        0.0007, 0.0649], device='cuda:0') for source tokens [The original language of De finibus bonorum et mal]
[2024-07-23 21:05:33,148][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [orum] are: tensor([0.0044, 0.2094, 0.0600, 0.0694, 0.0283, 0.0510, 0.0317, 0.0311, 0.0271,
        0.0584, 0.2472, 0.1820], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,150][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [orum] are: tensor([0.2442, 0.0323, 0.4465, 0.0624, 0.0186, 0.0687, 0.0133, 0.0043, 0.0234,
        0.0102, 0.0333, 0.0429], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,152][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [orum] are: tensor([0.0123, 0.0493, 0.2332, 0.0423, 0.0319, 0.0930, 0.0318, 0.0307, 0.0355,
        0.0118, 0.3553, 0.0729], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,153][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [orum] are: tensor([0.0122, 0.0242, 0.6510, 0.1977, 0.0204, 0.0111, 0.0050, 0.0026, 0.0050,
        0.0157, 0.0358, 0.0191], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,154][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [orum] are: tensor([6.0897e-05, 4.6741e-02, 9.3669e-02, 1.3678e-02, 1.4702e-01, 4.3599e-02,
        2.7556e-02, 2.2997e-01, 4.2233e-02, 4.7089e-02, 1.9469e-01, 1.1370e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,156][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [orum] are: tensor([0.0132, 0.0928, 0.1861, 0.2286, 0.0825, 0.0377, 0.0289, 0.0292, 0.0655,
        0.1325, 0.0380, 0.0650], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,158][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [orum] are: tensor([0.0133, 0.0402, 0.5567, 0.0391, 0.0151, 0.0490, 0.0293, 0.0404, 0.0202,
        0.0117, 0.1503, 0.0348], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,159][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [orum] are: tensor([0.2358, 0.1338, 0.2258, 0.1172, 0.0409, 0.0564, 0.0292, 0.0250, 0.0542,
        0.0151, 0.0375, 0.0290], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,160][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [orum] are: tensor([2.8388e-02, 3.2908e-02, 4.5931e-01, 8.0370e-03, 3.4666e-02, 9.7559e-02,
        1.0937e-02, 3.7466e-04, 3.0271e-04, 1.9735e-01, 1.2965e-01, 5.1508e-04],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,160][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [orum] are: tensor([0.0176, 0.0425, 0.2350, 0.1468, 0.0542, 0.0492, 0.0343, 0.0468, 0.0658,
        0.0547, 0.1302, 0.1231], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,161][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [orum] are: tensor([0.0920, 0.1647, 0.1444, 0.1178, 0.0069, 0.0158, 0.0511, 0.0271, 0.0729,
        0.0166, 0.1614, 0.1293], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,161][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [orum] are: tensor([0.1046, 0.1951, 0.2186, 0.0137, 0.0083, 0.0909, 0.0489, 0.0485, 0.0434,
        0.0050, 0.1530, 0.0700], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum]
[2024-07-23 21:05:33,161][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ is] are: tensor([0.0412, 0.2221, 0.0711, 0.1434, 0.0215, 0.0344, 0.0354, 0.0250, 0.0241,
        0.0484, 0.2066, 0.0875, 0.0394], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,162][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ is] are: tensor([0.0698, 0.0465, 0.6350, 0.0507, 0.0123, 0.0447, 0.0067, 0.0039, 0.0164,
        0.0071, 0.0485, 0.0421, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,162][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ is] are: tensor([0.0036, 0.0181, 0.2422, 0.0443, 0.0161, 0.0254, 0.0141, 0.0200, 0.0318,
        0.0260, 0.4362, 0.0829, 0.0393], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,163][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ is] are: tensor([0.0452, 0.0651, 0.4081, 0.1422, 0.0188, 0.0212, 0.0079, 0.0053, 0.0091,
        0.0299, 0.0253, 0.0473, 0.1747], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,163][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ is] are: tensor([4.5980e-05, 1.9232e-02, 4.4785e-02, 3.1051e-02, 2.3505e-01, 3.7571e-02,
        1.5042e-02, 1.5741e-01, 1.8338e-02, 5.0972e-02, 1.3100e-01, 6.2305e-02,
        1.9719e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,165][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ is] are: tensor([0.0070, 0.0589, 0.1792, 0.2867, 0.0624, 0.0255, 0.0115, 0.0136, 0.0553,
        0.0650, 0.1306, 0.0664, 0.0379], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,167][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ is] are: tensor([0.0097, 0.0533, 0.7003, 0.0309, 0.0125, 0.0210, 0.0089, 0.0173, 0.0070,
        0.0135, 0.0904, 0.0153, 0.0200], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,168][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ is] are: tensor([0.0457, 0.0429, 0.1794, 0.1262, 0.0740, 0.0635, 0.0209, 0.0625, 0.0479,
        0.0330, 0.2155, 0.0308, 0.0578], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,170][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ is] are: tensor([0.0780, 0.0643, 0.2181, 0.0213, 0.0488, 0.1094, 0.0237, 0.0037, 0.0029,
        0.1387, 0.1690, 0.0044, 0.1176], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,171][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ is] are: tensor([0.0361, 0.0675, 0.2555, 0.2131, 0.0381, 0.0379, 0.0172, 0.0304, 0.0471,
        0.0597, 0.1072, 0.0421, 0.0482], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,173][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ is] are: tensor([0.3535, 0.1175, 0.1427, 0.1369, 0.0012, 0.0119, 0.0182, 0.0070, 0.0337,
        0.0109, 0.0425, 0.0835, 0.0406], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,175][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ is] are: tensor([0.0373, 0.1711, 0.2524, 0.0197, 0.0134, 0.0303, 0.0263, 0.0608, 0.0451,
        0.0159, 0.1966, 0.0984, 0.0328], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is]
[2024-07-23 21:05:33,176][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0125, 0.0524, 0.0348, 0.1843, 0.0055, 0.0058, 0.0025, 0.0015, 0.0045,
        0.0100, 0.0278, 0.0180, 0.0142, 0.6263], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,178][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0122, 0.1379, 0.6458, 0.0886, 0.0024, 0.0224, 0.0016, 0.0010, 0.0054,
        0.0014, 0.0267, 0.0129, 0.0084, 0.0332], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,180][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0022, 0.0378, 0.2383, 0.0582, 0.0044, 0.0070, 0.0029, 0.0035, 0.0067,
        0.0049, 0.3331, 0.0231, 0.0260, 0.2520], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,181][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([4.5439e-03, 1.1110e-02, 4.1970e-01, 1.4468e-01, 3.0556e-03, 6.4931e-03,
        5.1997e-04, 2.6623e-04, 7.2541e-04, 7.0115e-03, 2.6737e-02, 6.9071e-03,
        7.6594e-02, 2.9165e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,182][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([3.3159e-05, 3.9044e-02, 2.7527e-02, 2.5571e-02, 7.0494e-02, 1.5624e-02,
        3.3222e-03, 6.9528e-02, 5.6183e-03, 2.0894e-02, 1.1842e-01, 2.3453e-02,
        1.9934e-01, 3.8113e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,184][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0028, 0.2042, 0.0981, 0.1705, 0.0111, 0.0083, 0.0025, 0.0046, 0.0214,
        0.0149, 0.0185, 0.0141, 0.0310, 0.3981], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,186][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0080, 0.0843, 0.7126, 0.0183, 0.0032, 0.0096, 0.0019, 0.0034, 0.0017,
        0.0030, 0.1112, 0.0043, 0.0097, 0.0288], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,187][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0737, 0.2403, 0.1802, 0.1313, 0.0168, 0.0291, 0.0078, 0.0374, 0.0117,
        0.0088, 0.1955, 0.0059, 0.0290, 0.0326], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,189][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2175, 0.0417, 0.0991, 0.0301, 0.0449, 0.0920, 0.0210, 0.0046, 0.0042,
        0.0743, 0.1246, 0.0058, 0.0745, 0.1656], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,191][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0112, 0.2196, 0.2448, 0.1814, 0.0133, 0.0304, 0.0087, 0.0131, 0.0172,
        0.0192, 0.0800, 0.0150, 0.0230, 0.1232], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,192][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([7.3482e-01, 3.0500e-02, 1.0029e-01, 5.6077e-02, 1.5261e-04, 2.4743e-03,
        1.2462e-03, 5.2888e-04, 3.4889e-03, 1.2400e-03, 1.4581e-02, 1.1559e-02,
        1.1300e-02, 3.1742e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,194][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0384, 0.2506, 0.5173, 0.0101, 0.0020, 0.0292, 0.0049, 0.0110, 0.0086,
        0.0019, 0.0719, 0.0202, 0.0041, 0.0297], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the]
[2024-07-23 21:05:33,196][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ same] are: tensor([0.0264, 0.0609, 0.0489, 0.0454, 0.0115, 0.0150, 0.0122, 0.0134, 0.0081,
        0.0206, 0.2694, 0.0359, 0.0142, 0.3894, 0.0287], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,197][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ same] are: tensor([0.3139, 0.0432, 0.4133, 0.0316, 0.0157, 0.0337, 0.0072, 0.0055, 0.0096,
        0.0071, 0.0760, 0.0226, 0.0103, 0.0073, 0.0030], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,199][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ same] are: tensor([0.0101, 0.0589, 0.2942, 0.0323, 0.0118, 0.0274, 0.0126, 0.0134, 0.0134,
        0.0137, 0.4303, 0.0301, 0.0141, 0.0178, 0.0199], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,201][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ same] are: tensor([0.2642, 0.0263, 0.2884, 0.0334, 0.0072, 0.0044, 0.0039, 0.0020, 0.0016,
        0.0090, 0.0107, 0.0070, 0.1679, 0.1409, 0.0330], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,202][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ same] are: tensor([6.2015e-05, 2.0901e-02, 4.6084e-02, 2.6588e-02, 1.4156e-01, 1.4465e-02,
        8.0887e-03, 1.0424e-01, 1.2154e-02, 3.7011e-02, 1.2799e-01, 3.9202e-02,
        1.1230e-01, 1.0778e-01, 2.0158e-01], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,204][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ same] are: tensor([0.0304, 0.0898, 0.1144, 0.1844, 0.0971, 0.0263, 0.0249, 0.0488, 0.0248,
        0.1285, 0.0243, 0.0171, 0.0917, 0.0802, 0.0171], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,205][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ same] are: tensor([0.0095, 0.0578, 0.6737, 0.0404, 0.0141, 0.0246, 0.0087, 0.0114, 0.0039,
        0.0084, 0.0826, 0.0068, 0.0141, 0.0164, 0.0275], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,207][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ same] are: tensor([0.1924, 0.0924, 0.1766, 0.1062, 0.0457, 0.0280, 0.0262, 0.0564, 0.0265,
        0.0145, 0.1426, 0.0165, 0.0406, 0.0186, 0.0168], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,208][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ same] are: tensor([1.7506e-02, 4.0089e-02, 3.2934e-01, 4.8618e-03, 2.3238e-02, 7.8705e-02,
        9.6896e-03, 4.6711e-04, 3.2894e-04, 1.2278e-01, 1.0254e-01, 5.5258e-04,
        5.9299e-02, 2.1045e-01, 1.4871e-04], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,210][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ same] are: tensor([0.0569, 0.1215, 0.2868, 0.1419, 0.0511, 0.0660, 0.0230, 0.0960, 0.0137,
        0.0409, 0.0386, 0.0089, 0.0317, 0.0152, 0.0078], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,212][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ same] are: tensor([0.3898, 0.1017, 0.1769, 0.0314, 0.0026, 0.0077, 0.0196, 0.0421, 0.0159,
        0.0064, 0.0978, 0.0261, 0.0243, 0.0134, 0.0444], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,214][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ same] are: tensor([0.1726, 0.1966, 0.2864, 0.0133, 0.0036, 0.0669, 0.0375, 0.0514, 0.0261,
        0.0015, 0.0522, 0.0399, 0.0059, 0.0299, 0.0162], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same]
[2024-07-23 21:05:33,215][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ as] are: tensor([0.0233, 0.1015, 0.0270, 0.0784, 0.0096, 0.0062, 0.0091, 0.0054, 0.0090,
        0.0253, 0.0587, 0.0352, 0.0131, 0.5273, 0.0331, 0.0377],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,217][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ as] are: tensor([0.0272, 0.1054, 0.4722, 0.0918, 0.0110, 0.0315, 0.0058, 0.0042, 0.0131,
        0.0065, 0.0361, 0.0413, 0.0148, 0.0383, 0.0046, 0.0961],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,219][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ as] are: tensor([0.0048, 0.0538, 0.2172, 0.0652, 0.0124, 0.0177, 0.0132, 0.0175, 0.0197,
        0.0206, 0.2731, 0.0409, 0.0344, 0.1024, 0.0275, 0.0798],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,220][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ as] are: tensor([0.0346, 0.0253, 0.1607, 0.1168, 0.0045, 0.0063, 0.0041, 0.0021, 0.0022,
        0.0083, 0.0135, 0.0115, 0.0809, 0.2796, 0.0776, 0.1718],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,222][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ as] are: tensor([1.3908e-04, 2.7885e-02, 6.2837e-02, 2.7246e-02, 8.4213e-02, 1.6446e-02,
        7.8695e-03, 7.6291e-02, 1.0483e-02, 2.4130e-02, 1.1748e-01, 3.6402e-02,
        1.0393e-01, 1.8628e-01, 1.7042e-01, 4.7961e-02], device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,224][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ as] are: tensor([0.0026, 0.0510, 0.0872, 0.1607, 0.0466, 0.0234, 0.0271, 0.0273, 0.0318,
        0.0514, 0.0208, 0.0217, 0.0413, 0.1872, 0.0411, 0.1787],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,225][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ as] are: tensor([0.0074, 0.0445, 0.5437, 0.0448, 0.0084, 0.0262, 0.0122, 0.0276, 0.0063,
        0.0131, 0.0742, 0.0113, 0.0208, 0.0496, 0.0395, 0.0705],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,227][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ as] are: tensor([0.0216, 0.0582, 0.1680, 0.1870, 0.0509, 0.0421, 0.0140, 0.0324, 0.0157,
        0.0090, 0.1413, 0.0103, 0.0301, 0.0464, 0.0325, 0.1405],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,227][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ as] are: tensor([0.2283, 0.0327, 0.1030, 0.0146, 0.0321, 0.0714, 0.0156, 0.0033, 0.0023,
        0.0821, 0.1358, 0.0034, 0.0565, 0.1574, 0.0019, 0.0596],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,228][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ as] are: tensor([0.0141, 0.0933, 0.1587, 0.1537, 0.0257, 0.0626, 0.0294, 0.0609, 0.0277,
        0.0381, 0.0401, 0.0186, 0.0256, 0.0506, 0.0277, 0.1732],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,228][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ as] are: tensor([0.0790, 0.0885, 0.1749, 0.0822, 0.0015, 0.0072, 0.0098, 0.0095, 0.0203,
        0.0045, 0.0281, 0.0430, 0.0407, 0.0743, 0.0507, 0.2858],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,228][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ as] are: tensor([0.0448, 0.2604, 0.1830, 0.0216, 0.0089, 0.0366, 0.0276, 0.0933, 0.0320,
        0.0044, 0.0599, 0.0611, 0.0134, 0.0448, 0.0659, 0.0424],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as]
[2024-07-23 21:05:33,229][circuit_model.py][line:1570][INFO] ##11-th layer ##Weight##: The head1 weight before mlp for token [ the] are: tensor([0.0044, 0.0171, 0.0123, 0.1210, 0.0037, 0.0025, 0.0012, 0.0006, 0.0024,
        0.0078, 0.0160, 0.0101, 0.0096, 0.5220, 0.0084, 0.0357, 0.2252],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,229][circuit_model.py][line:1573][INFO] ##11-th layer ##Weight##: The head2 weight before mlp for token [ the] are: tensor([0.0093, 0.1508, 0.3298, 0.0841, 0.0041, 0.0291, 0.0020, 0.0027, 0.0079,
        0.0031, 0.0322, 0.0219, 0.0152, 0.0478, 0.0049, 0.1041, 0.1509],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,230][circuit_model.py][line:1576][INFO] ##11-th layer ##Weight##: The head3 weight before mlp for token [ the] are: tensor([0.0026, 0.0508, 0.1264, 0.0637, 0.0063, 0.0066, 0.0039, 0.0059, 0.0093,
        0.0110, 0.1675, 0.0254, 0.0374, 0.2257, 0.0232, 0.0654, 0.1691],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,233][circuit_model.py][line:1579][INFO] ##11-th layer ##Weight##: The head4 weight before mlp for token [ the] are: tensor([0.0183, 0.0071, 0.1206, 0.0609, 0.0022, 0.0041, 0.0008, 0.0003, 0.0007,
        0.0085, 0.0131, 0.0038, 0.0940, 0.1393, 0.0140, 0.2232, 0.2891],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,233][circuit_model.py][line:1582][INFO] ##11-th layer ##Weight##: The head5 weight before mlp for token [ the] are: tensor([2.2844e-05, 2.0840e-02, 2.0651e-02, 1.4958e-02, 4.4770e-02, 8.4550e-03,
        2.6255e-03, 4.3059e-02, 4.8451e-03, 1.3783e-02, 7.5932e-02, 1.7403e-02,
        1.1201e-01, 2.2265e-01, 1.5925e-01, 3.9620e-02, 1.9912e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,235][circuit_model.py][line:1585][INFO] ##11-th layer ##Weight##: The head6 weight before mlp for token [ the] are: tensor([0.0013, 0.0951, 0.0332, 0.1089, 0.0095, 0.0045, 0.0021, 0.0051, 0.0120,
        0.0126, 0.0099, 0.0075, 0.0225, 0.2825, 0.0321, 0.0601, 0.3012],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,236][circuit_model.py][line:1588][INFO] ##11-th layer ##Weight##: The head7 weight before mlp for token [ the] are: tensor([0.0084, 0.0899, 0.4386, 0.0376, 0.0050, 0.0132, 0.0038, 0.0061, 0.0028,
        0.0064, 0.0631, 0.0049, 0.0155, 0.0637, 0.0429, 0.0656, 0.1324],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,239][circuit_model.py][line:1591][INFO] ##11-th layer ##Weight##: The head8 weight before mlp for token [ the] are: tensor([0.0464, 0.1541, 0.1159, 0.1203, 0.0229, 0.0244, 0.0066, 0.0426, 0.0111,
        0.0097, 0.1720, 0.0054, 0.0250, 0.0379, 0.0183, 0.0984, 0.0890],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,240][circuit_model.py][line:1594][INFO] ##11-th layer ##Weight##: The head9 weight before mlp for token [ the] are: tensor([0.2478, 0.0325, 0.0741, 0.0195, 0.0320, 0.0754, 0.0150, 0.0038, 0.0032,
        0.0606, 0.1116, 0.0044, 0.0594, 0.1163, 0.0022, 0.0502, 0.0921],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,242][circuit_model.py][line:1597][INFO] ##11-th layer ##Weight##: The head10 weight before mlp for token [ the] are: tensor([0.0038, 0.1017, 0.0675, 0.1112, 0.0104, 0.0205, 0.0086, 0.0166, 0.0120,
        0.0203, 0.0274, 0.0099, 0.0183, 0.1327, 0.0197, 0.0960, 0.3235],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,243][circuit_model.py][line:1600][INFO] ##11-th layer ##Weight##: The head11 weight before mlp for token [ the] are: tensor([5.6408e-01, 2.6050e-02, 5.7554e-02, 3.2140e-02, 1.8916e-04, 2.4809e-03,
        1.4379e-03, 8.8648e-04, 4.6736e-03, 1.7134e-03, 4.7993e-03, 9.7530e-03,
        1.5825e-02, 3.9678e-02, 6.8294e-03, 1.2343e-01, 1.0848e-01],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,245][circuit_model.py][line:1603][INFO] ##11-th layer ##Weight##: The head12 weight before mlp for token [ the] are: tensor([0.0890, 0.2058, 0.1899, 0.0105, 0.0046, 0.0410, 0.0105, 0.0385, 0.0268,
        0.0062, 0.0545, 0.0466, 0.0160, 0.0662, 0.0183, 0.0491, 0.1266],
       device='cuda:0') for source tokens [The original language of De finibus bonorum et malorum is the same as the]
[2024-07-23 21:05:33,246][circuit_model.py][line:1378][INFO] ############showing the lable-rank of each circuit
[2024-07-23 21:05:33,249][circuit_model.py][line:1466][INFO] The CircuitSUM has label_rank 
 tensor([[213],
        [  1],
        [ 44],
        [ 13],
        [ 23],
        [256],
        [ 24],
        [ 14],
        [ 10],
        [ 15],
        [ 44],
        [ 65],
        [  5],
        [  2],
        [ 64],
        [ 54],
        [  1]], device='cuda:0')
[2024-07-23 21:05:33,251][circuit_model.py][line:1468][INFO] The Circuit0 has label_rank 
 tensor([[211],
        [  1],
        [ 42],
        [  9],
        [ 17],
        [314],
        [ 17],
        [ 17],
        [ 10],
        [ 19],
        [ 74],
        [ 50],
        [  2],
        [  2],
        [ 52],
        [ 47],
        [  2]], device='cuda:0')
[2024-07-23 21:05:33,252][circuit_model.py][line:1470][INFO] The Circuit1 has label_rank 
 tensor([[ 658],
        [1032],
        [1780],
        [1302],
        [1565],
        [1756],
        [2098],
        [1668],
        [2247],
        [2098],
        [2725],
        [3067],
        [2465],
        [1728],
        [2587],
        [1947],
        [1972]], device='cuda:0')
[2024-07-23 21:05:33,254][circuit_model.py][line:1472][INFO] The Circuit2 has label_rank 
 tensor([[ 6399],
        [34714],
        [22397],
        [15470],
        [12992],
        [13645],
        [15545],
        [12031],
        [14149],
        [14496],
        [10658],
        [13625],
        [13858],
        [14942],
        [13165],
        [14638],
        [14496]], device='cuda:0')
[2024-07-23 21:05:33,256][circuit_model.py][line:1474][INFO] The Circuit3 has label_rank 
 tensor([[32721],
        [29567],
        [32389],
        [30495],
        [30715],
        [32199],
        [32708],
        [33491],
        [31970],
        [30851],
        [31423],
        [29172],
        [27977],
        [26989],
        [29063],
        [28662],
        [25510]], device='cuda:0')
[2024-07-23 21:05:33,257][circuit_model.py][line:1476][INFO] The Circuit4 has label_rank 
 tensor([[21512],
        [44127],
        [43436],
        [42751],
        [42952],
        [44491],
        [43824],
        [42786],
        [43476],
        [44298],
        [42348],
        [43300],
        [44431],
        [44606],
        [45027],
        [45301],
        [45429]], device='cuda:0')
[2024-07-23 21:05:33,259][circuit_model.py][line:1478][INFO] The Circuit5 has label_rank 
 tensor([[  831],
        [ 1690],
        [ 1972],
        [ 3624],
        [17089],
        [11471],
        [ 9269],
        [11738],
        [11126],
        [ 9076],
        [ 4820],
        [ 5678],
        [ 7405],
        [ 6949],
        [ 4520],
        [ 4561],
        [ 6062]], device='cuda:0')
[2024-07-23 21:05:33,260][circuit_model.py][line:1480][INFO] The Circuit6 has label_rank 
 tensor([[ 3967],
        [50236],
        [50167],
        [39801],
        [47023],
        [36270],
        [43321],
        [36632],
        [31382],
        [44126],
        [47829],
        [32750],
        [26479],
        [27275],
        [26391],
        [16287],
        [13355]], device='cuda:0')
[2024-07-23 21:05:33,262][circuit_model.py][line:1482][INFO] The Circuit7 has label_rank 
 tensor([[21528],
        [ 9947],
        [19934],
        [20143],
        [19949],
        [18128],
        [18781],
        [19149],
        [18386],
        [18704],
        [17375],
        [17459],
        [18897],
        [18968],
        [19255],
        [19270],
        [20085]], device='cuda:0')
[2024-07-23 21:05:33,264][circuit_model.py][line:1484][INFO] The Circuit8 has label_rank 
 tensor([[11613],
        [41394],
        [21697],
        [20758],
        [21103],
        [27909],
        [23316],
        [24767],
        [24611],
        [26946],
        [21112],
        [22874],
        [24451],
        [29183],
        [25128],
        [22944],
        [26569]], device='cuda:0')
[2024-07-23 21:05:33,265][circuit_model.py][line:1486][INFO] The Circuit9 has label_rank 
 tensor([[525],
        [479],
        [456],
        [432],
        [418],
        [415],
        [419],
        [414],
        [413],
        [444],
        [423],
        [413],
        [429],
        [503],
        [413],
        [471],
        [502]], device='cuda:0')
[2024-07-23 21:05:33,267][circuit_model.py][line:1488][INFO] The Circuit10 has label_rank 
 tensor([[36366],
        [34209],
        [20657],
        [24158],
        [22806],
        [21177],
        [20984],
        [19261],
        [19675],
        [20217],
        [22112],
        [22913],
        [23664],
        [26549],
        [22078],
        [24633],
        [24227]], device='cuda:0')
[2024-07-23 21:05:33,268][circuit_model.py][line:1490][INFO] The Circuit11 has label_rank 
 tensor([[ 1615],
        [ 3541],
        [17030],
        [14072],
        [20848],
        [12974],
        [16499],
        [ 5755],
        [19953],
        [17737],
        [19451],
        [22465],
        [19551],
        [15316],
        [21768],
        [27229],
        [23571]], device='cuda:0')
[2024-07-23 21:05:33,270][circuit_model.py][line:1492][INFO] The Circuit12 has label_rank 
 tensor([[13756],
        [ 8650],
        [ 9501],
        [12142],
        [12245],
        [ 9954],
        [12069],
        [12228],
        [ 9448],
        [11969],
        [12633],
        [12777],
        [13204],
        [13542],
        [12871],
        [12986],
        [14437]], device='cuda:0')
[2024-07-23 21:05:33,272][circuit_model.py][line:1494][INFO] The Circuit13 has label_rank 
 tensor([[7210],
        [1709],
        [1812],
        [1583],
        [2041],
        [1232],
        [2117],
        [ 239],
        [3388],
        [ 298],
        [ 777],
        [3453],
        [ 603],
        [ 906],
        [1970],
        [1875],
        [1948]], device='cuda:0')
[2024-07-23 21:05:33,273][circuit_model.py][line:1496][INFO] The Circuit14 has label_rank 
 tensor([[13678],
        [ 9283],
        [ 7629],
        [ 9551],
        [ 9609],
        [11130],
        [ 9460],
        [10082],
        [ 9004],
        [ 7383],
        [ 8132],
        [ 6888],
        [ 7637],
        [ 5584],
        [ 5811],
        [ 5175],
        [ 4734]], device='cuda:0')
[2024-07-23 21:05:33,275][circuit_model.py][line:1498][INFO] The Circuit15 has label_rank 
 tensor([[ 6935],
        [13796],
        [16434],
        [17415],
        [17610],
        [18095],
        [17543],
        [17562],
        [17407],
        [17080],
        [16891],
        [17992],
        [17884],
        [17770],
        [19224],
        [18677],
        [19463]], device='cuda:0')
[2024-07-23 21:05:33,277][circuit_model.py][line:1500][INFO] The Circuit16 has label_rank 
 tensor([[1018],
        [ 606],
        [ 500],
        [ 485],
        [ 467],
        [ 253],
        [ 374],
        [ 316],
        [ 285],
        [ 414],
        [ 558],
        [ 587],
        [ 737],
        [ 842],
        [ 710],
        [ 733],
        [ 944]], device='cuda:0')
[2024-07-23 21:05:33,278][circuit_model.py][line:1502][INFO] The Circuit17 has label_rank 
 tensor([[315],
        [277],
        [302],
        [331],
        [344],
        [286],
        [306],
        [356],
        [317],
        [289],
        [369],
        [319],
        [305],
        [311],
        [326],
        [352],
        [373]], device='cuda:0')
[2024-07-23 21:05:33,280][circuit_model.py][line:1504][INFO] The Circuit18 has label_rank 
 tensor([[ 356],
        [1694],
        [ 211],
        [ 121],
        [  49],
        [  88],
        [ 129],
        [ 309],
        [ 278],
        [ 344],
        [1045],
        [ 647],
        [ 328],
        [ 351],
        [ 572],
        [ 560],
        [ 501]], device='cuda:0')
[2024-07-23 21:05:33,281][circuit_model.py][line:1506][INFO] The Circuit19 has label_rank 
 tensor([[3411],
        [2022],
        [2249],
        [3748],
        [3248],
        [3334],
        [2757],
        [2709],
        [2685],
        [2550],
        [2304],
        [2154],
        [1908],
        [1750],
        [2438],
        [1514],
        [1386]], device='cuda:0')
[2024-07-23 21:05:33,283][circuit_model.py][line:1508][INFO] The Circuit20 has label_rank 
 tensor([[3404],
        [ 356],
        [1499],
        [1357],
        [1495],
        [1731],
        [1526],
        [1575],
        [1832],
        [1430],
        [1444],
        [1696],
        [1520],
        [1473],
        [1649],
        [1885],
        [2469]], device='cuda:0')
[2024-07-23 21:05:33,285][circuit_model.py][line:1510][INFO] The Circuit21 has label_rank 
 tensor([[2442],
        [1840],
        [1402],
        [1949],
        [1883],
        [2314],
        [2332],
        [3214],
        [3018],
        [3600],
        [2750],
        [3136],
        [5127],
        [4216],
        [4538],
        [5292],
        [5648]], device='cuda:0')
[2024-07-23 21:05:33,286][circuit_model.py][line:1512][INFO] The Circuit22 has label_rank 
 tensor([[17242],
        [16925],
        [16599],
        [16470],
        [16319],
        [16332],
        [16425],
        [16282],
        [16229],
        [16608],
        [16383],
        [16209],
        [16355],
        [16544],
        [16173],
        [16567],
        [16615]], device='cuda:0')
[2024-07-23 21:05:33,288][circuit_model.py][line:1514][INFO] The Circuit23 has label_rank 
 tensor([[ 695],
        [1594],
        [1046],
        [1691],
        [1629],
        [1469],
        [1657],
        [1696],
        [1770],
        [1769],
        [1519],
        [1778],
        [1877],
        [1761],
        [1760],
        [1951],
        [1980]], device='cuda:0')
[2024-07-23 21:05:33,289][circuit_model.py][line:1516][INFO] The Circuit24 has label_rank 
 tensor([[11367],
        [ 1028],
        [ 1396],
        [ 2444],
        [ 2021],
        [ 1477],
        [ 1331],
        [ 5608],
        [ 1299],
        [ 1910],
        [ 1994],
        [ 1685],
        [ 2217],
        [ 3975],
        [ 1817],
        [ 1281],
        [ 1505]], device='cuda:0')
[2024-07-23 21:05:33,291][circuit_model.py][line:1518][INFO] The Circuit25 has label_rank 
 tensor([[1941],
        [1363],
        [1066],
        [ 635],
        [ 616],
        [ 983],
        [ 874],
        [ 773],
        [1057],
        [ 684],
        [ 694],
        [ 731],
        [ 652],
        [ 576],
        [ 740],
        [ 905],
        [ 912]], device='cuda:0')
[2024-07-23 21:05:33,293][circuit_model.py][line:1520][INFO] The Circuit26 has label_rank 
 tensor([[17225],
        [27293],
        [26736],
        [21883],
        [23117],
        [23516],
        [22660],
        [19736],
        [21049],
        [22816],
        [24040],
        [21022],
        [18880],
        [18102],
        [18569],
        [18399],
        [16036]], device='cuda:0')
[2024-07-23 21:05:33,294][circuit_model.py][line:1522][INFO] The Circuit27 has label_rank 
 tensor([[14205],
        [11493],
        [ 4310],
        [ 8810],
        [ 6483],
        [ 8637],
        [ 6071],
        [26813],
        [ 8390],
        [21248],
        [19678],
        [10650],
        [16586],
        [15075],
        [12281],
        [ 9510],
        [14684]], device='cuda:0')
[2024-07-23 21:05:33,296][circuit_model.py][line:1524][INFO] The Circuit28 has label_rank 
 tensor([[2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215],
        [2215]], device='cuda:0')
